{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be76afd1-30c0-4f6a-9107-bf9eef74f904",
   "metadata": {},
   "source": [
    "## Ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fea144b-381f-4fcd-ac3d-e9f1d364d660",
   "metadata": {},
   "source": [
    "#### Was ist Ray?\n",
    "\n",
    "Den ganzen Kurs √ºber haben wir das Ray-Paket benutzt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41754b57-9c36-48f7-843a-4989c90f357b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray.rllib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4357c3f6-bc60-4eb4-b4be-6aac32900960",
   "metadata": {},
   "source": [
    "![](img/ray-logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173eca68-40f8-43ce-9889-a4d3b6f96505",
   "metadata": {},
   "source": [
    "Was ist Ray? Aus den [docs](https://docs.ray.io/en/latest/):\n",
    "\n",
    "&gt; Ray ist ein universell einsetzbares, verteiltes Computing-Framework.\n",
    "\n",
    "Ray ist auch:\n",
    "\n",
    "- Ein [aktives Open-Source-Projekt](https://github.com/ray-project/ray) mit √ºber 20k Sternen auf GitHub ü§©\n",
    "- Unterst√ºtzt von dem Einhorn-Startup [Anyscale](https://www.anyscale.com/), das diesen Kurs produziert hat ü¶Ñ\n",
    "\n",
    "Anmerkungen:\n",
    "\n",
    "Aber zur√ºck zum verteilten Rechnen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bceac41-d9d5-46e4-8e3a-21d5a914a986",
   "metadata": {},
   "source": [
    "#### Was ist verteiltes Rechnen?\n",
    "\n",
    "_Verteiltes Rechnen_ ist das Rechnen mit mehreren Rechnern (Knoten), die √ºber ein Netzwerk verteilt sind.\n",
    "\n",
    "![](img/supercomputer.png)\n",
    "\n",
    "Vorteile:\n",
    "\n",
    "- Massiv verbesserte F√§higkeiten\n",
    "\n",
    "Nachteile/Herausforderungen:\n",
    "\n",
    "- Synchronisation\n",
    "- Misserfolg\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d345f4-5b01-49f2-9635-130f4d580f21",
   "metadata": {},
   "source": [
    "#### Ray macht verteiltes Rechnen einfach\n",
    "\n",
    "- Das Ziel von Ray ist es, verteiltes Rechnen einfach und zug√§nglich zu machen.\n",
    "- Ray √ºbernimmt die meisten Herausforderungen f√ºr die Nutzer.\n",
    "- RLlib, tune und die anderen Unterpakete wurden auf der Grundlage von Ray entwickelt.\n",
    "- Das bedeutet, dass _RLlib und tune automatisch √ºber verteilte F√§higkeiten verf√ºgen._\n",
    "\n",
    "Anmerkungen:\n",
    "\n",
    "√úberraschung! RLlib ist einfach zu benutzen und implementiert viele moderne RL-Algorithmen, aber es hat noch einen weiteren Vorteil, den wir bisher nicht erw√§hnt haben: nat√ºrliche verteilte Berechnungsm√∂glichkeiten. Damit ist sie konkurrierenden Paketen weit voraus, wenn es darum geht, die Berechnungen zu verteilen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275fe8fc-292d-4622-b7c9-5e6d0c0f4022",
   "metadata": {},
   "source": [
    "#### RLlib, verteilt\n",
    "\n",
    "- In diesem Kurs haben wir viele Algorithmus-Konfigurationen erstellt.\n",
    "- Aber es gibt einige Parameter, die wir noch nicht verwendet haben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a1e8fe8-c52d-4cf6-a846-75f6ca83e381",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68ff95da-eee7-45eb-9694-ac4fd5420c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_config = (\n",
    "    PPOConfig()\n",
    "    .framework(\"torch\")\n",
    "    .rollouts(num_rollout_workers=4, num_envs_per_worker=2)\n",
    "    .resources(num_gpus=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e10ccc-cc94-41a3-bcda-66de6cf68fcc",
   "metadata": {},
   "source": [
    "Du kannst mehr √ºber die Angabe von Ressourcen [hier](https://docs.ray.io/en/master/rllib/rllib-training.html#specifying-resources) und √ºber Skalierung [hier](https://docs.ray.io/en/master/rllib/rllib-training.html#scaling-guide) lesen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec03fa7b-c78f-446d-9be7-deac2c895ce2",
   "metadata": {},
   "source": [
    "Aber... was ist ein \"Rollout Worker\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ebdf4e-e1c0-41c9-b923-1a3917106f9e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Rollout Arbeiter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5ea641-4738-4220-8adc-78d5a65141e0",
   "metadata": {},
   "source": [
    "- Rollout Worker sammeln parallel Daten aus der Environment (Simulator).\n",
    "- F√ºr die meisten Simulatorumgebungen kann man die Environment in einem Cluster replizieren.\n",
    "- So kannst du viel schneller Daten sammeln und Engp√§sse beim Training vermeiden.\n",
    "- Egal, mit welchem Cluster Ray im Backend verbunden ist, `num_rollout_workers=4` funktioniert nahtlos.\n",
    "\n",
    "Anmerkungen:\n",
    "\n",
    "Wenn du beim √ºberwachten Lernen wartest, wei√üt du, dass du wahrscheinlich darauf wartest, dass das Modell trainiert wird. Im RL k√∂nnte der Engpass die Datensammlung oder die Modellaktualisierung sein. Die M√∂glichkeit, Rollouts zu parallelisieren, lindert den Engpass bei der Datenerfassung "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925d5abd-b478-43a7-bb71-7eeb9ae4276d",
   "metadata": {},
   "source": [
    "#### Ray tune, revisited\n",
    "\n",
    "- Vergiss nicht, dass das Abstimmen von Hyperparametern wie die Rastersuche auch leicht verteilt werden kann.\n",
    "- Gl√ºcklicherweise ist `tune` auch Teil von Ray und k√ºmmert sich, wie RLlib, f√ºr dich darum! \n",
    "- Wie du siehst, ist Ray + tune + RLlib eine ziemlich m√§chtige Kombination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac61bd0-31ac-47e7-a51b-6ff1b92cb753",
   "metadata": {},
   "source": [
    "#### Treiber\n",
    "\n",
    "In allen unseren Konfigurationen haben wir\n",
    "\n",
    "```python\n",
    "create_env_on_driver = True\n",
    "```\n",
    "\n",
    "Das bedeutet, dass wir die Environmentsvariable auf denselben \"Treiber\"-Prozess setzen, der das Training ausf√ºhrt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fac297-c378-4ad3-b58f-65da9042333c",
   "metadata": {},
   "source": [
    "#### Zusammenfassung\n",
    "\n",
    "- Ray ist unglaublich m√§chtig, und wir haben nur die Spitze des Eisbergs angekratzt.\n",
    "- Einige andere Ressourcen:\n",
    "  - [ray.io](https://www.ray.io/)\n",
    "  - [Learning Ray](https://www.oreilly.com/library/view/learning-ray/9781098117214/) (Buch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76648d6e-5b58-4241-aeb3-84d4dc6c661f",
   "metadata": {},
   "source": [
    "#### Lass uns das Gelernte anwenden!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d0b93e-69ad-4338-b937-62bb19fc506b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Was ist Ray?\n",
    "<!-- multiple choice -->\n",
    "\n",
    "Was ist Ray?\n",
    "\n",
    "- [ ] Das Unternehmen, das RLlib entwickelt. | Du denkst vielleicht an Anyscale, die Firma hinter Ray!\n",
    "- [Ein Unterpaket der RLlib, das sich mit verteiltem Rechnen besch√§ftigt.\n",
    "- [Ein allgemeines Paket, das die RLlib enth√§lt, die sich mit verteiltem Rechnen befasst.\n",
    "- [x] Ein Algorithmus f√ºr Reinforcement Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67749f9a-fc60-436d-878d-ee060772cf9a",
   "metadata": {},
   "source": [
    "## RLlib verteilen\n",
    "<!-- multiple choice -->\n",
    "\n",
    "Wie nutzt RLlib in erster Linie die M√∂glichkeiten des verteilten Rechnens?\n",
    "\n",
    "- [x] Verteilte Rollout-Worker generieren Daten von Environmentsklonen, die in den Lernalgorithmus eingespeist werden \n",
    "- [Das Training des neuronalen Netzwerks wird auf mehrere Knotenpunkte verteilt.\n",
    "- [Jeder Knoten verf√ºgt √ºber ein separates neuronales Netzwerk, das unabh√§ngig auf seinem eigenen Knoten trainiert wird."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7a1f3a-18b5-4d99-ad1f-409bf87b59ad",
   "metadata": {},
   "source": [
    "## Experimentieren mit Rollout-Arbeitern\n",
    "<!-- coding exercise -->\n",
    "\n",
    "Der folgende Code erstellt zwei Instanzen des PPO-Algorithmus: eine, die zwei Rollout-Worker mit zwei Environmenten pro Worker verwenden soll, und eine, die nur einen Rollout-Worker mit einer Environment pro Worker verwendet. Anschlie√üend wird die Zeit ausgedruckt, die f√ºr das Training der beiden Instanzen f√ºr 5 Iterationen ben√∂tigt wurde. Vervollst√§ndige den Code, f√ºhre ihn aus und vergleiche dann die Zeiten \n",
    "\n",
    "Hinweis: Dieses Experiment funktioniert _normalerweise_. Allerdings l√§uft dieser Code auf einem Server, der m√∂glicherweise von mehreren Lernenden gleichzeitig genutzt wird. Au√üerdem ist dieser Server kein richtiger Cluster, sodass die Parallelisierung auf einem Rechner nur funktioniert, wenn mehrere CPU-Kerne zur Verf√ºgung stehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2689cc8b-4ce7-4b7c-a943-0e937d37ccec",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PPOConfig' object has no attribute '____'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m      6\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mDeprecationWarning\u001b[39;00m) \n\u001b[1;32m      8\u001b[0m ppo_config_many \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m----> 9\u001b[0m     \u001b[43mPPOConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m____\u001b[49m(____)\\\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;241m.\u001b[39mtraining(model\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfcnet_hiddens\u001b[39m\u001b[38;5;124m\"\u001b[39m : [\u001b[38;5;241m32\u001b[39m,\u001b[38;5;241m32\u001b[39m]})\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m ppo_config_single \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     16\u001b[0m     PPOConfig()\\\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;241m.\u001b[39mframework(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;241m.\u001b[39m____(num_rollout_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, num_envs_per_worker\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\\\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;241m.\u001b[39mtraining(model\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfcnet_hiddens\u001b[39m\u001b[38;5;124m\"\u001b[39m : [\u001b[38;5;241m32\u001b[39m,\u001b[38;5;241m32\u001b[39m]})\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     22\u001b[0m ppo_many \u001b[38;5;241m=\u001b[39m ppo_config_many\u001b[38;5;241m.\u001b[39mbuild(env\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrozenLake-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PPOConfig' object has no attribute '____'"
     ]
    }
   ],
   "source": [
    "# EXERCISE\n",
    "\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "ppo_config_many = (\n",
    "    PPOConfig()\n",
    "    .framework(\"torch\")\n",
    "    .____(____)\n",
    "    .training(model={\"fcnet_hiddens\" : [32,32]})\n",
    ")\n",
    "\n",
    "ppo_config_single = (\n",
    "    PPOConfig()\n",
    "    .framework(\"torch\")\n",
    "    .____(num_rollout_workers=1, num_envs_per_worker=1)\n",
    "    .training(model={\"fcnet_hiddens\" : [32,32]})\n",
    ")\n",
    "\n",
    "ppo_many = ppo_config_many.build(env=\"FrozenLake-v1\")\n",
    "t = time.time()\n",
    "for i in range(5):\n",
    "    ppo_many.train()\n",
    "print(f\"Elapsed time with 2 workers, 2 envs each: {time.time()-t:.1f}s.\")\n",
    "ppo_many.stop()\n",
    "\n",
    "ppo_single = ppo_config_single.build(env=\"FrozenLake-v1\")\n",
    "t = time.time()\n",
    "for i in range(5):\n",
    "    ppo_single.train()\n",
    "print(f\"Elapsed time with 1 worker, 1 env: {time.time()-t:.1f}s.\")\n",
    "ppo_single.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2405893d-2dc6-4a91-8d22-bb4db1450b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-27 17:02:42,927\tINFO worker.py:1490 -- Started a local Ray instance.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=70484)\u001b[0m 2022-08-27 17:02:47,263\tWARNING env.py:154 -- Your env reset() method appears to take 'seed' or 'return_info' arguments. Note that these are not yet supported in RLlib. Seeding will take place using 'env.seed()' and the info dict will not be returned from reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=70484)\u001b[0m 2022-08-27 17:02:47,536\tWARNING deprecation.py:47 -- DeprecationWarning: `concat_samples` has been deprecated. Use `concat_samples() from rllib.policy.sample_batch` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time with 2 workers, 2 envs each: 11.3s.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=70561)\u001b[0m 2022-08-27 17:03:00,941\tWARNING env.py:154 -- Your env reset() method appears to take 'seed' or 'return_info' arguments. Note that these are not yet supported in RLlib. Seeding will take place using 'env.seed()' and the info dict will not be returned from reset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time with 1 worker, 1 env: 18.0s.\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "ppo_config_many = (\n",
    "    PPOConfig()\n",
    "    .framework(\"torch\")\n",
    "    .rollouts(num_rollout_workers=2, num_envs_per_worker=2)\n",
    "    .training(model={\"fcnet_hiddens\" : [32,32]})\n",
    ")\n",
    "\n",
    "ppo_config_single = (\n",
    "    PPOConfig()\n",
    "    .framework(\"torch\")\n",
    "    .rollouts(num_rollout_workers=1, num_envs_per_worker=1)\n",
    "    .training(model={\"fcnet_hiddens\" : [32,32]})\n",
    ")\n",
    "\n",
    "ppo_many = ppo_config_many.build(env=\"FrozenLake-v1\")\n",
    "t = time.time()\n",
    "for i in range(5):\n",
    "    ppo_many.train()\n",
    "print(f\"Elapsed time with 2 workers, 2 envs each: {time.time()-t:.1f}s.\")\n",
    "ppo_many.stop()\n",
    "\n",
    "ppo_single = ppo_config_single.build(env=\"FrozenLake-v1\")\n",
    "t = time.time()\n",
    "for i in range(5):\n",
    "    ppo_single.train()\n",
    "print(f\"Elapsed time with 1 worker, 1 env: {time.time()-t:.1f}s.\")\n",
    "ppo_single.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda099fd-12d1-42e3-a529-e757347def40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ray200]",
   "language": "python",
   "name": "conda-env-ray200-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
