{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c331561e-c62d-4486-9252-b9ff362f8fa4",
   "metadata": {},
   "source": [
    "## RL-Umgebungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "093747d0-0011-4b28-a683-74870bfdbd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de064a2a-0a07-4c94-a9ac-b6f2d978f412",
   "metadata": {},
   "source": [
    "#### Was ist eine Environment?\n",
    "\n",
    "- Eine Environment kann sein:\n",
    "  - ein Spiel, wie ein Videospiel.\n",
    "  - eine Simulation eines realen Szenarios, z. B. eines Roboters, des Nutzerverhaltens oder der Börse\n",
    "  - jede andere Environment mit einem _Agenten_, der _Aktionen_ durchführt, _Beobachtungen_ macht und _Belohnungen_ erhält\n",
    "  \n",
    "Eine Anmerkung zur Terminologie: Wir werden _Agent_ und _Spieler_ synonym verwenden "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5748d00-5c07-49db-aa9f-3cc1613bd0a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Laufendes Beispiel: Gefrorener See\n",
    "\n",
    "Als laufendes Beispiel für eine Environment verwenden wir die [Frozen Lake](https://www.gymlibrary.ml/environments/toy_text/frozen_lake/) Environment aus [OpenAI Gym](https://www.gymlibrary.ml/), die die Standardschnittstelle für RL-Probleme bietet. Wir können die Environment wie folgt visualisieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f87dba7-b5de-4ba9-b89d-b5ea2d069f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2eef796-f767-4d99-a454-0c3bd227bfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "from utils_01 import fix_frozen_lake_render\n",
    "fix_frozen_lake_render(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8579403b-4adf-4a22-8d27-d7f54fd52dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "P...\n",
      ".O.O\n",
      "...O\n",
      "O..G\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693db547-8f4d-49ae-b759-80308d6803e0",
   "metadata": {},
   "source": [
    "Das Ziel ist es, dass der Spieler (P) das Ziel (G) erreicht, indem er auf den gefrorenen Seesegmenten (O) läuft, ohne in die Löcher (O) zu fallen.\n",
    "\n",
    "**Hinweis**: Das Rendering der Environment wurde vom OpenAI Gym Rendering abgewandelt, damit es in dieser interaktiven Lernplattform deutlich dargestellt werden kann.\n",
    "\n",
    "Anmerkungen:\n",
    "\n",
    "Der Code für das Ersetzen des Renderings wird nicht angezeigt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a34b31-5bbb-45f4-9e7f-e925fc96d0ee",
   "metadata": {},
   "source": [
    "#### Bewegung\n",
    "\n",
    "Der Spieler kann sich auf dem gefrorenen See bewegen. Zum Beispiel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d5d954a-7890-427c-aea1-7daec590c91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "....\n",
      "PO.O\n",
      "...O\n",
      "O..G\n"
     ]
    }
   ],
   "source": [
    "env.step(1) # 1 -> Down\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb8ff83-f2b0-464f-934c-9e6144977c2c",
   "metadata": {},
   "source": [
    "Mach dir vorerst keine Gedanken über \"Schritt(1)\"; dazu kommen wir noch \n",
    "\n",
    "Du kannst sehen, dass der Spieler (P) sich nach unten bewegt hat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c7ab81-0402-4302-a46f-1b4bc115ca63",
   "metadata": {},
   "source": [
    "#### Ziel\n",
    "\n",
    "Spule viele Schritte vorwärts, und du hast das Puzzle vervollständigt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a799321-538a-4f9c-ae1d-31e15a9e0e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "....\n",
      ".O.O\n",
      "...O\n",
      "O..P\n"
     ]
    }
   ],
   "source": [
    "env.step(1)\n",
    "env.step(2)\n",
    "env.step(1)\n",
    "env.step(2)\n",
    "env.step(1)\n",
    "env.step(2)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98858094-a152-4e3e-bd03-ed5c81af095b",
   "metadata": {},
   "source": [
    "Du hast das Ziel erreicht, indem du die untere rechte Ecke erreicht hast."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1635d0e4-7d38-434c-a956-d47646e57b4a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Was macht eine Environment aus?\n",
    "\n",
    "Eine Environment besteht aus mehreren Schlüsselkomponenten, die wir auf den folgenden Folien erläutern werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73df9894-c37a-4d34-93d9-c4e43ecf0564",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Staaten\n",
    "\n",
    "- Wir verwenden den Begriff _Zustand_ informell für alles, was die Environment betrifft "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a57b3f1-c76e-4b2c-9d84-742326bb827b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "P...\n",
      ".O.O\n",
      "...O\n",
      "O..G\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16f9748-129e-45c7-9bc1-106e0493fdad",
   "metadata": {},
   "source": [
    "- Das ist zum Beispiel der Ausgangszustand der Environment.\n",
    "- Der Spieler befindet sich oben links, in der Nähe gibt es gefrorenes Eis usw.\n",
    "- Wir werden das Konzept des Zustands verwenden, um über unsere Environment zu sprechen, aber es wird nicht in der \"API\" erscheinen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0e72e4-8f86-4c05-9f33-7e888ef44045",
   "metadata": {},
   "source": [
    "#### Aktionen\n",
    "\n",
    "- Hier kann der Spieler zwischen 4 möglichen Aktionen wählen: links, unten, rechts, oben\n",
    "- Der Raum aller möglichen Aktionen wird **Aktionsraum** genannt.\n",
    "- In SL unterscheiden wir zwischen Regression (kontinuierlich $y$) und Klassifikation (kategorisch $y$)\n",
    "- Auch im RL kann der Aktionsraum kontinuierlich oder diskret sein\n",
    "- In diesem Fall ist er diskret (4 Möglichkeiten)\n",
    "- Der Code stimmt überein:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "757cfd69-d93d-4e8b-90d0-f3caf079f5a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1907e0-63fe-4c72-af65-f9869b590365",
   "metadata": {},
   "source": [
    "#### Beobachtungen\n",
    "\n",
    "- Die Beobachtungen sind die _Teile des Zustands, die der Agent sehen kann_.\n",
    "- Manchmal kann der Agent alles sehen; wir nennen das _voll beobachtbar_.\n",
    "- Oft gibt es aber auch _teilweise beobachtbare_ Umgebungen\n",
    "- Im Beispiel des gefrorenen Sees kann der Agent nur seinen eigenen Standort von den 16 Feldern sehen.\n",
    "- Dem Agenten wird nicht durch direkte Beobachtungen \"gesagt\", wo die Löcher sind, also muss er dies durch Versuch und Irrtum _erlernen_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659ebb9b-4c6a-43f6-96c1-eb5117c50a54",
   "metadata": {},
   "source": [
    "#### Beobachtungen\n",
    "\n",
    "- Der Raum aller möglichen Beobachtungen wird **Beobachtungsraum** genannt.\n",
    "- Du kannst dir den Aktionsraum als Analogon zum Ziel beim überwachten Lernen vorstellen.\n",
    "- Du kannst dir den Beobachtungsraum analog zu den Merkmalen beim überwachten Lernen vorstellen.\n",
    "\n",
    "\n",
    "Hier haben wir einen diskreten Beobachtungsraum, der aus den 16 möglichen Spielerpositionen besteht:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81aa1ad6-e1bb-492d-a5d0-a72c94a33e29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(16)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ee9cb4-8650-4d39-87b7-97202ffca199",
   "metadata": {},
   "source": [
    "#### Belohnungen\n",
    "\n",
    "- Beim überwachten Lernen besteht das Ziel in der Regel darin, gute Vorhersagen zu treffen.\n",
    "- Je nach Ziel kannst du verschiedene Verlustfunktionen ausprobieren, aber das allgemeine Konzept ist dasselbe.\n",
    "- Im RL kann das Ziel alles sein.\n",
    "- Aber genau wie bei SL musst du etwas _optimieren_.\n",
    "- Im RL versuchen wir, die **Belohnung** zu maximieren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf68c60b-f6fe-47e0-8482-bc51c4410846",
   "metadata": {},
   "source": [
    "#### Belohnungen \n",
    "\n",
    "Im Beispiel des Gefrorenen Sees erhält der Agent eine Belohnung, wenn er das Ziel erreicht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4807bbe7-7525-4772-9ce9-a0e06f4b21fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "P...\n",
      ".O.O\n",
      "...O\n",
      "O..G\n",
      "reward = 0.0\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "obs, reward, done, _ = env.step(0)\n",
    "env.render()\n",
    "print(\"reward =\", reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aec3a5cd-5607-41fb-a932-7e5e10bf6569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward = 0.0\n"
     ]
    }
   ],
   "source": [
    "obs, reward, done, _ = env.step(1)\n",
    "print(\"reward =\", reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceae768d-d87b-4611-afce-e02137b9fd72",
   "metadata": {},
   "source": [
    "Immer noch keine Belohnung, lass uns weitermachen..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4c7004-91a1-4dcc-ad52-943cfd179a97",
   "metadata": {},
   "source": [
    "#### Belohnungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89518a96-7c75-437d-9bb1-da7dbc7038b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "....\n",
      ".O.O\n",
      "...O\n",
      "O..P\n",
      "reward = 1.0\n"
     ]
    }
   ],
   "source": [
    "env.step(1)\n",
    "env.step(2)\n",
    "env.step(1)\n",
    "env.step(2)\n",
    "env.step(1)\n",
    "obs, reward, done, _ = env.step(2)\n",
    "env.render()\n",
    "print(\"reward =\", reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f9cfd3-9e69-4752-9927-ba108685aa83",
   "metadata": {},
   "source": [
    "Wir haben eine Belohnung von 1,0 für das Erreichen des Ziels erhalten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e9f4d6-ef36-415d-905b-ce1fcce042e6",
   "metadata": {},
   "source": [
    "#### Environment-Agent-Schleife\n",
    "\n",
    "Beachte, wie der Agent (in diesem Fall wir) und die Environment in einer Hin- und Her-Schleife kommunizieren:\n",
    "\n",
    "![](img/RL-loop.png)\n",
    "\n",
    "Das ist das klassische Diagramm, das du überall siehst.\n",
    "\n",
    "Das Ziel ist es, das Verhalten des Agenten automatisch zu lernen (bleib dran!)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e888ae70-a3f2-400b-8fd9-54dd27369beb",
   "metadata": {},
   "source": [
    "#### Aktionen repräsentieren\n",
    "\n",
    "- Um RL-Software zu verwenden, brauchen wir eine numerische Darstellung unseres Aktionsraums und unseres Beobachtungsraums.\n",
    "- In diesem Fall haben wir 4 mögliche diskrete Aktionen, die wir als {0,1,2,3} für (links, unten, rechts, oben) kodieren können.\n",
    "- Aus diesem Grund haben wir früher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "420363c1-9385-47e0-abaf-935a5c0fb4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5a3c2f-ec47-47c4-85e8-95945dfa7019",
   "metadata": {},
   "source": [
    "nach unten zu gehen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6419f73c-8431-4e46-87af-0a99a95a58ba",
   "metadata": {},
   "source": [
    "#### Darstellung von Beobachtungen\n",
    "\n",
    "- Wir brauchen auch eine numerische Darstellung unserer Beobachtungen.\n",
    "- Hier gibt es 16 mögliche Positionen des Spielers. Diese werden wie folgt von 0-15 kodiert:\n",
    "\n",
    "```\n",
    " 0 1 2 3\n",
    " 4 5 6 7\n",
    " 8 9 10 11\n",
    "12 13 14 15\n",
    "```\n",
    "\n",
    "Diese Details zur Frozen Lake-Environment findest du auch in der [Dokumentation](https://www.gymlibrary.ml/environments/toy_text/frozen_lake/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cecba2-2529-4f24-a5ef-d730457ac994",
   "metadata": {},
   "source": [
    "#### Darstellung von Beobachtungen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b616b8-5ccf-45bf-a4ac-65ae31f108e4",
   "metadata": {},
   "source": [
    "Am Anfang sehen wir \"0\", weil wir oben links beginnen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df316735-5a38-488f-80ae-dd97d715cc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "P...\n",
      ".O.O\n",
      "...O\n",
      "O..G\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b5dad3-f8bc-4298-8dc4-4a3f87285600",
   "metadata": {},
   "source": [
    "Nachdem wir uns nach unten bewegt haben (Aktion 1), gehen wir auf Position 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d540232-affa-45de-a10f-6f99f755d5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, reward, done, _ = env.step(1)\n",
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b84f58-bf29-4df0-9b85-08190d1815f0",
   "metadata": {},
   "source": [
    "Die Beobachtung wird von der Methode `step()` zurückgegeben."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f179e5d-68cb-4d31-a5e0-cd01f5b14124",
   "metadata": {},
   "source": [
    "#### Nicht-deterministische Umgebungen\n",
    "\n",
    "- Bisher hat die Ausführung einer bestimmten Aktion in einem bestimmten Zustand immer zum gleichen neuen Zustand geführt.\n",
    "- Mit anderen Worten: Unsere Frozen Lake-Environment war _deterministisch_.\n",
    "- Manche Umgebungen sind _nicht-deterministisch_, das heißt, das Ergebnis einer Aktion kann zufällig sein.\n",
    "- Wir können einen nicht-deterministischen Frozen Lake wie folgt initialisieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9bdde4c3-e7f9-419a-a25b-4f2d6027354e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_slippery = gym.make(\"FrozenLake-v1\", is_slippery=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a7d44ef-2828-4c7f-aae9-17eab6b7d7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "env_slippery.seed(4)\n",
    "fix_frozen_lake_render(env_slippery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3943d88f-a44b-4fba-96f8-20177b6317b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "P...\n",
      ".O.O\n",
      "...O\n",
      "O..G\n"
     ]
    }
   ],
   "source": [
    "env_slippery.reset()\n",
    "env_slippery.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b97800f-5c7a-43a8-9018-f8505bd9eafd",
   "metadata": {},
   "source": [
    "#### Nicht-deterministische Umgebungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35e5cb48-3e0f-4b5f-8f0a-663ff31e99e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "....\n",
      "PO.O\n",
      "...O\n",
      "O..G\n"
     ]
    }
   ],
   "source": [
    "env_slippery.step(1) # move down\n",
    "env_slippery.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0755c6-0d78-42d0-a9b4-cd4aec36db82",
   "metadata": {},
   "source": [
    "Der Umzug nach unten hat nicht wie geplant funktioniert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cba5c8e8-a7ab-4854-9c35-ca79584f7b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "....\n",
      ".P.O\n",
      "...O\n",
      "O..G\n"
     ]
    }
   ],
   "source": [
    "env_slippery.step(1) # move down\n",
    "env_slippery.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031a6a4e-f9a1-4965-86b8-d439a82491ad",
   "metadata": {},
   "source": [
    "Diesmal hat es geklappt.\n",
    "\n",
    "In dieser \"glitschigen\" Frozen Lake-Environment funktioniert die Bewegung nur in 1/3 der Fälle wie vorgesehen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0388fe5a-b6e4-484c-ac51-d376c353ffaa",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Episoden\n",
    "\n",
    "- Das Spielen des Gefrorenen Sees hat ein Ende - entweder du fällst in ein Loch oder du erreichst das Ziel.\n",
    "- Ein Durchlauf reicht jedoch nicht aus, damit ein RL-Algorithmus daraus lernen kann.\n",
    "- Er braucht mehrere Durchläufe, die **Episode** genannt werden.\n",
    "- Nach einer Episode wird die Environment zurückgesetzt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68508dc1-2009-48a7-85e1-3f4bba4b0220",
   "metadata": {},
   "source": [
    "#### Episoden\n",
    "\n",
    "Die Methode `step()` gibt ein Flag zurück, das uns sagt, ob die Episode vorbei ist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "534b8516-26ea-4df4-9062-e6fb1f57d3a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, reward, done, _ = env_slippery.step(1)\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0684e7c-b767-4907-965d-6bcc86d64c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "....\n",
      ".P.O\n",
      "...O\n",
      "O..G\n"
     ]
    }
   ],
   "source": [
    "env_slippery.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdbecc7-7aac-432a-9e6e-301af52e7d2a",
   "metadata": {},
   "source": [
    "Hier ist die Episode zu Ende, weil wir in ein Loch gefallen sind."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854521ed-7fb7-453b-9eb6-eaf61074f533",
   "metadata": {},
   "source": [
    "#### Episoden\n",
    "\n",
    "- In einigen Umgebungen (z. B. Frozen Lake) erhältst du die Belohnungen erst am Ende einer Episode.\n",
    "- In anderen Umgebungen können Belohnungen in jedem **Zeitschritt** (d.h. nach einer Aktion) erhalten werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea0a22c-c13f-407e-beb1-5fd0a985dc4a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Alles zusammenfügen\n",
    "\n",
    "- Wir haben jetzt über die wichtigsten Komponenten einer RL-Environment gesprochen:\n",
    "  - Zustände\n",
    "  - Handlungen\n",
    "  - Beobachtungen\n",
    "  - Belohnungen\n",
    "  - Episoden\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8de8cd1-8733-4c90-89d6-9586be21309a",
   "metadata": {},
   "source": [
    "#### SL-Datensätze vs. RL-Umgebungen\n",
    "\n",
    "- Beim überwachten Lernen erhältst du normalerweise einen Datensatz.\n",
    "- Beim RL fungiert die Environment als _Datengenerator_.\n",
    "  - Je öfter du die Environment durchspielst, desto mehr \"Daten\" generierst du und desto mehr kannst du lernen.\n",
    "- Man kann RL auch mit einem vorher gesammelten Datensatz durchführen (das nennt man _offline RL_), aber das ist für uns nicht relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be83b356-8218-44dd-9ccd-f0bc44ade74e",
   "metadata": {},
   "source": [
    "#### Lass uns das Gelernte anwenden!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c511b50-fae2-4764-8eaa-c47ca7dab1d7",
   "metadata": {},
   "source": [
    "## Selbstfahrendes Auto Environment\n",
    "<!-- multiple choice -->\n",
    "\n",
    "Du benutzt RL, um ein selbstfahrendes Auto zu trainieren. Die Auto-KI nutzt verschiedene Kameras und Sensoren als Input und muss den Winkel des Lenkrads sowie den Winkel der Gas- und Bremspedale am Boden bestimmen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a443612-50ff-4564-bdf4-a4e1ba79be93",
   "metadata": {},
   "source": [
    "#### Ist der Beobachtungsraum kontinuierlich oder diskret?\n",
    "\n",
    "- [x] Kontinuierlich\n",
    "- [ ] Diskret | In diesem Fall sind die Beobachtungen die Sensoreingaben, z. B. Tiefenschätzungen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cace1880-0440-42f1-a087-576821998a4a",
   "metadata": {},
   "source": [
    "#### Ist der Aktionsraum kontinuierlich oder diskret?\n",
    "\n",
    "- [x] Kontinuierlich\n",
    "- [ ] Diskret | Die Aktionen sind Winkel; sie stammen nicht aus einer diskreten Menge von Optionen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124e3763-a526-4cd7-84c1-721bb7b9f102",
   "metadata": {},
   "source": [
    "#### Was wäre die sinnvollste Belohnungsstruktur für diese Environment?\n",
    "\n",
    "- [ ] Die Belohnung entspricht der Zeit, die das Auto fahren konnte, ohne zu verunglücken | Wie hoch wäre die Belohnung, wenn sich das Auto nie bewegt?\n",
    "- [x] Die Belohnung entspricht der Strecke, die das Auto fahren konnte, ohne zu verunglücken | Ja, das klingt gut!\n",
    "- [ ] +1 Belohnung wenn das Auto crashed | Erinnnere Dich daran, dass wir versuchen die Belohnung zu maximieren und nicht zu minimieren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcfbbb6-1765-473c-8b7e-c24e914949d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Episoden vs. Zeitschritte\n",
    "<!-- multiple choice -->\n",
    "\n",
    "Fülle die Lücken im folgenden Satz aus \n",
    "\n",
    "*Beim Reinforcement Learning führt man wiederholt Aktionen aus, bis das \\_\\_\\_\\_\\_ endet. Dabei kann es sich um nur ein \\_\\_\\_\\_\\_ handeln oder um sehr viele.*\n",
    "\n",
    "- [ ] Zeitschritt / Belohnung | Prüfe das erste Leerzeichen sorgfältig!\n",
    "- [ ] Belohnung / Zeitschritt | Versuche es noch einmal!\n",
    "- [ ] Zeitschritt / Episode | Versuche es noch einmal!\n",
    "- [x] Folge / Zeitschritt | Du hast es geschafft!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5286ef-84b7-4b04-a74b-d90b0216abae",
   "metadata": {},
   "source": [
    "## Gym's taxi environment\n",
    "<!-- coding exercise -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22599988-4849-4a9b-aa83-2b792dedbc19",
   "metadata": {},
   "source": [
    "In dieser Übung schauen wir uns eine der textbasierten Umgebungen an, die mit\n",
    "OpenAI Gym mitgelieferten textbasierten Umgebungen, der sogenannten Taxi-Environment.\n",
    "Dokumentation [hier](https://www.gymlibrary.ml/environments/toy_text/taxi/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb32c250-3e05-4040-bad0-5f8f0d7e3123",
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001B[34;1mR\u001B[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001B[35m\u001B[43mY\u001B[0m\u001B[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE\n",
    "import gym\n",
    "\n",
    "taxi = gym.make(\"Taxi-v3\")\n",
    "obs = taxi.reset(seed=5)\n",
    "taxi.render()\n",
    "\n",
    "# Add calls to `step` here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413240b7-6df6-48a5-bb24-4f8a518dc81e",
   "metadata": {},
   "source": [
    "In dieser Übung wird das Taxi durch die gelbe Markierung dargestellt,\n",
    "das sich derzeit unten links auf dem Spielfeld befindet. Das \":\" kann überquert werden, aber das \"|\" nicht.\n",
    "Das Ziel ist es, Fahrgäste aufzunehmen und sie abzusetzen.\n",
    "\n",
    "Es gibt 6 mögliche Aktionen:\n",
    "runter (0), Hoch (1), Rechts (2), Links (3), Aufnehmen (4), Absetzen (5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "99f35e5c-5e97-4122-9ae0-44e849a51c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(6)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5201b9-8041-4044-b172-a736f43451df",
   "metadata": {},
   "source": [
    "Der folgende Code gibt die Beobachtung in einem besser lesbaren Format aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f0083927-ad7d-4e78-b04f-316c394f0139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taxi row: 4\n",
      "Taxi col: 0\n",
      "Passenger loc: 0\n",
      "Destination loc: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Taxi row: %d\\nTaxi col: %d\\nPassenger loc: %d\\nDestination loc: %d\" % tuple(taxi.decode(obs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2553c73a-e6e8-4096-9b67-1806dae7f79f",
   "metadata": {},
   "source": [
    "Die möglichen Orte sind \"R\" (0), \"G\" (1), \"Y\" (2), \"B\" (3) und im Taxi (4). Der Fahrgast befindet sich also gerade in \"R\" und ist auf dem Weg nach \"Y\".\n",
    "\n",
    "Um die folgende Frage zu beantworten, musst du den Code ändern, ihn ausführen und alle relevanten Ausgaben ausdrucken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a4208ce9-efb2-48e7-8e7a-3b6042288309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001B[34;1mR\u001B[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001B[35m\u001B[43mY\u001B[0m\u001B[0m| : |B: |\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|\u001B[34;1m\u001B[43mR\u001B[0m\u001B[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001B[35mY\u001B[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001B[35m\u001B[42mY\u001B[0m\u001B[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "import gym\n",
    "\n",
    "taxi = gym.make(\"Taxi-v3\")\n",
    "obs = taxi.reset(seed=5)\n",
    "taxi.render()\n",
    "taxi.step(1)\n",
    "taxi.step(1)\n",
    "taxi.step(1)\n",
    "taxi.step(1)\n",
    "taxi.render()\n",
    "taxi.step(4)\n",
    "taxi.step(0)\n",
    "taxi.step(0)\n",
    "taxi.step(0)\n",
    "taxi.step(0)\n",
    "taxi.render()\n",
    "obs, reward, done, _ = taxi.step(5)\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "598a018b-569e-4378-bb2a-0de521c18881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: in this case there is no testing of the code; the code is only used to explore\n",
    "# this may not work well with the framework - add an automated test to the code as well? \n",
    "# e.g. can check that the state of the env is correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9564fe-af4a-4ded-97d3-4b754df46265",
   "metadata": {},
   "source": [
    "#### Wie viel Belohnung erhält der Agent für das erfolgreiche Absetzen des Passagiers?\n",
    "\n",
    "- [ ] 0 | Versuche, die notwendigen Aktionen mit `taxi.step()` auszuführen und die Belohnungen auszudrucken.\n",
    "- [ ] 5 | Versuche, die notwendigen Aktionen mit `taxi.step()` auszuführen und die Belohnungen auszudrucken.\n",
    "- [ ] 10 | Versuche, die notwendigen Aktionen mit `taxi.step()` auszuführen und die Belohnungen auszudrucken.\n",
    "- [x] 20 | Du hast es geschafft!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fffe5b6-36c3-4b2b-9513-a6125f0eaac3",
   "metadata": {},
   "source": [
    "## Beobachtungen vs. Renderings\n",
    "<!-- coding exercise -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a7f02e-4367-48d5-b491-8ee5cce4f468",
   "metadata": {},
   "source": [
    "Die Environment des gefrorenen Sees ermöglicht es uns, eine visuelle Darstellung der Environment zu erstellen, die wir bereits gesehen haben. Diese Darstellung dient der Fehlersuche und wird vom Agenten/Algorithmus _nicht_ gesehen. Deine Aufgabe ist es, eine bestimmte Frozen Lake-Environment zu spielen, **ohne auf die visuelle Darstellung zu achten** (kein Schummeln!). Fülle die Liste \"Aktionen\" so auf, dass sie eine Reihe von Aktionen enthält, die den Agenten korrekt zum Ziel bringen. Was du hier erlebst, ist das, was ein RL-Algorithmus beim Lernen \"sieht\"!\n",
    "\n",
    "Beachte, dass die gegebene Frozen Lake Environment 3x3 statt 4x4 ist. Der Beobachtungsraum geht also von 0 bis 8 statt von 0 bis 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb2ba815",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '____' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [28]\u001B[0m, in \u001B[0;36m<cell line: 12>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      9\u001B[0m env\u001B[38;5;241m.\u001B[39mrender \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     11\u001B[0m obs \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mreset()\n\u001B[0;32m---> 12\u001B[0m actions \u001B[38;5;241m=\u001B[39m \u001B[43m____\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m action \u001B[38;5;129;01min\u001B[39;00m actions:\n\u001B[1;32m     14\u001B[0m     obs, reward, done, _ \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep(action)\n",
      "\u001B[0;31mNameError\u001B[0m: name '____' is not defined"
     ]
    }
   ],
   "source": [
    "# EXERCISE\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "env = gym.make(\"FrozenLake-v1\", \n",
    "               desc=gym.envs.toy_text.frozen_lake.generate_random_map(size=3, p=0.3), \n",
    "               is_slippery=False)\n",
    "env.render = None\n",
    "\n",
    "obs = env.reset()\n",
    "actions = ____\n",
    "for action in actions:\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    print(\"Obs:\", obs, \"Reward:\", reward, \"Done:\", done)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7a5f827-8aad-4f48-b762-1feb4b51748a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs: 3 Reward: 0.0 Done: False\n",
      "Obs: 4 Reward: 0.0 Done: False\n",
      "Obs: 7 Reward: 0.0 Done: False\n",
      "Obs: 8 Reward: 1.0 Done: True\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "\n",
    "import gym.envs.toy_text\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "env = gym.make(\"FrozenLake-v1\", desc=gym.envs.toy_text.frozen_lake.generate_random_map(size=3, p=0.3), is_slippery=False)\n",
    "env.render = None\n",
    "\n",
    "obs = env.reset()\n",
    "actions = []\n",
    "# BEGIN SOLUTION\n",
    "actions = [1,2,1,2]\n",
    "# END SOLUTION\n",
    "for action in actions:\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    print(\"Obs:\", obs, \"Reward:\", reward, \"Done:\", done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0c3a6fc-637a-46b7-92e0-ff08e576c65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PO.\n",
      "...\n",
      "O.G\n"
     ]
    }
   ],
   "source": [
    "# HIDDEN\n",
    "import numpy as np\n",
    "import gym.envs.toy_text\n",
    "from utils_01 import fix_frozen_lake_render\n",
    "np.random.seed(1)\n",
    "env = gym.make(\"FrozenLake-v1\", desc=gym.envs.toy_text.frozen_lake.generate_random_map(size=3, p=0.3), is_slippery=False)\n",
    "env.reset()\n",
    "fix_frozen_lake_render(env)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e8a4b0-b102-4047-8e67-942ef81b0ed9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Der beste Weg zum Ziel\n",
    "\n",
    "Wenn man bedenkt, dass die Aktionen 0, 1, 2, 3 jeweils für links, unten, rechts und oben stehen, welche der folgenden Aussagen beschreibt den besten Weg zum Ziel richtig?\n",
    "\n",
    "- [ ] Beginne mit der Bewegung nach unten, dann wieder nach unten\n",
    "- [x] Bewege dich zuerst nach unten, dann nach rechts\n",
    "- [ ] Erst nach rechts, dann wieder nach rechts\n",
    "- [ ] Erst nach rechts, dann nach unten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8690d35a-5d51-4452-bd9c-8b44161feb81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ray200]",
   "language": "python",
   "name": "conda-env-ray200-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}