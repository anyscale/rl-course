{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e23df55-837b-4ec9-86ad-3b60c6a7f00d",
   "metadata": {},
   "source": [
    "## Kodierung Beobachtungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58bc4b75-8b99-4450-9b25-6af748253913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import ray\n",
    "import logging\n",
    "ray.init(log_to_driver=False, ignore_reinit_error=True, logging_level=logging.ERROR); # logging.FATAL\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95a9faf-9b2c-45ec-a347-3aadca399516",
   "metadata": {},
   "source": [
    "#### √úberpr√ºfung: Was ist eine Policy?\n",
    "\n",
    "- Im RL versuchen wir, eine Policy zu lernen, was ist das nochmal genau?\n",
    "- Eine Policy bildet **Beobachtungen** auf **Aktionen** ab.\n",
    "- Mit anderen Worten: Die Beobachtungen sind alles, was die Policy \"sieht\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacebed5-729c-4ea8-8570-3ac70481723b",
   "metadata": {},
   "source": [
    "#### Zuf√§llige Seenpolicy\n",
    "\n",
    "- Was sind die Beobachtungen im Zufallssee?\n",
    "- Es ist der Standort des Spielers, dargestellt als ganze Zahl von 0 bis 15  \n",
    "- Zur Auffrischung von Modul 1: Eine deterministische Policy k√∂nnte wie folgt aussehen:\n",
    "\n",
    "| Beobachtung | Aktion |\n",
    "|------|-------|\n",
    "| 0 | 0 |\n",
    "| 1 | 3 |\n",
    "| 2 | 1 |\n",
    "| 3 | 1 |\n",
    "| ... | ... |\n",
    "| 14 | 2 |\n",
    "| 15 | 2 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0d4278-ce03-4e94-9f2a-667b88bf1e85",
   "metadata": {},
   "source": [
    "#### Zuf√§llige Seenpolicy\n",
    "\n",
    "Und eine nicht-deterministische Policy k√∂nnte so aussehen:\n",
    "\n",
    "| Beobachtung | P(links) | P(unten) | P(rechts) | P(oben) | \n",
    "|------------|-------|-----------|---------|-------|\n",
    "| 0 | 0 | 0.9 | 0.01 | 0.04 | 0.05\n",
    "| 1 | 3 | 0.05 | 0.05 | 0.05 | 0.85\n",
    "| ... | ... | ... | ...      | ...      | ...\n",
    "| 15 | 2 | 0.0 | 0.0 | 0.99 | 0.01\n",
    "\n",
    "Das bedeutet √ºbrigens nicht, dass RLlib eine solche Tabelle lernt, aber wir k√∂nnen uns diese Tabelle begrifflich vorstellen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b044dbcb-f7a2-4792-aaf2-2c72bda18af2",
   "metadata": {},
   "source": [
    "#### Zuf√§llige Seenpolicy\n",
    "\n",
    "- Im Random Lake muss unsere gesamte Entscheidung auf der Position des Spielers basieren.\n",
    "- Manchmal reicht das schon aus: Von Position 11 aus solltest du nach unten gehen.\n",
    "\n",
    "```\n",
    " 0 1 2 3\n",
    " 4 5 6 7\n",
    " 8 9 10 11\n",
    "12 13 14 15\n",
    "```\n",
    "\n",
    "- Aber was ist mit Position 5, was solltest du von dort aus tun?\n",
    "- Antwort: _Es kommt darauf an_. Wenn es an Position 9 ein Loch gibt, willst du nicht nach unten gehen. Das Gleiche gilt f√ºr die 6 \n",
    "- Wie kann ich das entscheiden, _ohne zu wissen, wo die L√∂cher sind_?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fac00c2-6d9b-442f-ad3c-f9212a0dffac",
   "metadata": {},
   "source": [
    "#### State vs. Beobachtung, eine Zusammenfassung\n",
    "\n",
    "- In Modul 1 haben wir den Zustand informell als alles √ºber die Environment definiert.\n",
    "- In diesem Fall w√§ren das der Standort des Spielers und die L√∂cher.\n",
    "- Die Beobachtung hingegen kodiert nur einen Teil des Zustands: in diesem Fall den Standort des Spielers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbe12b1-eded-4618-a5dc-77dfd9fb75a0",
   "metadata": {},
   "source": [
    "#### Beobachtung = Zustand? Problem 1.\n",
    "\n",
    "- Okay, warum dann nicht einfach die Beobachtung auf den Zustand setzen? \n",
    "- Hier gibt es zwei Probleme.\n",
    "- Problem 1: Wenn das RL-System eingesetzt wird, kennst du vielleicht nicht den gesamten Status.\n",
    "  - Beispiel: In einem Empfehlungssystem hat der Agent (Empfehlungsgeber) keinen Zugriff auf die Stimmung des Nutzers (ein Teil des Zustands, der das Ergebnis beeinflusst)\n",
    "  - Beim √ºberwachten Lernen wollen wir nicht auf Merkmale trainieren, auf die wir im Einsatz keinen Zugriff haben\n",
    "    - Auch hier muss die Beobachtung etwas sein, auf das wir im Einsatz zugreifen k√∂nnen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617197ea-fcfe-457e-a567-a1f876b0bb86",
   "metadata": {},
   "source": [
    "#### Beobachtung = Zustand? Problem 2.\n",
    "\n",
    "- Problem 2: Es kann schwierig sein, von einer wirklich komplexen Beobachtung zu verallgemeinern.\n",
    "  - Es gibt Hunderttausende von m√∂glichen Zust√§nden in diesem kleinen 4x4 Random Lake Spiel.\n",
    "  - Zu viele Informationen k√∂nnten f√ºr den Agenten verwirrend sein oder es k√∂nnten unangemessen viele Daten (Simulationen) ben√∂tigt werden, um sie zu verstehen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6e363c-9255-442c-a598-1d33199b317b",
   "metadata": {},
   "source": [
    "#### Beobachtungen zur Kodierung\n",
    "\n",
    "- Ein Teil unserer Aufgabe als RL-Praktiker ist es, eine Darstellung (oder Kodierung) f√ºr die Beobachtung zu w√§hlen.\n",
    "- Finde aus den Informationen, die der Spieler wissen darf, eine sinnvolle Darstellung dessen, was der Spieler wissen muss.\n",
    "- In unserem Fall probieren wir einen Ansatz aus: Der Spieler kann \"sehen\", ob die 4 angrenzenden Felder L√∂cher sind oder nicht.\n",
    "- Wir kodieren dies als 4 Bin√§rzahlen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb9fb21-9ad8-4800-ab56-fc70eec7b5ec",
   "metadata": {},
   "source": [
    "#### Beobachtungen zur Kodierung\n",
    "\n",
    "```\n",
    ".OO.\n",
    "....\n",
    "O.P.\n",
    "...G\n",
    "```\n",
    "\n",
    "- In dieser Situation gibt es keine L√∂cher um den Spieler herum, also \"sieht\" der Spieler `[0 0 0 0]` \n",
    "- Mit anderen Worten: Die Beobachtung ist hier `[0 0 0 0]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a2fb46-4e64-47d7-9a3c-2af76319760c",
   "metadata": {},
   "source": [
    "#### Beobachtungen zur Kodierung\n",
    "\n",
    "```\n",
    ".OO.\n",
    "..P.\n",
    "O.O.\n",
    "...G\n",
    "```\n",
    "\n",
    "- Hier \"sieht\" der Spieler L√∂cher nach oben und unten, also ist die Beobachtung `[0 1 0 1]` (links, unten, rechts, oben)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114c2ad7-dc3d-45c8-9f59-46e55fd87e07",
   "metadata": {},
   "source": [
    "#### Beobachtungen zur Kodierung\n",
    "\n",
    "Was ist mit Kanten?\n",
    "\n",
    "```\n",
    "....\n",
    "..OP\n",
    "O.OO\n",
    "...G\n",
    "```\n",
    "\n",
    "- Dies ist unsere Wahl, wenn wir den Beobachtungsraum gestalten.\n",
    "- Ich entscheide mich daf√ºr, \"abseits des Rasters\" als L√∂cher darzustellen, d.h. wir tun so, als ob der See so aussieht:\n",
    " \n",
    "```\n",
    "OOOOOO\n",
    "O....O\n",
    "O..OPO\n",
    "OO.OOO\n",
    "O...GO\n",
    "OOOOOO\n",
    "```\n",
    "\n",
    "- Hier sieht der Spieler L√∂cher links, unten und rechts, also lautet die Beobachtung `[1 1 1 0]` (links, unten, rechts, oben)\n",
    "- Es gibt aber vielleicht bessere Ans√§tze, denn in ein Loch zu fallen ist schlimmer (die Episode endet) als √ºber den Rand zu laufen (es passiert nichts)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415a2223-64cb-43ca-befb-2269d33f9639",
   "metadata": {},
   "source": [
    "#### Kodierung unserer Beobachtungen\n",
    "\n",
    "- Jetzt, wo wir einen Plan haben, wie √§ndern wir den Code?\n",
    "- Da wir unsere Klasse so strukturiert haben, dass sie eine Methode \"Beobachtung\" hat, m√ºssen wir nur diese √§ndern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6046cff0-b83f-45b8-96ca-ab092b3c8599",
   "metadata": {},
   "outputs": [],
   "source": [
    "from envs_03 import RandomLake\n",
    "\n",
    "class RandomLakeObs(RandomLake):\n",
    "    def observation(self):\n",
    "        i, j = self.player\n",
    "\n",
    "        obs = []\n",
    "        obs.append(1 if j==0 else self.holes[i,j-1]) # left\n",
    "        obs.append(1 if i==3 else self.holes[i+1,j]) # down\n",
    "        obs.append(1 if j==3 else self.holes[i,j+1]) # right\n",
    "        obs.append(1 if i==0 else self.holes[i-1,j]) # up\n",
    "        \n",
    "        obs = np.array(obs, dtype=int) # cast to numpy array (optional)\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ee30d8-f9f9-4655-817d-84aebd8ca444",
   "metadata": {},
   "source": [
    "- Der Code erstellt eine Variable `obs`, in der jeder Eintrag 1 ist, wenn diese Richtung von der Kante wegf√ºhrt **oder** dort ein Loch vorhanden ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88f95238-ed09-4582-ab28-b9a85b48d992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53303bf5-5453-4798-92f3-618bb2774151",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Kodierung unserer Beobachtungen\n",
    "\n",
    "- Es ist noch eine weitere Code√§nderung erforderlich, und zwar der Konstruktor, in dem der Beobachtungsraum definiert wird.\n",
    "- Unsere Beobachtungen waren bisher eine Ganzzahl von 0 bis 15, also haben wir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0b2ebd3-1775-4bdf-9c93-c5af274dbc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space = gym.spaces.Discrete(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459308c9-a85a-4fcb-a8da-02a91401113f",
   "metadata": {},
   "source": [
    "Das gilt auch f√ºr Aktionen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3cc9d30-715e-40a6-baa4-03496e91f9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = gym.spaces.Discrete(4)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53434076-3bb8-4fd1-9e8e-e23eb2be2808",
   "metadata": {},
   "source": [
    "- Allerdings sind unsere Beobachtungen jetzt Arrays aus 4 Zahlen und nicht mehr eine einzelne Zahl.\n",
    "- Um dies zu verdeutlichen, verwenden wir \"gym.spaces.MultiDiscrete\" anstelle von \"gym.spaces.Discrete\".\n",
    "- Multi, weil wir mehrere Zahlen haben, aber immer noch diskret, weil jede der 4 Zahlen nur 2 m√∂gliche Werte annehmen kann (0 oder 1).\n",
    "- Hier ist der Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b119b15e-4d45-4087-8895-0ef3c694f62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomLakeObs(RandomLake):\n",
    "    def __init__(self, env_config=None):\n",
    "        self.observation_space = gym.spaces.MultiDiscrete([2,2,2,2])\n",
    "        self.action_space = gym.spaces.Discrete(4)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47022e95-090c-494b-955e-7e4ccadf088e",
   "metadata": {},
   "source": [
    "(Beachte, dass `gym` auch einen `MultiBinary`-Raumtyp hat, aber dieser wird derzeit nicht von RLlib unterst√ºtzt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f5ebb5-6a8a-448e-a692-335ae56f4377",
   "metadata": {},
   "source": [
    "#### Teste unsere neue Environment\n",
    "\n",
    "Testen wir es aus!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "466d94ac-5b1b-4ce5-96ec-2b99b6849c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afccc03d-e0f8-4b8b-9be7-d13aad969084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from envs_03 import RandomLakeObs\n",
    "\n",
    "env = RandomLakeObs()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de7de150-479c-4a5c-98cf-4adf0abd378b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßëüßäüßäüßä\n",
      "üï≥üï≥üï≥üßä\n",
      "üßäüßäüï≥üßä\n",
      "üßäüßäüï≥‚õ≥Ô∏è\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b6f296-1805-4467-8b34-fd61ebcdd732",
   "metadata": {},
   "source": [
    "Hier sehen wir die erwartete Beobachtung, die \"L√∂cher\" nach links, unten und oben anzeigt.\n",
    "\n",
    "Anmerkungen \n",
    "\n",
    "Links und oben sind die Kartenr√§nder, und unten ist ein tats√§chliches Loch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef2d1ac-4700-4dee-8aeb-23d09f18414f",
   "metadata": {},
   "source": [
    "#### Teste unsere neue Environment\n",
    "\n",
    "Lass uns versuchen, nach rechts zu gehen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db75ac7f-c4db-4e2e-8dbb-734fb0ff4ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 0, 1]), 0, False, {'player': (0, 1), 'goal': (3, 3)})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24287850-3c51-469c-86cd-17f2faa8cf3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßäüßëüßäüßä\n",
      "üï≥üï≥üï≥üßä\n",
      "üßäüßäüï≥üßä\n",
      "üßäüßäüï≥‚õ≥Ô∏è\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bac1dc-f61f-41c5-85d5-095f880079e8",
   "metadata": {},
   "source": [
    "Jetzt sehen wir L√∂cher in der Abw√§rts- und Aufw√§rtsrichtung, wie erwartet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3483468e-5524-4e8b-912e-ed2ab4e5a1ba",
   "metadata": {},
   "source": [
    "#### Training mit unseren neuen Beobachtungen\n",
    "\n",
    "- Unsere neuen Beobachtungen scheinen zu funktionieren, aber helfen sie dem Agenten auch beim Lernen?\n",
    "- Erinnere dich daran, dass wir mit unserem `Diskreten(16)` Beobachtungsraum nicht viel mehr als eine Erfolgsquote von 30% erreichen konnten.\n",
    "- Versuchen wir es noch einmal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83267540-3d6c-43a1-a834-9fb258d0dd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "from utils_03 import lake_default_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "232e7a87-3c60-4ef3-91eb-e3b26db1e257",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo = lake_default_config.build(env=RandomLakeObs)\n",
    "\n",
    "for i in range(8):\n",
    "    ppo.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea82a7de-3556-4f7a-9e78-78ef708e3b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6420454545454546"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo.evaluate()[\"evaluation\"][\"episode_reward_mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c7214d-f48c-4f13-8bb1-a6de0fb92411",
   "metadata": {},
   "source": [
    "- Das ist viel besser als die ~30%, die wir vorher bekommen haben!\n",
    "- Das macht Sinn... unser Agent kann die L√∂cher jetzt \"sehen\", anstatt blindlings zu laufen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86425828-0bef-4b47-ad0e-29d8afaf465d",
   "metadata": {},
   "source": [
    "#### Lass uns das Gelernte anwenden!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4842178d-aa77-46b2-9da3-7deaea3d40f9",
   "metadata": {},
   "source": [
    "## Analogie zum √ºberwachten Lernen: Beobachtungsraum\n",
    "<!-- multiple choice -->\n",
    "\n",
    "In den Folien haben wir den Beobachtungsraum f√ºr unseren Agenten ver√§ndert und dadurch eine h√∂here Belohnung erzielt. Welchem Aspekt des √ºberwachten Lernens √§hnelt das am meisten?\n",
    "\n",
    "- [x] Feature Engineering | Du hast es erfasst! Unser Beobachtungsraum dient als Merkmalsraum, auf den unsere Policy einwirken soll.\n",
    "- [Modellauswahl | Nicht ganz. Aber wie wir sehen werden, gibt es auch im RL einen Platz f√ºr die Modellauswahl!\n",
    "- [Hyperparameter-Abstimmung | Nicht ganz. Aber wie wir noch sehen werden, gibt es auch im RL einen Platz f√ºr Hyperparameter-Tuning!\n",
    "- [Ausw√§hlen einer Verlustfunktion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6438bf5-8970-4b51-a1b4-011f067d8903",
   "metadata": {},
   "source": [
    "## Einschlie√ülich der Position des Spielers\n",
    "<!-- multiple choice -->\n",
    "\n",
    "In unserer neuen Beobachtungsdarstellung haben wir den Standort des Spielers aus der Beobachtung _entfernt_ und _nur_ das Vorhandensein der L√∂cher in der N√§he ber√ºcksichtigt. Wenn wir einen Beobachtungsraum brauchen, der sowohl die W√§nde in der N√§he als auch den Standort des Spielers enth√§lt, welchen der folgenden Turnhallenr√§ume k√∂nnten wir dann verwenden?\n",
    "\n",
    "- [ ] `gym.spaces.Discrete(5)` | Versuch es noch einmal!\n",
    "- [x] `gym.spaces.MultiDiscrete([2,2,2,2,2,16])` | Ja! Die ersten 4 Zahlen stehen f√ºr die L√∂cher und die letzte Zahl f√ºr den Standort des Spielers.\n",
    "- [ ] `gym.spaces.MultiDiscrete([2,2,2,2]) + gym.spaces.Discrete(16)` | Versuche es noch einmal; leider k√∂nnen wir keine Turnhallenpl√§tze hinzuf√ºgen.\n",
    "- [ ] `gym.spaces.MultiDiscrete([32,32,32,32])` | Das k√∂nnte funktionieren, ist aber eine verwirrende/redundante Darstellung."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03e0f1b-e397-467a-8f62-8537173201c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Behandlung der Kanten\n",
    "<!-- multiple choice -->\n",
    "\n",
    "In den Folien haben wir uns entschieden, Kanten wie L√∂cher zu behandeln. Erinnere dich an dieses Bild:\n",
    "\n",
    "```\n",
    "OOOOOO\n",
    "O....O\n",
    "O..OPO\n",
    "OO.OOO\n",
    "O...GO\n",
    "OOOOOO\n",
    "```\n",
    "\n",
    "Kanten und L√∂cher unterscheiden sich jedoch voneinander: Wenn du in eine Kante l√§ufst, bewirkt das nichts, w√§hrend du in ein Loch l√§ufst, dass die Episode zu Ende ist. Das k√∂nnte ein wichtiger Unterschied sein, besonders in einer \"schl√ºpfrigen\" Version der Environment, in der die Ergebnisse von Aktionen nicht deterministisch sind \n",
    "\n",
    "Um dieses Problem zu l√∂sen, beschlie√üen wir, den Beobachtungsraum zu √§ndern. Der Agent \"sieht\" immer noch nur die vier Quadrate um ihn herum, aber jetzt sieht er, ob jedes Quadrat ein leeres Feld, ein Loch oder eine Kante ist. Welchen der folgenden Turnhallen-Beobachtungsr√§ume k√∂nnten wir f√ºr diese Darstellung verwenden?\n",
    "\n",
    "- [ ] `gym.spaces.MultiDiscrete([2,2,2,2,2,2,2,2,2])` | Versuch es noch einmal. Denke daran, dass der Agent immer noch nur 4 Quadrate \"sieht\".\n",
    "- [gym.spaces.MultiDiscrete([3,3,3,3,3,3,3,3,3])` | Versuche es noch einmal!\n",
    "- [ ] `gym.spaces.MultiDiscrete([2,2,2,2])` | Dies ist dasselbe wie das vorherige Feld, aber wir haben eine √Ñnderung vorgenommen.\n",
    "- [x] `gym.spaces.MultiDiscrete([3,3,3,3])` | Du hast es geschafft! Es gibt jetzt 3 m√∂gliche Optionen f√ºr das, was der Agent an jedem Platz \"sehen\" kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "485875e4-6694-4330-838c-1a54ed0499de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO / note to self\n",
    "# query_policy(trainer, RandomLakeObs(), [1,1,1,1])\n",
    "# shows that it wants to go up. this is because the above \"hole\" is probably an edge based on its learning. fascinating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c89bbb1-d5b4-41ef-bdd9-1f9ef08aa1eb",
   "metadata": {},
   "source": [
    "## Implementierung der Kanten\n",
    "<!-- coding exercise -->\n",
    "\n",
    "Der folgende Code zeigt die Funktion `Beobachtung` f√ºr den aktuellen Beobachtungsraum. √Ñndere den Code so, dass er den neuen Beobachtungsraum verwendet, wobei 0 f√ºr einen leeren Raum, 1 f√ºr ein Loch und 2 f√ºr eine Kante steht "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87cf9e9c-550c-43cb-8725-aa60d0cb91ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßëüßäüßäüßä\n",
      "üï≥üï≥üï≥üßä\n",
      "üßäüßäüï≥üßä\n",
      "üßäüßäüï≥‚õ≥Ô∏è\n",
      "[1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE\n",
    "\n",
    "from envs_03 import RandomLake\n",
    "\n",
    "class RandomLakeObs2(RandomLakeObs):\n",
    "    def observation(self):\n",
    "        i, j = self.player\n",
    "\n",
    "        obs = []\n",
    "        obs.append(1 if j==0 else self.holes[i,j-1]) # left\n",
    "        obs.append(1 if i==3 else self.holes[i+1,j]) # down\n",
    "        obs.append(1 if j==3 else self.holes[i,j+1]) # right\n",
    "        obs.append(1 if i==0 else self.holes[i-1,j]) # up\n",
    "        \n",
    "        obs = np.array(obs, dtype=int) # cast to numpy array\n",
    "        return obs\n",
    "\n",
    "np.random.seed(42)\n",
    "env = RandomLakeObs2()\n",
    "obs = env.reset()\n",
    "env.render()\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06328a38-f19c-4c71-82b6-31e056f28f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßëüßäüßäüßä\n",
      "üï≥üï≥üï≥üßä\n",
      "üßäüßäüï≥üßä\n",
      "üßäüßäüï≥‚õ≥Ô∏è\n",
      "[2 1 0 2]\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "\n",
    "from envs_03 import RandomLake\n",
    "\n",
    "class RandomLakeObs2(RandomLakeObs):\n",
    "    def observation(self):\n",
    "        i, j = self.player\n",
    "\n",
    "        obs = []\n",
    "        obs.append(2 if j==0 else self.holes[i,j-1]) # left\n",
    "        obs.append(2 if i==3 else self.holes[i+1,j]) # down\n",
    "        obs.append(2 if j==3 else self.holes[i,j+1]) # right\n",
    "        obs.append(2 if i==0 else self.holes[i-1,j]) # up\n",
    "        \n",
    "        obs = np.array(obs, dtype=int) # cast to numpy array\n",
    "        return obs\n",
    "\n",
    "np.random.seed(42)\n",
    "env = RandomLakeObs2()\n",
    "obs = env.reset()\n",
    "env.render()\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0f55b9-a660-4a04-8caf-52d13ced50e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Was der Agent sieht\n",
    "<!-- coding exercise -->\n",
    "\n",
    "Mit unserer neuen Kodierung des Beobachtungsraums \"sieht\" der Agent nur die 4 R√§ume um ihn herum und hat nur diese Informationen zur Verf√ºgung, um seine Entscheidungen zu treffen. Die Codezelle unten zeigt, was der Agent \"sieht\", w√§hrend er auf dem Zufallssee navigiert. Du kannst Aktionen mit der Tastatur eingeben, indem du die W√∂rter \"links\", \"unten\", \"rechts\" oder \"oben\" (oder kurz \"l\", \"d\", \"r\", \"u\") eintippst und die Simulation wird dir das Ergebnis zeigen. (Tippe \"quit\", um zu beenden.) Spiele das Spiel, bis du das Ziel erreicht hast. Versuche w√§hrenddessen, den See zu kartografieren (vielleicht durch Zeichnen auf einem Blatt Papier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d5b24c4-0df7-47e3-8155-895ccbb9e5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO / NOTE:\n",
    "# THIS EXERCISE DOES NOT HAVE A \"solution\"\n",
    "# the code is here ONLY to help them answer the multiple choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8341e39f-8684-4106-94cc-a4a422f476b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation:\n",
      ".O.\n",
      "OP.\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE\n",
    "\n",
    "import numpy as np\n",
    "from envs_03 import RandomLakeObs\n",
    "\n",
    "actions = {\"left\" : 0, \"down\" : 1, \"right\" : 2, \"up\" : 3, \n",
    "           \"l\" : 0, \"d\" : 1, \"r\" : 2, \"u\" : 3}\n",
    "\n",
    "np.random.seed(45)\n",
    "env = RandomLakeObs()\n",
    "obs = env.reset()\n",
    "\n",
    "act = \"start\"\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "   \n",
    "    obs_print = [['.']*3 for i in range(3)]\n",
    "    obs_print[1][1] = \"P\"\n",
    "    if obs[0]:\n",
    "        obs_print[1][0] = \"O\"\n",
    "    if obs[1]:\n",
    "        obs_print[2][1] = \"O\"\n",
    "    if obs[2]:\n",
    "        obs_print[1][2] = \"O\"\n",
    "    if obs[3]:\n",
    "        obs_print[0][1] = \"O\"\n",
    "    print(\"Observation:\")\n",
    "    print(\"\\n\".join(list(map(lambda c: \"\".join(c), obs_print))))\n",
    "    print()\n",
    "    \n",
    "    while act != \"quit\" and act not in actions: \n",
    "        act = input() # gather keyboard input \n",
    "    \n",
    "    if act == \"quit\":\n",
    "        break\n",
    "        \n",
    "    obs, rew, done, _ = env.step(act)\n",
    "    \n",
    "if done:\n",
    "    if rew > 0:\n",
    "        print(\"You win! +1 reward üéâ\")\n",
    "    else:\n",
    "        print(\"You fell into the lake üò¢\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c8469b-8040-44db-82cd-174ca76b82c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "import numpy as np\n",
    "from envs_03 import RandomLakeObs\n",
    "\n",
    "actions = {\"left\" : 0, \"down\" : 1, \"right\" : 2, \"up\" : 3, \n",
    "           \"l\" : 0, \"d\" : 1, \"r\" : 2, \"u\" : 3}\n",
    "\n",
    "np.random.seed(45)\n",
    "env = RandomLakeObs()\n",
    "obs = env.reset()\n",
    "\n",
    "act = \"start\"\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "   \n",
    "    obs_print = [['.']*3 for i in range(3)]\n",
    "    obs_print[1][1] = \"P\"\n",
    "    if obs[0]:\n",
    "        obs_print[1][0] = \"O\"\n",
    "    if obs[1]:\n",
    "        obs_print[2][1] = \"O\"\n",
    "    if obs[2]:\n",
    "        obs_print[1][2] = \"O\"\n",
    "    if obs[3]:\n",
    "        obs_print[0][1] = \"O\"\n",
    "    print(\"Observation:\")\n",
    "    print(\"\\n\".join(list(map(lambda c: \"\".join(c), obs_print))))\n",
    "    print()\n",
    "    \n",
    "    while act != \"quit\" and act not in actions: \n",
    "        act = input() # gather keyboard input \n",
    "    \n",
    "    if act == \"quit\":\n",
    "        break\n",
    "        \n",
    "    obs, rew, done, _ = env.step(act)\n",
    "    \n",
    "if done:\n",
    "    if rew > 0:\n",
    "        print(\"You win! +1 reward üéâ\")\n",
    "    else:\n",
    "        print(\"You fell into the lake üò¢\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7024e752-4e71-4162-886e-c5be693c9ee5",
   "metadata": {},
   "source": [
    "#### Wie sieht der See aus?\n",
    "\n",
    "Welche Karte des Sees in der obigen Frage ist nach deinen Erkundungen die richtige?\n",
    "\n",
    "```\n",
    " (A) (B) (C) (D)\n",
    "P..O P.OO P..O P.OO\n",
    "..OO .OOO ..OO .OOO\n",
    "O...     O...     O...     O..O\n",
    "...G ...G ..OG ...G\n",
    "```\n",
    "\n",
    "- [x] (A)\n",
    "- [ ] (B)\n",
    "- [ ] (C)\n",
    "- [ ] (D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbad4252-94ef-480a-938d-6e4858ed3675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# could also considering showing a BAD environment encoding to contrast with this reasonable one, as in the next slide deck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ray200]",
   "language": "python",
   "name": "conda-env-ray200-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
