{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e23df55-837b-4ec9-86ad-3b60c6a7f00d",
   "metadata": {},
   "source": [
    "## Kodierung Beobachtungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58bc4b75-8b99-4450-9b25-6af748253913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import ray\n",
    "import logging\n",
    "ray.init(log_to_driver=False, ignore_reinit_error=True, logging_level=logging.ERROR); # logging.FATAL\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95a9faf-9b2c-45ec-a347-3aadca399516",
   "metadata": {},
   "source": [
    "#### Überprüfung: Was ist eine Policy?\n",
    "\n",
    "- Im RL versuchen wir, eine Policy zu lernen, was ist das nochmal genau?\n",
    "- Eine Policy bildet **Beobachtungen** auf **Aktionen** ab.\n",
    "- Mit anderen Worten: Die Beobachtungen sind alles, was die Policy \"sieht\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacebed5-729c-4ea8-8570-3ac70481723b",
   "metadata": {},
   "source": [
    "#### Zufällige Seenpolicy\n",
    "\n",
    "- Was sind die Beobachtungen im Zufallssee?\n",
    "- Es ist der Standort des Spielers, dargestellt als ganze Zahl von 0 bis 15  \n",
    "- Zur Auffrischung von Modul 1: Eine deterministische Policy könnte wie folgt aussehen:\n",
    "\n",
    "| Beobachtung | Aktion |\n",
    "|------|-------|\n",
    "| 0 | 0 |\n",
    "| 1 | 3 |\n",
    "| 2 | 1 |\n",
    "| 3 | 1 |\n",
    "| ... | ... |\n",
    "| 14 | 2 |\n",
    "| 15 | 2 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0d4278-ce03-4e94-9f2a-667b88bf1e85",
   "metadata": {},
   "source": [
    "#### Zufällige Seenpolicy\n",
    "\n",
    "Und eine nicht-deterministische Policy könnte so aussehen:\n",
    "\n",
    "| Beobachtung | P(links) | P(unten) | P(rechts) | P(oben) | \n",
    "|------------|-------|-----------|---------|-------|\n",
    "| 0 | 0 | 0.9 | 0.01 | 0.04 | 0.05\n",
    "| 1 | 3 | 0.05 | 0.05 | 0.05 | 0.85\n",
    "| ... | ... | ... | ...      | ...      | ...\n",
    "| 15 | 2 | 0.0 | 0.0 | 0.99 | 0.01\n",
    "\n",
    "Das bedeutet übrigens nicht, dass RLlib eine solche Tabelle lernt, aber wir können uns diese Tabelle begrifflich vorstellen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b044dbcb-f7a2-4792-aaf2-2c72bda18af2",
   "metadata": {},
   "source": [
    "#### Zufällige Seenpolicy\n",
    "\n",
    "- Im Random Lake muss unsere gesamte Entscheidung auf der Position des Spielers basieren.\n",
    "- Manchmal reicht das schon aus: Von Position 11 aus solltest du nach unten gehen.\n",
    "\n",
    "```\n",
    " 0 1 2 3\n",
    " 4 5 6 7\n",
    " 8 9 10 11\n",
    "12 13 14 15\n",
    "```\n",
    "\n",
    "- Aber was ist mit Position 5, was solltest du von dort aus tun?\n",
    "- Antwort: _Es kommt darauf an_. Wenn es an Position 9 ein Loch gibt, willst du nicht nach unten gehen. Das Gleiche gilt für die 6 \n",
    "- Wie kann ich das entscheiden, _ohne zu wissen, wo die Löcher sind_?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fac00c2-6d9b-442f-ad3c-f9212a0dffac",
   "metadata": {},
   "source": [
    "#### State vs. Beobachtung, eine Zusammenfassung\n",
    "\n",
    "- In Modul 1 haben wir den Zustand informell als alles über die Environment definiert.\n",
    "- In diesem Fall wären das der Standort des Spielers und die Löcher.\n",
    "- Die Beobachtung hingegen kodiert nur einen Teil des Zustands: in diesem Fall den Standort des Spielers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbe12b1-eded-4618-a5dc-77dfd9fb75a0",
   "metadata": {},
   "source": [
    "#### Beobachtung = Zustand? Problem 1.\n",
    "\n",
    "- Okay, warum dann nicht einfach die Beobachtung auf den Zustand setzen? \n",
    "- Hier gibt es zwei Probleme.\n",
    "- Problem 1: Wenn das RL-System eingesetzt wird, kennst du vielleicht nicht den gesamten Status.\n",
    "  - Beispiel: In einem Empfehlungssystem hat der Agent (Empfehlungsgeber) keinen Zugriff auf die Stimmung des Nutzers (ein Teil des Zustands, der das Ergebnis beeinflusst)\n",
    "  - Beim überwachten Lernen wollen wir nicht auf Merkmale trainieren, auf die wir im Einsatz keinen Zugriff haben\n",
    "    - Auch hier muss die Beobachtung etwas sein, auf das wir im Einsatz zugreifen können."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617197ea-fcfe-457e-a567-a1f876b0bb86",
   "metadata": {},
   "source": [
    "#### Beobachtung = Zustand? Problem 2.\n",
    "\n",
    "- Problem 2: Es kann schwierig sein, von einer wirklich komplexen Beobachtung zu verallgemeinern.\n",
    "  - Es gibt Hunderttausende von möglichen Zuständen in diesem kleinen 4x4 Random Lake Spiel.\n",
    "  - Zu viele Informationen könnten für den Agenten verwirrend sein oder es könnten unangemessen viele Daten (Simulationen) benötigt werden, um sie zu verstehen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6e363c-9255-442c-a598-1d33199b317b",
   "metadata": {},
   "source": [
    "#### Beobachtungen zur Kodierung\n",
    "\n",
    "- Ein Teil unserer Aufgabe als RL-Praktiker ist es, eine Darstellung (oder Kodierung) für die Beobachtung zu wählen.\n",
    "- Finde aus den Informationen, die der Spieler wissen darf, eine sinnvolle Darstellung dessen, was der Spieler wissen muss.\n",
    "- In unserem Fall probieren wir einen Ansatz aus: Der Spieler kann \"sehen\", ob die 4 angrenzenden Felder Löcher sind oder nicht.\n",
    "- Wir kodieren dies als 4 Binärzahlen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb9fb21-9ad8-4800-ab56-fc70eec7b5ec",
   "metadata": {},
   "source": [
    "#### Beobachtungen zur Kodierung\n",
    "\n",
    "```\n",
    ".OO.\n",
    "....\n",
    "O.P.\n",
    "...G\n",
    "```\n",
    "\n",
    "- In dieser Situation gibt es keine Löcher um den Spieler herum, also \"sieht\" der Spieler `[0 0 0 0]` \n",
    "- Mit anderen Worten: Die Beobachtung ist hier `[0 0 0 0]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a2fb46-4e64-47d7-9a3c-2af76319760c",
   "metadata": {},
   "source": [
    "#### Beobachtungen zur Kodierung\n",
    "\n",
    "```\n",
    ".OO.\n",
    "..P.\n",
    "O.O.\n",
    "...G\n",
    "```\n",
    "\n",
    "- Hier \"sieht\" der Spieler Löcher nach oben und unten, also ist die Beobachtung `[0 1 0 1]` (links, unten, rechts, oben)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114c2ad7-dc3d-45c8-9f59-46e55fd87e07",
   "metadata": {},
   "source": [
    "#### Beobachtungen zur Kodierung\n",
    "\n",
    "Was ist mit Kanten?\n",
    "\n",
    "```\n",
    "....\n",
    "..OP\n",
    "O.OO\n",
    "...G\n",
    "```\n",
    "\n",
    "- Dies ist unsere Wahl, wenn wir den Beobachtungsraum gestalten.\n",
    "- Ich entscheide mich dafür, \"abseits des Rasters\" als Löcher darzustellen, d.h. wir tun so, als ob der See so aussieht:\n",
    " \n",
    "```\n",
    "OOOOOO\n",
    "O....O\n",
    "O..OPO\n",
    "OO.OOO\n",
    "O...GO\n",
    "OOOOOO\n",
    "```\n",
    "\n",
    "- Hier sieht der Spieler Löcher links, unten und rechts, also lautet die Beobachtung `[1 1 1 0]` (links, unten, rechts, oben)\n",
    "- Es gibt aber vielleicht bessere Ansätze, denn in ein Loch zu fallen ist schlimmer (die Episode endet) als über den Rand zu laufen (es passiert nichts)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415a2223-64cb-43ca-befb-2269d33f9639",
   "metadata": {},
   "source": [
    "#### Kodierung unserer Beobachtungen\n",
    "\n",
    "- Jetzt, wo wir einen Plan haben, wie ändern wir den Code?\n",
    "- Da wir unsere Klasse so strukturiert haben, dass sie eine Methode \"Beobachtung\" hat, müssen wir nur diese ändern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6046cff0-b83f-45b8-96ca-ab092b3c8599",
   "metadata": {},
   "outputs": [],
   "source": [
    "from envs_03 import RandomLake\n",
    "\n",
    "class RandomLakeObs(RandomLake):\n",
    "    def observation(self):\n",
    "        i, j = self.player\n",
    "\n",
    "        obs = []\n",
    "        obs.append(1 if j==0 else self.holes[i,j-1]) # left\n",
    "        obs.append(1 if i==3 else self.holes[i+1,j]) # down\n",
    "        obs.append(1 if j==3 else self.holes[i,j+1]) # right\n",
    "        obs.append(1 if i==0 else self.holes[i-1,j]) # up\n",
    "        \n",
    "        obs = np.array(obs, dtype=int) # cast to numpy array (optional)\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ee30d8-f9f9-4655-817d-84aebd8ca444",
   "metadata": {},
   "source": [
    "- Der Code erstellt eine Variable `obs`, in der jeder Eintrag 1 ist, wenn diese Richtung von der Kante wegführt **oder** dort ein Loch vorhanden ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88f95238-ed09-4582-ab28-b9a85b48d992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53303bf5-5453-4798-92f3-618bb2774151",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Kodierung unserer Beobachtungen\n",
    "\n",
    "- Es ist noch eine weitere Codeänderung erforderlich, und zwar der Konstruktor, in dem der Beobachtungsraum definiert wird.\n",
    "- Unsere Beobachtungen waren bisher eine Ganzzahl von 0 bis 15, also haben wir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0b2ebd3-1775-4bdf-9c93-c5af274dbc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space = gym.spaces.Discrete(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459308c9-a85a-4fcb-a8da-02a91401113f",
   "metadata": {},
   "source": [
    "Das gilt auch für Aktionen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3cc9d30-715e-40a6-baa4-03496e91f9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = gym.spaces.Discrete(4)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53434076-3bb8-4fd1-9e8e-e23eb2be2808",
   "metadata": {},
   "source": [
    "- Allerdings sind unsere Beobachtungen jetzt Arrays aus 4 Zahlen und nicht mehr eine einzelne Zahl.\n",
    "- Um dies zu verdeutlichen, verwenden wir \"gym.spaces.MultiDiscrete\" anstelle von \"gym.spaces.Discrete\".\n",
    "- Multi, weil wir mehrere Zahlen haben, aber immer noch diskret, weil jede der 4 Zahlen nur 2 mögliche Werte annehmen kann (0 oder 1).\n",
    "- Hier ist der Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b119b15e-4d45-4087-8895-0ef3c694f62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomLakeObs(RandomLake):\n",
    "    def __init__(self, env_config=None):\n",
    "        self.observation_space = gym.spaces.MultiDiscrete([2,2,2,2])\n",
    "        self.action_space = gym.spaces.Discrete(4)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47022e95-090c-494b-955e-7e4ccadf088e",
   "metadata": {},
   "source": [
    "(Beachte, dass `gym` auch einen `MultiBinary`-Raumtyp hat, aber dieser wird derzeit nicht von RLlib unterstützt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f5ebb5-6a8a-448e-a692-335ae56f4377",
   "metadata": {},
   "source": [
    "#### Teste unsere neue Environment\n",
    "\n",
    "Testen wir es aus!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "466d94ac-5b1b-4ce5-96ec-2b99b6849c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afccc03d-e0f8-4b8b-9be7-d13aad969084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from envs_03 import RandomLakeObs\n",
    "\n",
    "env = RandomLakeObs()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de7de150-479c-4a5c-98cf-4adf0abd378b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧑🧊🧊🧊\n",
      "🕳🕳🕳🧊\n",
      "🧊🧊🕳🧊\n",
      "🧊🧊🕳⛳️\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b6f296-1805-4467-8b34-fd61ebcdd732",
   "metadata": {},
   "source": [
    "Hier sehen wir die erwartete Beobachtung, die \"Löcher\" nach links, unten und oben anzeigt.\n",
    "\n",
    "Anmerkungen \n",
    "\n",
    "Links und oben sind die Kartenränder, und unten ist ein tatsächliches Loch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef2d1ac-4700-4dee-8aeb-23d09f18414f",
   "metadata": {},
   "source": [
    "#### Teste unsere neue Environment\n",
    "\n",
    "Lass uns versuchen, nach rechts zu gehen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db75ac7f-c4db-4e2e-8dbb-734fb0ff4ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 0, 1]), 0, False, {'player': (0, 1), 'goal': (3, 3)})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24287850-3c51-469c-86cd-17f2faa8cf3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧊🧑🧊🧊\n",
      "🕳🕳🕳🧊\n",
      "🧊🧊🕳🧊\n",
      "🧊🧊🕳⛳️\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bac1dc-f61f-41c5-85d5-095f880079e8",
   "metadata": {},
   "source": [
    "Jetzt sehen wir Löcher in der Abwärts- und Aufwärtsrichtung, wie erwartet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3483468e-5524-4e8b-912e-ed2ab4e5a1ba",
   "metadata": {},
   "source": [
    "#### Training mit unseren neuen Beobachtungen\n",
    "\n",
    "- Unsere neuen Beobachtungen scheinen zu funktionieren, aber helfen sie dem Agenten auch beim Lernen?\n",
    "- Erinnere dich daran, dass wir mit unserem `Diskreten(16)` Beobachtungsraum nicht viel mehr als eine Erfolgsquote von 30% erreichen konnten.\n",
    "- Versuchen wir es noch einmal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83267540-3d6c-43a1-a834-9fb258d0dd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "from utils_03 import lake_default_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "232e7a87-3c60-4ef3-91eb-e3b26db1e257",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo = lake_default_config.build(env=RandomLakeObs)\n",
    "\n",
    "for i in range(8):\n",
    "    ppo.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea82a7de-3556-4f7a-9e78-78ef708e3b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6420454545454546"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo.evaluate()[\"evaluation\"][\"episode_reward_mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c7214d-f48c-4f13-8bb1-a6de0fb92411",
   "metadata": {},
   "source": [
    "- Das ist viel besser als die ~30%, die wir vorher bekommen haben!\n",
    "- Das macht Sinn... unser Agent kann die Löcher jetzt \"sehen\", anstatt blindlings zu laufen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86425828-0bef-4b47-ad0e-29d8afaf465d",
   "metadata": {},
   "source": [
    "#### Lass uns das Gelernte anwenden!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4842178d-aa77-46b2-9da3-7deaea3d40f9",
   "metadata": {},
   "source": [
    "## Analogie zum überwachten Lernen: Beobachtungsraum\n",
    "<!-- multiple choice -->\n",
    "\n",
    "In den Folien haben wir den Beobachtungsraum für unseren Agenten verändert und dadurch eine höhere Belohnung erzielt. Welchem Aspekt des überwachten Lernens ähnelt das am meisten?\n",
    "\n",
    "- [x] Feature Engineering | Du hast es erfasst! Unser Beobachtungsraum dient als Merkmalsraum, auf den unsere Policy einwirken soll.\n",
    "- [Modellauswahl | Nicht ganz. Aber wie wir sehen werden, gibt es auch im RL einen Platz für die Modellauswahl!\n",
    "- [Hyperparameter-Abstimmung | Nicht ganz. Aber wie wir noch sehen werden, gibt es auch im RL einen Platz für Hyperparameter-Tuning!\n",
    "- [Auswählen einer Verlustfunktion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6438bf5-8970-4b51-a1b4-011f067d8903",
   "metadata": {},
   "source": [
    "## Einschließlich der Position des Spielers\n",
    "<!-- multiple choice -->\n",
    "\n",
    "In unserer neuen Beobachtungsdarstellung haben wir den Standort des Spielers aus der Beobachtung _entfernt_ und _nur_ das Vorhandensein der Löcher in der Nähe berücksichtigt. Wenn wir einen Beobachtungsraum brauchen, der sowohl die Wände in der Nähe als auch den Standort des Spielers enthält, welchen der folgenden Turnhallenräume könnten wir dann verwenden?\n",
    "\n",
    "- [ ] `gym.spaces.Discrete(5)` | Versuch es noch einmal!\n",
    "- [x] `gym.spaces.MultiDiscrete([2,2,2,2,2,16])` | Ja! Die ersten 4 Zahlen stehen für die Löcher und die letzte Zahl für den Standort des Spielers.\n",
    "- [ ] `gym.spaces.MultiDiscrete([2,2,2,2]) + gym.spaces.Discrete(16)` | Versuche es noch einmal; leider können wir keine Turnhallenplätze hinzufügen.\n",
    "- [ ] `gym.spaces.MultiDiscrete([32,32,32,32])` | Das könnte funktionieren, ist aber eine verwirrende/redundante Darstellung."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03e0f1b-e397-467a-8f62-8537173201c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Behandlung der Kanten\n",
    "<!-- multiple choice -->\n",
    "\n",
    "In den Folien haben wir uns entschieden, Kanten wie Löcher zu behandeln. Erinnere dich an dieses Bild:\n",
    "\n",
    "```\n",
    "OOOOOO\n",
    "O....O\n",
    "O..OPO\n",
    "OO.OOO\n",
    "O...GO\n",
    "OOOOOO\n",
    "```\n",
    "\n",
    "Kanten und Löcher unterscheiden sich jedoch voneinander: Wenn du in eine Kante läufst, bewirkt das nichts, während du in ein Loch läufst, dass die Episode zu Ende ist. Das könnte ein wichtiger Unterschied sein, besonders in einer \"schlüpfrigen\" Version der Environment, in der die Ergebnisse von Aktionen nicht deterministisch sind \n",
    "\n",
    "Um dieses Problem zu lösen, beschließen wir, den Beobachtungsraum zu ändern. Der Agent \"sieht\" immer noch nur die vier Quadrate um ihn herum, aber jetzt sieht er, ob jedes Quadrat ein leeres Feld, ein Loch oder eine Kante ist. Welchen der folgenden Turnhallen-Beobachtungsräume könnten wir für diese Darstellung verwenden?\n",
    "\n",
    "- [ ] `gym.spaces.MultiDiscrete([2,2,2,2,2,2,2,2,2])` | Versuch es noch einmal. Denke daran, dass der Agent immer noch nur 4 Quadrate \"sieht\".\n",
    "- [gym.spaces.MultiDiscrete([3,3,3,3,3,3,3,3,3])` | Versuche es noch einmal!\n",
    "- [ ] `gym.spaces.MultiDiscrete([2,2,2,2])` | Dies ist dasselbe wie das vorherige Feld, aber wir haben eine Änderung vorgenommen.\n",
    "- [x] `gym.spaces.MultiDiscrete([3,3,3,3])` | Du hast es geschafft! Es gibt jetzt 3 mögliche Optionen für das, was der Agent an jedem Platz \"sehen\" kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "485875e4-6694-4330-838c-1a54ed0499de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO / note to self\n",
    "# query_policy(trainer, RandomLakeObs(), [1,1,1,1])\n",
    "# shows that it wants to go up. this is because the above \"hole\" is probably an edge based on its learning. fascinating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c89bbb1-d5b4-41ef-bdd9-1f9ef08aa1eb",
   "metadata": {},
   "source": [
    "## Implementierung der Kanten\n",
    "<!-- coding exercise -->\n",
    "\n",
    "Der folgende Code zeigt die Funktion `Beobachtung` für den aktuellen Beobachtungsraum. Ändere den Code so, dass er den neuen Beobachtungsraum verwendet, wobei 0 für einen leeren Raum, 1 für ein Loch und 2 für eine Kante steht "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87cf9e9c-550c-43cb-8725-aa60d0cb91ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧑🧊🧊🧊\n",
      "🕳🕳🕳🧊\n",
      "🧊🧊🕳🧊\n",
      "🧊🧊🕳⛳️\n",
      "[1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE\n",
    "\n",
    "from envs_03 import RandomLake\n",
    "\n",
    "class RandomLakeObs2(RandomLakeObs):\n",
    "    def observation(self):\n",
    "        i, j = self.player\n",
    "\n",
    "        obs = []\n",
    "        obs.append(1 if j==0 else self.holes[i,j-1]) # left\n",
    "        obs.append(1 if i==3 else self.holes[i+1,j]) # down\n",
    "        obs.append(1 if j==3 else self.holes[i,j+1]) # right\n",
    "        obs.append(1 if i==0 else self.holes[i-1,j]) # up\n",
    "        \n",
    "        obs = np.array(obs, dtype=int) # cast to numpy array\n",
    "        return obs\n",
    "\n",
    "np.random.seed(42)\n",
    "env = RandomLakeObs2()\n",
    "obs = env.reset()\n",
    "env.render()\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06328a38-f19c-4c71-82b6-31e056f28f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧑🧊🧊🧊\n",
      "🕳🕳🕳🧊\n",
      "🧊🧊🕳🧊\n",
      "🧊🧊🕳⛳️\n",
      "[2 1 0 2]\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "\n",
    "from envs_03 import RandomLake\n",
    "\n",
    "class RandomLakeObs2(RandomLakeObs):\n",
    "    def observation(self):\n",
    "        i, j = self.player\n",
    "\n",
    "        obs = []\n",
    "        obs.append(2 if j==0 else self.holes[i,j-1]) # left\n",
    "        obs.append(2 if i==3 else self.holes[i+1,j]) # down\n",
    "        obs.append(2 if j==3 else self.holes[i,j+1]) # right\n",
    "        obs.append(2 if i==0 else self.holes[i-1,j]) # up\n",
    "        \n",
    "        obs = np.array(obs, dtype=int) # cast to numpy array\n",
    "        return obs\n",
    "\n",
    "np.random.seed(42)\n",
    "env = RandomLakeObs2()\n",
    "obs = env.reset()\n",
    "env.render()\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0f55b9-a660-4a04-8caf-52d13ced50e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Was der Agent sieht\n",
    "<!-- coding exercise -->\n",
    "\n",
    "Mit unserer neuen Kodierung des Beobachtungsraums \"sieht\" der Agent nur die 4 Räume um ihn herum und hat nur diese Informationen zur Verfügung, um seine Entscheidungen zu treffen. Die Codezelle unten zeigt, was der Agent \"sieht\", während er auf dem Zufallssee navigiert. Du kannst Aktionen mit der Tastatur eingeben, indem du die Wörter \"links\", \"unten\", \"rechts\" oder \"oben\" (oder kurz \"l\", \"d\", \"r\", \"u\") eintippst und die Simulation wird dir das Ergebnis zeigen. (Tippe \"quit\", um zu beenden.) Spiele das Spiel, bis du das Ziel erreicht hast. Versuche währenddessen, den See zu kartografieren (vielleicht durch Zeichnen auf einem Blatt Papier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d5b24c4-0df7-47e3-8155-895ccbb9e5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO / NOTE:\n",
    "# THIS EXERCISE DOES NOT HAVE A \"solution\"\n",
    "# the code is here ONLY to help them answer the multiple choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8341e39f-8684-4106-94cc-a4a422f476b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation:\n",
      ".O.\n",
      "OP.\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE\n",
    "\n",
    "import numpy as np\n",
    "from envs_03 import RandomLakeObs\n",
    "\n",
    "actions = {\"left\" : 0, \"down\" : 1, \"right\" : 2, \"up\" : 3, \n",
    "           \"l\" : 0, \"d\" : 1, \"r\" : 2, \"u\" : 3}\n",
    "\n",
    "np.random.seed(45)\n",
    "env = RandomLakeObs()\n",
    "obs = env.reset()\n",
    "\n",
    "act = \"start\"\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "   \n",
    "    obs_print = [['.']*3 for i in range(3)]\n",
    "    obs_print[1][1] = \"P\"\n",
    "    if obs[0]:\n",
    "        obs_print[1][0] = \"O\"\n",
    "    if obs[1]:\n",
    "        obs_print[2][1] = \"O\"\n",
    "    if obs[2]:\n",
    "        obs_print[1][2] = \"O\"\n",
    "    if obs[3]:\n",
    "        obs_print[0][1] = \"O\"\n",
    "    print(\"Observation:\")\n",
    "    print(\"\\n\".join(list(map(lambda c: \"\".join(c), obs_print))))\n",
    "    print()\n",
    "    \n",
    "    while act != \"quit\" and act not in actions: \n",
    "        act = input() # gather keyboard input \n",
    "    \n",
    "    if act == \"quit\":\n",
    "        break\n",
    "        \n",
    "    obs, rew, done, _ = env.step(act)\n",
    "    \n",
    "if done:\n",
    "    if rew > 0:\n",
    "        print(\"You win! +1 reward 🎉\")\n",
    "    else:\n",
    "        print(\"You fell into the lake 😢\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c8469b-8040-44db-82cd-174ca76b82c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "import numpy as np\n",
    "from envs_03 import RandomLakeObs\n",
    "\n",
    "actions = {\"left\" : 0, \"down\" : 1, \"right\" : 2, \"up\" : 3, \n",
    "           \"l\" : 0, \"d\" : 1, \"r\" : 2, \"u\" : 3}\n",
    "\n",
    "np.random.seed(45)\n",
    "env = RandomLakeObs()\n",
    "obs = env.reset()\n",
    "\n",
    "act = \"start\"\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "   \n",
    "    obs_print = [['.']*3 for i in range(3)]\n",
    "    obs_print[1][1] = \"P\"\n",
    "    if obs[0]:\n",
    "        obs_print[1][0] = \"O\"\n",
    "    if obs[1]:\n",
    "        obs_print[2][1] = \"O\"\n",
    "    if obs[2]:\n",
    "        obs_print[1][2] = \"O\"\n",
    "    if obs[3]:\n",
    "        obs_print[0][1] = \"O\"\n",
    "    print(\"Observation:\")\n",
    "    print(\"\\n\".join(list(map(lambda c: \"\".join(c), obs_print))))\n",
    "    print()\n",
    "    \n",
    "    while act != \"quit\" and act not in actions: \n",
    "        act = input() # gather keyboard input \n",
    "    \n",
    "    if act == \"quit\":\n",
    "        break\n",
    "        \n",
    "    obs, rew, done, _ = env.step(act)\n",
    "    \n",
    "if done:\n",
    "    if rew > 0:\n",
    "        print(\"You win! +1 reward 🎉\")\n",
    "    else:\n",
    "        print(\"You fell into the lake 😢\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7024e752-4e71-4162-886e-c5be693c9ee5",
   "metadata": {},
   "source": [
    "#### Wie sieht der See aus?\n",
    "\n",
    "Welche Karte des Sees in der obigen Frage ist nach deinen Erkundungen die richtige?\n",
    "\n",
    "```\n",
    " (A) (B) (C) (D)\n",
    "P..O P.OO P..O P.OO\n",
    "..OO .OOO ..OO .OOO\n",
    "O...     O...     O...     O..O\n",
    "...G ...G ..OG ...G\n",
    "```\n",
    "\n",
    "- [x] (A)\n",
    "- [ ] (B)\n",
    "- [ ] (C)\n",
    "- [ ] (D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbad4252-94ef-480a-938d-6e4858ed3675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# could also considering showing a BAD environment encoding to contrast with this reasonable one, as in the next slide deck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ray200]",
   "language": "python",
   "name": "conda-env-ray200-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
