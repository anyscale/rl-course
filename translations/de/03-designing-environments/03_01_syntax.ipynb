{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e23df55-837b-4ec9-86ad-3b60c6a7f00d",
   "metadata": {},
   "source": [
    "## Environments syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2e57cc-3491-4489-af86-6052b1efcc1b",
   "metadata": {},
   "source": [
    "#### Motivation\n",
    "\n",
    "- Bislang haben wir vordefinierte Environmenten wie Frozen Like und Google RecSim verwendet.\n",
    "- Um RL für unser eigenes Problem zu nutzen, können wir keine dieser Environmenten verwenden.\n",
    "- Wir müssen unsere eigene Environment mit Python definieren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0797f5a9-c856-49f2-9a3c-419d816b6ec1",
   "metadata": {},
   "source": [
    "#### Gefrorener See Rückblick\n",
    "\n",
    "- Erinnere dich an die Environment des Gefrorenen Sees aus Modul 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69171cee-dc7b-433d-85c5-d0d40486a43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"FrozenLake-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a41dffc-8451-4d7e-a40a-23fd1315ffba",
   "metadata": {},
   "source": [
    "#### Gefrorener See Rückblick\n",
    "\n",
    "- OpenAI Gym ist quelloffen, also könnten wir uns den [Frozen Lake Quellcode](https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py) ansehen.\n",
    "- Er ist jedoch kompliziert und enthält viel mehr, als wir brauchen.\n",
    "- Lass uns unsere eigene Environment namens Frozen Pond mit den Grundkomponenten von Frozen Lake erstellen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735eb606-b1dd-4dba-be55-d6105cc47536",
   "metadata": {},
   "source": [
    "#### Komponenten eines Env\n",
    "\n",
    "Konzeptionelle Entscheidungen:\n",
    "\n",
    "- Beobachtungsraum\n",
    "- Handlungsraum\n",
    "\n",
    "In Python müssen wir zumindest implementieren:\n",
    "\n",
    "- konstruktor\n",
    "- `Rücksetzen()`\n",
    "- `step()`\n",
    "\n",
    "In der Praxis werden wir vielleicht auch andere Methoden brauchen, wie `render()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9cf2ed-dc7e-4bbd-a5f8-9b78b8a38710",
   "metadata": {},
   "source": [
    "#### Konzeptionelle Entscheidungen\n",
    "\n",
    "Da wir in diesem Fall den Gefrorenen See nachahmen, sind der Beobachtungsraum und der Aktionsraum bereits festgelegt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34c0aa1d-8ab0-462a-8bdd-8ba12484a5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space = gym.spaces.Discrete(16)\n",
    "action_space = gym.spaces.Discrete(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef0fa48-25fb-4360-a1d8-54ca675dc786",
   "metadata": {},
   "source": [
    "Im weiteren Verlauf des Kurses werden wir uns mit diesen Entscheidungen näher beschäftigen!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c88c68-7f74-490e-beda-9e199d0c3822",
   "metadata": {},
   "source": [
    "#### Verschlüsseln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db413de7-7df3-4d77-9ae8-5c8e95a39068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "class FrozenPond(gym.Env):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b3bbec-72c8-48dd-bdc4-6f537dacda5c",
   "metadata": {},
   "source": [
    "- Beachte, dass wir mit der Unterklasse `gym.Env` beginnen.\n",
    "- Optional: Du kannst über Objekte, Vererbung und Unterklassen lesen.\n",
    "- Kurz und bündig: Dies ist eine grundlegende `gym.Env` und wir können ihre Eigenschaften überschreiben."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b2467a-4906-473f-8207-cde646def666",
   "metadata": {},
   "source": [
    "#### Konstrukteur\n",
    "\n",
    "- Der Konstruktor wird aufgerufen, wenn wir ein neues `FrozenPond` Objekt erstellen.\n",
    "- Hier definieren wir den Beobachtungsraum und den Aktionsraum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2417aa6-93e3-46c9-984b-f98e24ece49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "class FrozenPond(gym.Env):\n",
    "    def __init__(self, env_config=None):\n",
    "        self.observation_space = gym.spaces.Discrete(16)\n",
    "        self.action_space = gym.spaces.Discrete(4)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359ab164-11d8-4614-a4f1-f875fac08071",
   "metadata": {},
   "source": [
    "- Aus Gründen der RLlib-Kompatibilität muss der Konstruktor eine \"env_config\" aufnehmen \n",
    "- Wir werden dieses Argument vorerst ignorieren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285e5eca-3aa8-472a-a6bc-037b411f83a5",
   "metadata": {},
   "source": [
    "#### Zurücksetzen\n",
    "\n",
    "- Die nächste Methode, die wir brauchen, ist reset.\n",
    "- Der Konstruktor setzt permanente Parameter wie den Beobachtungsraum.\n",
    "- der \"Reset\" legt jede neue Episode fest.\n",
    "- Zwischen den beiden gibt es einige Freiheiten, z. B. die Festlegung des Zielorts.\n",
    "- Wenn sich etwas ändern _könnte_, legen wir es in `reset` fest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce9715c1-8865-4376-97d5-4c0f7b253e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c438cc9b-d6db-4412-8b29-80288a2cf288",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def reset(self):\n",
    "        self.player = (0, 0) # the player starts at the top-left\n",
    "        self.goal = (3, 3)   # goal is at the bottom-right\n",
    "        \n",
    "        self.holes = np.array([\n",
    "            [0,0,0,0], # FFFF \n",
    "            [0,1,0,1], # FHFH\n",
    "            [0,0,0,1], # FFFH\n",
    "            [1,0,0,0]  # HFFF\n",
    "        ])\n",
    "        \n",
    "        return 0 # to be changed to return self.observation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206ed3bf-aa1d-4833-84fa-e9be1f64648b",
   "metadata": {},
   "source": [
    "#### Zurücksetzen\n",
    "\n",
    "Testen wir das mal aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c1c6161-2b90-47bf-b002-d3ff6fc62464",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = FrozenPond()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50989bc9-5d31-480a-88ac-dbf85f2ff390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edbdb5ee-0af9-43e2-8833-c1c945eae535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0],\n",
       "       [0, 1, 0, 1],\n",
       "       [0, 0, 0, 1],\n",
       "       [1, 0, 0, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp.holes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79c6258-6353-42ac-81d6-51423385972a",
   "metadata": {},
   "source": [
    "Sieht gut aus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a138d14-cf05-4d44-b315-4e708692e001",
   "metadata": {},
   "source": [
    "#### Schritt\n",
    "\n",
    "- Die letzte Methode, die wir brauchen, ist `step`.\n",
    "- Dies ist die komplizierteste Methode, die die Kernlogik enthält.\n",
    "- Erinnere dich daran, dass \"step\" 4 Dinge zurückgibt:\n",
    "  1. Beobachtung\n",
    "  2. Belohnung\n",
    "  3. Erledigt-Flagge\n",
    "  4. Extra Info (wird ignoriert)\n",
    "- Der Übersichtlichkeit halber schreiben wir Hilfsmethoden für Observation, Reward und Done sowie eine zusätzliche Hilfsmethode "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5e1f9a-6f10-4038-9c02-2a6e966ecac5",
   "metadata": {},
   "source": [
    "#### Schritt: Beobachtung\n",
    "\n",
    "Erinnere dich daran, dass die Beobachtung ein Index von 0 bis 15 ist:\n",
    "\n",
    "```\n",
    " 0 1 2 3\n",
    " 4 5 6 7\n",
    " 8 9 10 11\n",
    "12 13 14 15\n",
    "```\n",
    "\n",
    "Wir können dies wie folgt kodieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a66c82f8-21a4-403d-8172-2b878488e155",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def observation(self):\n",
    "        return 4*self.player[0] + self.player[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921f3bc3-6200-4167-839b-a068e88240c1",
   "metadata": {},
   "source": [
    "Wenn der Spieler zum Beispiel auf (2,1) steht, geben wir zurück"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b95ad09b-a77d-4039-afa6-0695a8427601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4*2 + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae722e0-9d57-414c-ade2-53b361b8e590",
   "metadata": {},
   "source": [
    "Hinweis: Jetzt, wo `self.observation` implementiert ist, sollten wir `reset` in `return self.observation()` statt in `return 0` ändern, um die Codequalität zu verbessern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b84fcf-c40f-4513-8cd0-3bb2b0a8d32a",
   "metadata": {},
   "source": [
    "#### Schritt: Belohnung\n",
    "\n",
    "In Anlehnung an das Beispiel des Gefrorenen Sees ist die Belohnung 1, wenn der Agent das Ziel erreicht, und 0, wenn nicht:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ee1fd77-5923-4db1-abe3-96c47360366a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def reward(self):\n",
    "        return int(self.player == self.goal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d71262-1a9e-4daf-940e-df02c2c2631d",
   "metadata": {},
   "source": [
    "Wir werden diese Belohnungsfunktion später im Modul ändern!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb7d07c-9153-4933-94f6-c117ca6993ff",
   "metadata": {},
   "source": [
    "#### Schritt: erledigt\n",
    "\n",
    "- Wir müssen auch wissen, wann eine Episode abgeschlossen ist \n",
    "- Nach Frozen Lake ist die Episode beendet, wenn der Agent das Ziel erreicht oder in den Teich fällt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab0ed064-4f50-425e-8f0f-302a44c2d259",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def done(self):\n",
    "        return self.player == self.goal or self.holes[self.player] == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8829aa45-d26c-4707-ab51-38c38353ce00",
   "metadata": {},
   "source": [
    "#### Schritt: gültige Standorte\n",
    "\n",
    "Um die Methode `step` einfacher zu machen, schreiben wir eine Hilfsmethode namens `is_valid_loc`, die überprüft, ob ein bestimmter Ort innerhalb der Grenzen liegt (von 0 bis 3 in jeder Dimension)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1872595-ab21-4b4e-a431-c15fc1ea7595",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def is_valid_loc(self, location):\n",
    "        if 0 <= location[0] <= 3 and 0 <= location[1] <= 3:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce94f18-ffb6-4f93-ba45-f7084c3805c5",
   "metadata": {},
   "source": [
    "#### Schritt: Zusammenbau\n",
    "\n",
    "- Mit den obigen Teilen können wir nun die Methode \"step\" schreiben.\n",
    "- schritt\" nimmt eine _Aktion_ auf, aktualisiert den _Zustand_ und gibt die Beobachtung, die Belohnung, das Erledigt-Flag und zusätzliche Informationen (ignoriert) zurück.\n",
    "- Erinnere dich daran, wie Aktionen kodiert werden: 0 für links, 1 für unten, 2 für rechts, 3 für oben.\n",
    "- Wir werden einen **nicht rutschigen** gefrorenen Teich implementieren, d.h. deterministisch und nicht stochastisch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c083279a-b950-45e6-9cfc-265885c80c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def step(self, action):\n",
    "        # Compute the new player location\n",
    "        if action == 0:   # left\n",
    "            new_loc = (self.player[0], self.player[1]-1)\n",
    "        elif action == 1: # down\n",
    "            new_loc = (self.player[0]+1, self.player[1])\n",
    "        elif action == 2: # right\n",
    "            new_loc = (self.player[0], self.player[1]+1)\n",
    "        elif action == 3: # up\n",
    "            new_loc = (self.player[0]-1, self.player[1])\n",
    "        else:\n",
    "            raise ValueError(\"Action must be in {0,1,2,3}\")\n",
    "        \n",
    "        # Update the player location only if you stayed in bounds\n",
    "        # (if you try to move out of bounds, the action does nothing)\n",
    "        if self.is_valid_loc(new_loc):\n",
    "            self.player = new_loc\n",
    "        \n",
    "        # Return observation/reward/done\n",
    "        return self.observation(), self.reward(), self.done(), {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a4a4ec-cfe6-4d3e-80a1-a606afc87c0e",
   "metadata": {},
   "source": [
    "#### Erfolg!\n",
    "\n",
    "- Das war's! Wir haben die notwendigen Teile in Frozen Pond implementiert \n",
    "  - konstruktor\n",
    "  - `Rücksetzen`\n",
    "  - schritt\n",
    "- Außerdem fügen wir eine optionale Funktion `render` hinzu, damit wir den Zustand zeichnen können:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dace259a-1e8f-4d77-af39-3f1cb6cdc2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def render(self):\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                if (i,j) == self.goal:\n",
    "                    print(\"⛳️\", end=\"\")\n",
    "                elif (i,j) == self.player:\n",
    "                    print(\"🧑\", end=\"\")\n",
    "                elif self.holes[i,j]:\n",
    "                    print(\"🕳\", end=\"\")\n",
    "                else:\n",
    "                    print(\"🧊\", end=\"\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925146e2-188f-4b78-acb5-df30214dad54",
   "metadata": {},
   "source": [
    "- Zum Spaß werden wir Emojis in unserem Kundenrendering verwenden.\n",
    "- Spieler ist 🧑, Tor ist ⛳️, gefrorenes See-Segment ist 🧊, Loch ist 🕳."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d03db1-e53b-435b-a0fb-aa67bdd99161",
   "metadata": {},
   "source": [
    "#### Testen unserer Implementierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a079671-3fc3-46eb-aaa3-e0629201de06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "from envs_03 import FrozenPond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c03550a-fde5-4885-883a-030a2530488f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧑🧊🧊🧊\n",
      "🧊🕳🧊🕳\n",
      "🧊🧊🧊🕳\n",
      "🕳🧊🧊⛳️\n"
     ]
    }
   ],
   "source": [
    "env = FrozenPond()\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88bcc59-f6c1-4722-a255-ef30a49c2616",
   "metadata": {},
   "source": [
    "#### Testen unserer Implementierung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2301a03f-f40e-492d-ae42-0f99b07ee09f",
   "metadata": {},
   "source": [
    "Lass uns die Methode \"step\" testen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1cc81874-c11f-430f-904b-1d108c170589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0, False, {'player': (0, 1), 'goal': (3, 3)})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(2) # 0=left / 1=down / 2=right / 3=up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89d4775d-fe17-4628-a97f-33ab2799573d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧊🧑🧊🧊\n",
      "🧊🕳🧊🕳\n",
      "🧊🧊🧊🕳\n",
      "🕳🧊🧊⛳️\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b46b9c2-6ebe-4d37-b809-c38bd711126c",
   "metadata": {},
   "source": [
    "Sieht gut aus!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be4064b-3f24-48b1-aa6e-184f655067d0",
   "metadata": {},
   "source": [
    "#### Testen unserer Implementierung\n",
    "\n",
    "Lass uns die beiden Environmenten direkt miteinander vergleichen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3aff2e55-17f6-48bc-8fb8-eaade93881a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter | gym obs / our obs | gym reward / our reward | gym done / our done\n",
      " 0   |       0 /  0      |          0 / 0        |      False / False\n",
      " 1   |       1 /  1      |          0 / 0        |      False / False\n",
      " 2   |       2 /  2      |          0 / 0        |      False / False\n",
      " 3   |       6 /  6      |          0 / 0        |      False / False\n",
      " 4   |      10 / 10      |          0 / 0        |      False / False\n",
      " 5   |      14 / 14      |          0 / 0        |      False / False\n",
      " 6   |      14 / 14      |          0 / 0        |      False / False\n",
      " 7   |      15 / 15      |          1 / 1        |       True /  True\n"
     ]
    }
   ],
   "source": [
    "lake = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
    "pond = FrozenPond()\n",
    "\n",
    "lake.reset()\n",
    "pond.reset()\n",
    "\n",
    "print(\"Iter | gym obs / our obs | gym reward / our reward | gym done / our done\")\n",
    "for i, a in enumerate([0, 2, 2, 1, 1, 1, 1, 2]):\n",
    "    lake_obs, lake_rew, lake_done, _ = lake.step(a)\n",
    "    pond_obs, pond_rew, pond_done, _ = pond.step(a)\n",
    "    print(\"%2d   |      %2d / %2d      |          %d / %d        |      %5s / %5s\" % \\\n",
    "          (i, lake_obs, pond_obs, lake_rew, pond_rew, lake_done, pond_done))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87185e66-37b4-4337-9046-17a17cc6efb2",
   "metadata": {},
   "source": [
    "Sie sehen gleich aus!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5deb1e8-3de2-42dc-8363-aee8acf534e3",
   "metadata": {},
   "source": [
    "#### Testen unserer Implementierung\n",
    "\n",
    "- RLlib kommt auch mit einem env checker\n",
    "- Dieser kann uns nicht sagen, ob unsere Environment mit der von Frozen Lake identisch ist\n",
    "- Aber er führt mehrere nützliche Prüfungen durch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57e3cbe2-8354-4533-872d-41bd06ce2948",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.utils.pre_checks.env import check_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f4ebd66-5e52-4724-a9c3-df6efdb66eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-27 16:08:55,501\tWARNING env.py:142 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n"
     ]
    }
   ],
   "source": [
    "check_env(pond)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f674689-9df5-4da7-9379-478c2b802571",
   "metadata": {},
   "source": [
    "- Alle Prüfungen wurden bestanden, bis auf diese Warnung über die maximale Episodenlänge.\n",
    "- Wir können/sollten diese festlegen, damit die Episoden nicht beliebig lang werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29218be1-9c76-4f04-bffd-e7288098bb74",
   "metadata": {},
   "source": [
    "#### Maximale Schritte pro Episode\n",
    "\n",
    "- Um eine maximale Anzahl von Schritten pro Episode festzulegen, können wir einen `gym` _wrapper_ verwenden.\n",
    "- Wrapper sind bequeme Wege, um Environmenten zu verändern, einschließlich Beobachtungen, Aktionen und Belohnungen.\n",
    "- Hier verwenden wir den Wrapper `TimeLimit`, um ein Schrittlimit festzulegen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8edeeaf-5dd3-4eee-9ed0-747880709912",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import TimeLimit\n",
    "\n",
    "pond_5 = TimeLimit(pond, max_episode_steps=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37ac862-4df2-4f76-bbe0-b718e575f0e3",
   "metadata": {},
   "source": [
    "Wir können überprüfen, dass es nach 5 Schritten erledigt ist, auch wenn das Ziel nicht erreicht wird:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be71dbd1-7668-4499-85d2-68a5b70fa1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, False, {'player': (0, 0), 'goal': (3, 3)})\n",
      "(0, 0, False, {'player': (0, 0), 'goal': (3, 3)})\n",
      "(0, 0, False, {'player': (0, 0), 'goal': (3, 3)})\n",
      "(0, 0, False, {'player': (0, 0), 'goal': (3, 3)})\n",
      "(0, 0, True, {'player': (0, 0), 'goal': (3, 3), 'TimeLimit.truncated': True})\n"
     ]
    }
   ],
   "source": [
    "pond_5.reset()\n",
    "for i in range(5):\n",
    "    print(pond_5.step(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f54f62a-2022-4bdf-926e-b855dc374ec6",
   "metadata": {},
   "source": [
    "#### Maximale Schritte pro Episode\n",
    "\n",
    "Eine vernünftigere Schrittgrenze wäre 50 statt 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff4847c8-ff8e-4e15-96b3-b0eb38781266",
   "metadata": {},
   "outputs": [],
   "source": [
    "pond_50 = TimeLimit(pond, max_episode_steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9eadc6-381d-47ac-82ec-f418b6830e6f",
   "metadata": {},
   "source": [
    "- Zu deiner Information: Es ist auch möglich, diese Grenze in RLlib zu setzen, nur für Trainingszwecke.\n",
    "- Dies geschieht mit dem Parameter \"horizon\" in der Trainer-Konfiguration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aed5b6c-a486-4f1c-96e0-e027710461f6",
   "metadata": {},
   "source": [
    "#### Lass uns das Gelernte anwenden!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456c51dc-3d2c-4e06-be76-62c279e976b0",
   "metadata": {},
   "source": [
    "## Gefrorener Teich Belohnungen\n",
    "<!-- multiple choice -->\n",
    "\n",
    "Im Gefrorenen See (und Teich) ist die Belohnung 1, wenn der Agent das Ziel erreicht, und sonst 0. Der Agent muss lernen, die Löcher zu vermeiden, aber es gibt eigentlich keine negative Belohnung, wenn er in ein Loch fällt - es ist die gleiche Null-Belohnung, wie wenn er in ein sicheres Stück gefrorenen See läuft! Warum funktioniert dieses System trotzdem, obwohl die Belohnung für das Laufen in ein Loch oder in trockenes Land dieselbe ist?\n",
    "\n",
    "- [ ] Sobald der Agent in ein Loch fällt, steckt er fest. Er kann zwar weitere Aktionen durchführen, aber die bringen nichts mehr. Deshalb lernt der Agent, Löcher zu vermeiden.\n",
    "- [Eine Belohnung von 0 ist die niedrigste mögliche Belohnung; wenn der Agent also eine Belohnung von 0 erhält, wenn er in ein Loch fällt, weiß er sofort, dass es schlecht ist, in ein Loch zu fallen.\n",
    "- [Die Strafe für das Hineinfallen in ein Loch ist indirekt, da die Episode mit einer Belohnung von Null endet und damit die potenzielle Belohnung von 1 für das erfolgreiche Erreichen des Ziels verwirkt wird. Der Agent lernt, dass er, wenn er in ein Loch fällt, _zukünftige_ Belohnungen einbüßt.\n",
    "- [RL-Agenten bevorzugen längere Episoden. Wenn der Agent in das Loch fällt, endet die Episode sofort, was der Agent zu vermeiden lernt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d53229-6adf-4b71-b8dd-40157b9a703d",
   "metadata": {},
   "source": [
    "## Teich vs. Labyrinth\n",
    "<!-- coding exercise -->\n",
    "\n",
    "Nehmen wir an, wir wollen unsere Teichumgebung in ein _Labyrinth_ umwandeln. In diesem Fall haben wir Wände anstelle von Löchern. Der einzige Unterschied zwischen dem Teich und dem Labyrinth ist das Verhalten von Löchern und Wänden. Wenn du im gefrorenen Teich in ein Loch trittst, endet die Episode. Im Labyrinth bewirkt das Betreten einer Wand nichts (d. h. die Aktion ändert den Standort des Agenten nicht, genau wie der Versuch, über den Rand der Karte zu gehen). Um unseren Gefrorenen See in ein Labyrinth zu verwandeln, müssen wir zwei Methoden ändern: `done` und `is_valid_loc`.\n",
    "\n",
    "Unten findest du die Methoden `done` und `step`, die wir in den Folien oben gesehen haben. Ändere sie so ab, dass wir jetzt ein Labyrinth mit dem oben beschriebenen Verhalten haben: Wenn du gegen eine Wand läufst, passiert nichts.\n",
    "\n",
    "Beachte, dass die Klasse `Maze` alle anderen Methoden von `FrozenPond` erbt, also kannst du sie ausprobieren!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "238c70d1-4cd2-4cb0-ba58-3599ffa0879e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 0, True, {'player': (1, 1), 'goal': (3, 3)})\n",
      "(5, 0, True, {'player': (1, 1), 'goal': (3, 3)})\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE\n",
    "from envs_03 import FrozenPond\n",
    "\n",
    "\n",
    "class Maze(FrozenPond):\n",
    "    def done(self):\n",
    "        return self.player == self.goal or self.holes[self.player] == 1\n",
    "    def is_valid_loc(self, location):\n",
    "        if 0 <= location[0] <= 3 and 0 <= location[1] <= 3:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "pond = FrozenPond()\n",
    "pond.reset()\n",
    "pond.step(1)\n",
    "print(pond.step(2))\n",
    "\n",
    "maze = Maze()\n",
    "maze.reset()\n",
    "maze.step(1)\n",
    "print(maze.step(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a483f55-b956-44c1-bf8e-d534e6ad3ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 0, True, {'player': (1, 1), 'goal': (3, 3)})\n",
      "(4, 0, False, {'player': (1, 0), 'goal': (3, 3)})\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "from envs_03 import FrozenPond\n",
    "\n",
    "\n",
    "class Maze(FrozenPond):   \n",
    "    def done(self):\n",
    "        return self.player == self.goal\n",
    "    def is_valid_loc(self, location):\n",
    "        if 0 <= location[0] <= 3 and 0 <= location[1] <= 3 and not self.holes[location]:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "pond = FrozenPond()\n",
    "pond.reset()\n",
    "pond.step(1)\n",
    "print(pond.step(2))\n",
    "\n",
    "maze = Maze()\n",
    "maze.reset()\n",
    "maze.step(1)\n",
    "print(maze.step(2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ray200]",
   "language": "python",
   "name": "conda-env-ray200-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
