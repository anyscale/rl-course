{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e23df55-837b-4ec9-86ad-3b60c6a7f00d",
   "metadata": {},
   "source": [
    "## Environments syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2e57cc-3491-4489-af86-6052b1efcc1b",
   "metadata": {},
   "source": [
    "#### Motivation\n",
    "\n",
    "- Bislang haben wir vordefinierte Environmenten wie Frozen Like und Google RecSim verwendet.\n",
    "- Um RL f√ºr unser eigenes Problem zu nutzen, k√∂nnen wir keine dieser Environmenten verwenden.\n",
    "- Wir m√ºssen unsere eigene Environment mit Python definieren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0797f5a9-c856-49f2-9a3c-419d816b6ec1",
   "metadata": {},
   "source": [
    "#### Gefrorener See R√ºckblick\n",
    "\n",
    "- Erinnere dich an die Environment des Gefrorenen Sees aus Modul 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69171cee-dc7b-433d-85c5-d0d40486a43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"FrozenLake-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a41dffc-8451-4d7e-a40a-23fd1315ffba",
   "metadata": {},
   "source": [
    "#### Gefrorener See R√ºckblick\n",
    "\n",
    "- OpenAI Gym ist quelloffen, also k√∂nnten wir uns den [Frozen Lake Quellcode](https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py) ansehen.\n",
    "- Er ist jedoch kompliziert und enth√§lt viel mehr, als wir brauchen.\n",
    "- Lass uns unsere eigene Environment namens Frozen Pond mit den Grundkomponenten von Frozen Lake erstellen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735eb606-b1dd-4dba-be55-d6105cc47536",
   "metadata": {},
   "source": [
    "#### Komponenten eines Env\n",
    "\n",
    "Konzeptionelle Entscheidungen:\n",
    "\n",
    "- Beobachtungsraum\n",
    "- Handlungsraum\n",
    "\n",
    "In Python m√ºssen wir zumindest implementieren:\n",
    "\n",
    "- konstruktor\n",
    "- `R√ºcksetzen()`\n",
    "- `step()`\n",
    "\n",
    "In der Praxis werden wir vielleicht auch andere Methoden brauchen, wie `render()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9cf2ed-dc7e-4bbd-a5f8-9b78b8a38710",
   "metadata": {},
   "source": [
    "#### Konzeptionelle Entscheidungen\n",
    "\n",
    "Da wir in diesem Fall den Gefrorenen See nachahmen, sind der Beobachtungsraum und der Aktionsraum bereits festgelegt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34c0aa1d-8ab0-462a-8bdd-8ba12484a5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space = gym.spaces.Discrete(16)\n",
    "action_space = gym.spaces.Discrete(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef0fa48-25fb-4360-a1d8-54ca675dc786",
   "metadata": {},
   "source": [
    "Im weiteren Verlauf des Kurses werden wir uns mit diesen Entscheidungen n√§her besch√§ftigen!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c88c68-7f74-490e-beda-9e199d0c3822",
   "metadata": {},
   "source": [
    "#### Verschl√ºsseln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db413de7-7df3-4d77-9ae8-5c8e95a39068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "class FrozenPond(gym.Env):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b3bbec-72c8-48dd-bdc4-6f537dacda5c",
   "metadata": {},
   "source": [
    "- Beachte, dass wir mit der Unterklasse `gym.Env` beginnen.\n",
    "- Optional: Du kannst √ºber Objekte, Vererbung und Unterklassen lesen.\n",
    "- Kurz und b√ºndig: Dies ist eine grundlegende `gym.Env` und wir k√∂nnen ihre Eigenschaften √ºberschreiben."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b2467a-4906-473f-8207-cde646def666",
   "metadata": {},
   "source": [
    "#### Konstrukteur\n",
    "\n",
    "- Der Konstruktor wird aufgerufen, wenn wir ein neues `FrozenPond` Objekt erstellen.\n",
    "- Hier definieren wir den Beobachtungsraum und den Aktionsraum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2417aa6-93e3-46c9-984b-f98e24ece49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "class FrozenPond(gym.Env):\n",
    "    def __init__(self, env_config=None):\n",
    "        self.observation_space = gym.spaces.Discrete(16)\n",
    "        self.action_space = gym.spaces.Discrete(4)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359ab164-11d8-4614-a4f1-f875fac08071",
   "metadata": {},
   "source": [
    "- Aus Gr√ºnden der RLlib-Kompatibilit√§t muss der Konstruktor eine \"env_config\" aufnehmen \n",
    "- Wir werden dieses Argument vorerst ignorieren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285e5eca-3aa8-472a-a6bc-037b411f83a5",
   "metadata": {},
   "source": [
    "#### Zur√ºcksetzen\n",
    "\n",
    "- Die n√§chste Methode, die wir brauchen, ist reset.\n",
    "- Der Konstruktor setzt permanente Parameter wie den Beobachtungsraum.\n",
    "- der \"Reset\" legt jede neue Episode fest.\n",
    "- Zwischen den beiden gibt es einige Freiheiten, z. B. die Festlegung des Zielorts.\n",
    "- Wenn sich etwas √§ndern _k√∂nnte_, legen wir es in `reset` fest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce9715c1-8865-4376-97d5-4c0f7b253e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c438cc9b-d6db-4412-8b29-80288a2cf288",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def reset(self):\n",
    "        self.player = (0, 0) # the player starts at the top-left\n",
    "        self.goal = (3, 3)   # goal is at the bottom-right\n",
    "        \n",
    "        self.holes = np.array([\n",
    "            [0,0,0,0], # FFFF \n",
    "            [0,1,0,1], # FHFH\n",
    "            [0,0,0,1], # FFFH\n",
    "            [1,0,0,0]  # HFFF\n",
    "        ])\n",
    "        \n",
    "        return 0 # to be changed to return self.observation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206ed3bf-aa1d-4833-84fa-e9be1f64648b",
   "metadata": {},
   "source": [
    "#### Zur√ºcksetzen\n",
    "\n",
    "Testen wir das mal aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c1c6161-2b90-47bf-b002-d3ff6fc62464",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = FrozenPond()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50989bc9-5d31-480a-88ac-dbf85f2ff390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edbdb5ee-0af9-43e2-8833-c1c945eae535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0],\n",
       "       [0, 1, 0, 1],\n",
       "       [0, 0, 0, 1],\n",
       "       [1, 0, 0, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp.holes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79c6258-6353-42ac-81d6-51423385972a",
   "metadata": {},
   "source": [
    "Sieht gut aus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a138d14-cf05-4d44-b315-4e708692e001",
   "metadata": {},
   "source": [
    "#### Schritt\n",
    "\n",
    "- Die letzte Methode, die wir brauchen, ist `step`.\n",
    "- Dies ist die komplizierteste Methode, die die Kernlogik enth√§lt.\n",
    "- Erinnere dich daran, dass \"step\" 4 Dinge zur√ºckgibt:\n",
    "  1. Beobachtung\n",
    "  2. Belohnung\n",
    "  3. Erledigt-Flagge\n",
    "  4. Extra Info (wird ignoriert)\n",
    "- Der √úbersichtlichkeit halber schreiben wir Hilfsmethoden f√ºr Observation, Reward und Done sowie eine zus√§tzliche Hilfsmethode "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5e1f9a-6f10-4038-9c02-2a6e966ecac5",
   "metadata": {},
   "source": [
    "#### Schritt: Beobachtung\n",
    "\n",
    "Erinnere dich daran, dass die Beobachtung ein Index von 0 bis 15 ist:\n",
    "\n",
    "```\n",
    " 0 1 2 3\n",
    " 4 5 6 7\n",
    " 8 9 10 11\n",
    "12 13 14 15\n",
    "```\n",
    "\n",
    "Wir k√∂nnen dies wie folgt kodieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a66c82f8-21a4-403d-8172-2b878488e155",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def observation(self):\n",
    "        return 4*self.player[0] + self.player[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921f3bc3-6200-4167-839b-a068e88240c1",
   "metadata": {},
   "source": [
    "Wenn der Spieler zum Beispiel auf (2,1) steht, geben wir zur√ºck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b95ad09b-a77d-4039-afa6-0695a8427601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4*2 + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae722e0-9d57-414c-ade2-53b361b8e590",
   "metadata": {},
   "source": [
    "Hinweis: Jetzt, wo `self.observation` implementiert ist, sollten wir `reset` in `return self.observation()` statt in `return 0` √§ndern, um die Codequalit√§t zu verbessern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b84fcf-c40f-4513-8cd0-3bb2b0a8d32a",
   "metadata": {},
   "source": [
    "#### Schritt: Belohnung\n",
    "\n",
    "In Anlehnung an das Beispiel des Gefrorenen Sees ist die Belohnung 1, wenn der Agent das Ziel erreicht, und 0, wenn nicht:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ee1fd77-5923-4db1-abe3-96c47360366a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def reward(self):\n",
    "        return int(self.player == self.goal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d71262-1a9e-4daf-940e-df02c2c2631d",
   "metadata": {},
   "source": [
    "Wir werden diese Belohnungsfunktion sp√§ter im Modul √§ndern!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb7d07c-9153-4933-94f6-c117ca6993ff",
   "metadata": {},
   "source": [
    "#### Schritt: erledigt\n",
    "\n",
    "- Wir m√ºssen auch wissen, wann eine Episode abgeschlossen ist \n",
    "- Nach Frozen Lake ist die Episode beendet, wenn der Agent das Ziel erreicht oder in den Teich f√§llt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab0ed064-4f50-425e-8f0f-302a44c2d259",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def done(self):\n",
    "        return self.player == self.goal or self.holes[self.player] == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8829aa45-d26c-4707-ab51-38c38353ce00",
   "metadata": {},
   "source": [
    "#### Schritt: g√ºltige Standorte\n",
    "\n",
    "Um die Methode `step` einfacher zu machen, schreiben wir eine Hilfsmethode namens `is_valid_loc`, die √ºberpr√ºft, ob ein bestimmter Ort innerhalb der Grenzen liegt (von 0 bis 3 in jeder Dimension)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1872595-ab21-4b4e-a431-c15fc1ea7595",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def is_valid_loc(self, location):\n",
    "        if 0 <= location[0] <= 3 and 0 <= location[1] <= 3:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce94f18-ffb6-4f93-ba45-f7084c3805c5",
   "metadata": {},
   "source": [
    "#### Schritt: Zusammenbau\n",
    "\n",
    "- Mit den obigen Teilen k√∂nnen wir nun die Methode \"step\" schreiben.\n",
    "- schritt\" nimmt eine _Aktion_ auf, aktualisiert den _Zustand_ und gibt die Beobachtung, die Belohnung, das Erledigt-Flag und zus√§tzliche Informationen (ignoriert) zur√ºck.\n",
    "- Erinnere dich daran, wie Aktionen kodiert werden: 0 f√ºr links, 1 f√ºr unten, 2 f√ºr rechts, 3 f√ºr oben.\n",
    "- Wir werden einen **nicht rutschigen** gefrorenen Teich implementieren, d.h. deterministisch und nicht stochastisch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c083279a-b950-45e6-9cfc-265885c80c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def step(self, action):\n",
    "        # Compute the new player location\n",
    "        if action == 0:   # left\n",
    "            new_loc = (self.player[0], self.player[1]-1)\n",
    "        elif action == 1: # down\n",
    "            new_loc = (self.player[0]+1, self.player[1])\n",
    "        elif action == 2: # right\n",
    "            new_loc = (self.player[0], self.player[1]+1)\n",
    "        elif action == 3: # up\n",
    "            new_loc = (self.player[0]-1, self.player[1])\n",
    "        else:\n",
    "            raise ValueError(\"Action must be in {0,1,2,3}\")\n",
    "        \n",
    "        # Update the player location only if you stayed in bounds\n",
    "        # (if you try to move out of bounds, the action does nothing)\n",
    "        if self.is_valid_loc(new_loc):\n",
    "            self.player = new_loc\n",
    "        \n",
    "        # Return observation/reward/done\n",
    "        return self.observation(), self.reward(), self.done(), {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a4a4ec-cfe6-4d3e-80a1-a606afc87c0e",
   "metadata": {},
   "source": [
    "#### Erfolg!\n",
    "\n",
    "- Das war's! Wir haben die notwendigen Teile in Frozen Pond implementiert \n",
    "  - konstruktor\n",
    "  - `R√ºcksetzen`\n",
    "  - schritt\n",
    "- Au√üerdem f√ºgen wir eine optionale Funktion `render` hinzu, damit wir den Zustand zeichnen k√∂nnen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dace259a-1e8f-4d77-af39-3f1cb6cdc2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def render(self):\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                if (i,j) == self.goal:\n",
    "                    print(\"‚õ≥Ô∏è\", end=\"\")\n",
    "                elif (i,j) == self.player:\n",
    "                    print(\"üßë\", end=\"\")\n",
    "                elif self.holes[i,j]:\n",
    "                    print(\"üï≥\", end=\"\")\n",
    "                else:\n",
    "                    print(\"üßä\", end=\"\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925146e2-188f-4b78-acb5-df30214dad54",
   "metadata": {},
   "source": [
    "- Zum Spa√ü werden wir Emojis in unserem Kundenrendering verwenden.\n",
    "- Spieler ist üßë, Tor ist ‚õ≥Ô∏è, gefrorenes See-Segment ist üßä, Loch ist üï≥."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d03db1-e53b-435b-a0fb-aa67bdd99161",
   "metadata": {},
   "source": [
    "#### Testen unserer Implementierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a079671-3fc3-46eb-aaa3-e0629201de06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "from envs_03 import FrozenPond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c03550a-fde5-4885-883a-030a2530488f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßëüßäüßäüßä\n",
      "üßäüï≥üßäüï≥\n",
      "üßäüßäüßäüï≥\n",
      "üï≥üßäüßä‚õ≥Ô∏è\n"
     ]
    }
   ],
   "source": [
    "env = FrozenPond()\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88bcc59-f6c1-4722-a255-ef30a49c2616",
   "metadata": {},
   "source": [
    "#### Testen unserer Implementierung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2301a03f-f40e-492d-ae42-0f99b07ee09f",
   "metadata": {},
   "source": [
    "Lass uns die Methode \"step\" testen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1cc81874-c11f-430f-904b-1d108c170589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0, False, {'player': (0, 1), 'goal': (3, 3)})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(2) # 0=left / 1=down / 2=right / 3=up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89d4775d-fe17-4628-a97f-33ab2799573d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßäüßëüßäüßä\n",
      "üßäüï≥üßäüï≥\n",
      "üßäüßäüßäüï≥\n",
      "üï≥üßäüßä‚õ≥Ô∏è\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b46b9c2-6ebe-4d37-b809-c38bd711126c",
   "metadata": {},
   "source": [
    "Sieht gut aus!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be4064b-3f24-48b1-aa6e-184f655067d0",
   "metadata": {},
   "source": [
    "#### Testen unserer Implementierung\n",
    "\n",
    "Lass uns die beiden Environmenten direkt miteinander vergleichen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3aff2e55-17f6-48bc-8fb8-eaade93881a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter | gym obs / our obs | gym reward / our reward | gym done / our done\n",
      " 0   |       0 /  0      |          0 / 0        |      False / False\n",
      " 1   |       1 /  1      |          0 / 0        |      False / False\n",
      " 2   |       2 /  2      |          0 / 0        |      False / False\n",
      " 3   |       6 /  6      |          0 / 0        |      False / False\n",
      " 4   |      10 / 10      |          0 / 0        |      False / False\n",
      " 5   |      14 / 14      |          0 / 0        |      False / False\n",
      " 6   |      14 / 14      |          0 / 0        |      False / False\n",
      " 7   |      15 / 15      |          1 / 1        |       True /  True\n"
     ]
    }
   ],
   "source": [
    "lake = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
    "pond = FrozenPond()\n",
    "\n",
    "lake.reset()\n",
    "pond.reset()\n",
    "\n",
    "print(\"Iter | gym obs / our obs | gym reward / our reward | gym done / our done\")\n",
    "for i, a in enumerate([0, 2, 2, 1, 1, 1, 1, 2]):\n",
    "    lake_obs, lake_rew, lake_done, _ = lake.step(a)\n",
    "    pond_obs, pond_rew, pond_done, _ = pond.step(a)\n",
    "    print(\"%2d   |      %2d / %2d      |          %d / %d        |      %5s / %5s\" % \\\n",
    "          (i, lake_obs, pond_obs, lake_rew, pond_rew, lake_done, pond_done))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87185e66-37b4-4337-9046-17a17cc6efb2",
   "metadata": {},
   "source": [
    "Sie sehen gleich aus!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5deb1e8-3de2-42dc-8363-aee8acf534e3",
   "metadata": {},
   "source": [
    "#### Testen unserer Implementierung\n",
    "\n",
    "- RLlib kommt auch mit einem env checker\n",
    "- Dieser kann uns nicht sagen, ob unsere Environment mit der von Frozen Lake identisch ist\n",
    "- Aber er f√ºhrt mehrere n√ºtzliche Pr√ºfungen durch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57e3cbe2-8354-4533-872d-41bd06ce2948",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.utils.pre_checks.env import check_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f4ebd66-5e52-4724-a9c3-df6efdb66eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-27 16:08:55,501\tWARNING env.py:142 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n"
     ]
    }
   ],
   "source": [
    "check_env(pond)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f674689-9df5-4da7-9379-478c2b802571",
   "metadata": {},
   "source": [
    "- Alle Pr√ºfungen wurden bestanden, bis auf diese Warnung √ºber die maximale Episodenl√§nge.\n",
    "- Wir k√∂nnen/sollten diese festlegen, damit die Episoden nicht beliebig lang werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29218be1-9c76-4f04-bffd-e7288098bb74",
   "metadata": {},
   "source": [
    "#### Maximale Schritte pro Episode\n",
    "\n",
    "- Um eine maximale Anzahl von Schritten pro Episode festzulegen, k√∂nnen wir einen `gym` _wrapper_ verwenden.\n",
    "- Wrapper sind bequeme Wege, um Environmenten zu ver√§ndern, einschlie√ülich Beobachtungen, Aktionen und Belohnungen.\n",
    "- Hier verwenden wir den Wrapper `TimeLimit`, um ein Schrittlimit festzulegen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8edeeaf-5dd3-4eee-9ed0-747880709912",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import TimeLimit\n",
    "\n",
    "pond_5 = TimeLimit(pond, max_episode_steps=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37ac862-4df2-4f76-bbe0-b718e575f0e3",
   "metadata": {},
   "source": [
    "Wir k√∂nnen √ºberpr√ºfen, dass es nach 5 Schritten erledigt ist, auch wenn das Ziel nicht erreicht wird:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be71dbd1-7668-4499-85d2-68a5b70fa1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, False, {'player': (0, 0), 'goal': (3, 3)})\n",
      "(0, 0, False, {'player': (0, 0), 'goal': (3, 3)})\n",
      "(0, 0, False, {'player': (0, 0), 'goal': (3, 3)})\n",
      "(0, 0, False, {'player': (0, 0), 'goal': (3, 3)})\n",
      "(0, 0, True, {'player': (0, 0), 'goal': (3, 3), 'TimeLimit.truncated': True})\n"
     ]
    }
   ],
   "source": [
    "pond_5.reset()\n",
    "for i in range(5):\n",
    "    print(pond_5.step(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f54f62a-2022-4bdf-926e-b855dc374ec6",
   "metadata": {},
   "source": [
    "#### Maximale Schritte pro Episode\n",
    "\n",
    "Eine vern√ºnftigere Schrittgrenze w√§re 50 statt 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff4847c8-ff8e-4e15-96b3-b0eb38781266",
   "metadata": {},
   "outputs": [],
   "source": [
    "pond_50 = TimeLimit(pond, max_episode_steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9eadc6-381d-47ac-82ec-f418b6830e6f",
   "metadata": {},
   "source": [
    "- Zu deiner Information: Es ist auch m√∂glich, diese Grenze in RLlib zu setzen, nur f√ºr Trainingszwecke.\n",
    "- Dies geschieht mit dem Parameter \"horizon\" in der Trainer-Konfiguration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aed5b6c-a486-4f1c-96e0-e027710461f6",
   "metadata": {},
   "source": [
    "#### Lass uns das Gelernte anwenden!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456c51dc-3d2c-4e06-be76-62c279e976b0",
   "metadata": {},
   "source": [
    "## Gefrorener Teich Belohnungen\n",
    "<!-- multiple choice -->\n",
    "\n",
    "Im Gefrorenen See (und Teich) ist die Belohnung 1, wenn der Agent das Ziel erreicht, und sonst 0. Der Agent muss lernen, die L√∂cher zu vermeiden, aber es gibt eigentlich keine negative Belohnung, wenn er in ein Loch f√§llt - es ist die gleiche Null-Belohnung, wie wenn er in ein sicheres St√ºck gefrorenen See l√§uft! Warum funktioniert dieses System trotzdem, obwohl die Belohnung f√ºr das Laufen in ein Loch oder in trockenes Land dieselbe ist?\n",
    "\n",
    "- [ ] Sobald der Agent in ein Loch f√§llt, steckt er fest. Er kann zwar weitere Aktionen durchf√ºhren, aber die bringen nichts mehr. Deshalb lernt der Agent, L√∂cher zu vermeiden.\n",
    "- [Eine Belohnung von 0 ist die niedrigste m√∂gliche Belohnung; wenn der Agent also eine Belohnung von 0 erh√§lt, wenn er in ein Loch f√§llt, wei√ü er sofort, dass es schlecht ist, in ein Loch zu fallen.\n",
    "- [Die Strafe f√ºr das Hineinfallen in ein Loch ist indirekt, da die Episode mit einer Belohnung von Null endet und damit die potenzielle Belohnung von 1 f√ºr das erfolgreiche Erreichen des Ziels verwirkt wird. Der Agent lernt, dass er, wenn er in ein Loch f√§llt, _zuk√ºnftige_ Belohnungen einb√º√üt.\n",
    "- [RL-Agenten bevorzugen l√§ngere Episoden. Wenn der Agent in das Loch f√§llt, endet die Episode sofort, was der Agent zu vermeiden lernt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d53229-6adf-4b71-b8dd-40157b9a703d",
   "metadata": {},
   "source": [
    "## Teich vs. Labyrinth\n",
    "<!-- coding exercise -->\n",
    "\n",
    "Nehmen wir an, wir wollen unsere Teichumgebung in ein _Labyrinth_ umwandeln. In diesem Fall haben wir W√§nde anstelle von L√∂chern. Der einzige Unterschied zwischen dem Teich und dem Labyrinth ist das Verhalten von L√∂chern und W√§nden. Wenn du im gefrorenen Teich in ein Loch trittst, endet die Episode. Im Labyrinth bewirkt das Betreten einer Wand nichts (d. h. die Aktion √§ndert den Standort des Agenten nicht, genau wie der Versuch, √ºber den Rand der Karte zu gehen). Um unseren Gefrorenen See in ein Labyrinth zu verwandeln, m√ºssen wir zwei Methoden √§ndern: `done` und `is_valid_loc`.\n",
    "\n",
    "Unten findest du die Methoden `done` und `step`, die wir in den Folien oben gesehen haben. √Ñndere sie so ab, dass wir jetzt ein Labyrinth mit dem oben beschriebenen Verhalten haben: Wenn du gegen eine Wand l√§ufst, passiert nichts.\n",
    "\n",
    "Beachte, dass die Klasse `Maze` alle anderen Methoden von `FrozenPond` erbt, also kannst du sie ausprobieren!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "238c70d1-4cd2-4cb0-ba58-3599ffa0879e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 0, True, {'player': (1, 1), 'goal': (3, 3)})\n",
      "(5, 0, True, {'player': (1, 1), 'goal': (3, 3)})\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE\n",
    "from envs_03 import FrozenPond\n",
    "\n",
    "\n",
    "class Maze(FrozenPond):\n",
    "    def done(self):\n",
    "        return self.player == self.goal or self.holes[self.player] == 1\n",
    "    def is_valid_loc(self, location):\n",
    "        if 0 <= location[0] <= 3 and 0 <= location[1] <= 3:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "pond = FrozenPond()\n",
    "pond.reset()\n",
    "pond.step(1)\n",
    "print(pond.step(2))\n",
    "\n",
    "maze = Maze()\n",
    "maze.reset()\n",
    "maze.step(1)\n",
    "print(maze.step(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a483f55-b956-44c1-bf8e-d534e6ad3ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 0, True, {'player': (1, 1), 'goal': (3, 3)})\n",
      "(4, 0, False, {'player': (1, 0), 'goal': (3, 3)})\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "from envs_03 import FrozenPond\n",
    "\n",
    "\n",
    "class Maze(FrozenPond):   \n",
    "    def done(self):\n",
    "        return self.player == self.goal\n",
    "    def is_valid_loc(self, location):\n",
    "        if 0 <= location[0] <= 3 and 0 <= location[1] <= 3 and not self.holes[location]:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "pond = FrozenPond()\n",
    "pond.reset()\n",
    "pond.step(1)\n",
    "print(pond.step(2))\n",
    "\n",
    "maze = Maze()\n",
    "maze.reset()\n",
    "maze.step(1)\n",
    "print(maze.step(2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ray200]",
   "language": "python",
   "name": "conda-env-ray200-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
