{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5aa21ad-6bb9-4990-ab4d-fb9435b0bc00",
   "metadata": {},
   "source": [
    "## Hors ligne RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "204bbdb7-e15f-4e5c-be77-b58b39dd3dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f337b2ca-8719-4938-8983-3a02bc76626f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import ray\n",
    "import logging\n",
    "ray.init(log_to_driver=False, ignore_reinit_error=True, logging_level=logging.ERROR); # logging.FATAL\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fd0b8c-0fe5-4295-b752-061d9e199421",
   "metadata": {},
   "source": [
    "#### Est-ce que c'est r√©aliste ?\n",
    "\n",
    "- Jusqu'√† pr√©sent, nous avons construit une simulation du comportement de l'utilisateur\n",
    "- Dans certaines applications, nous pourrions √™tre en mesure de construire des simulations pr√©cises :\n",
    "  - simulations de physique (par exemple, les robots)\n",
    "  - jeux\n",
    "  - simulations √©conomiques/financi√®res ?\n",
    "- Cependant, pour le comportement de l'utilisateur, c'est difficile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb08a59-42f4-4fce-b709-1907ea5ef3c9",
   "metadata": {},
   "source": [
    "#### Est-ce que c'est r√©aliste ? \n",
    "\n",
    "- Le mieux serait de d√©ployer RL en direct, mais ce n'est pas pratique\n",
    "- Une autre possibilit√© : apprendre √† partir des donn√©es des utilisateurs ?\n",
    "- Nous pouvons le faire avec **l'apprentissage par renforcement en ligne**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba4b7ef-32c4-4caa-bb22-eace97236370",
   "metadata": {},
   "source": [
    "#### RL hors ligne\n",
    "\n",
    "- Qu'est-ce que le RL hors ligne ?\n",
    "- Rappelle-toi notre boucle RL :\n",
    "\n",
    "[](img/RL-loop-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d89bcc9-17c8-4dbc-afb0-9f9e4810dad0",
   "metadata": {},
   "source": [
    "#### RL hors ligne\n",
    "\n",
    "- Dans la RL hors ligne, nous n'avons pas d'environnement avec lequel interagir dans une boucle de r√©troaction :\n",
    "\n",
    "[](img/offline-RL-loop.png)\n",
    "\n",
    "Ces donn√©es historiques ont √©t√© g√©n√©r√©es par une/des autre(s) politique(s) inconnue(s).\n",
    "\n",
    "Notes :\n",
    "\n",
    "Peut √™tre g√©n√©r√© par des utilisateurs r√©els, ou par une source diff√©rente (al√©atoire, ou agent RL !)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d6cd2e-9bff-4b43-a49f-38e3211c4a09",
   "metadata": {},
   "source": [
    "#### Le d√©fi de la RL hors ligne\n",
    "\n",
    "- On ne peut pas r√©pondre aux questions \"et si\"\n",
    "- Nous ne pouvons voir que les r√©sultats des actions tent√©es dans l'ensemble de donn√©es\n",
    "\n",
    "Remarques :\n",
    "\n",
    "Peut-√™tre que cela nous fait appr√©cier √† quel point il est pr√©cieux/important d'avoir r√©ellement un env √† disposition, ce que nous avons eu pendant tout le reste du cours. Cela nous permet d'essayer n'importe quoi sans aucun co√ªt, sauf le co√ªt de calcul (en supposant qu'il s'agit d'un simulateur et non d'un environnement du monde r√©el) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f9ff23b-8073-44b0-bb32-320ab7fa0a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # HIDDEN\n",
    "\n",
    "# # generate the offline dataset\n",
    "# env_config = {\n",
    "#     \"num_candidates\" : 2,\n",
    "#     \"alpha\"          : 0.5,\n",
    "#     \"seed\"           : 42\n",
    "# }\n",
    "\n",
    "# from ray.rllib.algorithms.ppo import PPO, PPOConfig\n",
    "\n",
    "# ppo_config = (\n",
    "#     PPOConfig()\\\n",
    "#     .framework(\"torch\")\\\n",
    "#     # need to set num_rollout_workers=1 for now per https://github.com/ray-project/ray/issues/25696\n",
    "#     .rollouts(create_env_on_local_worker=True, num_rollout_workers=1)\\\n",
    "#     .debugging(seed=0, log_level=\"ERROR\")\\\n",
    "#     .training(model={\"fcnet_hiddens\" : [64, 64]}, lr=0.001)\\\n",
    "#     # .environment(env_config=env_config)\\\n",
    "#     .offline_data(output=\"data/recommender2\")\n",
    "# )\n",
    "\n",
    "# from envs_04 import BasicRecommenderWithHistory\n",
    "\n",
    "# ppo_history = ppo_config.build(env=\"CartPole-v1\")\n",
    "\n",
    "# rewards_history = []\n",
    "# for i in range(25):\n",
    "#     result = ppo_history.train()\n",
    "#     rewards_history.append(result[\"episode_reward_mean\"])\n",
    "    \n",
    "# ppo_history.evaluate(duration_fn=1000)[\"evaluation\"][\"episode_reward_mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23be8a47-e338-408d-abac-63bbefe41dac",
   "metadata": {},
   "source": [
    "#### Jeu de donn√©es Recommender\n",
    "\n",
    "- Explorons un ensemble de donn√©es hors ligne dont nous pouvons tirer des enseignements.\n",
    "- Nous aurons besoin d'un peu de code pour lire tous les objets JSON du fichier :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8610f1a2-b704-4b22-aeba-3eb3826693e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json_dataset_file = \"data/recommender_offline.json\"\n",
    "\n",
    "rollouts = []\n",
    "with open(json_dataset_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        rollouts.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9540859-74d2-4c4f-a06c-1cd6fd848994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rollouts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6f10f9-acb9-4aff-9694-622faf8792e1",
   "metadata": {},
   "source": [
    "Nous avons 50 \"d√©ploiements\" de donn√©es.\n",
    "\n",
    "Notes :\n",
    "\n",
    "Le fichier est dans le format √† partir duquel RLlib apprend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433484e4-8250-473f-9579-f712e9fa9e17",
   "metadata": {},
   "source": [
    "#### Jeu de donn√©es Recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b685e2f5-4381-40ca-bff8-cc1398d76ddb",
   "metadata": {},
   "source": [
    "Chaque rollout est un dict contenant des informations sur le pas de temps :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c1e82cd-f6f4-4076-aa36-2a16dc761447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.rllib.utils.compression import unpack, pack\n",
    "\n",
    "obs = unpack(rollouts[0][\"obs\"])\n",
    "obs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ea21ad-940d-4aa0-8a63-43c527d233bd",
   "metadata": {},
   "source": [
    "- Nous avons 200 √©tapes temporelles de donn√©es dans chaque d√©ploiement\n",
    "- Examinons d'abord les observations\n",
    "\n",
    "Notes :\n",
    "\n",
    "Ce nombre 200 est d√©fini par le param√®tre de configuration de l'algorithme \"rollout_fragment_length\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fce3e9-db24-4050-bcd4-fc08f760bf5d",
   "metadata": {},
   "source": [
    "#### Jeu de donn√©es Recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a579d766-069c-446c-aa3d-7798335ece38",
   "metadata": {},
   "source": [
    "Voici les 3 premi√®res observations :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7946e442-7f6a-4653-8e84-75bc37fe952a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6545137 , 0.29728338],\n",
       "       [0.5238871 , 0.5144319 ],\n",
       "       [0.6741674 , 0.10163702]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93775945-d5ff-4f9f-96c1-3a04e4ac2a85",
   "metadata": {},
   "source": [
    "\n",
    "Nous pouvons voir que `num_candidats` a √©t√© fix√© √† 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a1b0f3-6394-4afe-b444-37914d84ac89",
   "metadata": {},
   "source": [
    "#### Jeu de donn√©es Recommender\n",
    "\n",
    "Nous pouvons √©galement examiner les 3 premi√®res actions, les r√©compenses et les dons :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ace3caf4-ef70-4393-9846-e68648b41bfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rollouts[0][\"actions\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29ff0ca7-6fe4-4786-a19f-c9bee9bf0b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6545137166976929, 0.3524414300918579, 0.05838315561413765]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rollouts[0][\"rewards\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37cafb64-4837-40c3-aa09-7b9e9871906d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, False, False]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rollouts[0][\"dones\"][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff27284-e91c-4ef1-9b01-7e950eda2cd4",
   "metadata": {},
   "source": [
    "Notes :\n",
    "\n",
    "Donc, l'agent a d'abord vu l'observation [0.65, 0.297] de la diapositive pr√©c√©dente, puis il a fait l'action 0, a obtenu une r√©compense de 0.65, et l'√©pisode n'a pas √©t√© fait.\n",
    "\n",
    "Il y a plus d'informations stock√©es dans l'ensemble de donn√©es que ce qui pr√©c√®de, mais ce sont les points cl√©s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a206ba60-13ae-47b3-a406-eb06e701af5a",
   "metadata": {},
   "source": [
    "#### Apprentissage supervis√©\n",
    "\n",
    "- Attends, c'est un ensemble de donn√©es... ne pouvons-nous pas simplement faire de l'apprentissage supervis√© pour faire correspondre les √©tats aux actions ? ü§î\n",
    "- Oui, nous le pouvons, et cela viserait √† r√©cup√©rer la politique qui a g√©n√©r√© l'ensemble de donn√©es (_apprentissage par imitation_).\n",
    "- Mais il est en fait possible de faire _mieux_... et c'est notre objectif avec le RL hors ligne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59b604b7-484a-4ad8-976d-6f49c941f3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# maybe going overboard here, but for a synthetic example we could actually show it does better than the policy that generated the data,\n",
    "# since we generated the data ourselves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e35709e-45fd-4d70-a111-172580296fea",
   "metadata": {},
   "source": [
    "#### Formation RL hors ligne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8049ef58-4f5e-4116-abd7-4aad06f65ee0",
   "metadata": {},
   "source": [
    "- Beaucoup d'informations sur le RL hors ligne avec RLlib [ici] (https://docs.ray.io/en/latest/rllib/rllib-offline.html).\n",
    "- Tout d'abord, nous avons besoin d'un algorithme.\n",
    "- Pour le RL hors ligne, nous ne pouvons pas utiliser `PPO`.\n",
    "- Nous utiliserons l'algorithme `MARWIL` qui est inclus dans RLlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34d0e449-de74-444c-894e-4d58c6e33c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.marwil import MARWIL, MARWILConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0385a4b2-594b-46e0-bd2f-2854a6935111",
   "metadata": {},
   "source": [
    "#### Formation RL hors ligne\n",
    "\n",
    "Ensuite, nous cr√©ons la config, en commen√ßant par `MARWILConfig` au lieu de `PPOConfig`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56e22e98-6ca9-4c34-850e-488c4bb38db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the same as before\n",
    "offline_config = ( \n",
    "    MARWILConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(create_env_on_local_worker=True)\\\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\\\n",
    "    .training(model={\"fcnet_hiddens\" : [64, 64]})\\\n",
    ")\n",
    "\n",
    "# This is new for offline RL\n",
    "num_candidates = 2\n",
    "offline_config = offline_config.environment(\n",
    "    observation_space = gym.spaces.Box(low=0, high=1, shape=(num_candidates,)), \n",
    "    action_space = gym.spaces.Discrete(num_candidates),\n",
    ").offline_data(\n",
    "    input_ = [json_dataset_file],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108dd79c-dddb-4153-a89d-258ee623da8a",
   "metadata": {},
   "source": [
    "Notes :\n",
    "\n",
    "- Les √©l√©ments de configuration du haut devraient te sembler familiers. Sur la deuxi√®me moiti√©, les choses sont un peu diff√©rentes :\n",
    "  - Nous devons lui donner le chemin d'acc√®s au fichier de l'ensemble de donn√©es\n",
    "  - Comme il n'y a pas d'env, nous devons sp√©cifier manuellement les espaces d'observation et d'action\n",
    "- Nous n'avons pas de config d'environnement car il n'y a pas d'environnement !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c811603-4264-479d-bd0a-b5ec9ff38de2",
   "metadata": {},
   "source": [
    "#### Formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7280c730-eeee-49a2-a84b-7c42388df308",
   "metadata": {},
   "outputs": [],
   "source": [
    "marwil = offline_config.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8172e9d-bc26-4e00-b431-c9438863028d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    marwil.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d7651b-e0cb-4e37-bec3-976ac37af9cb",
   "metadata": {},
   "source": [
    "#### √âvaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fe0e89-2c77-4a99-98e7-307e7502693f",
   "metadata": {},
   "source": [
    "_Comment √©valuer sans simulateur ?\n",
    "\n",
    "- C'est ce qu'on appelle l'estimation hors politique.\n",
    "- C'est une technique plus sophistiqu√©e qui sort du cadre de ce cours.\n",
    "- Voir la documentation de RLlib [ici] (https://docs.ray.io/en/latest/rllib/rllib-offline.html).\n",
    "- Ce que nous allons faire, c'est √©valuer avec notre simulateur, puisque nous en avons un.\n",
    "\n",
    "Notes :\n",
    "\n",
    "Dans un sc√©nario r√©aliste de RL hors ligne, tu n'as pas de simulateur √† ta disposition. Tu dois alors utiliser des techniques d'√©valuation plus complexes appel√©es estimation hors ligne. Tu auras des ensembles de donn√©es d'entra√Ænement/de test comme dans l'apprentissage supervis√© \n",
    "\n",
    "Pour nos besoins, puisque nous avons un simulateur, nous l'utiliserons pour l'√©valuation. RLlib a en fait une option pour permettre cela, car c'est utile pour le d√©bogage et autres. Nous verrons cela sur la prochaine diapositive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202d38ea-9ad6-4d17-9137-720251d1f6a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### √âvaluation avec notre simulateur\n",
    "\n",
    "Nous pouvons √©valuer l'algorithme √† l'aide de notre simulateur env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5da21ac0-fcba-4d96-8b23-721f98dc1638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "from envs_04 import BasicRecommender\n",
    "\n",
    "env_config = {\n",
    "    \"num_candidates\" : 2,\n",
    "    \"alpha\"          : 0.5,\n",
    "    \"seed\"           : 42\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b582c0ac-88f4-40e0-8078-31ee041bfe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BasicRecommender(env_config)\n",
    "\n",
    "def get_episode_reward(env, algo):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = algo.compute_single_action(obs)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a8671a-26c8-45f5-a468-944dfa4c01b6",
   "metadata": {},
   "source": [
    "- Ci-dessus : configure une fonction qui ex√©cute un √©pisode avec le simulateur.\n",
    "- Ci-dessous : ex√©cute cette fonction pour 100 √©pisodes afin d'obtenir la moyenne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "632fbd40-c2bb-44fe-af47-39b9507530dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.537665635536726"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([get_episode_reward(env, marwil) for i in range(100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8818fa-2e46-4d04-85c7-d55797fc169c",
   "metadata": {},
   "source": [
    "Cela semble faire √† peu pr√®s la m√™me chose que le hasard √† nouveau (25,5)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e507580-77c8-47c1-a9a5-8259ee4bdcd9",
   "metadata": {},
   "source": [
    "#### RLlib pour l'√©valuation sur simulateur\n",
    "\n",
    "RLlib propose √©galement cette fonctionnalit√© :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7468c451-e68b-446b-a0f2-5dc86bb1af11",
   "metadata": {},
   "outputs": [],
   "source": [
    "offline_config = offline_config.evaluation(\n",
    "    off_policy_estimation_methods={},\n",
    "    evaluation_config={\n",
    "        \"input\": \"sampler\",\n",
    "        \"env\": BasicRecommender,\n",
    "        \"env_config\" : env_config\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0db4ec-8023-4ada-83ac-e4f0f42acb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "#marwil.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dfe966-9734-42c7-bf5f-bf8655944f36",
   "metadata": {},
   "source": [
    "Si nous avions mis en place la configuration de cette fa√ßon et form√© :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5bccaf63-54f6-41da-a034-3da05a1ae5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "marwil = offline_config.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "56152208-587f-4306-8e53-9c0eb9f99f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    marwil.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d980136-8c96-42e9-90fb-9d4a6a564922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.3457144503546"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marwil.evaluate()[\"evaluation\"][\"episode_reward_mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d21639-5531-46b6-8cce-68b3cfb013f0",
   "metadata": {},
   "source": [
    "Ici, nous voyons √† nouveau un r√©sultat similaire."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e979de95-c039-4493-b958-e2e5d85098ec",
   "metadata": {},
   "source": [
    "#### Appliquons ce que nous avons appris !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643f5645-5a4d-4fcd-aa28-32894a99ffc2",
   "metadata": {},
   "source": [
    "## Exemple de RL hors ligne\n",
    "<!-- multiple choice -->\n",
    "\n",
    "- [ ] Apprendre √† une IA √† jouer aux √©checs en la faisant jouer contre d'autres IA √† plusieurs reprises.\n",
    "- [ ] Apprendre √† une IA √† jouer aux √©checs en se basant sur les parties pass√©es de joueurs d'√©checs professionnels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcb97ca-5ccd-4eed-b3ac-b58cee110c78",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Les donn√©es hors ligne t'aident-elles ?\n",
    "<!-- multiple choice -->\n",
    "\n",
    "Imagine que tu as un simulateur qui repr√©sente parfaitement ton environnement ; par exemple, tu entra√Ænes peut-√™tre une IA √† jouer √† un jeu solo comme [Atari Breakout](https://en.wikipedia.org/wiki/Breakout_(video_game)). En plus du simulateur, tu as aussi des donn√©es hors ligne √† ta disposition. Bien que cela ne soit pas abord√© dans les diapositives ci-dessus, il est possible de combiner un simulateur avec des donn√©es hors ligne pour former conjointement un agent √† l'aide de RL (voir [ici](https://docs.ray.io/en/latest/rllib/rllib-offline.html#mixing-simulation-and-offline-data)). Si tu as d√©j√† un simulateur parfait, les donn√©es hors ligne pourraient-elles apporter une utilit√© suppl√©mentaire, si elles sont combin√©es avec le simulateur pendant la formation ?\n",
    "\n",
    "#### Meilleure politique apr√®s l'entra√Ænement\n",
    "\n",
    "Choisis l'affirmation correcte concernant la recherche de la meilleure politique apr√®s une formation illimit√©e. Tu peux supposer que tu as un algorithme RL \"parfait\", ce qui signifie qu'il peut repr√©senter n'importe quelle politique et est capable d'optimiser n'importe quelle fonction.\n",
    "\n",
    "- [Avec un algorithme RL \"parfait\" et un temps de calcul suffisant, il n'y a aucun avantage √† utiliser les donn√©es historiques car le simulateur contient tout ce qu'il y a √† savoir sur l'environnement.\n",
    "- [ ] Avec un algorithme RL \"parfait\" et un temps de calcul illimit√©, les donn√©es historiques peuvent t'aider √† trouver une meilleure politique qu'en utilisant uniquement le simulateur. | Avec un algorithme RL \"parfait\", tu devrais finir par trouver la politique optimale. Cela revient √† avoir des donn√©es infinies, un temps de calcul infini, un mod√®le arbitrairement complexe et une m√©thode d'optimisation parfaite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bf6575-837f-458d-be6f-924ee4659254",
   "metadata": {},
   "source": [
    "#### Vitesse d'entra√Ænement\n",
    "\n",
    "Choisis l'affirmation correcte concernant le fait de trouver la meilleure politique apr√®s un peu d'entra√Ænement. Tu peux supposer que tu as un algorithme RL \"parfait\", c'est-√†-dire qu'il peut repr√©senter n'importe quelle politique et est capable d'optimiser n'importe quelle fonction.\n",
    "\n",
    "- [Avec un algorithme RL \"parfait\" mais seulement un peu de temps de calcul, il n'y a aucun avantage √† utiliser les donn√©es historiques car le simulateur contient tout ce qu'il y a √† savoir sur l'environnement. | Et si les donn√©es historiques √©taient g√©n√©r√©es √† l'aide de la politique *optimale* ?\n",
    "- [Avec un algorithme RL \"parfait\" mais un temps de calcul limit√©, les donn√©es historiques peuvent t'aider √† trouver une meilleure politique qu'en utilisant uniquement le simulateur. | Si les donn√©es historiques ont √©t√© g√©n√©r√©es √† l'aide d'une tr√®s bonne politique, tu pourrais en tirer des le√ßons rapidement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab3ebb6-c70d-44d8-93e1-ae4d0f7b54a7",
   "metadata": {},
   "source": [
    "## Politique de donn√©es historiques\n",
    "<!-- multiple choice -->\n",
    "\n",
    "La RL hors ligne s'appuie sur des donn√©es g√©n√©r√©es par une certaine politique qui interagit avec l'environnement. Lequel des √©l√©ments suivants est **NON** une propri√©t√© souhaitable de cet ensemble de donn√©es / politique historique ?\n",
    " \n",
    "- [x] L'environnement et la politique historique sont tous deux d√©terministes. | Dans ce cas, nous n'explorerions qu'une seule trajectoire √† travers l'environnement \n",
    "- [ ] L'ensemble de donn√©es contient un grand nombre d'√©pisodes.\n",
    "- [ ] La politique historique explore une vari√©t√© d'√©tats dans l'environnement.\n",
    "- [ ] La politique historique obtient une r√©compense √©lev√©e dans certains √©pisodes.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06aebc9-1031-4635-bb8a-43417dffcb2c",
   "metadata": {},
   "source": [
    "## RL hors ligne pour Cartpole\n",
    "<!-- coding exercise -->\n",
    "\n",
    "Dans cet exercice, nous allons nous attaquer au c√©l√®bre [probl√®me de r√©f√©rence Cartpole] (https://www.gymlibrary.ml/environments/classic_control/cart_pole/) qui est fourni avec la biblioth√®que `gym`. Le but est d'emp√™cher le pendule invers√© de tomber en appliquant une force, √† gauche ou √† droite, √† chaque pas de temps \n",
    "\n",
    "Nous entra√Ænerons l'agent en utilisant des donn√©es hors ligne contenues dans un fichier `cartpolev1_offline.json` qui est lu par le code.\n",
    "\n",
    "Nous _√©valuerons_ l'agent en utilisant le vrai simulateur Cartpole. (Encore une fois, dans la r√©alit√©, si nous utilisions RL hors ligne, nous n'aurions probablement pas acc√®s au vrai simulateur, mais nous l'incluons ici afin de pouvoir faire une √©valuation de terrain de notre agent)\n",
    "\n",
    "Remplis le code manquant. Ensuite, ex√©cute le code et r√©ponds √† la question √† choix multiple ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd176dc-9a29-4813-b10e-224a61c37aaa",
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ray.rllib.algorithms.marwil import MARWIL, MARWILConfig\n",
    "\n",
    "offline_trainer_config = {\n",
    "    # These should look familiar:\n",
    "    \"framework\"             : \"torch\",\n",
    "    \"create_env_on_driver\"  : True,\n",
    "    \"seed\"                  : 0,\n",
    "    \"model\"                 : {\n",
    "        \"fcnet_hiddens\"     : [64, 64]\n",
    "    },\n",
    "    \n",
    "    # These are new for offline RL:\n",
    "    ____: [\"data/cartpolev1_offline.json\"],\n",
    "    \"observation_space\": gym.spaces.Box(low=____, \n",
    "                                        high=np.array([4.8,  np.inf,  0.42,  np.inf])),\n",
    "    \"action_space\": gym.spaces.Discrete(2),\n",
    "    \"input_evaluation\" : [\"simulation\", \"is\", \"wis\"],\n",
    "    \"env\" : \"CartPole-v1\" # for evaluation only\n",
    "}\n",
    "\n",
    "algo = ...\n",
    "\n",
    "# Training (and storing results)\n",
    "results_off = []\n",
    "results_sim = []\n",
    "for i in range(200):\n",
    "    r = algo.____()\n",
    "    results_off.append(r[\"off_policy_estimator\"][\"wis\"]['V_gain_est'])\n",
    "    results_sim.append(r[\"episode_reward_mean\"])\n",
    "\n",
    "plt.plot(results_sim);\n",
    "plt.xlabel('iterations') \n",
    "plt.ylabel('simulator reward') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e100ab56-3d05-494e-a151-2e82701842ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# This is new for offline RL\u001b[39;00m\n\u001b[1;32m     17\u001b[0m offline_config \u001b[38;5;241m=\u001b[39m offline_config\u001b[38;5;241m.\u001b[39menvironment(\n\u001b[1;32m     18\u001b[0m     observation_space \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mspaces\u001b[38;5;241m.\u001b[39mBox(low\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4.8\u001b[39m, \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.42\u001b[39m, \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf]), \n\u001b[1;32m     19\u001b[0m                                         high\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m4.8\u001b[39m,  np\u001b[38;5;241m.\u001b[39minf,  \u001b[38;5;241m0.42\u001b[39m,  np\u001b[38;5;241m.\u001b[39minf])),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     off_policy_estimation_methods\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msimulation\u001b[39m\u001b[38;5;124m'\u001b[39m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msimulation\u001b[39m\u001b[38;5;124m'\u001b[39m}, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwis\u001b[39m\u001b[38;5;124m'\u001b[39m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwis\u001b[39m\u001b[38;5;124m'\u001b[39m}}\n\u001b[1;32m     26\u001b[0m )\n\u001b[0;32m---> 28\u001b[0m algo \u001b[38;5;241m=\u001b[39m \u001b[43moffline_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Training (and storing results)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m results_off \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/rllib/algorithms/algorithm_config.py:307\u001b[0m, in \u001b[0;36mAlgorithmConfig.build\u001b[0;34m(self, env, logger_creator)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logger_creator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger_creator \u001b[38;5;241m=\u001b[39m logger_creator\n\u001b[0;32m--> 307\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malgo_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/rllib/algorithms/algorithm.py:308\u001b[0m, in \u001b[0;36mAlgorithm.__init__\u001b[0;34m(self, config, env, logger_creator, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;66;03m# Initialize common evaluation_metrics to nan, before they become\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;66;03m# available. We want to make sure the metrics are always present\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;66;03m# (although their values may be nan), so that Tune does not complain\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;66;03m# when we use these as stopping criteria.\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_metrics \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode_reward_max\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mnan,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m     }\n\u001b[1;32m    306\u001b[0m }\n\u001b[0;32m--> 308\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m# Check, whether `training_iteration` is still a tune.Trainable property\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# and has not been overridden by the user in the attempt to implement the\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;66;03m# algos logic (this should be done now inside `training_step`).\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/tune/trainable/trainable.py:157\u001b[0m, in \u001b[0;36mTrainable.__init__\u001b[0;34m(self, config, logger_creator, remote_checkpoint_dir, custom_syncer)\u001b[0m\n\u001b[1;32m    155\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_local_ip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_current_ip()\n\u001b[0;32m--> 157\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m setup_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m setup_time \u001b[38;5;241m>\u001b[39m SETUP_TIME_THRESHOLD:\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/rllib/algorithms/crr/crr.py:163\u001b[0m, in \u001b[0;36mCRR.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetup\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: PartialAlgorithmConfigDict):\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_network_update_freq\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_network_update_freq\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    166\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_batch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m    167\u001b[0m         )\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/rllib/algorithms/algorithm.py:572\u001b[0m, in \u001b[0;36mAlgorithm.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(method_type, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    571\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to import from string: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m method_type)\n\u001b[0;32m--> 572\u001b[0m     mod, obj \u001b[38;5;241m=\u001b[39m method_type\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    573\u001b[0m     mod \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(mod)\n\u001b[1;32m    574\u001b[0m     method_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(mod, obj)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ray.rllib.algorithms.marwil import MARWIL, MARWILConfig\n",
    "from ray.rllib.algorithms.crr import CRR, CRRConfig\n",
    "\n",
    "# This is the same as before\n",
    "offline_config = ( \n",
    "    CRRConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(create_env_on_local_worker=True)\\\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\\\n",
    "    .training(model={\"fcnet_hiddens\" : [32, 32]})\\\n",
    ")\n",
    "# This is new for offline RL\n",
    "offline_config = offline_config.environment(\n",
    "    observation_space = gym.spaces.Box(low=np.array([-4.8, -np.inf, -0.42, -np.inf]), \n",
    "                                        high=np.array([4.8,  np.inf,  0.42,  np.inf])),\n",
    "    action_space = gym.spaces.Discrete(2),\n",
    "    env = \"CartPole-v1\" # for evaluation only\n",
    ").offline_data(\n",
    "    input_ = [\"data/cartpolev1_offline.json\"]\n",
    ").evaluation(\n",
    "    off_policy_estimation_methods={'simulation': {'type': 'simulation'}}\n",
    ")\n",
    "\n",
    "algo = offline_config.build()\n",
    "\n",
    "# Training (and storing results)\n",
    "results_off = []\n",
    "results_sim = []\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "    r = algo.train()\n",
    "    results_off.append(r[\"\"][\"wis\"]['v_new_mean'])\n",
    "    results_sim.append(r[\"episode_reward_mean\"])\n",
    "\n",
    "plt.plot(results_sim);\n",
    "plt.xlabel('iterations') \n",
    "plt.ylabel('simulator reward') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bcc68e84-014c-46ba-ba70-37bf63f6234a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# see the tuned examples here: https://docs.ray.io/en/master/rllib/rllib-algorithms.html#offline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad6fe1d-dc59-41ff-99cb-c688dc12fd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# - ALSO: this might be a great example to try supervised learning and show why it doesn't work...???\n",
    "#   - in some ways this is a way better example than frozen lake... because there IS a short-term reward, it's just not what you should look at.\n",
    "#   - with frozen lake there is no short-term reward, so RL seems \"obvious\"\n",
    "# Or, that could go in the offline RL section, since we already have a data file there and could do SL directly on it\n",
    "# Yes, that seems cool.\n",
    "# could use scipy/numpy to just do the normal equations if we want to avoid adding a dependency on sklearn \n",
    "\n",
    "# Supervised learning only gives you imitation learning, which can only get as good as the policy that generated the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766cc60b-fb10-411c-a798-18ae6f447369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34ae237-8b75-48b9-a998-2914625e71e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ray200]",
   "language": "python",
   "name": "conda-env-ray200-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
