{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90c01ab9-1cd0-47d8-a198-e07e36e2c1fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Politiques RL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddd909a-c86f-4e5c-9ba0-8b0ef1152730",
   "metadata": {},
   "source": [
    "#### Qu'est-ce qu'une politique ?\n",
    "\n",
    "Revenons à l'\"API\" de RL :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50604c8f-7c62-495e-b9c6-f3b408a440d9",
   "metadata": {},
   "source": [
    "[](img/RL-API.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ac6598-7965-4496-ac1e-6b71cc7e487b",
   "metadata": {},
   "source": [
    "- La politique est la sortie de RL\n",
    "- Elle fait correspondre les observations aux actions\n",
    "- La politique est comme le cerveau de l'agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a207992-104b-4ecd-b470-341860f09ad6",
   "metadata": {},
   "source": [
    "#### Qu'est-ce qu'une politique ? Détails.\n",
    "\n",
    "- Une politique est une fonction qui met en correspondance des observations et des actions.\n",
    "- Considérons à nouveau l'environnement Frozen Lake :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2a869f8-2fb6-4ef5-bd1c-d8c74493e93c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
    "obs = env.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbfda97-e12c-4284-b0b2-78a8f21c7ede",
   "metadata": {},
   "source": [
    "- Nous avons reçu une observation 0. Que devons-nous faire ensuite ? \n",
    "- La politique nous le dira."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a98f1c-a7b6-40bc-9d18-dce492473ea9",
   "metadata": {},
   "source": [
    "#### Exemple de politique\n",
    "\n",
    "Une politique peut ressembler à ceci\n",
    "\n",
    "| Observation | Action |\n",
    "|------|-------|\n",
    "| 0 | 0 |\n",
    "| 1 | 3 |\n",
    "| 2 | 1 |\n",
    "| 3 | 1 |\n",
    "| ... | ... |\n",
    "| 14 | 2 |\n",
    "| 15 | 2 |\n",
    "\n",
    "- À gauche, nous avons toutes les observations possibles (15 pour Frozen Lake).\n",
    "- À droite, nous avons l'action correspondante que nous ferons _si nous voyons cette observation_.\n",
    "- \"Si je vois 0, je ferai 0 ; si je vois 1, je ferai 3\", etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c510ca5c-a9d5-44e3-9afb-c4a15979f874",
   "metadata": {},
   "source": [
    "#### Objectif de RL\n",
    "\n",
    "**Le but de RL est d'apprendre une bonne politique étant donné un environnement.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dac4894-b44a-42bc-8269-54f8a7d93c8e",
   "metadata": {},
   "source": [
    "#### Politiques non déterministes\n",
    "\n",
    "- Précédemment, nous avons appris à connaître les environnements déterministes et non déterministes \n",
    "- De manière analogue, nous avons des _politiques_ déterministes et non-déterministes.\n",
    "- Avant, nous avons vu une politique déterministe : une observation donnée suscite une action fixe.\n",
    "- Voici un exemple de politique non déterministe :\n",
    "\n",
    "| Observation | P(gauche) | P(bas) | P(droite) | P(haut) | \n",
    "|------------|-------|-----------|---------|-------|\n",
    "| 0 | 0 | 0.9 | 0.01 | 0.04 | 0.05\n",
    "| 1 | 3 | 0.05 | 0.05 | 0.05 | 0.85\n",
    "| ... | ... | ... | ...      | ...      | ...\n",
    "| 15 | 2 | 0.0 | 0.0 | 0.99 | 0.01\n",
    "\n",
    "\"Si je vois 0, je me déplacerai à gauche 99% du temps, vers le bas 1% du temps, à droite 4% du temps et vers le haut 5% du temps\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34003758-53d7-4832-8e82-f805d723db9c",
   "metadata": {},
   "source": [
    "#### Espaces d'action continus\n",
    "\n",
    "Et si notre espace d'action est continu ? Nous pouvons quand même avoir une politique. Par exemple :\n",
    "\n",
    "| Observation | Action |\n",
    "|------|-------|\n",
    "| 0 | 0.42 |\n",
    "| 1 | -3.99 |\n",
    "| ... | ... |\n",
    "| 15 | 2.24 |\n",
    "\n",
    "Une politique non déterministe devrait cependant puiser dans une distribution de probabilité."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8212fc5-3270-49b9-8e82-44e04ebc892c",
   "metadata": {},
   "source": [
    "#### Espaces d'observation continus\n",
    "\n",
    "- Et si notre _espace d'observation_ était continu ? \n",
    "- Eh bien, maintenant nous ne pouvons plus dessiner la politique sous forme de tableau...\n",
    "- Dans ce cas, notre politique est une _fonction_ de la valeur d'observation \n",
    "- Par exemple, \"angle de la pédale d'accélérateur avec le sol (action) = 1,5 x distance de l'obstacle le plus proche (observation)\" \n",
    "- Cet exemple de jouet dit que si l'obstacle le plus proche est loin, tu peux accélérer la voiture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed8dfeec-6879-4d40-9c9e-b28e4bb260b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# too much text around here, need more code and/or images\n",
    "# I think we can remove some of this stuff (continuous, beyond scalars) and add it back it when it's paired with a concrete example\n",
    "# it's not very helpful/interesting as just ideas in isolation..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0043038d-d6f4-4364-b348-e968fdb37d3a",
   "metadata": {},
   "source": [
    "#### Au-delà des scalaires\n",
    "\n",
    "- Jusqu'à présent, nous avons supposé que l'observation est un nombre unique et que l'action est un nombre unique.\n",
    "- Cependant, ces deux éléments peuvent être des types de données plus complexes : images, vecteurs, etc \n",
    "- Les observations réelles d'une voiture autopilotée peuvent être des dizaines de mesures, d'images, etc.\n",
    "- Les actions réelles d'une voiture autonome peuvent consister à définir plusieurs valeurs à chaque pas de temps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dc8172-ae79-405f-801f-a2b750a62e31",
   "metadata": {},
   "source": [
    "#### Penser les politiques comme des fonctions\n",
    "\n",
    "- En général, c'est une façon utile de penser : la politique est une fonction qui fait correspondre les observations aux actions.\n",
    "- Dans **l'apprentissage par renforcement profond**, cette fonction est un réseau neuronal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a75dce-81a4-4ff6-923c-d5e0b736384c",
   "metadata": {},
   "source": [
    "#### Résumé\n",
    "\n",
    "- L'\"agent\" ou le \"joueur\" sont des personnifications de la politique\n",
    "- Il n'y a pas d'\"intelligence\" ou de prise de décision supplémentaire au-delà de la politique\n",
    "- Par conséquent, nous n'avons techniquement pas besoin de la notion d'agent/joueur\n",
    "- La politique est le résultat de RL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7210fa7d-3ebb-4906-83cb-853e8f4d91d3",
   "metadata": {},
   "source": [
    "#### Appliquons ce que nous avons appris !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5780af6f-fa34-4b85-af30-32e20755f1c0",
   "metadata": {},
   "source": [
    "## Politique de Frozen Lake\n",
    "<!-- multiple choice -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a299f97-00fa-4e31-ad66-5e843b63e1d1",
   "metadata": {},
   "source": [
    "Rappelle-toi l'environnement du lac gelé :\n",
    "\n",
    "```\n",
    "P...\n",
    ".O.O\n",
    "...O\n",
    "O..G\n",
    "```\n",
    "\n",
    "avec son espace d'observation représenté comme :\n",
    "\n",
    "```\n",
    " 0 1 2 3\n",
    " 4 5 6 7\n",
    " 8 9 10 11\n",
    "12 13 14 15\n",
    "```\n",
    "\n",
    "et les actions représentées comme\n",
    "\n",
    "| Action | Signification |\n",
    "|------|------|\n",
    "| 0 | gauche |\n",
    "| 1 | bas |\n",
    "| 2 | droite |\n",
    "| 3 | haut |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8eca95c-cc54-42d3-a442-f41fb0c27b5e",
   "metadata": {},
   "source": [
    "#### Question 1\n",
    "\n",
    "La police ci-dessous contient une entrée manquante représentée par un symbole `?`.\n",
    "\n",
    "| Observation | Action |\n",
    "|------|-------|\n",
    "| 0 | 0 |\n",
    "| 1 | 2 |\n",
    "| ... | ... |\n",
    "| 13 | ? |\n",
    "| 14 | 2 |\n",
    "| 15 | 0 |\n",
    "\n",
    "Choisis le meilleur choix pour remplir l'entrée `?`.\n",
    "\n",
    "- [ ] 0 | Essaie encore !\n",
    "- [ ] 1 | Essaie encore !\n",
    "- [ ] 2 | Oui ! En te déplaçant vers la droite, tu te diriges vers l'objectif.\n",
    "- [ ] 3 | Essaie encore !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c41f87-143b-409f-a16c-b8b1ac4b0cc7",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Question 2\n",
    "\n",
    "_Dans la version glissante du lac gelé, l'agent a une probabilité de 1/3 de se déplacer dans la direction voulue et 1/3 dans chacune des deux directions perpendiculaires._\n",
    "\n",
    "L'affirmation ci-dessus concerne-t-elle l'environnement ou l'agent ?\n",
    "\n",
    "- [ ] Environnement | Tu as compris ! \n",
    "- [ ] Politique | Rappelle-toi que la politique décrit comment l'agent réagit aux observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefe9bb5-2162-44df-9559-e98566491dff",
   "metadata": {},
   "source": [
    "#### Question 3\n",
    "\n",
    "_Dans la version glissante du Lac gelé, il est parfois préférable de ne pas marcher dans la direction où tu veux vraiment aller, parce qu'il est plus important d'éviter le risque de glisser dans un trou._\n",
    "\n",
    "La phrase ci-dessus est-elle une déclaration sur l'environnement ou sur la politique ?\n",
    "\n",
    "- [ ] Environnement | La déclaration ci-dessus concerne la meilleure action à entreprendre dans une situation donnée ; celle-ci est déterminée par la politique.\n",
    "- [ ] Politique | Tu as compris !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2b5b57-e0e8-40c6-858c-34bc20ce5112",
   "metadata": {},
   "source": [
    "## Calculer la récompense attendue\n",
    "<!-- coding exercise -->\n",
    "\n",
    "Complète le code ci-dessous de façon à ce qu'il calcule la récompense attendue dans l'environnement glissant (non déterministe) de\n",
    "Environnement Frozen Lake sur 1000 épisodes. La boucle intérieure boucle sur les étapes d'un seul épisode.\n",
    "La boucle extérieure est sur les épisodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca010dee-36a5-452c-84db-9bd7059f28b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 0.047\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "\n",
    "rewards = []\n",
    "N = 1000\n",
    "\n",
    "for ____:\n",
    "\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while ____:\n",
    "        action = np.random.randint(low=0, high=4)\n",
    "        obs, reward, done, _ = env.step(____)\n",
    "    \n",
    "    rewards.append(reward)\n",
    "    \n",
    "print(\"Average reward:\", sum(rewards)/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99564a1c-076f-4167-858f-ce8ce47107f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 0.014\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "\n",
    "rewards = []\n",
    "N = 1000\n",
    "\n",
    "for i in range(N): # loop over N episodes\n",
    "\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.random.randint(low=0, high=4)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "    \n",
    "    rewards.append(reward)\n",
    "    \n",
    "print(\"Average reward:\", sum(rewards)/N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8a29b5-f8b4-452c-9cd2-a045ba136d04",
   "metadata": {},
   "source": [
    "## Politique de fabrication à la main\n",
    "<!-- coding exercise -->\n",
    "\n",
    "Le code ci-dessous charge l'environnement (non déterministe) glissant Frozen Lake.\n",
    "Une politique (déterministe) est définie comme un dictionnaire Python qui fait correspondre des observations à des actions.\n",
    "Le code boucle sur 1000 épisodes. Dans chaque épisode, il itère à travers les étapes de temps (observations et actions) jusqu'à ce que l'épisode soit terminé et qu'une récompense soit obtenue. Il imprime ensuite la récompense moyenne sur les 1000 épisodes. Il obtient généralement une récompense moyenne d'environ 0,05, ce qui signifie que l'objectif est atteint environ 5 % du temps.\n",
    "\n",
    "**Ta tâche:** modifie la politique de façon à obtenir une récompense moyenne d'au moins 0,02 (c'est-à-dire que l'agent atteint l'objectif 20 % du temps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35b4667",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "\n",
    "import gym\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d9fcefc-ba10-45a3-9e03-adb283d6c08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 0.034\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "\n",
    "import gym\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "\n",
    "policy = {\n",
    "    0 : 2,\n",
    "    1 : 2,\n",
    "    2 : 2,\n",
    "    3 : 2,\n",
    "    4 : 1,\n",
    "    5 : 1,\n",
    "    6 : 1,\n",
    "    7 : 1,\n",
    "    8 : 2,\n",
    "    9 : 2,\n",
    "    10: 2,\n",
    "    11: 0,\n",
    "    12: 2,\n",
    "    13: 2,\n",
    "    14: 2,\n",
    "    15: 2\n",
    "}\n",
    "\n",
    "rewards = []\n",
    "N = 1000\n",
    "for i in range(N): # loop over N episodes\n",
    "\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = policy[obs]\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "    \n",
    "    rewards.append(reward)\n",
    "    \n",
    "print(\"Average reward:\", sum(rewards)/N)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ray200]",
   "language": "python",
   "name": "conda-env-ray200-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
