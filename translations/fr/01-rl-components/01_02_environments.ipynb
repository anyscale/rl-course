{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c331561e-c62d-4486-9252-b9ff362f8fa4",
   "metadata": {},
   "source": [
    "## Environnements RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "093747d0-0011-4b28-a683-74870bfdbd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de064a2a-0a07-4c94-a9ac-b6f2d978f412",
   "metadata": {},
   "source": [
    "#### Qu'est-ce qu'un environnement ?\n",
    "\n",
    "- Un environnement peut être :\n",
    "  - un jeu, comme un jeu vidéo.\n",
    "  - une simulation d'un scénario du monde réel, comme un robot, le comportement d'un utilisateur ou le marché boursier\n",
    "  - toute autre configuration avec un _agent_ qui prend des _actions_, voit des _observations_ et reçoit des _récompenses_\n",
    "  \n",
    "Une note sur la terminologie : nous utiliserons indifféremment _agent_ et _joueur_ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5748d00-5c07-49db-aa9f-3cc1613bd0a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Exemple de course : lac gelé\n",
    "\n",
    "Comme exemple de fonctionnement d'un environnement, nous allons utiliser l'environnement [Frozen Lake](https://www.gymlibrary.ml/environments/toy_text/frozen_lake/) de [OpenAI Gym](https://www.gymlibrary.ml/), qui fournit l'interface standard pour les problèmes RL. Nous pouvons visualiser l'environnement comme suit :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f87dba7-b5de-4ba9-b89d-b5ea2d069f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2eef796-f767-4d99-a454-0c3bd227bfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "from utils_01 import fix_frozen_lake_render\n",
    "fix_frozen_lake_render(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8579403b-4adf-4a22-8d27-d7f54fd52dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "P...\n",
      ".O.O\n",
      "...O\n",
      "O..G\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693db547-8f4d-49ae-b759-80308d6803e0",
   "metadata": {},
   "source": [
    "Le but est que le joueur (`P`) atteigne le but (`G`) en marchant sur les segments du lac gelé (`.`) sans tomber dans les trous (`O`).\n",
    "\n",
    "**Note** : le rendu de l'environnement a été modifié par rapport au rendu d'OpenAI Gym afin qu'il puisse être affiché clairement dans cette plateforme d'apprentissage interactive.\n",
    "\n",
    "Notes :\n",
    "\n",
    "Le code n'est pas montré pour le remplacement du rendu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a34b31-5bbb-45f4-9e7f-e925fc96d0ee",
   "metadata": {},
   "source": [
    "#### Mouvement\n",
    "\n",
    "Le joueur peut se déplacer sur le lac gelé. Par exemple :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d5d954a-7890-427c-aea1-7daec590c91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "....\n",
      "PO.O\n",
      "...O\n",
      "O..G\n"
     ]
    }
   ],
   "source": [
    "env.step(1) # 1 -> Down\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb8ff83-f2b0-464f-934c-9e6144977c2c",
   "metadata": {},
   "source": [
    "Ne t'inquiète pas de `step(1)` pour l'instant ; nous y reviendrons \n",
    "\n",
    "Ce que tu peux voir, c'est que le joueur (`P`) s'est déplacé vers le bas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c7ab81-0402-4302-a46f-1b4bc115ca63",
   "metadata": {},
   "source": [
    "#### Objectif\n",
    "\n",
    "Avance d'un grand nombre d'étapes et tu as terminé le puzzle :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a799321-538a-4f9c-ae1d-31e15a9e0e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "....\n",
      ".O.O\n",
      "...O\n",
      "O..P\n"
     ]
    }
   ],
   "source": [
    "env.step(1)\n",
    "env.step(2)\n",
    "env.step(1)\n",
    "env.step(2)\n",
    "env.step(1)\n",
    "env.step(2)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98858094-a152-4e3e-bd03-ed5c81af095b",
   "metadata": {},
   "source": [
    "Tu as atteint l'objectif en atteignant la partie inférieure droite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1635d0e4-7d38-434c-a956-d47646e57b4a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Qu'est-ce qui fait un environnement ?\n",
    "\n",
    "Un environnement comprend plusieurs composants clés, que nous allons passer en revue dans les diapositives suivantes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73df9894-c37a-4d34-93d9-c4e43ecf0564",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### États\n",
    "\n",
    "- Nous utiliserons le terme _état_ de manière informelle pour désigner tout ce qui concerne l'environnement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a57b3f1-c76e-4b2c-9d84-742326bb827b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "P...\n",
      ".O.O\n",
      "...O\n",
      "O..G\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16f9748-129e-45c7-9bc1-106e0493fdad",
   "metadata": {},
   "source": [
    "- Par exemple, voici l'état de départ de l'environnement.\n",
    "- Le joueur se trouve en haut à gauche, il y a de la glace gelée à proximité, etc.\n",
    "- Nous utiliserons le concept d'état pour parler de notre environnement, mais il n'apparaîtra pas dans l'\"API\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0e72e4-8f86-4c05-9f33-7e888ef44045",
   "metadata": {},
   "source": [
    "#### Actions\n",
    "\n",
    "- Ici, le joueur peut choisir entre 4 actions possibles : gauche, bas, droite, haut\n",
    "- L'espace de toutes les actions possibles s'appelle l' **espace des actions**.\n",
    "- Dans SL, nous distinguons la régression ($y$ continu) et la classification ($y$ catégorique)\n",
    "- De même, dans RL, l'espace d'action peut être continu ou discret\n",
    "- Dans ce cas, il est discret (4 possibilités)\n",
    "- Le code est d'accord :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "757cfd69-d93d-4e8b-90d0-f3caf079f5a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1907e0-63fe-4c72-af65-f9869b590365",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "- Les observations sont les _parties de l'état que l'agent peut voir_.\n",
    "- Parfois, l'agent peut tout voir ; nous appelons cela _tout à fait observable_.\n",
    "- Souvent, nous avons des environnements _partiellement observables_ \n",
    "- Dans l'exemple du lac gelé, l'agent ne peut voir que son propre emplacement parmi les 16 carrés.\n",
    "- On ne \"dit\" pas à l'agent où se trouvent les trous par le biais d'observations directes, il devra donc _apprendre_ cela par essais et erreurs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659ebb9b-4c6a-43f6-96c1-eb5117c50a54",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "- L'espace de toutes les observations possibles s'appelle l' **espace d'observation**.\n",
    "- Tu peux considérer l'espace d'action comme analogue à la cible dans l'apprentissage supervisé.\n",
    "- Tu peux considérer l'espace d'observation comme analogue aux caractéristiques de l'apprentissage supervisé.\n",
    "\n",
    "\n",
    "Ici, nous avons un espace d'observation discret composé des 16 positions possibles du joueur :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81aa1ad6-e1bb-492d-a5d0-a72c94a33e29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(16)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ee9cb4-8650-4d39-87b7-97202ffca199",
   "metadata": {},
   "source": [
    "#### Récompenses\n",
    "\n",
    "- Dans l'apprentissage supervisé, l'objectif est généralement de faire de bonnes prédictions.\n",
    "- Tu peux toujours essayer différentes fonctions de perte en fonction de ton objectif spécifique, mais le concept général est le même.\n",
    "- En RL, l'objectif peut être n'importe quoi.\n",
    "- Mais, comme dans SL, tu devras _optimiser_ quelque chose.\n",
    "- En RL, nous cherchons à maximiser la **récompense**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf68c60b-f6fe-47e0-8482-bc51c4410846",
   "metadata": {},
   "source": [
    "#### Récompenses \n",
    "\n",
    "Dans l'exemple de Frozen Lake, l'agent reçoit une récompense lorsqu'il atteint l'objectif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4807bbe7-7525-4772-9ce9-a0e06f4b21fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "P...\n",
      ".O.O\n",
      "...O\n",
      "O..G\n",
      "reward = 0.0\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "obs, reward, done, _ = env.step(0)\n",
    "env.render()\n",
    "print(\"reward =\", reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aec3a5cd-5607-41fb-a932-7e5e10bf6569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward = 0.0\n"
     ]
    }
   ],
   "source": [
    "obs, reward, done, _ = env.step(1)\n",
    "print(\"reward =\", reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceae768d-d87b-4611-afce-e02137b9fd72",
   "metadata": {},
   "source": [
    "Toujours pas de récompense, continuons..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4c7004-91a1-4dcc-ad52-943cfd179a97",
   "metadata": {},
   "source": [
    "#### Récompenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89518a96-7c75-437d-9bb1-da7dbc7038b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "....\n",
      ".O.O\n",
      "...O\n",
      "O..P\n",
      "reward = 1.0\n"
     ]
    }
   ],
   "source": [
    "env.step(1)\n",
    "env.step(2)\n",
    "env.step(1)\n",
    "env.step(2)\n",
    "env.step(1)\n",
    "obs, reward, done, _ = env.step(2)\n",
    "env.render()\n",
    "print(\"reward =\", reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f9cfd3-9e69-4752-9927-ba108685aa83",
   "metadata": {},
   "source": [
    "Nous avons reçu une récompense de 1.0 pour avoir atteint l'objectif."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e9f4d6-ef36-415d-905b-ce1fcce042e6",
   "metadata": {},
   "source": [
    "#### Boucle environnement-agent\n",
    "\n",
    "Remarque comment l'agent (dans ce cas, nous) et l'environnement communiquent dans une boucle aller-retour :\n",
    "\n",
    "[](img/RL-loop.png)\n",
    "\n",
    "C'est le diagramme classique que tu verras partout.\n",
    "\n",
    "L'objectif sera d'apprendre automatiquement le comportement de l'agent (reste à l'écoute !)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e888ae70-a3f2-400b-8fd9-54dd27369beb",
   "metadata": {},
   "source": [
    "#### Représenter des actions\n",
    "\n",
    "- Pour utiliser le logiciel RL, nous aurons besoin d'une représentation numérique de notre espace d'action et de notre espace d'observation.\n",
    "- Dans ce cas, nous avons 4 actions discrètes possibles, nous pouvons donc les coder sous la forme {0,1,2,3} pour (gauche, bas, droite, haut).\n",
    "- C'est pourquoi, plus tôt, nous avons fait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "420363c1-9385-47e0-abaf-935a5c0fb4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5a3c2f-ec47-47c4-85e8-95945dfa7019",
   "metadata": {},
   "source": [
    "pour marcher vers le bas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6419f73c-8431-4e46-87af-0a99a95a58ba",
   "metadata": {},
   "source": [
    "#### Représenter des observations\n",
    "\n",
    "- De même, nous aurons besoin d'une représentation numérique de nos observations.\n",
    "- Ici, il y a 16 positions possibles du joueur. Elles sont codées de 0 à 15 comme suit :\n",
    "\n",
    "```\n",
    " 0 1 2 3\n",
    " 4 5 6 7\n",
    " 8 9 10 11\n",
    "12 13 14 15\n",
    "```\n",
    "\n",
    "Ces détails sur l'environnement Frozen Lake sont également disponibles dans la [documentation] (https://www.gymlibrary.ml/environments/toy_text/frozen_lake/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cecba2-2529-4f24-a5ef-d730457ac994",
   "metadata": {},
   "source": [
    "#### Représenter des observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b616b8-5ccf-45bf-a4ac-65ae31f108e4",
   "metadata": {},
   "source": [
    "Au départ, nous observons \"0\" car nous commençons en haut à gauche :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df316735-5a38-488f-80ae-dd97d715cc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "P...\n",
      ".O.O\n",
      "...O\n",
      "O..G\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b5dad3-f8bc-4298-8dc4-4a3f87285600",
   "metadata": {},
   "source": [
    "Après s'être déplacé vers le bas (action 1), nous nous déplaçons vers la position 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d540232-affa-45de-a10f-6f99f755d5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, reward, done, _ = env.step(1)\n",
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b84f58-bf29-4df0-9b85-08190d1815f0",
   "metadata": {},
   "source": [
    "L'observation est renvoyée par la méthode `step()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f179e5d-68cb-4d31-a5e0-cd01f5b14124",
   "metadata": {},
   "source": [
    "#### Environnements non déterministes\n",
    "\n",
    "- Jusqu'à présent, entreprendre une action particulière à partir d'un état particulier a toujours abouti au même nouvel état.\n",
    "- En d'autres termes, notre environnement Frozen Lake était _déterministe_.\n",
    "- Certains environnements sont _non-déterministes_, ce qui signifie que le résultat d'une action peut être aléatoire.\n",
    "- Nous pouvons initialiser un Frozen Lake non déterministe comme ceci :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9bdde4c3-e7f9-419a-a25b-4f2d6027354e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_slippery = gym.make(\"FrozenLake-v1\", is_slippery=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a7d44ef-2828-4c7f-aae9-17eab6b7d7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "env_slippery.seed(4)\n",
    "fix_frozen_lake_render(env_slippery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3943d88f-a44b-4fba-96f8-20177b6317b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "P...\n",
      ".O.O\n",
      "...O\n",
      "O..G\n"
     ]
    }
   ],
   "source": [
    "env_slippery.reset()\n",
    "env_slippery.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b97800f-5c7a-43a8-9018-f8505bd9eafd",
   "metadata": {},
   "source": [
    "#### Environnements non déterministes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35e5cb48-3e0f-4b5f-8f0a-663ff31e99e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "....\n",
      "PO.O\n",
      "...O\n",
      "O..G\n"
     ]
    }
   ],
   "source": [
    "env_slippery.step(1) # move down\n",
    "env_slippery.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0755c6-0d78-42d0-a9b4-cd4aec36db82",
   "metadata": {},
   "source": [
    "Le déménagement n'a pas fonctionné comme prévu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cba5c8e8-a7ab-4854-9c35-ca79584f7b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "....\n",
      ".P.O\n",
      "...O\n",
      "O..G\n"
     ]
    }
   ],
   "source": [
    "env_slippery.step(1) # move down\n",
    "env_slippery.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031a6a4e-f9a1-4965-86b8-d439a82491ad",
   "metadata": {},
   "source": [
    "Le déplacement vers le bas a fonctionné cette fois.\n",
    "\n",
    "Dans cet environnement \"glissant\" de Frozen Lake, le mouvement ne fonctionne comme prévu qu'un tiers du temps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0388fe5a-b6e4-484c-ac51-d376c353ffaa",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Episodes\n",
    "\n",
    "- Jouer au lac gelé a une fin - soit tu tombes dans un trou, soit tu atteins l'objectif.\n",
    "- Cependant, un seul jeu n'est pas suffisant pour qu'un algorithme RL puisse en tirer des leçons.\n",
    "- Il aura besoin de plusieurs sessions de jeu, appelées **épisodes**.\n",
    "- Après un épisode, l'environnement est réinitialisé."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68508dc1-2009-48a7-85e1-3f4bba4b0220",
   "metadata": {},
   "source": [
    "#### Episodes\n",
    "\n",
    "La méthode `step()` renvoie un drapeau nous indiquant si l'épisode est terminé :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "534b8516-26ea-4df4-9062-e6fb1f57d3a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, reward, done, _ = env_slippery.step(1)\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0684e7c-b767-4907-965d-6bcc86d64c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "....\n",
      ".P.O\n",
      "...O\n",
      "O..G\n"
     ]
    }
   ],
   "source": [
    "env_slippery.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdbecc7-7aac-432a-9e6e-301af52e7d2a",
   "metadata": {},
   "source": [
    "Ici, l'épisode est terminé parce que nous sommes tombés dans un trou."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854521ed-7fb7-453b-9eb6-eaf61074f533",
   "metadata": {},
   "source": [
    "#### Episodes\n",
    "\n",
    "- Dans certains environnements (comme Frozen Lake), les récompenses ne sont reçues qu'à la fin d'un épisode.\n",
    "- Dans d'autres environnements, les récompenses peuvent être reçues à n'importe quel **pas de temps** (c'est-à-dire après une action)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea0a22c-c13f-407e-beb1-5fd0a985dc4a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Mettre tout en place\n",
    "\n",
    "- Nous avons maintenant parlé des principaux composants d'un environnement RL :\n",
    "  - États\n",
    "  - Actions\n",
    "  - Observations\n",
    "  - Récompenses\n",
    "  - Épisodes\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8de8cd1-8733-4c90-89d6-9586be21309a",
   "metadata": {},
   "source": [
    "#### Jeux de données SL vs. environnements RL\n",
    "\n",
    "- Dans l'apprentissage supervisé, on te donne généralement un ensemble de données.\n",
    "- En RL, l'environnement agit comme un _générateur de données_.\n",
    "  - Plus tu joues dans l'environnement, plus tu génères de \"données\" et plus tu peux apprendre.\n",
    "- On peut aussi faire du RL sur un ensemble de données pré-collectées (appelé _RL hors ligne_), mais cela ne nous concerne pas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be83b356-8218-44dd-9ccd-f0bc44ade74e",
   "metadata": {},
   "source": [
    "#### Appliquons ce que nous avons appris !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c511b50-fae2-4764-8eaa-c47ca7dab1d7",
   "metadata": {},
   "source": [
    "## Environnement de la voiture auto-conductrice\n",
    "<!-- multiple choice -->\n",
    "\n",
    "Tu utilises RL pour entraîner une voiture à conduite autonome. L'IA de la voiture utilise diverses caméras et capteurs comme entrées et doit décider de l'angle du volant ainsi que de l'angle des pédales d'accélérateur/frein au sol."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a443612-50ff-4564-bdf4-a4e1ba79be93",
   "metadata": {},
   "source": [
    "#### L'espace d'observation est-il continu ou discret ?\n",
    "\n",
    "- [x] Continu\n",
    "- [ ] Discret | Dans ce cas, les observations sont les entrées du capteur, par exemple les estimations de la profondeur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cace1880-0440-42f1-a087-576821998a4a",
   "metadata": {},
   "source": [
    "#### L'espace d'action est-il continu ou discret ?\n",
    "\n",
    "- [x] Continu\n",
    "- [ ] Discret | Les actions sont des angles ; elles ne proviennent pas d'un ensemble discret d'options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124e3763-a526-4cd7-84c1-721bb7b9f102",
   "metadata": {},
   "source": [
    "#### Quelle serait la structure de récompense la plus raisonnable pour cet environnement ?\n",
    "\n",
    "- [La récompense est égale à la durée pendant laquelle la voiture a pu rouler sans s'écraser | Quelle serait la récompense si la voiture ne bouge jamais ?\n",
    "- [ ] La récompense est égale à la distance que la voiture a pu parcourir sans s'écraser | Oui, ça sonne bien !\n",
    "- [ ] +1 récompense chaque fois que la voiture s'écrase | N'oublie pas que nous voulons maximiser la récompense, pas la minimiser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcfbbb6-1765-473c-8b7e-c24e914949d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Épisodes en fonction des pas de temps\n",
    "<!-- multiple choice -->\n",
    "\n",
    "Remplis les espaces vides de la phrase suivante \n",
    "\n",
    "*Dans un environnement d'apprentissage par renforcement, on effectue des actions de manière répétée jusqu'à ce que l'\\_\\_\\_\\_\\_\\_\\_s'arrête. Cela peut impliquer une seule \\_\\_\\_\\_\\_\\_\\_, ou de très nombreuses.*\n",
    "\n",
    "- [ ] pas de temps / récompense | Vérifie bien le premier blanc !\n",
    "- [ ] récompense / étape de temps | Essaie encore !\n",
    "- [ ] étape temporelle / épisode | Essaie encore !\n",
    "- [ ] épisode / étape de temps | Tu l'as eu !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5286ef-84b7-4b04-a74b-d90b0216abae",
   "metadata": {},
   "source": [
    "## L'environnement du taxi de Gym\n",
    "<!-- coding exercise -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22599988-4849-4a9b-aa83-2b792dedbc19",
   "metadata": {},
   "source": [
    "Dans cet exercice, nous allons examiner l'un des environnements textuels fournis avec\n",
    "OpenAI gym, appelé l'environnement taxi.\n",
    "Documentation [ici](https://www.gymlibrary.ml/environments/toy_text/taxi/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb32c250-3e05-4040-bad0-5f8f0d7e3123",
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE\n",
    "import gym\n",
    "\n",
    "taxi = gym.make(\"Taxi-v3\")\n",
    "obs = taxi.reset(seed=5)\n",
    "taxi.render()\n",
    "\n",
    "# Add calls to `step` here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413240b7-6df6-48a5-bb24-4f8a518dc81e",
   "metadata": {},
   "source": [
    "Dans cet exercice, le taxi est représenté par la surbrillance jaune,\n",
    "qui se trouve actuellement en bas à gauche de l'arène. Le `:` peut être traversé mais pas le `|`.\n",
    "Le but est de prendre des passagers et de les déposer.\n",
    "\n",
    "Il y a 6 actions possibles :\n",
    "descendre (0), monter (1), droite (2), gauche (3), ramasser (4), déposer (5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "99f35e5c-5e97-4122-9ae0-44e849a51c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(6)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5201b9-8041-4044-b172-a736f43451df",
   "metadata": {},
   "source": [
    "Le code suivant imprime l'observation dans un format plus lisible pour l'homme :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f0083927-ad7d-4e78-b04f-316c394f0139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taxi row: 4\n",
      "Taxi col: 0\n",
      "Passenger loc: 0\n",
      "Destination loc: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Taxi row: %d\\nTaxi col: %d\\nPassenger loc: %d\\nDestination loc: %d\" % tuple(taxi.decode(obs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2553c73a-e6e8-4096-9b67-1806dae7f79f",
   "metadata": {},
   "source": [
    "Les emplacements possibles sont `R` (0), `G` (1), `Y` (2), `B` (3) et en taxi (4). Ainsi, le passager se trouve actuellement à `R` et se dirige vers `Y`.\n",
    "\n",
    "Pour répondre à la question suivante, tu devras modifier le code, l'exécuter et imprimer tout résultat pertinent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a4208ce9-efb2-48e7-8e7a-3b6042288309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[42mY\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "import gym\n",
    "\n",
    "taxi = gym.make(\"Taxi-v3\")\n",
    "obs = taxi.reset(seed=5)\n",
    "taxi.render()\n",
    "taxi.step(1)\n",
    "taxi.step(1)\n",
    "taxi.step(1)\n",
    "taxi.step(1)\n",
    "taxi.render()\n",
    "taxi.step(4)\n",
    "taxi.step(0)\n",
    "taxi.step(0)\n",
    "taxi.step(0)\n",
    "taxi.step(0)\n",
    "taxi.render()\n",
    "obs, reward, done, _ = taxi.step(5)\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "598a018b-569e-4378-bb2a-0de521c18881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: in this case there is no testing of the code; the code is only used to explore\n",
    "# this may not work well with the framework - add an automated test to the code as well? \n",
    "# e.g. can check that the state of the env is correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9564fe-af4a-4ded-97d3-4b754df46265",
   "metadata": {},
   "source": [
    "#### Quelle récompense l'agent reçoit-il pour avoir réussi à déposer le passager ?\n",
    "\n",
    "- [ ] 0 | Essaie d'effectuer les actions nécessaires avec `taxi.step()` et d'imprimer les récompenses.\n",
    "- [ ] 5 | Essaie d'effectuer les actions nécessaires avec `taxi.step()` et d'imprimer les récompenses.\n",
    "- [ ] 10 | Essaie d'effectuer les actions nécessaires avec `taxi.step()` et d'imprimer les récompenses.\n",
    "- [ ] 20 | Tu as réussi !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fffe5b6-36c3-4b2b-9513-a6125f0eaac3",
   "metadata": {},
   "source": [
    "## Observations vs. rendus\n",
    "<!-- coding exercise -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a7f02e-4367-48d5-b491-8ee5cce4f468",
   "metadata": {},
   "source": [
    "L'environnement du lac gelé nous permet de créer un rendu visuel de l'environnement, que nous avons vu précédemment. Ce rendu est destiné à des fins humaines/de débogage, et n'est _pas_ vu par l'agent/algorithme. Ta tâche ici est de jouer un environnement Frozen Lake donné **sans regarder le rendu visuel** (pas de triche !). Remplis la liste `actions` pour qu'elle contienne un ensemble d'actions qui amènent correctement l'agent au but. Ce que tu vis est ce qu'un algorithme RL \"voit\" lorsqu'il apprend !\n",
    "\n",
    "Note que l'environnement donné de Frozen Lake est 3x3 au lieu de 4x4. Ainsi, l'espace d'observation va de 0 à 8 au lieu de 0 à 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb2ba815",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '____' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m env\u001b[38;5;241m.\u001b[39mrender \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     11\u001b[0m obs \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m---> 12\u001b[0m actions \u001b[38;5;241m=\u001b[39m \u001b[43m____\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m actions:\n\u001b[1;32m     14\u001b[0m     obs, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "\u001b[0;31mNameError\u001b[0m: name '____' is not defined"
     ]
    }
   ],
   "source": [
    "# EXERCISE\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "env = gym.make(\"FrozenLake-v1\", \n",
    "               desc=gym.envs.toy_text.frozen_lake.generate_random_map(size=3, p=0.3), \n",
    "               is_slippery=False)\n",
    "env.render = None\n",
    "\n",
    "obs = env.reset()\n",
    "actions = ____\n",
    "for action in actions:\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    print(\"Obs:\", obs, \"Reward:\", reward, \"Done:\", done)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7a5f827-8aad-4f48-b762-1feb4b51748a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs: 3 Reward: 0.0 Done: False\n",
      "Obs: 4 Reward: 0.0 Done: False\n",
      "Obs: 7 Reward: 0.0 Done: False\n",
      "Obs: 8 Reward: 1.0 Done: True\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "\n",
    "import gym.envs.toy_text\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "env = gym.make(\"FrozenLake-v1\", desc=gym.envs.toy_text.frozen_lake.generate_random_map(size=3, p=0.3), is_slippery=False)\n",
    "env.render = None\n",
    "\n",
    "obs = env.reset()\n",
    "actions = []\n",
    "# BEGIN SOLUTION\n",
    "actions = [1,2,1,2]\n",
    "# END SOLUTION\n",
    "for action in actions:\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    print(\"Obs:\", obs, \"Reward:\", reward, \"Done:\", done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0c3a6fc-637a-46b7-92e0-ff08e576c65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PO.\n",
      "...\n",
      "O.G\n"
     ]
    }
   ],
   "source": [
    "# HIDDEN\n",
    "import numpy as np\n",
    "import gym.envs.toy_text\n",
    "from utils_01 import fix_frozen_lake_render\n",
    "np.random.seed(1)\n",
    "env = gym.make(\"FrozenLake-v1\", desc=gym.envs.toy_text.frozen_lake.generate_random_map(size=3, p=0.3), is_slippery=False)\n",
    "env.reset()\n",
    "fix_frozen_lake_render(env)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e8a4b0-b102-4047-8e67-942ef81b0ed9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Meilleur chemin vers l'objectif\n",
    "\n",
    "En rappelant que les actions 0, 1, 2, 3 représentent respectivement la gauche, le bas, la droite et le haut, laquelle des affirmations suivantes décrit correctement le meilleur chemin vers le but ?\n",
    "\n",
    "- [ ] Commence par te déplacer vers le bas, puis vers le bas à nouveau\n",
    "- [ ] Commence par te déplacer vers le bas, puis vers la droite\n",
    "- [ ] Commence par te déplacer vers la droite, puis à nouveau vers la droite\n",
    "- [ ] Commence par te déplacer vers la droite, puis vers le bas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8690d35a-5d51-4452-bd9c-8b44161feb81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ray200]",
   "language": "python",
   "name": "conda-env-ray200-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
