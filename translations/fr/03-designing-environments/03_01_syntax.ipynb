{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e23df55-837b-4ec9-86ad-3b60c6a7f00d",
   "metadata": {},
   "source": [
    "## Syntaxe des environnements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2e57cc-3491-4489-af86-6052b1efcc1b",
   "metadata": {},
   "source": [
    "#### Motivation\n",
    "\n",
    "- Jusqu'à présent, nous avons utilisé des environnements prédéfinis comme Frozen Like et Google RecSim.\n",
    "- Pour utiliser RL sur notre propre problème, nous ne pouvons utiliser aucun de ces environnements.\n",
    "- Nous devrons définir notre propre environnement avec Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0797f5a9-c856-49f2-9a3c-419d816b6ec1",
   "metadata": {},
   "source": [
    "#### Revue de Frozen Lake\n",
    "\n",
    "- Rappelle-toi l'environnement de Frozen Lake, du module 1 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69171cee-dc7b-433d-85c5-d0d40486a43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"FrozenLake-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a41dffc-8451-4d7e-a40a-23fd1315ffba",
   "metadata": {},
   "source": [
    "#### Revue de Frozen Lake\n",
    "\n",
    "- OpenAI Gym est une source ouverte, nous avons donc pu consulter le [code source de Frozen Lake] (https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py).\n",
    "- Cependant, il est compliqué et contient bien plus que ce dont nous avons besoin.\n",
    "- Créons notre propre environnement appelé Frozen Pond avec les composants de base de Frozen Lake."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735eb606-b1dd-4dba-be55-d6105cc47536",
   "metadata": {},
   "source": [
    "#### Composants d'une Env\n",
    "\n",
    "Décisions conceptuelles :\n",
    "\n",
    "- Espace d'observation\n",
    "- Espace d'action\n",
    "\n",
    "En Python, nous devrons implémenter, au moins :\n",
    "\n",
    "- constructeur\n",
    "- `reset()`\n",
    "- `step()`\n",
    "\n",
    "En pratique, nous pouvons aussi vouloir d'autres méthodes, comme `render()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9cf2ed-dc7e-4bbd-a5f8-9b78b8a38710",
   "metadata": {},
   "source": [
    "#### Décisions conceptuelles\n",
    "\n",
    "Dans ce cas, comme nous imitons le Lac gelé, l'espace d'observation et l'espace d'action sont déjà décidés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34c0aa1d-8ab0-462a-8bdd-8ba12484a5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space = gym.spaces.Discrete(16)\n",
    "action_space = gym.spaces.Discrete(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef0fa48-25fb-4360-a1d8-54ca675dc786",
   "metadata": {},
   "source": [
    "Plus tard dans ce cours, nous nous plongerons plus profondément dans ces décisions !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c88c68-7f74-490e-beda-9e199d0c3822",
   "metadata": {},
   "source": [
    "#### Coding it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db413de7-7df3-4d77-9ae8-5c8e95a39068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "class FrozenPond(gym.Env):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b3bbec-72c8-48dd-bdc4-6f537dacda5c",
   "metadata": {},
   "source": [
    "- Remarque que nous commençons par sous-classer `gym.Env`.\n",
    "- Facultatif : Tu peux lire sur les objets, l'héritage et les sous-classes.\n",
    "- L'essentiel : Il s'agit d'un `gym.Env` de base et nous pouvons en écraser les caractéristiques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b2467a-4906-473f-8207-cde646def666",
   "metadata": {},
   "source": [
    "#### Constructeur\n",
    "\n",
    "- Le constructeur est appelé lorsque nous créons un nouvel objet `FrozenPond`.\n",
    "- C'est ici que nous définissons l'espace d'observation et l'espace d'action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2417aa6-93e3-46c9-984b-f98e24ece49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "class FrozenPond(gym.Env):\n",
    "    def __init__(self, env_config=None):\n",
    "        self.observation_space = gym.spaces.Discrete(16)\n",
    "        self.action_space = gym.spaces.Discrete(4)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359ab164-11d8-4614-a4f1-f875fac08071",
   "metadata": {},
   "source": [
    "- Pour la compatibilité avec RLlib, le constructeur doit prendre un `env_config` \n",
    "- Nous allons simplement ignorer cet argument pour l'instant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285e5eca-3aa8-472a-a6bc-037b411f83a5",
   "metadata": {},
   "source": [
    "_COPY #### Reset\n",
    "\n",
    "- La prochaine méthode dont nous aurons besoin est la réinitialisation.\n",
    "- Le constructeur définit des paramètres permanents comme l'espace d'observation.\n",
    "- `reset` configure chaque nouvel épisode.\n",
    "- Il y a une certaine liberté entre les deux, par exemple pour définir l'emplacement du but.\n",
    "- Si quelque chose _pourrait_ changer, nous le mettrons dans `reset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce9715c1-8865-4376-97d5-4c0f7b253e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c438cc9b-d6db-4412-8b29-80288a2cf288",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def reset(self):\n",
    "        self.player = (0, 0) # the player starts at the top-left\n",
    "        self.goal = (3, 3)   # goal is at the bottom-right\n",
    "        \n",
    "        self.holes = np.array([\n",
    "            [0,0,0,0], # FFFF \n",
    "            [0,1,0,1], # FHFH\n",
    "            [0,0,0,1], # FFFH\n",
    "            [1,0,0,0]  # HFFF\n",
    "        ])\n",
    "        \n",
    "        return 0 # to be changed to return self.observation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206ed3bf-aa1d-4833-84fa-e9be1f64648b",
   "metadata": {},
   "source": [
    "_COPY #### Reset\n",
    "\n",
    "Testons cela :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c1c6161-2b90-47bf-b002-d3ff6fc62464",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = FrozenPond()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50989bc9-5d31-480a-88ac-dbf85f2ff390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edbdb5ee-0af9-43e2-8833-c1c945eae535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0],\n",
       "       [0, 1, 0, 1],\n",
       "       [0, 0, 0, 1],\n",
       "       [1, 0, 0, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp.holes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79c6258-6353-42ac-81d6-51423385972a",
   "metadata": {},
   "source": [
    "Ça a l'air bien."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a138d14-cf05-4d44-b315-4e708692e001",
   "metadata": {},
   "source": [
    "#### Étape\n",
    "\n",
    "- La dernière méthode dont nous avons besoin est `step`.\n",
    "- C'est la méthode la plus compliquée qui contient la logique de base.\n",
    "- Rappelle-toi que `step` renvoie 4 choses :\n",
    "  1. Observation\n",
    "  2. Récompense\n",
    "  3. Drapeau fait\n",
    "  4. Info supplémentaire (nous l'ignorerons)\n",
    "- Pour plus de clarté, nous allons écrire des méthodes d'aide pour observation, récompense et fait, plus une méthode d'aide supplémentaire "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5e1f9a-6f10-4038-9c02-2a6e966ecac5",
   "metadata": {},
   "source": [
    "#### Étape : observation\n",
    "\n",
    "Rappelle-toi que l'observation est un indice de 0 à 15 :\n",
    "\n",
    "```\n",
    " 0 1 2 3\n",
    " 4 5 6 7\n",
    " 8 9 10 11\n",
    "12 13 14 15\n",
    "```\n",
    "\n",
    "Nous pouvons coder cela comme suit :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a66c82f8-21a4-403d-8172-2b878488e155",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def observation(self):\n",
    "        return 4*self.player[0] + self.player[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921f3bc3-6200-4167-839b-a068e88240c1",
   "metadata": {},
   "source": [
    "Par exemple, si le joueur se trouve à (2,1), nous renvoyons le message suivant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b95ad09b-a77d-4039-afa6-0695a8427601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4*2 + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae722e0-9d57-414c-ade2-53b361b8e590",
   "metadata": {},
   "source": [
    "Remarque : maintenant que `self.observation` est implémenté, nous devrions changer `reset` en `return self.observation()` plutôt que `return 0` pour un code de meilleure qualité."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b84fcf-c40f-4513-8cd0-3bb2b0a8d32a",
   "metadata": {},
   "source": [
    "#### Étape : récompense\n",
    "\n",
    "En suivant l'exemple de Frozen Lake, la récompense sera de 1 si l'agent atteint l'objectif, et de 0 sinon :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ee1fd77-5923-4db1-abe3-96c47360366a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def reward(self):\n",
    "        return int(self.player == self.goal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d71262-1a9e-4daf-940e-df02c2c2631d",
   "metadata": {},
   "source": [
    "Nous modifierons cette fonction de récompense plus tard dans le module !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb7d07c-9153-4933-94f6-c117ca6993ff",
   "metadata": {},
   "source": [
    "####. Étape : faite\n",
    "\n",
    "- Nous avons également besoin de savoir quand un épisode est terminé \n",
    "- Selon Frozen Lake, l'épisode est terminé lorsque l'agent atteint l'objectif ou tombe dans l'étang."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab0ed064-4f50-425e-8f0f-302a44c2d259",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def done(self):\n",
    "        return self.player == self.goal or self.holes[self.player] == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8829aa45-d26c-4707-ab51-38c38353ce00",
   "metadata": {},
   "source": [
    "#### Étape : emplacements valides\n",
    "\n",
    "Enfin, pour simplifier la méthode `step`, nous allons écrire une méthode auxiliaire appelée `is_valid_loc` qui vérifie si un emplacement particulier est dans les limites (de 0 à 3 dans chaque dimension)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1872595-ab21-4b4e-a431-c15fc1ea7595",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def is_valid_loc(self, location):\n",
    "        if 0 <= location[0] <= 3 and 0 <= location[1] <= 3:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce94f18-ffb6-4f93-ba45-f7084c3805c5",
   "metadata": {},
   "source": [
    "#### Étape : assembler le tout\n",
    "\n",
    "- En utilisant les éléments ci-dessus, nous pouvons maintenant écrire la méthode `step`.\n",
    "- `step` prend une _action_, met à jour l'_état_ et renvoie l'observation, la récompense, le drapeau \"done\" et des informations supplémentaires (ignorées).\n",
    "- Rappelle-toi comment les actions sont codées : 0 pour gauche, 1 pour bas, 2 pour droite, 3 pour haut.\n",
    "- Nous allons implémenter un étang gelé **non glissant** ; en d'autres termes, déterministe plutôt que stochastique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c083279a-b950-45e6-9cfc-265885c80c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def step(self, action):\n",
    "        # Compute the new player location\n",
    "        if action == 0:   # left\n",
    "            new_loc = (self.player[0], self.player[1]-1)\n",
    "        elif action == 1: # down\n",
    "            new_loc = (self.player[0]+1, self.player[1])\n",
    "        elif action == 2: # right\n",
    "            new_loc = (self.player[0], self.player[1]+1)\n",
    "        elif action == 3: # up\n",
    "            new_loc = (self.player[0]-1, self.player[1])\n",
    "        else:\n",
    "            raise ValueError(\"Action must be in {0,1,2,3}\")\n",
    "        \n",
    "        # Update the player location only if you stayed in bounds\n",
    "        # (if you try to move out of bounds, the action does nothing)\n",
    "        if self.is_valid_loc(new_loc):\n",
    "            self.player = new_loc\n",
    "        \n",
    "        # Return observation/reward/done\n",
    "        return self.observation(), self.reward(), self.done(), {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a4a4ec-cfe6-4d3e-80a1-a606afc87c0e",
   "metadata": {},
   "source": [
    "#### Succès !\n",
    "\n",
    "- C'est fait ! Nous avons implémenté les pièces nécessaires dans Frozen Pond \n",
    "  - constructeur\n",
    "  - `reset`\n",
    "  - `step`\n",
    "- Nous ajouterons également une fonction facultative `render` afin de pouvoir dessiner l'état :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dace259a-1e8f-4d77-af39-3f1cb6cdc2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def render(self):\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                if (i,j) == self.goal:\n",
    "                    print(\"⛳️\", end=\"\")\n",
    "                elif (i,j) == self.player:\n",
    "                    print(\"🧑\", end=\"\")\n",
    "                elif self.holes[i,j]:\n",
    "                    print(\"🕳\", end=\"\")\n",
    "                else:\n",
    "                    print(\"🧊\", end=\"\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925146e2-188f-4b78-acb5-df30214dad54",
   "metadata": {},
   "source": [
    "- Pour le plaisir, nous allons utiliser des émojis dans le rendu de notre client.\n",
    "- Le joueur est 🧑, le but est ⛳️, le segment de lac gelé est 🧊, le trou est 🕳."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d03db1-e53b-435b-a0fb-aa67bdd99161",
   "metadata": {},
   "source": [
    "#### Tester notre implémentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a079671-3fc3-46eb-aaa3-e0629201de06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "from envs_03 import FrozenPond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c03550a-fde5-4885-883a-030a2530488f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧑🧊🧊🧊\n",
      "🧊🕳🧊🕳\n",
      "🧊🧊🧊🕳\n",
      "🕳🧊🧊⛳️\n"
     ]
    }
   ],
   "source": [
    "env = FrozenPond()\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88bcc59-f6c1-4722-a255-ef30a49c2616",
   "metadata": {},
   "source": [
    "#### Tester notre implémentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2301a03f-f40e-492d-ae42-0f99b07ee09f",
   "metadata": {},
   "source": [
    "Testons la méthode `step` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1cc81874-c11f-430f-904b-1d108c170589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0, False, {'player': (0, 1), 'goal': (3, 3)})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(2) # 0=left / 1=down / 2=right / 3=up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89d4775d-fe17-4628-a97f-33ab2799573d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧊🧑🧊🧊\n",
      "🧊🕳🧊🕳\n",
      "🧊🧊🧊🕳\n",
      "🕳🧊🧊⛳️\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b46b9c2-6ebe-4d37-b809-c38bd711126c",
   "metadata": {},
   "source": [
    "Ça a l'air bien !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be4064b-3f24-48b1-aa6e-184f655067d0",
   "metadata": {},
   "source": [
    "#### Tester notre implémentation\n",
    "\n",
    "Comparons directement les deux environnements :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3aff2e55-17f6-48bc-8fb8-eaade93881a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter | gym obs / our obs | gym reward / our reward | gym done / our done\n",
      " 0   |       0 /  0      |          0 / 0        |      False / False\n",
      " 1   |       1 /  1      |          0 / 0        |      False / False\n",
      " 2   |       2 /  2      |          0 / 0        |      False / False\n",
      " 3   |       6 /  6      |          0 / 0        |      False / False\n",
      " 4   |      10 / 10      |          0 / 0        |      False / False\n",
      " 5   |      14 / 14      |          0 / 0        |      False / False\n",
      " 6   |      14 / 14      |          0 / 0        |      False / False\n",
      " 7   |      15 / 15      |          1 / 1        |       True /  True\n"
     ]
    }
   ],
   "source": [
    "lake = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
    "pond = FrozenPond()\n",
    "\n",
    "lake.reset()\n",
    "pond.reset()\n",
    "\n",
    "print(\"Iter | gym obs / our obs | gym reward / our reward | gym done / our done\")\n",
    "for i, a in enumerate([0, 2, 2, 1, 1, 1, 1, 2]):\n",
    "    lake_obs, lake_rew, lake_done, _ = lake.step(a)\n",
    "    pond_obs, pond_rew, pond_done, _ = pond.step(a)\n",
    "    print(\"%2d   |      %2d / %2d      |          %d / %d        |      %5s / %5s\" % \\\n",
    "          (i, lake_obs, pond_obs, lake_rew, pond_rew, lake_done, pond_done))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87185e66-37b4-4337-9046-17a17cc6efb2",
   "metadata": {},
   "source": [
    "Ils sont identiques !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5deb1e8-3de2-42dc-8363-aee8acf534e3",
   "metadata": {},
   "source": [
    "#### Tester notre implémentation\n",
    "\n",
    "- RLlib est également livré avec un vérificateur d'env\n",
    "- Cela ne nous dira pas si notre env est identique à celle de Frozen Lake\n",
    "- Mais il effectuera plusieurs vérifications utiles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57e3cbe2-8354-4533-872d-41bd06ce2948",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.utils.pre_checks.env import check_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f4ebd66-5e52-4724-a9c3-df6efdb66eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-27 16:08:55,501\tWARNING env.py:142 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n"
     ]
    }
   ],
   "source": [
    "check_env(pond)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f674689-9df5-4da7-9379-478c2b802571",
   "metadata": {},
   "source": [
    "- Toutes les vérifications ont été passées, sauf cet avertissement concernant la longueur maximale d'un épisode.\n",
    "- Nous pouvons/devrions le définir pour que les épisodes ne puissent pas devenir arbitrairement longs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29218be1-9c76-4f04-bffd-e7288098bb74",
   "metadata": {},
   "source": [
    "#### Nombre maximum d'étapes par épisode\n",
    "\n",
    "- Pour définir un nombre maximum de pas par épisode, nous pouvons utiliser un `gym` _wrapper_.\n",
    "- Les wrappers sont des moyens pratiques de modifier les environnements, y compris les observations, les actions et les récompenses.\n",
    "- Ici, nous allons utiliser le wrapper `TimeLimit` pour définir une limite de pas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8edeeaf-5dd3-4eee-9ed0-747880709912",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import TimeLimit\n",
    "\n",
    "pond_5 = TimeLimit(pond, max_episode_steps=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37ac862-4df2-4f76-bbe0-b718e575f0e3",
   "metadata": {},
   "source": [
    "Nous pouvons vérifier que cela sera fait après 5 étapes, même si le but n'est pas atteint :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be71dbd1-7668-4499-85d2-68a5b70fa1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, False, {'player': (0, 0), 'goal': (3, 3)})\n",
      "(0, 0, False, {'player': (0, 0), 'goal': (3, 3)})\n",
      "(0, 0, False, {'player': (0, 0), 'goal': (3, 3)})\n",
      "(0, 0, False, {'player': (0, 0), 'goal': (3, 3)})\n",
      "(0, 0, True, {'player': (0, 0), 'goal': (3, 3), 'TimeLimit.truncated': True})\n"
     ]
    }
   ],
   "source": [
    "pond_5.reset()\n",
    "for i in range(5):\n",
    "    print(pond_5.step(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f54f62a-2022-4bdf-926e-b855dc374ec6",
   "metadata": {},
   "source": [
    "#### Nombre maximum d'étapes par épisode\n",
    "\n",
    "Une limite de pas plus raisonnable pourrait être de 50, plutôt que de 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff4847c8-ff8e-4e15-96b3-b0eb38781266",
   "metadata": {},
   "outputs": [],
   "source": [
    "pond_50 = TimeLimit(pond, max_episode_steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9eadc6-381d-47ac-82ec-f418b6830e6f",
   "metadata": {},
   "source": [
    "- Pour info : il est également possible de définir cette limite dans RLlib, juste à des fins d'entraînement.\n",
    "- Cela se fait avec le paramètre `\"horizon\"` dans la configuration de l'entraîneur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aed5b6c-a486-4f1c-96e0-e027710461f6",
   "metadata": {},
   "source": [
    "#### Appliquons ce que nous avons appris !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456c51dc-3d2c-4e06-be76-62c279e976b0",
   "metadata": {},
   "source": [
    "## Les récompenses de Frozen Pond\n",
    "<!-- multiple choice -->\n",
    "\n",
    "Dans le lac (et l'étang) gelé, la récompense est de 1 lorsque l'agent atteint l'objectif, et de 0 sinon. L'agent doit apprendre à éviter les trous, mais il n'y a en fait aucune récompense négative à tomber dans un trou - c'est la même récompense nulle que de marcher dans un morceau sûr du lac gelé ! Pourquoi cette configuration fonctionne-t-elle toujours, même si la récompense est la même pour marcher dans un trou ou sur la terre ferme ?\n",
    "\n",
    "- [ ] Une fois que l'agent est tombé dans un trou, il est coincé. Il peut faire d'autres actions, mais elles ne font rien. L'agent apprend donc à éviter les trous.\n",
    "- [ ] Une récompense de 0 est la plus petite récompense possible ; par conséquent, lorsque l'agent reçoit une récompense de 0 en tombant dans un trou, il sait immédiatement que tomber dans un trou est une mauvaise chose.\n",
    "- [La pénalité liée au fait de tomber dans un trou est indirecte, car l'épisode se termine par une récompense de zéro, ce qui signifie qu'il renonce à la récompense potentielle de 1 qu'il pourrait obtenir en atteignant son objectif. L'agent apprend qu'en tombant dans un trou, il perd des récompenses _futures_.\n",
    "- [Les agents RL préfèrent les épisodes plus longs. Lorsque l'agent tombe dans le trou, l'épisode se termine immédiatement, ce que l'agent apprend à éviter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d53229-6adf-4b71-b8dd-40157b9a703d",
   "metadata": {},
   "source": [
    "## Étang contre labyrinthe\n",
    "<!-- coding exercise -->\n",
    "\n",
    "Disons que nous voulons transformer notre environnement d'étang en un environnement de labyrinthe. Dans ce cas, nous avons des murs au lieu de trous. La seule différence entre l'étang et le labyrinthe est le comportement des trous par rapport aux murs. Dans l'étang gelé, marcher dans un trou met fin à l'épisode. Dans l'environnement du labyrinthe, marcher dans un mur ne fait rien (c'est-à-dire que l'action ne change pas l'emplacement de l'agent, tout comme essayer de marcher sur le bord de la carte). Pour transformer notre lac gelé en labyrinthe, nous devrons modifier deux méthodes : `done` et `is_valid_loc`.\n",
    "\n",
    "Tu trouveras ci-dessous les méthodes `done` et `step` que nous avons vues dans les diapositives ci-dessus. Modifie-les pour que nous ayons maintenant un labyrinthe avec le comportement décrit ci-dessus : marcher dans un mur ne fait rien.\n",
    "\n",
    "Note que la classe `Maze` hérite de toutes les autres méthodes de `FrozenPond`, tu peux donc la tester !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "238c70d1-4cd2-4cb0-ba58-3599ffa0879e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 0, True, {'player': (1, 1), 'goal': (3, 3)})\n",
      "(5, 0, True, {'player': (1, 1), 'goal': (3, 3)})\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE\n",
    "from envs_03 import FrozenPond\n",
    "\n",
    "\n",
    "class Maze(FrozenPond):\n",
    "    def done(self):\n",
    "        return self.player == self.goal or self.holes[self.player] == 1\n",
    "    def is_valid_loc(self, location):\n",
    "        if 0 <= location[0] <= 3 and 0 <= location[1] <= 3:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "pond = FrozenPond()\n",
    "pond.reset()\n",
    "pond.step(1)\n",
    "print(pond.step(2))\n",
    "\n",
    "maze = Maze()\n",
    "maze.reset()\n",
    "maze.step(1)\n",
    "print(maze.step(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a483f55-b956-44c1-bf8e-d534e6ad3ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 0, True, {'player': (1, 1), 'goal': (3, 3)})\n",
      "(4, 0, False, {'player': (1, 0), 'goal': (3, 3)})\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "from envs_03 import FrozenPond\n",
    "\n",
    "\n",
    "class Maze(FrozenPond):   \n",
    "    def done(self):\n",
    "        return self.player == self.goal\n",
    "    def is_valid_loc(self, location):\n",
    "        if 0 <= location[0] <= 3 and 0 <= location[1] <= 3 and not self.holes[location]:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "pond = FrozenPond()\n",
    "pond.reset()\n",
    "pond.step(1)\n",
    "print(pond.step(2))\n",
    "\n",
    "maze = Maze()\n",
    "maze.reset()\n",
    "maze.step(1)\n",
    "print(maze.step(2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ray200]",
   "language": "python",
   "name": "conda-env-ray200-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
