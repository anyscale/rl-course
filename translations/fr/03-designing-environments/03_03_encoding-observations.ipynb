{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e23df55-837b-4ec9-86ad-3b60c6a7f00d",
   "metadata": {},
   "source": [
    "## Encodage des observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58bc4b75-8b99-4450-9b25-6af748253913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import ray\n",
    "import logging\n",
    "ray.init(log_to_driver=False, ignore_reinit_error=True, logging_level=logging.ERROR); # logging.FATAL\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95a9faf-9b2c-45ec-a347-3aadca399516",
   "metadata": {},
   "source": [
    "#### R√©vision : qu'est-ce qu'une politique ?\n",
    "\n",
    "- Dans RL, nous essayons d'apprendre une politique, qu'est-ce que c'est d√©j√†, exactement ?\n",
    "- Une politique fait correspondre des **observations** √† des **actions**.\n",
    "- En d'autres termes, les observations sont tout ce que la politique \"voit\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacebed5-729c-4ea8-8570-3ac70481723b",
   "metadata": {},
   "source": [
    "#### Politiques du lac al√©atoire\n",
    "\n",
    "- Quelles sont les observations dans le lac al√©atoire ?\n",
    "- Elles sont l'emplacement du joueur, repr√©sent√© par un nombre entier de 0 √† 15  \n",
    "- En guise de rappel du module 1, une politique d√©terministe pourrait ressembler √† ceci :\n",
    "\n",
    "| Observation | Action |\n",
    "|------|-------|\n",
    "| 0 | 0 |\n",
    "| 1 | 3 |\n",
    "| 2 | 1 |\n",
    "| 3 | 1 |\n",
    "| ... | ... |\n",
    "| 14 | 2 |\n",
    "| 15 | 2 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0d4278-ce03-4e94-9f2a-667b88bf1e85",
   "metadata": {},
   "source": [
    "#### Politiques du lac al√©atoire\n",
    "\n",
    "Une politique non d√©terministe pourrait ressembler √† ceci :\n",
    "\n",
    "| Observation | P(gauche) | P(bas) | P(droite) | P(haut) | P(haut) | \n",
    "|------------|-------|-----------|---------|-------|\n",
    "| 0 | 0 | 0.9 | 0.01 | 0.04 | 0.05\n",
    "| 1 | 3 | 0.05 | 0.05 | 0.05 | 0.85\n",
    "| ... | ... | ... | ...      | ...      | ...\n",
    "| 15 | 2 | 0.0 | 0.0 | 0.99 | 0.01\n",
    "\n",
    "Cela ne signifie pas que RLlib apprend un tel tableau, d'ailleurs, mais nous pouvons penser √† ce tableau de mani√®re conceptuelle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b044dbcb-f7a2-4792-aaf2-2c72bda18af2",
   "metadata": {},
   "source": [
    "#### Politiques du lac al√©atoire\n",
    "\n",
    "- Dans le lac al√©atoire, toute notre d√©cision doit √™tre bas√©e sur la position du joueur.\n",
    "- Parfois, cela suffit : depuis la position 11, tu devrais descendre.\n",
    "\n",
    "```\n",
    " 0 1 2 3\n",
    " 4 5 6 7\n",
    " 8 9 10 11\n",
    "12 13 14 15\n",
    "```\n",
    "\n",
    "- Mais qu'en est-il de la position 5, que dois-tu faire √† partir de l√† ?\n",
    "- R√©ponse : _Cela d√©pend_. S'il y a un trou √† la position 9, tu ne veux pas descendre. De m√™me pour la position 6 \n",
    "- Comment puis-je d√©cider _sans savoir o√π sont les trous_ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fac00c2-6d9b-442f-ad3c-f9212a0dffac",
   "metadata": {},
   "source": [
    "#### State vs. observation, un r√©capitulatif\n",
    "\n",
    "- Dans le module 1, nous avons d√©fini l'√©tat de mani√®re informelle comme tout ce qui concerne l'environnement.\n",
    "- Ici, cela comprendrait l'emplacement du joueur et des trous.\n",
    "- L'observation, quant √† elle, ne code qu'une partie de l'√©tat : dans ce cas, l'emplacement du joueur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbe12b1-eded-4618-a5dc-77dfd9fb75a0",
   "metadata": {},
   "source": [
    "#### Observation = √©tat ? Probl√®me 1.\n",
    "\n",
    "- OK alors, pourquoi ne pas simplement d√©finir l'observation sur l'√©tat ? \n",
    "- Il y a deux probl√®mes ici.\n",
    "- Probl√®me 1 : Lorsque le syst√®me RL est d√©ploy√©, il se peut que tu ne connaisses pas tout l'√©tat.\n",
    "  - Exemple : dans un syst√®me de recommandation, l'agent (le recommandeur) n'a pas acc√®s √† l'humeur de l'utilisateur (une partie de l'√©tat qui affecte les r√©sultats)\n",
    "  - Dans l'apprentissage supervis√©, nous ne voulons pas nous entra√Æner sur des caract√©ristiques auxquelles nous n'aurons pas acc√®s lors du d√©ploiement\n",
    "    - De m√™me ici, l'observation doit √™tre quelque chose √† laquelle nous pouvons acc√©der lors du d√©ploiement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617197ea-fcfe-457e-a567-a1f876b0bb86",
   "metadata": {},
   "source": [
    "#### Observation = √©tat ? Probl√®me 2.\n",
    "\n",
    "- Probl√®me 2 : Il peut √™tre difficile de g√©n√©raliser √† partir d'une observation vraiment complexe.\n",
    "  - Il y a des centaines de milliers d'√©tats possibles dans ce seul petit jeu de lac al√©atoire 4x4.\n",
    "  - Trop d'informations pourraient √™tre d√©routantes pour l'agent ou n√©cessiter des quantit√©s d√©raisonnables de donn√©es (simulations) pour avoir du sens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6e363c-9255-442c-a598-1d33199b317b",
   "metadata": {},
   "source": [
    "#### Observations sur le codage\n",
    "\n",
    "- Une partie de notre travail en tant que praticien du RL consiste √† choisir une repr√©sentation (ou codage) pour l'observation.\n",
    "- √Ä partir des informations que le joueur a permis de conna√Ætre, trouve une repr√©sentation utile de ce que le joueur doit savoir.\n",
    "- Dans notre cas, nous allons essayer une approche : le joueur a le droit de \"voir\" si les 4 espaces adjacents sont des trous ou non.\n",
    "- Nous coderons cela sous forme de 4 nombres binaires."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb9fb21-9ad8-4800-ab56-fc70eec7b5ec",
   "metadata": {},
   "source": [
    "#### Observations sur le codage\n",
    "\n",
    "```\n",
    "oO.\n",
    "....\n",
    "O.P.\n",
    "...G\n",
    "```\n",
    "\n",
    "- Dans cette situation, il n'y a pas de trous autour du joueur, donc le joueur \"voit\" `[0 0 0 0]` \n",
    "- En d'autres termes, l'observation ici est `[0 0 0 0]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a2fb46-4e64-47d7-9a3c-2af76319760c",
   "metadata": {},
   "source": [
    "#### Observations sur le codage\n",
    "\n",
    "```\n",
    ".OO.\n",
    "..P.\n",
    "O.O.\n",
    "...G\n",
    "```\n",
    "\n",
    "- Ici, le joueur \"voit\" les trous en haut et en bas, donc l'observation est `[0 1 0 1]` (gauche, bas, droite, haut)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114c2ad7-dc3d-45c8-9f59-46e55fd87e07",
   "metadata": {},
   "source": [
    "#### Observations sur le codage\n",
    "\n",
    "Qu'en est-il des bords ?\n",
    "\n",
    "```\n",
    "....\n",
    "..OP\n",
    "O.OO\n",
    "...G\n",
    "```\n",
    "\n",
    "- C'est notre choix lorsque nous concevons l'espace d'observation.\n",
    "- Je vais choisir de repr√©senter \"hors r√©seau\" comme des trous, ce qui signifie que nous pr√©tendons que le lac ressemble √† ceci :\n",
    " \n",
    "```\n",
    "OOOOOO\n",
    "O....O\n",
    "O..OPO\n",
    "OO.OOO\n",
    "O...GO\n",
    "OOOOOO\n",
    "```\n",
    "\n",
    "- Ici, le joueur voit des trous √† gauche, en bas et √† droite, donc l'observation est `[1 1 1 0]` (gauche, bas, droite, haut)\n",
    "- Il pourrait cependant y avoir de meilleures approches, car tomber dans un trou est pire (l'√©pisode se termine) que marcher sur le bord (il ne se passe rien)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415a2223-64cb-43ca-befb-2269d33f9639",
   "metadata": {},
   "source": [
    "#### Coder nos observations\n",
    "\n",
    "- Maintenant que nous avons un plan, comment modifier le code ?\n",
    "- Puisque nous avons structur√© notre classe pour avoir une m√©thode `observation`, c'est tout ce que nous devons modifier :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6046cff0-b83f-45b8-96ca-ab092b3c8599",
   "metadata": {},
   "outputs": [],
   "source": [
    "from envs_03 import RandomLake\n",
    "\n",
    "class RandomLakeObs(RandomLake):\n",
    "    def observation(self):\n",
    "        i, j = self.player\n",
    "\n",
    "        obs = []\n",
    "        obs.append(1 if j==0 else self.holes[i,j-1]) # left\n",
    "        obs.append(1 if i==3 else self.holes[i+1,j]) # down\n",
    "        obs.append(1 if j==3 else self.holes[i,j+1]) # right\n",
    "        obs.append(1 if i==0 else self.holes[i-1,j]) # up\n",
    "        \n",
    "        obs = np.array(obs, dtype=int) # cast to numpy array (optional)\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ee30d8-f9f9-4655-817d-84aebd8ca444",
   "metadata": {},
   "source": [
    "- Le code cr√©e une variable `obs` o√π chaque entr√©e vaut 1 si cette direction m√®ne au bord **ou** un trou est pr√©sent √† cet endroit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88f95238-ed09-4582-ab28-b9a85b48d992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53303bf5-5453-4798-92f3-618bb2774151",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Coder nos observations\n",
    "\n",
    "- Un autre changement de code est n√©cessaire, il s'agit du constructeur o√π l'espace d'observation est d√©fini.\n",
    "- Nos observations √©taient auparavant un nombre entier de 0 √† 15, nous avons donc utilis√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0b2ebd3-1775-4bdf-9c93-c5af274dbc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space = gym.spaces.Discrete(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459308c9-a85a-4fcb-a8da-02a91401113f",
   "metadata": {},
   "source": [
    "Et de m√™me pour les actions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3cc9d30-715e-40a6-baa4-03496e91f9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = gym.spaces.Discrete(4)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53434076-3bb8-4fd1-9e8e-e23eb2be2808",
   "metadata": {},
   "source": [
    "- Cependant, nos observations sont maintenant des tableaux de 4 nombres plut√¥t qu'un seul nombre.\n",
    "- Pour indiquer cela, nous utilisons `gym.spaces.MultiDiscrete` au lieu de `gym.spaces.Discrete`.\n",
    "- Multi, car nous avons plusieurs nombres, mais toujours discret, car chacun des 4 nombres ne peut prendre que 2 valeurs possibles (0 ou 1).\n",
    "- Voici le code :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b119b15e-4d45-4087-8895-0ef3c694f62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomLakeObs(RandomLake):\n",
    "    def __init__(self, env_config=None):\n",
    "        self.observation_space = gym.spaces.MultiDiscrete([2,2,2,2])\n",
    "        self.action_space = gym.spaces.Discrete(4)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47022e95-090c-494b-955e-7e4ccadf088e",
   "metadata": {},
   "source": [
    "(Note que `gym` poss√®de √©galement un type d'espace `MultiBinary`, mais celui-ci n'est actuellement pas pris en charge par RLlib)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f5ebb5-6a8a-448e-a692-335ae56f4377",
   "metadata": {},
   "source": [
    "#### Tester notre nouvel env\n",
    "\n",
    "Testons-le !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "466d94ac-5b1b-4ce5-96ec-2b99b6849c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afccc03d-e0f8-4b8b-9be7-d13aad969084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from envs_03 import RandomLakeObs\n",
    "\n",
    "env = RandomLakeObs()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de7de150-479c-4a5c-98cf-4adf0abd378b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßëüßäüßäüßä\n",
      "üï≥üï≥üï≥üßä\n",
      "üßäüßäüï≥üßä\n",
      "üßäüßäüï≥‚õ≥Ô∏è\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b6f296-1805-4467-8b34-fd61ebcdd732",
   "metadata": {},
   "source": [
    "Ici, nous voyons l'observation attendue indiquant des \"trous\" √† gauche, en bas et en haut.\n",
    "\n",
    "Notes \n",
    "\n",
    "La gauche et le haut sont les bords de la carte, et le bas est un trou r√©el."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef2d1ac-4700-4dee-8aeb-23d09f18414f",
   "metadata": {},
   "source": [
    "#### Tester notre nouvel env\n",
    "\n",
    "Essayons de faire un pas √† droite :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db75ac7f-c4db-4e2e-8dbb-734fb0ff4ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 0, 1]), 0, False, {'player': (0, 1), 'goal': (3, 3)})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24287850-3c51-469c-86cd-17f2faa8cf3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßäüßëüßäüßä\n",
      "üï≥üï≥üï≥üßä\n",
      "üßäüßäüï≥üßä\n",
      "üßäüßäüï≥‚õ≥Ô∏è\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bac1dc-f61f-41c5-85d5-095f880079e8",
   "metadata": {},
   "source": [
    "Nous voyons maintenant des trous dans les directions descendante et ascendante, comme pr√©vu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3483468e-5524-4e8b-912e-ed2ab4e5a1ba",
   "metadata": {},
   "source": [
    "#### Formation avec nos nouvelles observations\n",
    "\n",
    "- Nos nouvelles observations semblent fonctionner, mais aident-elles l'agent √† apprendre ?\n",
    "- Rappelle-toi qu'avec notre espace d'observation `Discret(16)` nous n'avons pas pu obtenir beaucoup plus qu'un taux de r√©ussite de 30%.\n",
    "- Essayons √† nouveau :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83267540-3d6c-43a1-a834-9fb258d0dd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "from utils_03 import lake_default_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "232e7a87-3c60-4ef3-91eb-e3b26db1e257",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo = lake_default_config.build(env=RandomLakeObs)\n",
    "\n",
    "for i in range(8):\n",
    "    ppo.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea82a7de-3556-4f7a-9e78-78ef708e3b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6420454545454546"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo.evaluate()[\"evaluation\"][\"episode_reward_mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c7214d-f48c-4f13-8bb1-a6de0fb92411",
   "metadata": {},
   "source": [
    "- C'est bien mieux que les ~30% que nous obtenions avant !\n",
    "- Ce qui est logique... notre agent peut \"voir\" les trous maintenant, au lieu de marcher √† l'aveuglette."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86425828-0bef-4b47-ad0e-29d8afaf465d",
   "metadata": {},
   "source": [
    "#### Appliquons ce que nous avons appris !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4842178d-aa77-46b2-9da3-7deaea3d40f9",
   "metadata": {},
   "source": [
    "## Analogie de l'apprentissage supervis√© : espace d'observation\n",
    "<!-- multiple choice -->\n",
    "\n",
    "Dans les diapositives, nous avons modifi√© l'espace d'observation de notre agent et, par cons√©quent, obtenu de meilleures r√©compenses. √Ä quel aspect du processus d'apprentissage supervis√© cela ressemble-t-il le plus ?\n",
    "\n",
    "- [L'ing√©nierie des caract√©ristiques | Tu as trouv√© ! Notre espace d'observation sert d'espace de caract√©ristiques sur lequel notre politique doit agir.\n",
    "- [ ] S√©lection de mod√®les | Pas tout √† fait. Mais, comme nous le verrons, la s√©lection de mod√®les a aussi sa place dans l'apprentissage automatique !\n",
    "- [R√©glage des hyperparam√®tres | Pas tout √† fait. Mais, comme nous le verrons, l'ajustement des hyperparam√®tres a √©galement sa place dans RL !\n",
    "- [ ] S√©lection d'une fonction de perte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6438bf5-8970-4b51-a1b4-011f067d8903",
   "metadata": {},
   "source": [
    "## Incluant l'emplacement du joueur\n",
    "<!-- multiple choice -->\n",
    "\n",
    "Dans notre nouvelle repr√©sentation de l'observation, nous avons en fait _supprim√©_ l'emplacement du joueur de l'observation et _seulement_ inclus la pr√©sence des trous √† proximit√©. Si nous voulions un espace d'observation qui inclut √† la fois les murs proches _et_ l'emplacement du joueur, lequel des espaces de gymnastique suivants pourrions-nous utiliser ?\n",
    "\n",
    "- [ ] `gym.spaces.Discrete(5)` | Essaie encore !\n",
    "- [ ] `gym.spaces.MultiDiscrete([2,2,2,2,16])` | Oui ! Les 4 premiers chiffres repr√©sentent les trous, et le dernier chiffre repr√©sente l'emplacement du joueur.\n",
    "- [ ] `gym.spaces.MultiDiscrete([2,2,2,2]) + gym.spaces.Discrete(16)` | R√©essaie ; malheureusement, nous ne pouvons pas ajouter d'espaces de gym.\n",
    "- [ ] `gym.spaces.MultiDiscrete([32,32,32,32])` | On pourrait faire en sorte que cela fonctionne, mais c'est une repr√©sentation confuse/redondante."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03e0f1b-e397-467a-8f62-8537173201c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Manipuler les bords\n",
    "<!-- multiple choice -->\n",
    "\n",
    "Dans les diapositives, nous avons d√©cid√© de traiter les bords comme des trous. Rappelle-toi cette image :\n",
    "\n",
    "```\n",
    "OOOOOO\n",
    "O....O\n",
    "O..OPO\n",
    "OO.OOO\n",
    "O...GO\n",
    "OOOOOO\n",
    "```\n",
    "\n",
    "Cependant, les bords et les trous sont en fait diff√©rents les uns des autres : marcher dans un bord ne fait rien, alors que marcher dans un trou provoque la fin de l'√©pisode. Cette distinction pourrait √™tre importante, surtout dans une version \"glissante\" de l'environnement o√π les r√©sultats des actions sont non d√©terministes \n",
    "\n",
    "Pour r√©soudre ce probl√®me, nous d√©cidons de modifier l'espace d'observation. L'agent ne \"voit\" toujours que les quatre carr√©s qui l'entourent, mais il voit maintenant si chaque carr√© est un espace vide, un trou ou un bord. Pour cette repr√©sentation, lequel des espaces d'observation des gymnases suivants pourrions-nous utiliser ?\n",
    "\n",
    "- [ ] `gym.spaces.MultiDiscrete([2,2,2,2,2,2,2,2,2])` | Essaie encore. Rappelle-toi que l'agent ne \"voit\" toujours que 4 carr√©s.\n",
    "- [ ] `gym.spaces.MultiDiscrete([3,3,3,3,3,3,3,3,3,3])` | Essaie encore !\n",
    "- [ ] `gym.spaces.MultiDiscrete([2,2,2,2])` | C'est le m√™me espace que le pr√©c√©dent, mais nous avons fait un changement.\n",
    "- [x] `gym.spaces.MultiDiscrete([3,3,3,3])` | Tu as trouv√© ! Il y a maintenant 3 options possibles pour ce que l'agent peut \"voir\" √† chaque case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "485875e4-6694-4330-838c-1a54ed0499de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO / note to self\n",
    "# query_policy(trainer, RandomLakeObs(), [1,1,1,1])\n",
    "# shows that it wants to go up. this is because the above \"hole\" is probably an edge based on its learning. fascinating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c89bbb1-d5b4-41ef-bdd9-1f9ef08aa1eb",
   "metadata": {},
   "source": [
    "## Mise en ≈ìuvre des bords\n",
    "<!-- coding exercise -->\n",
    "\n",
    "Le code ci-dessous montre la fonction `observation` pour l'espace d'observation actuel. Modifie le code pour qu'il utilise le nouvel espace d'observation, o√π 0 repr√©sente un espace vide, 1 repr√©sente un trou et 2 repr√©sente un bord "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87cf9e9c-550c-43cb-8725-aa60d0cb91ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßëüßäüßäüßä\n",
      "üï≥üï≥üï≥üßä\n",
      "üßäüßäüï≥üßä\n",
      "üßäüßäüï≥‚õ≥Ô∏è\n",
      "[1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE\n",
    "\n",
    "from envs_03 import RandomLake\n",
    "\n",
    "class RandomLakeObs2(RandomLakeObs):\n",
    "    def observation(self):\n",
    "        i, j = self.player\n",
    "\n",
    "        obs = []\n",
    "        obs.append(1 if j==0 else self.holes[i,j-1]) # left\n",
    "        obs.append(1 if i==3 else self.holes[i+1,j]) # down\n",
    "        obs.append(1 if j==3 else self.holes[i,j+1]) # right\n",
    "        obs.append(1 if i==0 else self.holes[i-1,j]) # up\n",
    "        \n",
    "        obs = np.array(obs, dtype=int) # cast to numpy array\n",
    "        return obs\n",
    "\n",
    "np.random.seed(42)\n",
    "env = RandomLakeObs2()\n",
    "obs = env.reset()\n",
    "env.render()\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06328a38-f19c-4c71-82b6-31e056f28f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßëüßäüßäüßä\n",
      "üï≥üï≥üï≥üßä\n",
      "üßäüßäüï≥üßä\n",
      "üßäüßäüï≥‚õ≥Ô∏è\n",
      "[2 1 0 2]\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "\n",
    "from envs_03 import RandomLake\n",
    "\n",
    "class RandomLakeObs2(RandomLakeObs):\n",
    "    def observation(self):\n",
    "        i, j = self.player\n",
    "\n",
    "        obs = []\n",
    "        obs.append(2 if j==0 else self.holes[i,j-1]) # left\n",
    "        obs.append(2 if i==3 else self.holes[i+1,j]) # down\n",
    "        obs.append(2 if j==3 else self.holes[i,j+1]) # right\n",
    "        obs.append(2 if i==0 else self.holes[i-1,j]) # up\n",
    "        \n",
    "        obs = np.array(obs, dtype=int) # cast to numpy array\n",
    "        return obs\n",
    "\n",
    "np.random.seed(42)\n",
    "env = RandomLakeObs2()\n",
    "obs = env.reset()\n",
    "env.render()\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0f55b9-a660-4a04-8caf-52d13ced50e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Ce que l'agent voit\n",
    "<!-- coding exercise -->\n",
    "\n",
    "Avec notre nouveau codage des espaces d'observation, l'agent ne \"voit\" que les 4 espaces qui l'entourent et ne dispose que de ces informations pour prendre ses d√©cisions. La cellule de code ci-dessous cr√©e un rendu de ce que l'agent \"voit\" pendant qu'il navigue sur le lac al√©atoire. Tu peux entrer des actions avec le clavier en tapant les mots \"gauche\", \"bas\", \"droite\" ou \"haut\" (ou \"l\", \"d\", \"r\", \"u\" pour faire court) et la simulation te montrera le r√©sultat. (Tape \"quit\" pour sortir.) Joue le jeu jusqu'√† ce que tu atteignes l'objectif. Au fur et √† mesure, essaie de cartographier le lac (peut-√™tre en dessinant sur une feuille de papier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d5b24c4-0df7-47e3-8155-895ccbb9e5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO / NOTE:\n",
    "# THIS EXERCISE DOES NOT HAVE A \"solution\"\n",
    "# the code is here ONLY to help them answer the multiple choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8341e39f-8684-4106-94cc-a4a422f476b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation:\n",
      ".O.\n",
      "OP.\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE\n",
    "\n",
    "import numpy as np\n",
    "from envs_03 import RandomLakeObs\n",
    "\n",
    "actions = {\"left\" : 0, \"down\" : 1, \"right\" : 2, \"up\" : 3, \n",
    "           \"l\" : 0, \"d\" : 1, \"r\" : 2, \"u\" : 3}\n",
    "\n",
    "np.random.seed(45)\n",
    "env = RandomLakeObs()\n",
    "obs = env.reset()\n",
    "\n",
    "act = \"start\"\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "   \n",
    "    obs_print = [['.']*3 for i in range(3)]\n",
    "    obs_print[1][1] = \"P\"\n",
    "    if obs[0]:\n",
    "        obs_print[1][0] = \"O\"\n",
    "    if obs[1]:\n",
    "        obs_print[2][1] = \"O\"\n",
    "    if obs[2]:\n",
    "        obs_print[1][2] = \"O\"\n",
    "    if obs[3]:\n",
    "        obs_print[0][1] = \"O\"\n",
    "    print(\"Observation:\")\n",
    "    print(\"\\n\".join(list(map(lambda c: \"\".join(c), obs_print))))\n",
    "    print()\n",
    "    \n",
    "    while act != \"quit\" and act not in actions: \n",
    "        act = input() # gather keyboard input \n",
    "    \n",
    "    if act == \"quit\":\n",
    "        break\n",
    "        \n",
    "    obs, rew, done, _ = env.step(act)\n",
    "    \n",
    "if done:\n",
    "    if rew > 0:\n",
    "        print(\"You win! +1 reward üéâ\")\n",
    "    else:\n",
    "        print(\"You fell into the lake üò¢\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c8469b-8040-44db-82cd-174ca76b82c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "import numpy as np\n",
    "from envs_03 import RandomLakeObs\n",
    "\n",
    "actions = {\"left\" : 0, \"down\" : 1, \"right\" : 2, \"up\" : 3, \n",
    "           \"l\" : 0, \"d\" : 1, \"r\" : 2, \"u\" : 3}\n",
    "\n",
    "np.random.seed(45)\n",
    "env = RandomLakeObs()\n",
    "obs = env.reset()\n",
    "\n",
    "act = \"start\"\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "   \n",
    "    obs_print = [['.']*3 for i in range(3)]\n",
    "    obs_print[1][1] = \"P\"\n",
    "    if obs[0]:\n",
    "        obs_print[1][0] = \"O\"\n",
    "    if obs[1]:\n",
    "        obs_print[2][1] = \"O\"\n",
    "    if obs[2]:\n",
    "        obs_print[1][2] = \"O\"\n",
    "    if obs[3]:\n",
    "        obs_print[0][1] = \"O\"\n",
    "    print(\"Observation:\")\n",
    "    print(\"\\n\".join(list(map(lambda c: \"\".join(c), obs_print))))\n",
    "    print()\n",
    "    \n",
    "    while act != \"quit\" and act not in actions: \n",
    "        act = input() # gather keyboard input \n",
    "    \n",
    "    if act == \"quit\":\n",
    "        break\n",
    "        \n",
    "    obs, rew, done, _ = env.step(act)\n",
    "    \n",
    "if done:\n",
    "    if rew > 0:\n",
    "        print(\"You win! +1 reward üéâ\")\n",
    "    else:\n",
    "        print(\"You fell into the lake üò¢\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7024e752-4e71-4162-886e-c5be693c9ee5",
   "metadata": {},
   "source": [
    "#### √Ä quoi ressemble le lac ?\n",
    "\n",
    "D'apr√®s tes explorations, quelle est la carte correcte du lac dans la question ci-dessus ?\n",
    "\n",
    "```\n",
    " (A) (B) (C) (D)\n",
    "P..O P.OO P..O P.OO\n",
    "..OO .OOO ..OO .OOO\n",
    "O...     O...     O...     O..O\n",
    "...G ...G ..OG ...G\n",
    "```\n",
    "\n",
    "- [x] (A)\n",
    "- [ ] (B)\n",
    "- [ ] (C)\n",
    "- [ ] (D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbad4252-94ef-480a-938d-6e4858ed3675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# could also considering showing a BAD environment encoding to contrast with this reasonable one, as in the next slide deck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ray200]",
   "language": "python",
   "name": "conda-env-ray200-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
