{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e23df55-837b-4ec9-86ad-3b60c6a7f00d",
   "metadata": {},
   "source": [
    "## Observaciones de codificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58bc4b75-8b99-4450-9b25-6af748253913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import ray\n",
    "import logging\n",
    "ray.init(log_to_driver=False, ignore_reinit_error=True, logging_level=logging.ERROR); # logging.FATAL\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95a9faf-9b2c-45ec-a347-3aadca399516",
   "metadata": {},
   "source": [
    "#### Revisión: ¿qué es una política?\n",
    "\n",
    "- En RL intentamos aprender una política, ¿qué es esto exactamente?\n",
    "- Una política asigna **observaciones** a **acciones**.\n",
    "- En otras palabras, las observaciones son todo lo que la política \"ve\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacebed5-729c-4ea8-8570-3ac70481723b",
   "metadata": {},
   "source": [
    "#### Políticas del lago aleatorio\n",
    "\n",
    "- ¿Qué son las observaciones del lago aleatorio?\n",
    "- Son la ubicación del jugador, representada como un número entero de 0 a 15  \n",
    "- Como repaso del módulo 1, una política determinista podría tener este aspecto\n",
    "\n",
    "| Observación | Acción |\n",
    "|------|-------|\n",
    "| 0 | 0 |\n",
    "| 1 | 3 |\n",
    "| 2 | 1 |\n",
    "| 3 | 1 |\n",
    "| ... | ... |\n",
    "| 14 | 2 |\n",
    "| 15 | 2 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0d4278-ce03-4e94-9f2a-667b88bf1e85",
   "metadata": {},
   "source": [
    "#### Políticas del lago aleatorio\n",
    "\n",
    "Y una política no determinista podría tener el siguiente aspecto\n",
    "\n",
    "| Observación | P(izquierda) | P(abajo) | P(derecha) | P(arriba) \n",
    "|------------|-------|-----------|---------|-------|\n",
    "| 0 | 0 | 0.9 | 0.01 | 0.04 | 0.05\n",
    "| 1 | 3 | 0.05 | 0.05 | 0.05 | 0.85\n",
    "| ... | ... | ... | ...      | ...      | ...\n",
    "| 15 | 2 | 0.0 | 0.0 | 0.99 | 0.01\n",
    "\n",
    "Por cierto, esto no significa que RLlib aprenda esa tabla, pero podemos pensar en ella conceptualmente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b044dbcb-f7a2-4792-aaf2-2c72bda18af2",
   "metadata": {},
   "source": [
    "#### Políticas del lago aleatorio\n",
    "\n",
    "- En el lago aleatorio, toda nuestra decisión debe basarse en la posición del jugador.\n",
    "- A veces esto es suficiente: desde la posición 11, debes bajar.\n",
    "\n",
    "```\n",
    " 0 1 2 3\n",
    " 4 5 6 7\n",
    " 8 9 10 11\n",
    "12 13 14 15\n",
    "```\n",
    "\n",
    "- Pero ¿qué pasa con la posición 5, qué debes hacer a partir de ahí?\n",
    "- Respuesta: _depende_. Si hay un agujero en la posición 9, no querrás bajar. Lo mismo ocurre con la 6 \n",
    "- ¿Cómo puedo decidir _sin saber dónde están los agujeros_?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fac00c2-6d9b-442f-ad3c-f9212a0dffac",
   "metadata": {},
   "source": [
    "#### Estado contra observación, un resumen\n",
    "\n",
    "- En el Módulo 1, definimos informalmente el estado como todo lo relacionado con el entorno.\n",
    "- Aquí, eso incluiría la ubicación del jugador y los agujeros.\n",
    "- La observación, en cambio, sólo codifica una parte del estado: en este caso, la ubicación del jugador."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbe12b1-eded-4618-a5dc-77dfd9fb75a0",
   "metadata": {},
   "source": [
    "#### ¿Observación = estado? Problema 1.\n",
    "\n",
    "- Entonces, ¿por qué no establecer la observación en el estado? \n",
    "- Aquí hay dos problemas.\n",
    "- Problema 1: cuando se despliega el sistema de RL, es posible que no se conozca todo el estado.\n",
    "  - Ejemplo: en un sistema de recomendación, el agente (recomendador) no tiene acceso al estado de ánimo del usuario (parte del estado que afecta a los resultados)\n",
    "  - En el aprendizaje supervisado, no queremos entrenar sobre características a las que no tendremos acceso en el despliegue\n",
    "    - Del mismo modo, aquí la observación tiene que ser algo a lo que podamos acceder en el despliegue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617197ea-fcfe-457e-a567-a1f876b0bb86",
   "metadata": {},
   "source": [
    "#### ¿Observación = estado? Problema 2.\n",
    "\n",
    "- Problema 2: Puede ser difícil generalizar a partir de una observación realmente compleja.\n",
    "  - Hay cientos de miles de estados posibles sólo en este pequeño juego de lago aleatorio de 4x4.\n",
    "  - Demasiada información podría ser confusa para el agente o podría requerir cantidades irrazonables de datos (simulaciones) para darle sentido."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6e363c-9255-442c-a598-1d33199b317b",
   "metadata": {},
   "source": [
    "#### Observaciones sobre la codificación\n",
    "\n",
    "- Parte de nuestro trabajo como practicante de RL es elegir una representación (o codificación) para la observación.\n",
    "- A partir de la información que el jugador permitió conocer, encontrar una representación útil de lo que el jugador necesita saber.\n",
    "- En nuestro caso, probaremos una aproximación: el jugador consigue \"ver\" si los 4 espacios adyacentes son agujeros o no.\n",
    "- Codificaremos esto como 4 números binarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb9fb21-9ad8-4800-ab56-fc70eec7b5ec",
   "metadata": {},
   "source": [
    "#### Observaciones sobre la codificación\n",
    "\n",
    "```\n",
    ".OO.\n",
    "....\n",
    "O.P.\n",
    "...G\n",
    "```\n",
    "\n",
    "- En esta situación, no hay agujeros alrededor del jugador, por lo que el jugador \"ve\" `[0 0 0]` \n",
    "- En otras palabras, la observación aquí es `[0 0 0 0]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a2fb46-4e64-47d7-9a3c-2af76319760c",
   "metadata": {},
   "source": [
    "#### Observaciones sobre la codificación\n",
    "\n",
    "```\n",
    ".OO.\n",
    "..P.\n",
    "O.O.\n",
    "...G\n",
    "```\n",
    "\n",
    "- Aquí, el jugador \"ve\" agujeros arriba y abajo, por lo que la observación es `[0 1 0 1]` (izquierda, abajo, derecha, arriba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114c2ad7-dc3d-45c8-9f59-46e55fd87e07",
   "metadata": {},
   "source": [
    "#### Observaciones sobre la codificación\n",
    "\n",
    "¿Qué pasa con las aristas?\n",
    "\n",
    "```\n",
    "....\n",
    "..OP\n",
    "O.OO\n",
    "...G\n",
    "```\n",
    "\n",
    "- Esta es nuestra elección al diseñar el espacio de observación.\n",
    "- Elegiré representar \"fuera de la red\" como agujeros, lo que significa que fingimos que el lago tiene este aspecto:\n",
    " \n",
    "```\n",
    "OOOOOO\n",
    "O....O\n",
    "O..OPO\n",
    "OO.OOO\n",
    "O...GO\n",
    "OOOOOO\n",
    "```\n",
    "\n",
    "- Aquí, el jugador ve agujeros a la izquierda, abajo y derecha, por lo que la observación es `[1 1 1 0]` (izquierda, abajo, derecha, arriba)\n",
    "- Sin embargo, puede haber enfoques mejores, porque caer en un agujero es peor (el episodio termina) que caminar por el borde (no pasa nada)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415a2223-64cb-43ca-befb-2269d33f9639",
   "metadata": {},
   "source": [
    "#### Codificar nuestras observaciones\n",
    "\n",
    "- Ahora que tenemos un plan, ¿cómo modificamos el código?\n",
    "- Como hemos estructurado nuestra clase para que tenga un método de `observación`, eso es todo lo que tenemos que modificar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6046cff0-b83f-45b8-96ca-ab092b3c8599",
   "metadata": {},
   "outputs": [],
   "source": [
    "from envs_03 import RandomLake\n",
    "\n",
    "class RandomLakeObs(RandomLake):\n",
    "    def observation(self):\n",
    "        i, j = self.player\n",
    "\n",
    "        obs = []\n",
    "        obs.append(1 if j==0 else self.holes[i,j-1]) # left\n",
    "        obs.append(1 if i==3 else self.holes[i+1,j]) # down\n",
    "        obs.append(1 if j==3 else self.holes[i,j+1]) # right\n",
    "        obs.append(1 if i==0 else self.holes[i-1,j]) # up\n",
    "        \n",
    "        obs = np.array(obs, dtype=int) # cast to numpy array (optional)\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ee30d8-f9f9-4655-817d-84aebd8ca444",
   "metadata": {},
   "source": [
    "- El código crea una variable `obs` en la que cada entrada es 1 si esa dirección sale de la arista **o** hay un agujero en ella."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88f95238-ed09-4582-ab28-b9a85b48d992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53303bf5-5453-4798-92f3-618bb2774151",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Codificar nuestras observaciones\n",
    "\n",
    "- Es necesario un cambio más en el código, que es el constructor donde se define el espacio de observación.\n",
    "- Nuestras observaciones eran antes un número entero de 0 a 15, por lo que utilizamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0b2ebd3-1775-4bdf-9c93-c5af274dbc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space = gym.spaces.Discrete(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459308c9-a85a-4fcb-a8da-02a91401113f",
   "metadata": {},
   "source": [
    "Y lo mismo para las acciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3cc9d30-715e-40a6-baa4-03496e91f9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = gym.spaces.Discrete(4)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53434076-3bb8-4fd1-9e8e-e23eb2be2808",
   "metadata": {},
   "source": [
    "- Sin embargo, nuestras observaciones son ahora matrices de 4 números en lugar de un único número.\n",
    "- Para indicarlo, utilizamos `gym.spaces.MultiDiscrete` en lugar de `gym.spaces.Discrete`.\n",
    "- Multi, porque tenemos varios números, pero sigue siendo discreto, porque cada uno de los 4 números sólo puede tomar 2 valores posibles (0 ó 1).\n",
    "- Este es el código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b119b15e-4d45-4087-8895-0ef3c694f62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomLakeObs(RandomLake):\n",
    "    def __init__(self, env_config=None):\n",
    "        self.observation_space = gym.spaces.MultiDiscrete([2,2,2,2])\n",
    "        self.action_space = gym.spaces.Discrete(4)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47022e95-090c-494b-955e-7e4ccadf088e",
   "metadata": {},
   "source": [
    "(Ten en cuenta que `gym` también tiene un tipo de espacio `MultiBinario`, pero actualmente no es compatible con RLlib)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f5ebb5-6a8a-448e-a692-335ae56f4377",
   "metadata": {},
   "source": [
    "#### Probando nuestro nuevo entorno\n",
    "\n",
    "¡Vamos a probarlo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "466d94ac-5b1b-4ce5-96ec-2b99b6849c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afccc03d-e0f8-4b8b-9be7-d13aad969084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from envs_03 import RandomLakeObs\n",
    "\n",
    "env = RandomLakeObs()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de7de150-479c-4a5c-98cf-4adf0abd378b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧑🧊🧊🧊\n",
      "🕳🕳🕳🧊\n",
      "🧊🧊🕳🧊\n",
      "🧊🧊🕳⛳️\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b6f296-1805-4467-8b34-fd61ebcdd732",
   "metadata": {},
   "source": [
    "Aquí, vemos la observación esperada que indica \"agujeros\" a la izquierda, abajo y arriba.\n",
    "\n",
    "Notas \n",
    "\n",
    "La izquierda y el arriba son los bordes del mapa, y el abajo es un agujero real."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef2d1ac-4700-4dee-8aeb-23d09f18414f",
   "metadata": {},
   "source": [
    "#### Probando nuestro nuevo entorno\n",
    "\n",
    "Intentemos dar un paso a la derecha:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db75ac7f-c4db-4e2e-8dbb-734fb0ff4ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 0, 1]), 0, False, {'player': (0, 1), 'goal': (3, 3)})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24287850-3c51-469c-86cd-17f2faa8cf3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧊🧑🧊🧊\n",
      "🕳🕳🕳🧊\n",
      "🧊🧊🕳🧊\n",
      "🧊🧊🕳⛳️\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bac1dc-f61f-41c5-85d5-095f880079e8",
   "metadata": {},
   "source": [
    "Ahora vemos agujeros en las direcciones descendente y ascendente, de nuevo como se esperaba."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3483468e-5524-4e8b-912e-ed2ab4e5a1ba",
   "metadata": {},
   "source": [
    "#### Formación con nuestras nuevas observaciones\n",
    "\n",
    "- Nuestras nuevas observaciones parecen funcionar, pero ¿ayudan al agente a aprender?\n",
    "- Recordemos que con nuestro espacio de observación `Discreto(16)` no pudimos conseguir mucho más que un 30% de éxito.\n",
    "- Intentémoslo de nuevo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83267540-3d6c-43a1-a834-9fb258d0dd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "from utils_03 import lake_default_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "232e7a87-3c60-4ef3-91eb-e3b26db1e257",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo = lake_default_config.build(env=RandomLakeObs)\n",
    "\n",
    "for i in range(8):\n",
    "    ppo.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea82a7de-3556-4f7a-9e78-78ef708e3b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6420454545454546"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo.evaluate()[\"evaluation\"][\"episode_reward_mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c7214d-f48c-4f13-8bb1-a6de0fb92411",
   "metadata": {},
   "source": [
    "- ¡Esto es mucho mejor que el ~30% que obteníamos antes!\n",
    "- Lo cual tiene sentido... nuestro agente puede \"ver\" los agujeros ahora, en lugar de caminar a ciegas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86425828-0bef-4b47-ad0e-29d8afaf465d",
   "metadata": {},
   "source": [
    "#### ¡Apliquemos lo que hemos aprendido!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4842178d-aa77-46b2-9da3-7deaea3d40f9",
   "metadata": {},
   "source": [
    "## Analogía del aprendizaje supervisado: espacio de observación\n",
    "<!-- multiple choice -->\n",
    "\n",
    "En las diapositivas hemos cambiado el espacio de observación de nuestro agente y, como resultado, hemos conseguido mayores recompensas. ¿A qué aspecto del proceso de aprendizaje supervisado es más análogo?\n",
    "\n",
    "- [x] Ingeniería de características | ¡Ya lo tienes! Nuestro espacio de observación actúa como el espacio de características sobre el que actúa nuestra política.\n",
    "- [ ] Selección de modelos | No del todo. Pero, como veremos, ¡la selección de modelos también tiene cabida en la RL!\n",
    "- [Ajuste de los hiperparámetros: no es así. Pero, como veremos, ¡el ajuste de hiperparámetros también tiene cabida en la RL!\n",
    "- [ ] Selección de una función de pérdida"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6438bf5-8970-4b51-a1b4-011f067d8903",
   "metadata": {},
   "source": [
    "## Incluyendo la ubicación del jugador\n",
    "<!-- multiple choice -->\n",
    "\n",
    "En nuestra nueva representación de la observación, en realidad _eliminamos_ la ubicación del jugador de la observación y _sólo_ incluimos la presencia de los agujeros cercanos. Si quisiéramos un espacio de observación que incluyera tanto las paredes cercanas _como_ la ubicación del jugador, ¿cuál de los siguientes espacios de gimnasio podríamos utilizar?\n",
    "\n",
    "- [ ] `gimnasio.espacios.Discreto(5)` | ¡Inténtalo de nuevo!\n",
    "- [x] `gimnasio.espacios.MultiDiscreto([2,2,2,2,16])` | ¡Sí! Los 4 primeros números representan los agujeros, y el último número representa la ubicación del jugador.\n",
    "- [ ] `gimnasio.espacios.MultiDiscreto([2,2,2,2]) + gimnasio.espacios.Discreto(16)` | Inténtalo de nuevo; lamentablemente no podemos añadir espacios de gimnasio.\n",
    "- [ ] `gimnasio.espacios.MultiDiscreto([32,32,32,32])` | Esto podría funcionar, pero es una representación confusa/redundante."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03e0f1b-e397-467a-8f62-8537173201c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Manejo de los bordes\n",
    "<!-- multiple choice -->\n",
    "\n",
    "En las diapositivas hemos decidido tratar las aristas como agujeros. Recuerda esta imagen:\n",
    "\n",
    "```\n",
    "OOOOOO\n",
    "O....O\n",
    "O..OPO\n",
    "OO.OOO\n",
    "O...GO\n",
    "OOOOOO\n",
    "```\n",
    "\n",
    "Sin embargo, los bordes y los agujeros son en realidad diferentes entre sí: entrar en un borde no hace nada, mientras que entrar en un agujero hace que el episodio termine. Ésta podría ser una distinción importante, especialmente en una versión \"resbaladiza\" del entorno, en la que los resultados de las acciones no son deterministas \n",
    "\n",
    "Para abordar esta cuestión, decidimos cambiar el espacio de observación. El agente sigue \"viendo\" sólo los cuatro cuadrados que le rodean, pero ahora ve si cada cuadrado es un espacio vacío, un agujero o un borde. Para esta representación, ¿cuál de los siguientes espacios de observación del gimnasio podríamos utilizar?\n",
    "\n",
    "- [ ] `gimnasio.espacios.MultiDiscreto([2,2,2,2,2,2,2,2])` | Inténtalo de nuevo. Recuerda que el agente sigue \"viendo\" sólo 4 casillas.\n",
    "- [ ] `gym.spaces.MultiDiscrete([3,3,3,3,3,3,3,3])` | ¡Inténtalo de nuevo!\n",
    "- [ ] `gimnasio.espacios.MultiDiscreto([2,2,2,2])` | Esto es lo mismo que el espacio anterior, pero hemos hecho un cambio.\n",
    "- [x] `gimnasio.espacios.MultiDiscreto([3,3,3,3])` | ¡Ya lo tienes! Ahora hay 3 opciones posibles para lo que el agente puede \"ver\" en cada casilla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "485875e4-6694-4330-838c-1a54ed0499de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO / note to self\n",
    "# query_policy(trainer, RandomLakeObs(), [1,1,1,1])\n",
    "# shows that it wants to go up. this is because the above \"hole\" is probably an edge based on its learning. fascinating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c89bbb1-d5b4-41ef-bdd9-1f9ef08aa1eb",
   "metadata": {},
   "source": [
    "## Implementación de los bordes\n",
    "<!-- coding exercise -->\n",
    "\n",
    "El código siguiente muestra la función \"observación\" para el espacio de observación actual. Modifica el código para que utilice el nuevo espacio de observación, en el que 0 representa un espacio vacío, 1 representa un agujero y 2 representa una arista "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87cf9e9c-550c-43cb-8725-aa60d0cb91ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧑🧊🧊🧊\n",
      "🕳🕳🕳🧊\n",
      "🧊🧊🕳🧊\n",
      "🧊🧊🕳⛳️\n",
      "[1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE\n",
    "\n",
    "from envs_03 import RandomLake\n",
    "\n",
    "class RandomLakeObs2(RandomLakeObs):\n",
    "    def observation(self):\n",
    "        i, j = self.player\n",
    "\n",
    "        obs = []\n",
    "        obs.append(1 if j==0 else self.holes[i,j-1]) # left\n",
    "        obs.append(1 if i==3 else self.holes[i+1,j]) # down\n",
    "        obs.append(1 if j==3 else self.holes[i,j+1]) # right\n",
    "        obs.append(1 if i==0 else self.holes[i-1,j]) # up\n",
    "        \n",
    "        obs = np.array(obs, dtype=int) # cast to numpy array\n",
    "        return obs\n",
    "\n",
    "np.random.seed(42)\n",
    "env = RandomLakeObs2()\n",
    "obs = env.reset()\n",
    "env.render()\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06328a38-f19c-4c71-82b6-31e056f28f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧑🧊🧊🧊\n",
      "🕳🕳🕳🧊\n",
      "🧊🧊🕳🧊\n",
      "🧊🧊🕳⛳️\n",
      "[2 1 0 2]\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "\n",
    "from envs_03 import RandomLake\n",
    "\n",
    "class RandomLakeObs2(RandomLakeObs):\n",
    "    def observation(self):\n",
    "        i, j = self.player\n",
    "\n",
    "        obs = []\n",
    "        obs.append(2 if j==0 else self.holes[i,j-1]) # left\n",
    "        obs.append(2 if i==3 else self.holes[i+1,j]) # down\n",
    "        obs.append(2 if j==3 else self.holes[i,j+1]) # right\n",
    "        obs.append(2 if i==0 else self.holes[i-1,j]) # up\n",
    "        \n",
    "        obs = np.array(obs, dtype=int) # cast to numpy array\n",
    "        return obs\n",
    "\n",
    "np.random.seed(42)\n",
    "env = RandomLakeObs2()\n",
    "obs = env.reset()\n",
    "env.render()\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0f55b9-a660-4a04-8caf-52d13ced50e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Lo que ve el agente\n",
    "<!-- coding exercise -->\n",
    "\n",
    "Con nuestra nueva codificación del espacio de observación, el agente sólo \"ve\" los 4 espacios que le rodean y sólo dispone de esta información para tomar sus decisiones. La celda de código siguiente crea una representación de lo que el agente \"ve\" mientras navega por el lago aleatorio. Puedes introducir acciones con el teclado escribiendo las palabras \"izquierda\", \"abajo\", \"derecha\" o \"arriba\" (o \"l\", \"d\", \"r\", \"u\" para abreviar) y la simulación te mostrará el resultado. (Escribe \"quit\" para salir.) Juega hasta que llegues a la meta. A medida que avanzas, intenta trazar un mapa del lago (quizás dibujando en un papel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d5b24c4-0df7-47e3-8155-895ccbb9e5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO / NOTE:\n",
    "# THIS EXERCISE DOES NOT HAVE A \"solution\"\n",
    "# the code is here ONLY to help them answer the multiple choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8341e39f-8684-4106-94cc-a4a422f476b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation:\n",
      ".O.\n",
      "OP.\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE\n",
    "\n",
    "import numpy as np\n",
    "from envs_03 import RandomLakeObs\n",
    "\n",
    "actions = {\"left\" : 0, \"down\" : 1, \"right\" : 2, \"up\" : 3, \n",
    "           \"l\" : 0, \"d\" : 1, \"r\" : 2, \"u\" : 3}\n",
    "\n",
    "np.random.seed(45)\n",
    "env = RandomLakeObs()\n",
    "obs = env.reset()\n",
    "\n",
    "act = \"start\"\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "   \n",
    "    obs_print = [['.']*3 for i in range(3)]\n",
    "    obs_print[1][1] = \"P\"\n",
    "    if obs[0]:\n",
    "        obs_print[1][0] = \"O\"\n",
    "    if obs[1]:\n",
    "        obs_print[2][1] = \"O\"\n",
    "    if obs[2]:\n",
    "        obs_print[1][2] = \"O\"\n",
    "    if obs[3]:\n",
    "        obs_print[0][1] = \"O\"\n",
    "    print(\"Observation:\")\n",
    "    print(\"\\n\".join(list(map(lambda c: \"\".join(c), obs_print))))\n",
    "    print()\n",
    "    \n",
    "    while act != \"quit\" and act not in actions: \n",
    "        act = input() # gather keyboard input \n",
    "    \n",
    "    if act == \"quit\":\n",
    "        break\n",
    "        \n",
    "    obs, rew, done, _ = env.step(act)\n",
    "    \n",
    "if done:\n",
    "    if rew > 0:\n",
    "        print(\"You win! +1 reward 🎉\")\n",
    "    else:\n",
    "        print(\"You fell into the lake 😢\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c8469b-8040-44db-82cd-174ca76b82c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "import numpy as np\n",
    "from envs_03 import RandomLakeObs\n",
    "\n",
    "actions = {\"left\" : 0, \"down\" : 1, \"right\" : 2, \"up\" : 3, \n",
    "           \"l\" : 0, \"d\" : 1, \"r\" : 2, \"u\" : 3}\n",
    "\n",
    "np.random.seed(45)\n",
    "env = RandomLakeObs()\n",
    "obs = env.reset()\n",
    "\n",
    "act = \"start\"\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "   \n",
    "    obs_print = [['.']*3 for i in range(3)]\n",
    "    obs_print[1][1] = \"P\"\n",
    "    if obs[0]:\n",
    "        obs_print[1][0] = \"O\"\n",
    "    if obs[1]:\n",
    "        obs_print[2][1] = \"O\"\n",
    "    if obs[2]:\n",
    "        obs_print[1][2] = \"O\"\n",
    "    if obs[3]:\n",
    "        obs_print[0][1] = \"O\"\n",
    "    print(\"Observation:\")\n",
    "    print(\"\\n\".join(list(map(lambda c: \"\".join(c), obs_print))))\n",
    "    print()\n",
    "    \n",
    "    while act != \"quit\" and act not in actions: \n",
    "        act = input() # gather keyboard input \n",
    "    \n",
    "    if act == \"quit\":\n",
    "        break\n",
    "        \n",
    "    obs, rew, done, _ = env.step(act)\n",
    "    \n",
    "if done:\n",
    "    if rew > 0:\n",
    "        print(\"You win! +1 reward 🎉\")\n",
    "    else:\n",
    "        print(\"You fell into the lake 😢\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7024e752-4e71-4162-886e-c5be693c9ee5",
   "metadata": {},
   "source": [
    "#### ¿Qué aspecto tiene el lago?\n",
    "\n",
    "Basándote en tus exploraciones, ¿cuál es el mapa correcto del lago en la pregunta anterior?\n",
    "\n",
    "```\n",
    " (A) (B) (C) (D)\n",
    "P..O P.OO P..O P.OO\n",
    "..OO .OOO ..OO .OOO\n",
    "O...     O...     O...     O..O\n",
    "...G ...G ..OG ...G\n",
    "```\n",
    "\n",
    "- [x] (A)\n",
    "- [ ] (B)\n",
    "- [ ] (C)\n",
    "- [ ] (D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbad4252-94ef-480a-938d-6e4858ed3675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# could also considering showing a BAD environment encoding to contrast with this reasonable one, as in the next slide deck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ray200]",
   "language": "python",
   "name": "conda-env-ray200-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
