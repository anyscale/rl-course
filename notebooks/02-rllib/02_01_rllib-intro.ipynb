{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90c01ab9-1cd0-47d8-a198-e07e36e2c1fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## RLlib intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e74417a3-63bc-4000-951a-72ab882b689e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fd0307b-9589-479a-85e9-0767e9f813af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import ray\n",
    "import logging\n",
    "ray.init(log_to_driver=False, ignore_reinit_error=True, logging_level=logging.ERROR); # logging.FATAL\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddd909a-c86f-4e5c-9ba0-8b0ef1152730",
   "metadata": {},
   "source": [
    "#### What about the learning?\n",
    "\n",
    "Let's return to the \"API\" of RL:\n",
    "\n",
    "![](img/RL-API.png)\n",
    "\n",
    "- We've talked about the input (environment) and output (policy)\n",
    "- Let's talk about the reinforcement learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7503593-2928-46b8-891a-913ff3fbc3c1",
   "metadata": {},
   "source": [
    "#### What we'll cover\n",
    "\n",
    "- Many, many supervised learning algorithms exist... random forests, logistic regression, neural networks, etc.\n",
    "- Likewise, there are many RL algorithms.\n",
    "- This is not a course on RL algorithms, though many good ones exist!\n",
    "- This course is about _applying_ RL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14840b72-5c0f-42a3-b29f-d9b14995d2c1",
   "metadata": {},
   "source": [
    "#### Introducing Ray RLlib\n",
    "\n",
    "![](img/rllib-logo.png)\n",
    "\n",
    "- In this course we'll use Ray RLlib as our \"scikit-learn of reinforcement learning\"\n",
    "- We will look under the hood only as needed, and focus on the inputs and outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251a0d49-fad7-4620-bc2f-71ab58d228f9",
   "metadata": {},
   "source": [
    "#### Our first RLlib code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497d2332-bdf4-43c2-ba76-c5e3c4d37afc",
   "metadata": {},
   "source": [
    "- First, we import RLlib\n",
    "- RLlib is part of the Ray project, hence `ray`\n",
    "- In this course we'll mainly be focussing on the [PPO algorithm](https://openai.com/blog/openai-baselines-ppo/), hence `PPO` and `PPOConfig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aacb650-3d39-4117-b9a9-2eccdbd9a350",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.ppo import PPO, PPOConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b300e4-266f-45ef-94c5-f72d1928f94b",
   "metadata": {},
   "source": [
    "Next, we create a trainer object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd844399-d193-4961-9bb3-8be1e2f95ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "algo_config = (\n",
    "    PPOConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(create_env_on_local_worker=True)\\\n",
    "    .debugging(seed=0, log_level=\"ERROR\", logger_config={\"log_to_driver\" : False})\\\n",
    "    .training(model={\"fcnet_hiddens\" : [32, 32]})\\\n",
    "    .environment(env_config={\"is_slippery\" : False})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cae604e-ed20-41f3-b7de-8e9ab550d34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo = PPO(env=\"FrozenLake-v1\", config=algo_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c04a477-934c-4317-a69b-9eaef6d7563f",
   "metadata": {},
   "source": [
    "- `PPO`: we're using the PPO algorithm\n",
    "- `env=\"FrozenLake-v1\"`: RLlib knows about OpenAI Gym environments\n",
    "  - In the next module we'll learn how to make our own environments!\n",
    "- `config=algo_config`: this contains all hyperparameters of the algorithm and the environment.\n",
    "  - For clarity we've hidden the config for now, but we'll get back to it soon.\n",
    "  \n",
    "Notes:\n",
    "\n",
    "- We can refer to gym environments by name\n",
    "- Later we'll show the new standard way of instantiating algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4400014-5524-4360-9c2f-988fa2fd114d",
   "metadata": {},
   "source": [
    "#### Using the policy\n",
    "\n",
    "- We haven't trained the agent yet, but we can still see what it does.\n",
    "- This is like calling `predict` before running `fit` with supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52c46765-0db9-4f0f-be98-6bd15fed1292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
    "obs = env.reset(seed=3)\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72c9f289-1c06-46a7-89bd-a8553104f1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "from utils import fix_frozen_lake_render\n",
    "fix_frozen_lake_render(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ce06add-1fc8-4211-9850-f419ddfca832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = ppo.compute_single_action(obs, explore=False)\n",
    "action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59ecbe8-4c8b-45d3-bfc7-d1f4e5d93684",
   "metadata": {},
   "source": [
    "- We gave the algorithm our initial observation, 0, and it recommended action 2 (right).\n",
    "- This action came from the initialized **policy**.\n",
    "- Remember, the policy maps observations to actions.\n",
    "- For now we'll ignore the `explore=False`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48087f0-f3bd-47fb-b368-51c1f28601ac",
   "metadata": {},
   "source": [
    "#### Using the policy\n",
    "\n",
    "We can see what happened after taking that action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b6a30b0-d591-4c30-b85e-fe839f518744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      ".P..\n",
      ".O.O\n",
      "...O\n",
      "O..G\n"
     ]
    }
   ],
   "source": [
    "obs, reward, done, _ = env.step(action)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7068f1-2f74-4d01-b6a6-455cdb964e9a",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "- So far our policy was just a random/arbitrary initialization.\n",
    "- What we want is to train it _based on experience interacting with the environment_.\n",
    "- In order to do this, RLlib will _play through many episodes_ and learn as it goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26694604-767d-437d-b29b-377d38c67999",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_info = ppo.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0024b5a6-b7de-4e0b-a2ac-1b17af442c99",
   "metadata": {},
   "source": [
    "- Note that, unlike sklearn's `fit()`, here we don't provide the dataset to `train()`.\n",
    "- We gave it the environment during initialization, and it uses the environment to generate data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67115d1f-5fe7-414e-9d56-41063ce0dedb",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "The `train` method returns a dictionary containing information about the iteration of training. We'll explore this next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c34155-1045-4582-8995-30ca063dd941",
   "metadata": {},
   "source": [
    "#### Training iterations\n",
    "\n",
    "- In fact, what we just did was one _iteration_ of training.\n",
    "- RLlib will play through a bunch of episodes per iteration, depending on its hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5e474bd-6aa7-4801-8268-f8b643d6d091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "536"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_info[\"hist_stats\"][\"episode_lengths\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e586701-6cc9-4706-83fc-66f8082c0ddd",
   "metadata": {},
   "source": [
    "Looks like it ran ~500 episodes in that one iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6461c7c3-4612-4649-be60-ef527c6abce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3991"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train_info[\"hist_stats\"][\"episode_lengths\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2786b671-c31c-4278-801e-41aa7a2a5f1b",
   "metadata": {},
   "source": [
    "For a total of ~4000 time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9af3fd-5931-471b-ba85-85de280e1501",
   "metadata": {},
   "source": [
    "#### RL mindset: data generation\n",
    "\n",
    "- Key departure from the supervised learning mindset\n",
    "- In SL, we take a fixed amount of data and train for some number of iterations\n",
    "- In RL, more iterations means more training _on more data_ because we learn from the environment as we interact with it\n",
    "- If you only play one episode, you might never see observation 10, so how can you learn what to do given observation 10?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dcef69-2b7c-4682-8163-ff2c84a7c43f",
   "metadata": {},
   "source": [
    "#### RL mindset\n",
    "\n",
    "What we had before, when we were controlling the agent manually:\n",
    "\n",
    "![](img/RL-loop-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa39ad76-d385-4048-a0f4-b52a5adb0596",
   "metadata": {},
   "source": [
    "#### RL mindset\n",
    "\n",
    "What we have now:\n",
    "\n",
    "![](img/RL-loop-3.png)\n",
    "\n",
    "We have an outer loop that involves training.\n",
    "\n",
    "Notes:\n",
    "\n",
    "Training / updating the agent may not necessarily occur after every single iteration, but this schematic gives a conceptual framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a4fb60-74bb-4e18-aac0-7f13ad5fa4a0",
   "metadata": {},
   "source": [
    "#### Training info: episode lengths\n",
    "\n",
    "Let's look at the lengths of the last 100 episodes we played:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69d6edba-b9d7-4e41-8fe5-c9181870faef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 9, 18, 3, 5, 2, 5, 5, 13, 18, 4, 6, 9, 3, 16, 4, 2, 3, 8, 4, 4, 4, 9, 6, 7, 2, 3, 6, 10, 9, 5, 7, 11, 19, 4, 7, 2, 2, 15, 6, 2, 10, 6, 13, 10, 11, 3, 10, 2, 8, 14, 12, 2, 4, 5, 2, 7, 4, 7, 5, 2, 3, 2, 6, 7, 2, 19, 2, 4, 12, 15, 7, 7, 11, 6, 2, 6, 6, 12, 6, 16, 8, 8, 4, 8, 4, 6, 4, 16, 2, 5, 11, 10, 6, 14, 2, 2, 6, 4, 14]\n"
     ]
    }
   ],
   "source": [
    "print(train_info[\"hist_stats\"][\"episode_lengths\"][-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb727bf9-bc04-4127-8290-4b4a76a961f2",
   "metadata": {},
   "source": [
    "- Remember that an episode ends when `.step()` returns `True` for the `done` flag.\n",
    "- We see some very short episodes, where the agent fell into a hole right away."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cdf5ae-db28-4da8-a55d-eb1c0222b277",
   "metadata": {},
   "source": [
    "#### Training info: episode rewards\n",
    "\n",
    "- For those longer episodes, did the agent reach the goal?\n",
    "- To assess this, we can print out the first 100 _episode rewards_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd6ffa8d-206d-47e6-8d5e-c2ce350c92a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print(train_info[\"hist_stats\"][\"episode_reward\"][-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8641cd6-7d8c-4d69-a4a9-0402f7ac0bde",
   "metadata": {},
   "source": [
    "And the average of these rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab826887-1e08-4821-9ea4-79baba755237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(train_info[\"hist_stats\"][\"episode_reward\"][-100:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c846b7-297a-4878-a6de-a91663176660",
   "metadata": {},
   "source": [
    "This is not very impressive. Let's keep training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb75a84-3cab-487b-8476-2740a686af92",
   "metadata": {},
   "source": [
    "#### More training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dcc1cbb9-0c16-447f-88a6-027bd5b05b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    train_info = ppo.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8413bd9d-59d5-4620-b5f1-2f4f466f2029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9778481012658228\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(train_info[\"hist_stats\"][\"episode_reward\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9e1c65-6eb7-4b72-adf6-f63845487fd4",
   "metadata": {},
   "source": [
    "- Nice! Now we're reaching the goal almost every time!\n",
    "- The _average reward_ (across the most recent training episodes) is 0.98.\n",
    "- Interpretation: we're reaching the goal 98% of the time.\n",
    "\n",
    "Notes:\n",
    "\n",
    "Why this interpretation? Because in this environment, we only receive a reward at the end of the episode, 0 for failure and 1 for success."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9227bbc-7b6b-40e2-b458-8bbe94b9a647",
   "metadata": {},
   "source": [
    "#### Training curves\n",
    "\n",
    "- In deep learning we often see diagrams of loss vs. epochs during training\n",
    "- We can get this in RL too by storing this each iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92699cd9-fc76-474c-9977-f1b7aa4a4035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ppo = PPO(env=\"FrozenLake-v1\", config=algo_config)\n",
    "\n",
    "rewards = []\n",
    "for i in range(10):\n",
    "    train_info = ppo.train()\n",
    "    rewards.append(train_info[\"episode_reward_mean\"])\n",
    "                   \n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(rewards);\n",
    "plt.xlabel(\"iterations\");\n",
    "plt.ylabel(\"average reward\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74335644-d4a4-41b2-9c52-b4fb334c6168",
   "metadata": {},
   "source": [
    "We can see the rewards increasing!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f4e2b6-da88-4245-8ccc-e81e53333491",
   "metadata": {},
   "source": [
    "Notes: If we store the episode reward mean for each training iteration, we can pr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134f484d-3aa5-4cc6-b878-869f591102ec",
   "metadata": {},
   "source": [
    "#### Declare victory?\n",
    "\n",
    "- We did well.\n",
    "- But, this non-slippery Frozen Lake is a very easy environment. \n",
    "- Kind of like a supervised learning dataset where y=x would be \"easy\".\n",
    "- Later we'll ramp up the difficulty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80d7905-3fab-417f-b9b1-c9ffa4d4cecf",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Using the policy\n",
    "\n",
    "We can run the **observation-policy-action loop** for multiple time steps to watch the policy in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06f47779-3725-4bb6-9c92-f1877fa148b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "....\n",
      "PO.O\n",
      "...O\n",
      "O..G\n",
      "  (Down)\n",
      "....\n",
      ".O.O\n",
      "P..O\n",
      "O..G\n",
      "  (Right)\n",
      "....\n",
      ".O.O\n",
      ".P.O\n",
      "O..G\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "\n",
    "for i in range(3):\n",
    "    action = ppo.compute_single_action(obs, explore=False)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed77ebb4-1aa0-432e-b725-79cae6f3c2b9",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "The relevant trainer method in RLlib is compute_single_action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f9fe3e-a69d-4a40-ba94-db9f3c39ee54",
   "metadata": {},
   "source": [
    "#### Using the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af239179-38df-4a33-bbea-4c7b4ae9c684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "....\n",
      ".O.O\n",
      "..PO\n",
      "O..G\n",
      "  (Down)\n",
      "....\n",
      ".O.O\n",
      "...O\n",
      "O.PG\n",
      "  (Right)\n",
      "....\n",
      ".O.O\n",
      "...O\n",
      "O..P\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    action = ppo.compute_single_action(obs, explore=False)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8753cf53-da7f-4fad-b67c-200a9e844a85",
   "metadata": {},
   "source": [
    "Using this policy we reliably reach the goal every time because the non-slippery Frozen Lake environment is deterministic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680904ff-bd6a-40f4-800f-5bc225b5d850",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Evaluation\n",
    "\n",
    "In RLlib, we can evaluate with `.evaluate()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20596f3c-893f-4b0d-88ba-7d7b9e349033",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = ppo.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed056a47-6dc8-4846-89b4-719537a0b9f6",
   "metadata": {},
   "source": [
    "Training does not occur during evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f94835f-b1cf-40de-a9f2-e5852bc06c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9501557632398754"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results[\"evaluation\"][\"episode_reward_mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dccc5c8-bf38-4efc-9a68-6d7af6a6b8ab",
   "metadata": {},
   "source": [
    "Here we get similar results to the output of `.train()`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c110e98d-2f5a-4750-acff-4703e239672a",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "The evaluation output contains a lot of info, including many of the same fields as the training output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6a8e52-ab9e-4a1f-b704-15525065dee1",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "Histogram of episode rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd354df4-96b4-45dc-9ede-f89089794093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(eval_results[\"evaluation\"][\"hist_stats\"][\"episode_reward\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144df6d5-e45e-4d6e-a2bf-a0332211105f",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "Histogram of episode _lengths_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62486690-de69-4b8c-94eb-e0290d5b90e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(eval_results[\"evaluation\"][\"hist_stats\"][\"episode_lengths\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0198fc0-5477-4bec-8680-2d6101fac0a8",
   "metadata": {},
   "source": [
    "- The short ones are the failures, since it's impossible to reach the goal in 4 steps.\n",
    "- Most of the time we reach the goal in the minimum number of steps (6)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c8070f-8ac9-4026-b049-ceeffa94775b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### sklearn / RLlib analogies\n",
    "\n",
    "- We've seen some analogies (and departures) between SL and RL, both in concept and syntax. \n",
    "- Let's compare:\n",
    "\n",
    "| SL/sklearn | RL/RLlib | Description  |\n",
    "|-------------|---------|--------------|\n",
    "| `ModelName(**hypers)` | `AlgoName(hypers, env)` | Initialize a model/algorithm |\n",
    "| `.fit(X,y)`  | `.train()` | Training (fully for sklearn, one iteration for RLlib) |\n",
    "| `.predict(x)` | `.compute_single_action(obs)` | Use the trained model once |\n",
    "| `.score(X,y)` | `.evaluate()` | Evaluate the model |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d205da70-1255-437c-8a99-6ea8a5e8d2a0",
   "metadata": {},
   "source": [
    "#### Let's apply what we learned!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd539d44-aa7c-4958-b448-7759f3adc20e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## RLlib algorithm methods\n",
    "<!-- multiple choice -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17580d1a-2245-4390-bfee-449b1bfff570",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "_Which of the following most accurately describes the role of `algorithm.train()` in RLlib?_\n",
    "\n",
    "- [ ] It neither collects a data set of episodes nor learns a policy. | Are you sure?\n",
    "- [ ] It learns a policy from a fixed data set of episodes. | Remember, calling train() causes the agent to play through episodes.\n",
    "- [ ] It creates a data set of episodes but does not learn a policy. | Remember, calling train() learns a policy.\n",
    "- [x] It simultaneously collects a data set of episodes and also learns a policy. | You got it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48245b9-3007-4d9c-a657-97978c4fea1e",
   "metadata": {},
   "source": [
    "#### Passing in the dataset\n",
    "\n",
    "_When using scikit-learn for supervised learning we call `fit(X,y)`, but with RLlib we call `train()` without passing in the dataset. Why?_\n",
    "\n",
    "- [x] Because RLlib was given access to the environment when the algorithm was initialized, and this is all it needs.\n",
    "- [ ] Because reinforcement learning does not involve data.\n",
    "- [ ] Because X and y are passed into a different RLlib method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc34288f-7416-4041-8b70-5182715bfd0e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Predicting\n",
    "\n",
    "_Which of the following RLlib algorithm methods is most analogous to scikit-learn's `.predict()` function?_\n",
    "\n",
    "- [ ] train() | This is more like _fit()_ in scikit-learn.\n",
    "- [x] compute_single_action()\n",
    "- [ ] evaluate() | This is more like _score()_ in scikit-learn. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34d5c5b-a1be-447a-abe6-e034c9487cce",
   "metadata": {},
   "source": [
    "## Slippery Frozen Lake\n",
    "<!-- coding exercise -->\n",
    "\n",
    "In the slides, we trained an agent to reliably reach the goal in the **non-slippery** Frozen Lake environment. Here, try the same thing with the **slippery** Frozen Lake. Train your agent for enough iterations such it reaches the goal at least 20% of the time. Then, answer the multiple choice question below.\n",
    "\n",
    "Note: we will discuss the algorithm config in the next set of slides. For now just note that we're using the slippery version of Frozen Lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2847c21-1709-4039-b77d-d634e43e6096",
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '____' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgorithms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mppo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PPO, PPOConfig\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m slippery_algo_config\n\u001b[0;32m----> 5\u001b[0m ppo \u001b[38;5;241m=\u001b[39m PPO(env\u001b[38;5;241m=\u001b[39m\u001b[43m____\u001b[49m, config\u001b[38;5;241m=\u001b[39mslippery_algo_config)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(____):\n\u001b[1;32m      8\u001b[0m     train_info \u001b[38;5;241m=\u001b[39m ppo\u001b[38;5;241m.\u001b[39m____()\n",
      "\u001b[0;31mNameError\u001b[0m: name '____' is not defined"
     ]
    }
   ],
   "source": [
    "# EXERCISE\n",
    "from ray.rllib.algorithms.ppo import PPO, PPOConfig\n",
    "from utils import slippery_algo_config\n",
    "\n",
    "ppo = PPO(env=____, config=slippery_algo_config)\n",
    "\n",
    "for i in range(____):\n",
    "    train_info = ppo.____()\n",
    "    \n",
    "eval_results = ____.evaluate()\n",
    "\n",
    "print(eval_results[\"evaluation\"][\"episode_reward_mean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e2f2fdf9-ec1a-488d-8864-0a929e42190c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of reaching goal: 49.2%\n",
      "Action performed from top-right: 3\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "from ray.rllib.algorithms.ppo import PPO, PPOConfig\n",
    "from utils import slippery_algo_config\n",
    "\n",
    "ppo = PPO(env=\"FrozenLake-v1\", config=slippery_algo_config)\n",
    "\n",
    "for i in range(50): # There is randomness here, but 20 should be enough most of the time\n",
    "    train_info = ppo.train()\n",
    "    \n",
    "eval_results = ppo.evaluate()\n",
    "\n",
    "print(\"Frequency of reaching goal: %.1f%%\" % (eval_results[\"evaluation\"][\"episode_reward_mean\"]*100))\n",
    "\n",
    "print(\"Action performed from top-right:\", ppo.compute_single_action(3, explore=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6485b269-1c62-4cd8-a8d2-0a6cea590aeb",
   "metadata": {},
   "source": [
    "#### Action performed from top-right\n",
    "\n",
    "According to the trained policy, what action is performed when the agent is at the top-right of the arena? This is printed out by the code.\n",
    "\n",
    "- [ ] left (0) | When we ran the code, we got something different here.\n",
    "- [ ] down (1) | When we ran the code, we got something different here.\n",
    "- [ ] right (2) | When we ran the code, we got something different here.\n",
    "- [x] up (3) | You got it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb1e37b-ca77-41ba-ba20-6a9558485b80",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Interpreting the policy\n",
    "\n",
    "Recall that the arena looks like this:\n",
    "\n",
    "```\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "```\n",
    "\n",
    "In this slippery environment, you do your intended action 1/3 of the time, and each of the two perpendicular directions 1/3 of the time. You never go the opposite of the intended direction.\n",
    "\n",
    "In the previous question, we saw that, from the **top-right corner** (observation 3), the agent tries to move up (action 3). Why do you think the agent tries to move up?\n",
    "\n",
    "- [ ] 'Up' is an arbitrary choice because we did not train the agent.\n",
    "- [ ] The agent receives a reward for the 'up' action. \n",
    "- [x] The agent wants to avoid falling into the hole below it, so the 'up' action is the safest choice.\n",
    "- [ ] Moving up brings the agent closer to the goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b069fd6a-beca-49f9-a1a5-c6d5396105db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # HIDDEN\n",
    "\n",
    "# from ray.rllib.algorithms.ppo import PPO, PPOConfig\n",
    "# from utils import slippery_algo_config\n",
    "\n",
    "# ppo = slippery_algo_config.build(env=\"FrozenLake-v1\")\n",
    "\n",
    "# for i in range(20):\n",
    "#     ppo.train()\n",
    "\n",
    "# ppo.save(\"models/FrozenLakeSlippery20\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c553472d-69ad-49af-8282-5671d6cb9c15",
   "metadata": {},
   "source": [
    "## Rendering the trained agent\n",
    "<!-- coding exercise -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3362ef16-d73a-47c3-8b1b-737fd6931041",
   "metadata": {},
   "source": [
    "Fill in the blanks in the code below so that the code performs the observation-action-reward loop for one episode. Then, run the code and watch the trained agent navigate the slippery Frozen Lake. Then, answer the multiple choice question below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2996d2f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=32950)\u001b[0m /Users/mike/git/anyscale/ray/python/ray/rllib/utils/debug/deterministic.py:42: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32950)\u001b[0m   if LooseVersion(torch.__version__) >= LooseVersion(\"1.8.0\"):\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32950)\u001b[0m /Users/mike/miniconda3/envs/ray2beta-rc1/lib/python3.9/site-packages/gym/core.py:172: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed) instead.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32950)\u001b[0m   deprecation(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32951)\u001b[0m /Users/mike/git/anyscale/ray/python/ray/rllib/utils/debug/deterministic.py:42: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32951)\u001b[0m   if LooseVersion(torch.__version__) >= LooseVersion(\"1.8.0\"):\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32951)\u001b[0m /Users/mike/miniconda3/envs/ray2beta-rc1/lib/python3.9/site-packages/gym/core.py:172: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed) instead.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32951)\u001b[0m   deprecation(\n",
      "2022-08-14 14:05:51,801\tINFO trainable.py:668 -- Restored on 127.0.0.1 from checkpoint: models/FrozenLakeSlippery20/checkpoint_000020\n",
      "2022-08-14 14:05:51,803\tINFO trainable.py:677 -- Current state after restoring: {'_iteration': 20, '_timesteps_total': None, '_time_total': 41.955684185028076, '_episodes_total': 7248}\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE\n",
    "from utils import slippery_algo_config\n",
    "import gym\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "ppo = slippery_algo_config.build(env=\"FrozenLake-v1\")\n",
    "ppo.restore(\"models/FrozenLakeSlippery50/checkpoint_000050/\")\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "\n",
    "obs = env.reset()\n",
    "env.seed(12)\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    action = ppo.compute_single_action(obs, explore=False)\n",
    "    obs, rewards, done, _ = env.step(action)\n",
    "\n",
    "    display.clear_output(wait=True);\n",
    "    print(env.render(mode=\"ansi\"))\n",
    "    time.sleep(0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3f5e7f1f-cde4-4a3e-83b9-ab6334c83cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "from ray.rllib.algorithms.ppo import PPO, PPOConfig\n",
    "from utils import slippery_algo_config\n",
    "\n",
    "import gym\n",
    "\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "ppo = slippery_algo_config.build(env=\"FrozenLake-v1\")\n",
    "ppo.restore(\"models/FrozenLakeSlippery50/checkpoint_000050/\")\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "\n",
    "obs = env.reset()\n",
    "env.seed(12)\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    action = ppo.compute_single_action(obs, explore=False)\n",
    "    obs, rewards, done, _ = env.step(action)\n",
    "\n",
    "    display.clear_output(wait=True);\n",
    "    print(env.render(mode=\"ansi\"))\n",
    "    time.sleep(0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ef9dd9-99ea-4acf-ad2a-b09f2b534f5e",
   "metadata": {},
   "source": [
    "#### Choose the option below that best describes the agent's treacherous journey to the goal.\n",
    "\n",
    "- [ ] The agent visits the top-right square in this episode.\n",
    "- [ ] The agent never returns to the start state after leaving it initially.\n",
    "- [ ] The agent does not reach the goal during the episode.\n",
    "- [x] The agent never sees observation 10 on its journey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca0f7c4-786f-4105-9121-5f8122357d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ray200]",
   "language": "python",
   "name": "conda-env-ray200-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
