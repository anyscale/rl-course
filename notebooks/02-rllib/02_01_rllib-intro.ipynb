{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90c01ab9-1cd0-47d8-a198-e07e36e2c1fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## RLlib intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e74417a3-63bc-4000-951a-72ab882b689e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.size'] = 16\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddd909a-c86f-4e5c-9ba0-8b0ef1152730",
   "metadata": {},
   "source": [
    "#### What about the learning?\n",
    "\n",
    "Let's return to the \"API\" of RL:\n",
    "\n",
    "![](img/RL-API.png)\n",
    "\n",
    "- We've talked about the input (environment) and output (policy)\n",
    "- Let's talk about the reinforcement learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7503593-2928-46b8-891a-913ff3fbc3c1",
   "metadata": {},
   "source": [
    "#### What we'll cover\n",
    "\n",
    "- Many, many supervised learning algorithms exist... random forests, logistic regression, neural networks, etc.\n",
    "- Likewise, there are many RL algorithms.\n",
    "- This is not a course on RL algorithms, though many good ones exist!\n",
    "- This course is about _applying_ RL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14840b72-5c0f-42a3-b29f-d9b14995d2c1",
   "metadata": {},
   "source": [
    "#### Introducing Ray RLlib\n",
    "\n",
    "![](img/rllib-logo.png)\n",
    "\n",
    "- In this course we'll use Ray RLlib as our \"scikit-learn of reinforcement learning\"\n",
    "- We will look under the hood only as needed, and focus on the inputs and outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251a0d49-fad7-4620-bc2f-71ab58d228f9",
   "metadata": {},
   "source": [
    "#### Our first RLlib code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497d2332-bdf4-43c2-ba76-c5e3c4d37afc",
   "metadata": {},
   "source": [
    "- First, we import RLlib\n",
    "- RLlib is part of the Ray project, hence `ray`\n",
    "- In this course we'll mainly be focussing on the [PPO algorithm](https://openai.com/blog/openai-baselines-ppo/), hence `PPO` and `PPOConfig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6aacb650-3d39-4117-b9a9-2eccdbd9a350",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.ppo import PPO, PPOConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b300e4-266f-45ef-94c5-f72d1928f94b",
   "metadata": {},
   "source": [
    "Next, we create a trainer object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd844399-d193-4961-9bb3-8be1e2f95ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "algo_config = (\n",
    "    PPOConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(create_env_on_local_worker=True)\\\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\\\n",
    "    .training(model={\"fcnet_hiddens\" : [32, 32]})\\\n",
    "    .environment(env_config={\"is_slippery\" : False})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cae604e-ed20-41f3-b7de-8e9ab550d34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=19405)\u001b[0m /Users/mike/git/anyscale/ray/python/ray/rllib/utils/debug/deterministic.py:42: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19405)\u001b[0m   if LooseVersion(torch.__version__) >= LooseVersion(\"1.8.0\"):\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19405)\u001b[0m /Users/mike/miniconda3/envs/ray2beta/lib/python3.9/site-packages/gym/core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19405)\u001b[0m   deprecation(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19405)\u001b[0m /Users/mike/miniconda3/envs/ray2beta/lib/python3.9/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19405)\u001b[0m   deprecation(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19405)\u001b[0m /Users/mike/miniconda3/envs/ray2beta/lib/python3.9/site-packages/gym/core.py:268: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19405)\u001b[0m   deprecation(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19406)\u001b[0m /Users/mike/git/anyscale/ray/python/ray/rllib/utils/debug/deterministic.py:42: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19406)\u001b[0m   if LooseVersion(torch.__version__) >= LooseVersion(\"1.8.0\"):\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19406)\u001b[0m /Users/mike/miniconda3/envs/ray2beta/lib/python3.9/site-packages/gym/core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19406)\u001b[0m   deprecation(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19406)\u001b[0m /Users/mike/miniconda3/envs/ray2beta/lib/python3.9/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19406)\u001b[0m   deprecation(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19406)\u001b[0m /Users/mike/miniconda3/envs/ray2beta/lib/python3.9/site-packages/gym/core.py:268: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19406)\u001b[0m   deprecation(\n"
     ]
    }
   ],
   "source": [
    "ppo = PPO(env=\"FrozenLake-v1\", config=algo_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7bc1a0f-6062-4cb8-8ff6-fb41c4713a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPOConfig().framework(\"torch\").\n",
    "#     \"framework\"             : \"torch\",\n",
    "#     \"create_env_on_driver\"  : True,\n",
    "#     \"seed\"                  : 0,\n",
    "#     \"model\"                 : {\"fcnet_hiddens\" : [32, 32]}, \n",
    "#     \"env_config\"            : {\"is_slippery\" : False}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4d16f4f-1910-48e2-be41-8d40596fee06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = config_train.build(\"FrozenLake-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fc412c4-e739-417a-b973-a0819bc5e8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = ray.rllib.algorithms.ppo.PPOConfig(env=\"FrozenLake-v1\", \n",
    "#                                       config=trainer_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c04a477-934c-4317-a69b-9eaef6d7563f",
   "metadata": {},
   "source": [
    "- `PPO`: we're using the PPO algorithm\n",
    "- `env=\"FrozenLake-v1\"`: RLlib knows about OpenAI Gym environments\n",
    "  - In the next module we'll learn how to make our own environments!\n",
    "- `config=algo_config`: this contains all hyperparameters of the algorithm and the environment.\n",
    "  - For clarity we've hidden the config for now, but we'll get back to it soon.\n",
    "  \n",
    "Notes:\n",
    "\n",
    "- We can refer to gym environments by name\n",
    "- Later we'll show the new standard way of instantiating algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4400014-5524-4360-9c2f-988fa2fd114d",
   "metadata": {},
   "source": [
    "#### Using the policy\n",
    "\n",
    "- We haven't trained the agent yet, but we can still see what it does.\n",
    "- This is like calling `predict` before running `fit` with supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52c46765-0db9-4f0f-be98-6bd15fed1292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
    "obs = env.reset(seed=3)\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72c9f289-1c06-46a7-89bd-a8553104f1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "from utils import fix_frozen_lake_render\n",
    "fix_frozen_lake_render(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ce06add-1fc8-4211-9850-f419ddfca832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = ppo.compute_single_action(obs, explore=False)\n",
    "action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59ecbe8-4c8b-45d3-bfc7-d1f4e5d93684",
   "metadata": {},
   "source": [
    "- We gave the algorithm our initial observation, 0, and it recommended action 2 (right).\n",
    "- This action came from the initialized **policy**.\n",
    "- Remember, the policy maps observations to actions.\n",
    "- For now we'll ignore the `explore=False`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48087f0-f3bd-47fb-b368-51c1f28601ac",
   "metadata": {},
   "source": [
    "#### Using the policy\n",
    "\n",
    "We can see what happened after taking that action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b6a30b0-d591-4c30-b85e-fe839f518744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "FPFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "obs, reward, done, _ = env.step(action)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7068f1-2f74-4d01-b6a6-455cdb964e9a",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "- So far our policy was just a random/arbitrary initialization.\n",
    "- What we want is to train it _based on experience interacting with the environment_.\n",
    "- In order to do this, RLlib will _play through many episodes_ and learn as it goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26694604-767d-437d-b29b-377d38c67999",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_info = ppo.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0024b5a6-b7de-4e0b-a2ac-1b17af442c99",
   "metadata": {},
   "source": [
    "- Note that, unlike sklearn's `fit()`, here we don't provide the dataset to `train()`.\n",
    "- We gave it the environment during initialization, and it uses the environment to generate data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67115d1f-5fe7-414e-9d56-41063ce0dedb",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "The `train` method returns a dictionary containing information about the iteration of training. We'll explore this next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c34155-1045-4582-8995-30ca063dd941",
   "metadata": {},
   "source": [
    "#### Training iterations\n",
    "\n",
    "- In fact, what we just did was one _iteration_ of training.\n",
    "- RLlib will play through a bunch of episodes per iteration, depending on its hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5e474bd-6aa7-4801-8268-f8b643d6d091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "536"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_info[\"hist_stats\"][\"episode_lengths\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e586701-6cc9-4706-83fc-66f8082c0ddd",
   "metadata": {},
   "source": [
    "Looks like it ran ~500 episodes in that one iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6461c7c3-4612-4649-be60-ef527c6abce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3991"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train_info[\"hist_stats\"][\"episode_lengths\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2786b671-c31c-4278-801e-41aa7a2a5f1b",
   "metadata": {},
   "source": [
    "For a total of ~4000 time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9af3fd-5931-471b-ba85-85de280e1501",
   "metadata": {},
   "source": [
    "#### RL mindset: data generation\n",
    "\n",
    "- Key departure from the supervised learning mindset\n",
    "- In SL, we take a fixed amount of data and train for some number of iterations\n",
    "- In RL, more iterations means more training _on more data_ because we learn from the environment as we interact with it\n",
    "- If you only play one episode, you might never see observation 10, so how can you learn what to do given observation 10?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dcef69-2b7c-4682-8163-ff2c84a7c43f",
   "metadata": {},
   "source": [
    "#### RL mindset\n",
    "\n",
    "What we had before, when we were controlling the agent manually:\n",
    "\n",
    "![](img/RL-loop-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa39ad76-d385-4048-a0f4-b52a5adb0596",
   "metadata": {},
   "source": [
    "#### RL mindset\n",
    "\n",
    "What we have now:\n",
    "\n",
    "![](img/RL-loop-3.png)\n",
    "\n",
    "We have an outer loop that involves training.\n",
    "\n",
    "Notes:\n",
    "\n",
    "Training / updating the agent may not necessarily occur after every single iteration, but this schematic gives a conceptual framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a4fb60-74bb-4e18-aac0-7f13ad5fa4a0",
   "metadata": {},
   "source": [
    "#### Training info: episode lengths\n",
    "\n",
    "Let's look at the lengths of the last 100 episodes we played:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69d6edba-b9d7-4e41-8fe5-c9181870faef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 9, 18, 3, 5, 2, 5, 5, 13, 18, 4, 6, 9, 3, 16, 4, 2, 3, 8, 4, 4, 4, 9, 6, 7, 2, 3, 6, 10, 9, 5, 7, 11, 19, 4, 7, 2, 2, 15, 6, 2, 10, 6, 13, 10, 11, 3, 10, 2, 8, 14, 12, 2, 4, 5, 2, 7, 4, 7, 5, 2, 3, 2, 6, 7, 2, 19, 2, 4, 12, 15, 7, 7, 11, 6, 2, 6, 6, 12, 6, 16, 8, 8, 4, 8, 4, 6, 4, 16, 2, 5, 11, 10, 6, 14, 2, 2, 6, 4, 14]\n"
     ]
    }
   ],
   "source": [
    "print(train_info[\"hist_stats\"][\"episode_lengths\"][-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb727bf9-bc04-4127-8290-4b4a76a961f2",
   "metadata": {},
   "source": [
    "- Remember that an episode ends when `.step()` returns `True` for the `done` flag.\n",
    "- We see some very short episodes, where the agent fell into a hole right away."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cdf5ae-db28-4da8-a55d-eb1c0222b277",
   "metadata": {},
   "source": [
    "#### Training info: episode rewards\n",
    "\n",
    "- For those longer episodes, did the agent reach the goal?\n",
    "- To assess this, we can print out the first 100 _episode rewards_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd6ffa8d-206d-47e6-8d5e-c2ce350c92a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print(train_info[\"hist_stats\"][\"episode_reward\"][-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8641cd6-7d8c-4d69-a4a9-0402f7ac0bde",
   "metadata": {},
   "source": [
    "And the average of these rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab826887-1e08-4821-9ea4-79baba755237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(train_info[\"hist_stats\"][\"episode_reward\"][-100:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c846b7-297a-4878-a6de-a91663176660",
   "metadata": {},
   "source": [
    "This is not very impressive. Let's keep training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb75a84-3cab-487b-8476-2740a686af92",
   "metadata": {},
   "source": [
    "#### More training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dcc1cbb9-0c16-447f-88a6-027bd5b05b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    train_info = ppo.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8413bd9d-59d5-4620-b5f1-2f4f466f2029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9778481012658228\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(train_info[\"hist_stats\"][\"episode_reward\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9e1c65-6eb7-4b72-adf6-f63845487fd4",
   "metadata": {},
   "source": [
    "- Nice! Now we're reaching the goal almost every time!\n",
    "- The _average reward_ (across the most recent training episodes) is 0.98.\n",
    "- Interpretation: we're reaching the goal 98% of the time.\n",
    "\n",
    "Notes:\n",
    "\n",
    "Why this interpretation? Because in this environment, we only receive a reward at the end of the episode, 0 for failure and 1 for success."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134f484d-3aa5-4cc6-b878-869f591102ec",
   "metadata": {},
   "source": [
    "#### Declare victory?\n",
    "\n",
    "- We did well.\n",
    "- But, this non-slippery Frozen Lake is a very easy environment. \n",
    "- Kind of like a supervised learning dataset where y=x would be \"easy\".\n",
    "- Later we'll ramp up the difficulty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80d7905-3fab-417f-b9b1-c9ffa4d4cecf",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Using the policy\n",
    "\n",
    "We can run the **observation-policy-action loop** for multiple time steps to watch the policy in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06f47779-3725-4bb6-9c92-f1877fa148b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "FFFF\n",
      "PHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "FFFF\n",
      "FHFH\n",
      "PFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "FFFF\n",
      "FHFH\n",
      "FPFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "\n",
    "for i in range(3):\n",
    "    action = ppo.compute_single_action(obs, explore=False)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed77ebb4-1aa0-432e-b725-79cae6f3c2b9",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "The relevant trainer method in RLlib is compute_single_action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f9fe3e-a69d-4a40-ba94-db9f3c39ee54",
   "metadata": {},
   "source": [
    "#### Using the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af239179-38df-4a33-bbea-4c7b4ae9c684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "FFFF\n",
      "FHFH\n",
      "FFPH\n",
      "HFFG\n",
      "  (Down)\n",
      "FFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFPG\n",
      "  (Right)\n",
      "FFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFP\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    action = ppo.compute_single_action(obs, explore=False)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8753cf53-da7f-4fad-b67c-200a9e844a85",
   "metadata": {},
   "source": [
    "Using this policy we reliably reach the goal every time because the non-slippery Frozen Lake environment is deterministic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680904ff-bd6a-40f4-800f-5bc225b5d850",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Evaluation\n",
    "\n",
    "In RLlib, we can evaluate with `.evaluate()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20596f3c-893f-4b0d-88ba-7d7b9e349033",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = ppo.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed056a47-6dc8-4846-89b4-719537a0b9f6",
   "metadata": {},
   "source": [
    "Training does not occur during evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f94835f-b1cf-40de-a9f2-e5852bc06c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9810725552050473"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results[\"evaluation\"][\"episode_reward_mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dccc5c8-bf38-4efc-9a68-6d7af6a6b8ab",
   "metadata": {},
   "source": [
    "Here we get similar results to the output of `.train()`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c110e98d-2f5a-4750-acff-4703e239672a",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "The evaluation output contains a lot of info, including many of the same fields as the training output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6a8e52-ab9e-4a1f-b704-15525065dee1",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "Histogram of episode rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd354df4-96b4-45dc-9ede-f89089794093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(eval_results[\"evaluation\"][\"hist_stats\"][\"episode_reward\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144df6d5-e45e-4d6e-a2bf-a0332211105f",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "Histogram of episode _lengths_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "62486690-de69-4b8c-94eb-e0290d5b90e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(eval_results[\"evaluation\"][\"hist_stats\"][\"episode_lengths\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0198fc0-5477-4bec-8680-2d6101fac0a8",
   "metadata": {},
   "source": [
    "- The short ones are the failures, since it's impossible to reach the goal in 4 steps.\n",
    "- Most of the time we reach the goal in the minimum number of steps (6)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c8070f-8ac9-4026-b049-ceeffa94775b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### sklearn / RLlib analogies\n",
    "\n",
    "- We've seen some analogies (and departures) between SL and RL, both in concept and syntax. \n",
    "- Let's compare:\n",
    "\n",
    "| SL/sklearn | RL/RLlib | Description  |\n",
    "|-------------|---------|--------------|\n",
    "| `ModelName(**hypers)` | `AlgoName(hypers, env)` | Initialize a model/algorithm |\n",
    "| `.fit(X,y)`  | `.train()` | Training (fully for sklearn, one iteration for RLlib) |\n",
    "| `.predict(x)` | `.compute_single_action(obs)` | Use the trained model once |\n",
    "| `.score(X,y)` | `.evaluate()` | Evaluate the model |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d205da70-1255-437c-8a99-6ea8a5e8d2a0",
   "metadata": {},
   "source": [
    "#### Let's apply what we learned!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd539d44-aa7c-4958-b448-7759f3adc20e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## RLlib algorithm methods\n",
    "<!-- multiple choice -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17580d1a-2245-4390-bfee-449b1bfff570",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "_Which of the following most accurately describes the role of `algorithm.train()` in RLlib?_\n",
    "\n",
    "- [ ] It neither collects a data set of episodes nor learns a policy. | Are you sure?\n",
    "- [ ] It learns a policy from a fixed data set of episodes. | Remember, calling train() causes the agent to play through episodes.\n",
    "- [ ] It creates a data set of episodes but does not learn a policy. | Remember, calling train() learns a policy.\n",
    "- [x] It simultaneously collects a data set of episodes and also learns a policy. | You got it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48245b9-3007-4d9c-a657-97978c4fea1e",
   "metadata": {},
   "source": [
    "#### Passing in the dataset\n",
    "\n",
    "_When using scikit-learn for supervised learning we call `fit(X,y)`, but with RLlib we call `train()` without passing in the dataset. Why?_\n",
    "\n",
    "- [x] Because RLlib was given access to the environment when the algorithm was initialized, and this is all it needs.\n",
    "- [ ] Because reinforcement learning does not involve data.\n",
    "- [ ] Because X and y are passed into a different RLlib method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc34288f-7416-4041-8b70-5182715bfd0e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Predicting\n",
    "\n",
    "_Which of the following RLlib algorithm methods is most analogous to scikit-learn's `.predict()` function?_\n",
    "\n",
    "- [ ] train() | This is more like _fit()_ in scikit-learn.\n",
    "- [x] compute_single_action()\n",
    "- [ ] evaluate() | This is more like _score()_ in scikit-learn. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34d5c5b-a1be-447a-abe6-e034c9487cce",
   "metadata": {},
   "source": [
    "## Slippery Frozen Lake\n",
    "<!-- coding exercise -->\n",
    "\n",
    "In the slides, we trained an agent to reliably reach the goal in the **non-slippery** Frozen Lake environment. Here, try the same thing with the **slippery** Frozen Lake. Train your agent for enough iterations such it reaches the goal at least 20% of the time. Then, answer the multiple choice question below.\n",
    "\n",
    "Note: we will discuss the algorithm config in the next set of slides. For now just note that we're using the slippery version of Frozen Lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2847c21-1709-4039-b77d-d634e43e6096",
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "from ray.rllib.algorithms.ppo import PPO, PPOConfig\n",
    "from utils import slippery_algo_config\n",
    "\n",
    "ppo = PPO(env=____, config=slippery_algo_config)\n",
    "\n",
    "for i in range(____):\n",
    "    train_info = ppo.____()\n",
    "    \n",
    "eval_results = ____.evaluate()\n",
    "\n",
    "print(eval_results[\"evaluation\"][\"episode_reward_mean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2f2fdf9-ec1a-488d-8864-0a929e42190c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=29935)\u001b[0m /Users/mike/git/anyscale/ray/python/ray/rllib/utils/debug/deterministic.py:42: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29935)\u001b[0m   if LooseVersion(torch.__version__) >= LooseVersion(\"1.8.0\"):\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29935)\u001b[0m /Users/mike/miniconda3/envs/ray2beta/lib/python3.9/site-packages/gym/core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29935)\u001b[0m   deprecation(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29935)\u001b[0m /Users/mike/miniconda3/envs/ray2beta/lib/python3.9/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29935)\u001b[0m   deprecation(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29935)\u001b[0m /Users/mike/miniconda3/envs/ray2beta/lib/python3.9/site-packages/gym/core.py:268: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29935)\u001b[0m   deprecation(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29934)\u001b[0m /Users/mike/git/anyscale/ray/python/ray/rllib/utils/debug/deterministic.py:42: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29934)\u001b[0m   if LooseVersion(torch.__version__) >= LooseVersion(\"1.8.0\"):\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29934)\u001b[0m /Users/mike/miniconda3/envs/ray2beta/lib/python3.9/site-packages/gym/core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29934)\u001b[0m   deprecation(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29934)\u001b[0m /Users/mike/miniconda3/envs/ray2beta/lib/python3.9/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29934)\u001b[0m   deprecation(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29934)\u001b[0m /Users/mike/miniconda3/envs/ray2beta/lib/python3.9/site-packages/gym/core.py:268: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29934)\u001b[0m   deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of reaching goal: 63.1%\n",
      "Action performed from top-right: 3\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "from ray.rllib.algorithms.ppo import PPO, PPOConfig\n",
    "# from utils import slippery_algo_config\n",
    "\n",
    "ppo = PPO(env=\"FrozenLake-v1\", config=slippery_algo_config)\n",
    "\n",
    "for i in range(30): # There is randomness here, but 20 should be enough most of the time\n",
    "    train_info = ppo.train()\n",
    "    \n",
    "eval_results = ppo.evaluate()\n",
    "\n",
    "print(\"Frequency of reaching goal: %.1f%%\" % (eval_results[\"evaluation\"][\"episode_reward_mean\"]*100))\n",
    "\n",
    "print(\"Action performed from top-right:\", ppo.compute_single_action(3, explore=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6485b269-1c62-4cd8-a8d2-0a6cea590aeb",
   "metadata": {},
   "source": [
    "#### Action performed from top-right\n",
    "\n",
    "According to the trained policy, what action is performed when the agent is at the top-right of the arena? This is printed out by the code.\n",
    "\n",
    "- [ ] left (0) | When we ran the code, we got something different here.\n",
    "- [ ] down (1) | When we ran the code, we got something different here.\n",
    "- [ ] right (2) | When we ran the code, we got something different here.\n",
    "- [x] up (3) | You got it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb1e37b-ca77-41ba-ba20-6a9558485b80",
   "metadata": {},
   "source": [
    "#### Interpreting the policy\n",
    "\n",
    "Recall that the arena looks like this:\n",
    "\n",
    "```\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "```\n",
    "\n",
    "In this slippery environment, you do your intended action 1/3 of the time, and each of the two perpendicular directions 1/3 of the time. You never go the opposite of the intended direction.\n",
    "\n",
    "In the previous question, we saw that, from the **top-right corner** (observation 3), the agent tries to move up (action 3). Why do you think the agent tries to move up?\n",
    "\n",
    "- [ ] 'Up' is an arbitrary choice because we did not train the agent.\n",
    "- [ ] The agent receives a reward for the 'up' action. \n",
    "- [x] The agent wants to avoid falling into the hole below it, so the 'up' action is the safest choice.\n",
    "- [ ] Moving up brings the agent closer to the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c553472d-69ad-49af-8282-5671d6cb9c15",
   "metadata": {},
   "source": [
    "## Rendering the trained agent\n",
    "<!-- coding exercise -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3362ef16-d73a-47c3-8b1b-737fd6931041",
   "metadata": {},
   "source": [
    "Fill in the blanks in the code below so that the code performs the observation-action-reward loop for one episode. Then, run the code and watch the trained agent navigate the slippery Frozen Lake. Then, answer the multiple choice question below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2996d2f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "from ray.rllib.algorithms.ppo import PPO, PPOConfig\n",
    "from utils import slippery_algo_config\n",
    "\n",
    "import gym\n",
    "\n",
    "from ipywidgets import Output\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "ppo = PPO(env=\"FrozenLake-v1\", config=slippery_algo_config)\n",
    "ppo.restore(\"models/FrozenLakeSlippery/checkpoint-20\")\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "\n",
    "out = Output();\n",
    "display.display(out);\n",
    "\n",
    "with out:\n",
    "    obs = env.reset()\n",
    "    env.seed(1)\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = ppo.____(____, explore=False)\n",
    "        obs, rewards, done, _ = env.step(____)\n",
    "        \n",
    "        out.clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5e7f1f-cde4-4a3e-83b9-ab6334c83cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "from ray.rllib.algorithms.ppo import PPO, PPOConfig\n",
    "from utils import slippery_algo_config\n",
    "\n",
    "from ipywidgets import Output\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "ppo = PPO(env=\"FrozenLake-v1\", config=slippery_algo_config)\n",
    "ppo.restore(\"models/FrozenLakeSlippery/checkpoint-20\")\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "\n",
    "out = Output();\n",
    "display.display(out);\n",
    "\n",
    "with out:\n",
    "    obs = env.reset()\n",
    "    env.seed(1)\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = slippery_trainer.compute_single_action(obs, explore=False)\n",
    "        obs, rewards, done, _ = env.step(action)\n",
    "        \n",
    "        out.clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e291218e-1c96-4163-b617-42450992927c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# we may be able to clean this up and remove the whole out/Output thing, and just use display for everything (see cartpole 2.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ef9dd9-99ea-4acf-ad2a-b09f2b534f5e",
   "metadata": {},
   "source": [
    "#### Choose the option below that best describes the agent's treacherous journey to the goal.\n",
    "\n",
    "- [ ] The agent visits the top-right square in this episode.\n",
    "- [ ] The agent never returns to the start state after leaving it initially.\n",
    "- [ ] The agent does not reach the goal during the episode.\n",
    "- [x] The agent sees observation 11 exactly once on its journey."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ray2beta]",
   "language": "python",
   "name": "conda-env-ray2beta-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
