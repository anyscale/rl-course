{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90c01ab9-1cd0-47d8-a198-e07e36e2c1fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## RLlib intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e74417a3-63bc-4000-951a-72ab882b689e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fd0307b-9589-479a-85e9-0767e9f813af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import ray\n",
    "import logging\n",
    "ray.init(log_to_driver=False, ignore_reinit_error=True, logging_level=logging.ERROR); # logging.FATAL\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddd909a-c86f-4e5c-9ba0-8b0ef1152730",
   "metadata": {},
   "source": [
    "#### What about the learning?\n",
    "\n",
    "Let's return to the \"API\" of RL:\n",
    "\n",
    "![](img/RL-API.png)\n",
    "\n",
    "- We've talked about the input (environment) and output (policy)\n",
    "- Let's talk about the reinforcement learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7503593-2928-46b8-891a-913ff3fbc3c1",
   "metadata": {},
   "source": [
    "#### What we'll cover\n",
    "\n",
    "- Many, many supervised learning algorithms exist... random forests, logistic regression, neural networks, etc.\n",
    "- Likewise, there are many RL algorithms.\n",
    "- This is not a course on RL algorithms, though many good ones exist!\n",
    "- This course is about _applying_ RL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14840b72-5c0f-42a3-b29f-d9b14995d2c1",
   "metadata": {},
   "source": [
    "#### Introducing Ray RLlib\n",
    "\n",
    "![](img/rllib-logo.png)\n",
    "\n",
    "- In this course we'll use Ray RLlib as our \"scikit-learn of reinforcement learning\"\n",
    "- We will look under the hood only as needed, and focus on the inputs and outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251a0d49-fad7-4620-bc2f-71ab58d228f9",
   "metadata": {},
   "source": [
    "#### Our first RLlib code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497d2332-bdf4-43c2-ba76-c5e3c4d37afc",
   "metadata": {},
   "source": [
    "- First, we import RLlib\n",
    "- RLlib is part of the Ray project, hence `ray`\n",
    "- In this course we'll mainly be focussing on the [PPO algorithm](https://openai.com/blog/openai-baselines-ppo/), hence `PPO` and `PPOConfig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aacb650-3d39-4117-b9a9-2eccdbd9a350",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.ppo import PPO, PPOConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b300e4-266f-45ef-94c5-f72d1928f94b",
   "metadata": {},
   "source": [
    "Next, we create a trainer object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd844399-d193-4961-9bb3-8be1e2f95ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "algo_config = (\n",
    "    PPOConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(create_env_on_local_worker=True)\\\n",
    "    .debugging(seed=0, log_level=\"ERROR\", logger_config={\"log_to_driver\" : False})\\\n",
    "    .training(model={\"fcnet_hiddens\" : [32, 32]})\\\n",
    "    .environment(env_config={\"is_slippery\" : False})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cae604e-ed20-41f3-b7de-8e9ab550d34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo = PPO(env=\"FrozenLake-v1\", config=algo_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c04a477-934c-4317-a69b-9eaef6d7563f",
   "metadata": {},
   "source": [
    "- `PPO`: we're using the PPO algorithm\n",
    "- `env=\"FrozenLake-v1\"`: RLlib knows about OpenAI Gym environments\n",
    "  - In the next module we'll learn how to make our own environments!\n",
    "- `config=algo_config`: this contains all hyperparameters of the algorithm and the environment.\n",
    "  - For clarity we've hidden the config for now, but we'll get back to it soon.\n",
    "  \n",
    "Notes:\n",
    "\n",
    "- We can refer to gym environments by name\n",
    "- Later we'll show the new standard way of instantiating algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4400014-5524-4360-9c2f-988fa2fd114d",
   "metadata": {},
   "source": [
    "#### Using the policy\n",
    "\n",
    "- We haven't trained the agent yet, but we can still see what it does.\n",
    "- This is like calling `predict` before running `fit` with supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52c46765-0db9-4f0f-be98-6bd15fed1292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
    "obs = env.reset(seed=3)\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72c9f289-1c06-46a7-89bd-a8553104f1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "from utils import fix_frozen_lake_render\n",
    "fix_frozen_lake_render(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ce06add-1fc8-4211-9850-f419ddfca832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = ppo.compute_single_action(obs, explore=False)\n",
    "action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59ecbe8-4c8b-45d3-bfc7-d1f4e5d93684",
   "metadata": {},
   "source": [
    "- We gave the algorithm our initial observation, 0, and it recommended action 2 (right).\n",
    "- This action came from the initialized **policy**.\n",
    "- Remember, the policy maps observations to actions.\n",
    "- For now we'll ignore the `explore=False`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48087f0-f3bd-47fb-b368-51c1f28601ac",
   "metadata": {},
   "source": [
    "#### Using the policy\n",
    "\n",
    "We can see what happened after taking that action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b6a30b0-d591-4c30-b85e-fe839f518744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "üßäüßëüßäüßä\n",
      "üßäüï≥üßäüï≥\n",
      "üßäüßäüßäüï≥\n",
      "üï≥üßäüßä‚õ≥Ô∏è\n"
     ]
    }
   ],
   "source": [
    "obs, reward, done, _ = env.step(action)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7068f1-2f74-4d01-b6a6-455cdb964e9a",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "- So far our policy was just a random/arbitrary initialization.\n",
    "- What we want is to train it _based on experience interacting with the environment_.\n",
    "- In order to do this, RLlib will _play through many episodes_ and learn as it goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26694604-767d-437d-b29b-377d38c67999",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_info = ppo.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0024b5a6-b7de-4e0b-a2ac-1b17af442c99",
   "metadata": {},
   "source": [
    "- Note that, unlike sklearn's `fit()`, here we don't provide the dataset to `train()`.\n",
    "- We gave it the environment during initialization, and it uses the environment to generate data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67115d1f-5fe7-414e-9d56-41063ce0dedb",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "The `train` method returns a dictionary containing information about the iteration of training. We'll explore this next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c34155-1045-4582-8995-30ca063dd941",
   "metadata": {},
   "source": [
    "#### Training iterations\n",
    "\n",
    "- In fact, what we just did was one _iteration_ of training.\n",
    "- RLlib will play through a bunch of episodes per iteration, depending on its hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5e474bd-6aa7-4801-8268-f8b643d6d091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "536"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_info[\"hist_stats\"][\"episode_lengths\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e586701-6cc9-4706-83fc-66f8082c0ddd",
   "metadata": {},
   "source": [
    "Looks like it ran ~500 episodes in that one iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6461c7c3-4612-4649-be60-ef527c6abce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3991"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train_info[\"hist_stats\"][\"episode_lengths\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2786b671-c31c-4278-801e-41aa7a2a5f1b",
   "metadata": {},
   "source": [
    "For a total of ~4000 time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9af3fd-5931-471b-ba85-85de280e1501",
   "metadata": {},
   "source": [
    "#### RL mindset: data generation\n",
    "\n",
    "- Key departure from the supervised learning mindset\n",
    "- In SL, we take a fixed amount of data and train for some number of iterations\n",
    "- In RL, more iterations means more training _on more data_ because we learn from the environment as we interact with it\n",
    "- If you only play one episode, you might never see observation 10, so how can you learn what to do given observation 10?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dcef69-2b7c-4682-8163-ff2c84a7c43f",
   "metadata": {},
   "source": [
    "#### RL mindset\n",
    "\n",
    "What we had before, when we were controlling the agent manually:\n",
    "\n",
    "![](img/RL-loop-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa39ad76-d385-4048-a0f4-b52a5adb0596",
   "metadata": {},
   "source": [
    "#### RL mindset\n",
    "\n",
    "What we have now:\n",
    "\n",
    "![](img/RL-loop-3.png)\n",
    "\n",
    "We have an outer loop that involves training.\n",
    "\n",
    "Notes:\n",
    "\n",
    "Training / updating the agent may not necessarily occur after every single iteration, but this schematic gives a conceptual framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a4fb60-74bb-4e18-aac0-7f13ad5fa4a0",
   "metadata": {},
   "source": [
    "#### Training info: episode lengths\n",
    "\n",
    "Let's look at the lengths of the last 100 episodes we played:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69d6edba-b9d7-4e41-8fe5-c9181870faef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 9, 18, 3, 5, 2, 5, 5, 13, 18, 4, 6, 9, 3, 16, 4, 2, 3, 8, 4, 4, 4, 9, 6, 7, 2, 3, 6, 10, 9, 5, 7, 11, 19, 4, 7, 2, 2, 15, 6, 2, 10, 6, 13, 10, 11, 3, 10, 2, 8, 14, 12, 2, 4, 5, 2, 7, 4, 7, 5, 2, 3, 2, 6, 7, 2, 19, 2, 4, 12, 15, 7, 7, 11, 6, 2, 6, 6, 12, 6, 16, 8, 8, 4, 8, 4, 6, 4, 16, 2, 5, 11, 10, 6, 14, 2, 2, 6, 4, 14]\n"
     ]
    }
   ],
   "source": [
    "print(train_info[\"hist_stats\"][\"episode_lengths\"][-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb727bf9-bc04-4127-8290-4b4a76a961f2",
   "metadata": {},
   "source": [
    "- Remember that an episode ends when `.step()` returns `True` for the `done` flag.\n",
    "- We see some very short episodes, where the agent fell into a hole right away."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cdf5ae-db28-4da8-a55d-eb1c0222b277",
   "metadata": {},
   "source": [
    "#### Training info: episode rewards\n",
    "\n",
    "- For those longer episodes, did the agent reach the goal?\n",
    "- To assess this, we can print out the first 100 _episode rewards_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd6ffa8d-206d-47e6-8d5e-c2ce350c92a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print(train_info[\"hist_stats\"][\"episode_reward\"][-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8641cd6-7d8c-4d69-a4a9-0402f7ac0bde",
   "metadata": {},
   "source": [
    "And the average of these rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab826887-1e08-4821-9ea4-79baba755237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(train_info[\"hist_stats\"][\"episode_reward\"][-100:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c846b7-297a-4878-a6de-a91663176660",
   "metadata": {},
   "source": [
    "This is not very impressive. Let's keep training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb75a84-3cab-487b-8476-2740a686af92",
   "metadata": {},
   "source": [
    "#### More training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dcc1cbb9-0c16-447f-88a6-027bd5b05b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    train_info = ppo.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8413bd9d-59d5-4620-b5f1-2f4f466f2029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9778481012658228\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(train_info[\"hist_stats\"][\"episode_reward\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9e1c65-6eb7-4b72-adf6-f63845487fd4",
   "metadata": {},
   "source": [
    "- Nice! Now we're reaching the goal almost every time!\n",
    "- The _average reward_ (across the most recent training episodes) is 0.98.\n",
    "- Interpretation: we're reaching the goal 98% of the time.\n",
    "\n",
    "Notes:\n",
    "\n",
    "Why this interpretation? Because in this environment, we only receive a reward at the end of the episode, 0 for failure and 1 for success."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9227bbc-7b6b-40e2-b458-8bbe94b9a647",
   "metadata": {},
   "source": [
    "#### Training curves\n",
    "\n",
    "- In deep learning we often see diagrams of loss vs. epochs during training\n",
    "- We can get this in RL too by storing this each iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92699cd9-fc76-474c-9977-f1b7aa4a4035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ppo = PPO(env=\"FrozenLake-v1\", config=algo_config)\n",
    "\n",
    "rewards = []\n",
    "for i in range(10):\n",
    "    train_info = ppo.train()\n",
    "    rewards.append(train_info[\"episode_reward_mean\"])\n",
    "                   \n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(rewards);\n",
    "plt.xlabel(\"iterations\");\n",
    "plt.ylabel(\"average reward\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74335644-d4a4-41b2-9c52-b4fb334c6168",
   "metadata": {},
   "source": [
    "We can see the rewards increasing!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f4e2b6-da88-4245-8ccc-e81e53333491",
   "metadata": {},
   "source": [
    "Notes: If we store the episode reward mean for each training iteration, we can pr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134f484d-3aa5-4cc6-b878-869f591102ec",
   "metadata": {},
   "source": [
    "#### Declare victory?\n",
    "\n",
    "- We did well.\n",
    "- But, this non-slippery Frozen Lake is a very easy environment. \n",
    "- Kind of like a supervised learning dataset where y=x would be \"easy\".\n",
    "- Later we'll ramp up the difficulty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80d7905-3fab-417f-b9b1-c9ffa4d4cecf",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Using the policy\n",
    "\n",
    "We can run the **observation-policy-action loop** for multiple time steps to watch the policy in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06f47779-3725-4bb6-9c92-f1877fa148b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "üßäüßäüßäüßä\n",
      "üßëüï≥üßäüï≥\n",
      "üßäüßäüßäüï≥\n",
      "üï≥üßäüßä‚õ≥Ô∏è\n",
      "  (Down)\n",
      "üßäüßäüßäüßä\n",
      "üßäüï≥üßäüï≥\n",
      "üßëüßäüßäüï≥\n",
      "üï≥üßäüßä‚õ≥Ô∏è\n",
      "  (Right)\n",
      "üßäüßäüßäüßä\n",
      "üßäüï≥üßäüï≥\n",
      "üßäüßëüßäüï≥\n",
      "üï≥üßäüßä‚õ≥Ô∏è\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "\n",
    "for i in range(3):\n",
    "    action = ppo.compute_single_action(obs, explore=False)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed77ebb4-1aa0-432e-b725-79cae6f3c2b9",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "The relevant trainer method in RLlib is compute_single_action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f9fe3e-a69d-4a40-ba94-db9f3c39ee54",
   "metadata": {},
   "source": [
    "#### Using the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af239179-38df-4a33-bbea-4c7b4ae9c684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "üßäüßäüßäüßä\n",
      "üßäüï≥üßäüï≥\n",
      "üßäüßäüßëüï≥\n",
      "üï≥üßäüßä‚õ≥Ô∏è\n",
      "  (Down)\n",
      "üßäüßäüßäüßä\n",
      "üßäüï≥üßäüï≥\n",
      "üßäüßäüßäüï≥\n",
      "üï≥üßäüßë‚õ≥Ô∏è\n",
      "  (Right)\n",
      "üßäüßäüßäüßä\n",
      "üßäüï≥üßäüï≥\n",
      "üßäüßäüßäüï≥\n",
      "üï≥üßäüßäüßë\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    action = ppo.compute_single_action(obs, explore=False)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8753cf53-da7f-4fad-b67c-200a9e844a85",
   "metadata": {},
   "source": [
    "Using this policy we reliably reach the goal every time because the non-slippery Frozen Lake environment is deterministic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680904ff-bd6a-40f4-800f-5bc225b5d850",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Evaluation\n",
    "\n",
    "In RLlib, we can evaluate with `.evaluate()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20596f3c-893f-4b0d-88ba-7d7b9e349033",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = ppo.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed056a47-6dc8-4846-89b4-719537a0b9f6",
   "metadata": {},
   "source": [
    "Training does not occur during evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f94835f-b1cf-40de-a9f2-e5852bc06c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9501557632398754"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results[\"evaluation\"][\"episode_reward_mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dccc5c8-bf38-4efc-9a68-6d7af6a6b8ab",
   "metadata": {},
   "source": [
    "Here we get similar results to the output of `.train()`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c110e98d-2f5a-4750-acff-4703e239672a",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "The evaluation output contains a lot of info, including many of the same fields as the training output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6a8e52-ab9e-4a1f-b704-15525065dee1",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "Histogram of episode rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bd354df4-96b4-45dc-9ede-f89089794093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAPbUlEQVR4nO3db4xcV33G8e+DHVJaUEnqTeTapusi0+JUjaFbNyptFUhFQnjhIBHktAILRTJVQwsSL3B40SBVloJUoKragAxEuBLFWBAat1Bo6kJTBCRskEnimBSXuMliK17+tPyplMrOry/2pkztXc94Z2aXPf5+pNXce+45c39Hu3r2+uyd61QVkqS2PGu5C5AkjZ7hLkkNMtwlqUGGuyQ1yHCXpAatXu4CANasWVOTk5PLXYYkrSgPPPDAt6tqYr5jPxHhPjk5yfT09HKXIUkrSpL/WOiYyzKS1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktSgvp9QTfJTwL3AxV3/j1fVbUkuBT4GTALHgNdV1fe6MbcCNwOngT+uqs+OpXpJGoHJXZ9atnMfu/3VY3nfQa7cnwJeUVVXAluA65JcBewCDlbVJuBgt0+SzcB24ArgOuCOJKvGULskaQF9w73m/LDbvaj7KmAbsLdr3wvc0G1vA/ZV1VNV9RhwFNg6yqIlSec20Jp7klVJDgEngXuq6j7g8qo6AdC9XtZ1Xwc80TN8pms78z13JplOMj07OzvEFCRJZxoo3KvqdFVtAdYDW5P8yjm6Z763mOc991TVVFVNTUzM+8RKSdIindfdMlX1n8DnmVtLfzLJWoDu9WTXbQbY0DNsPXB82EIlSYPrG+5JJpI8v9t+DvC7wNeBA8COrtsO4O5u+wCwPcnFSTYCm4D7R1y3JOkcBvnPOtYCe7s7Xp4F7K+qv0/yJWB/kpuBx4EbAarqcJL9wCPAKeCWqjo9nvIlSfPpG+5V9SDwknnavwNcs8CY3cDuoauTJC2Kn1CVpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAb1DfckG5J8LsmRJIeTvKVrf2eSbyU51H1d3zPm1iRHkzya5NpxTkCSdLbVA/Q5Bbytqr6a5HnAA0nu6Y69t6r+rLdzks3AduAK4OeBf0ryoqo6PcrCJUkL63vlXlUnquqr3fYPgCPAunMM2Qbsq6qnquox4CiwdRTFSpIGc15r7kkmgZcA93VNb07yYJI7k1zSta0DnugZNsM8vwyS7EwynWR6dnb2/CuXJC1o4HBP8lzgE8Bbq+r7wPuAFwJbgBPAu5/pOs/wOquhak9VTVXV1MTExPnWLUk6h4HCPclFzAX7R6rqLoCqerKqTlfV08AH+PHSywywoWf4euD46EqWJPUzyN0yAT4EHKmq9/S0r+3p9hrg4W77ALA9ycVJNgKbgPtHV7IkqZ9B7pZ5GfB64KEkh7q2dwA3JdnC3JLLMeBNAFV1OMl+4BHm7rS5xTtlJGlp9Q33qvoC86+jf/ocY3YDu4eoS5I0BD+hKkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkN6hvuSTYk+VySI0kOJ3lL135pknuSfKN7vaRnzK1JjiZ5NMm145yAJOlsg1y5nwLeVlUvBq4CbkmyGdgFHKyqTcDBbp/u2HbgCuA64I4kq8ZRvCRpfn3DvapOVNVXu+0fAEeAdcA2YG/XbS9wQ7e9DdhXVU9V1WPAUWDriOuWJJ3Dea25J5kEXgLcB1xeVSdg7hcAcFnXbR3wRM+wma7tzPfamWQ6yfTs7OwiSpckLWTgcE/yXOATwFur6vvn6jpPW53VULWnqqaqampiYmLQMiRJAxgo3JNcxFywf6Sq7uqan0yytju+FjjZtc8AG3qGrweOj6ZcSdIgBrlbJsCHgCNV9Z6eQweAHd32DuDunvbtSS5OshHYBNw/upIlSf2sHqDPy4DXAw8lOdS1vQO4Hdif5GbgceBGgKo6nGQ/8Ahzd9rcUlWnR124JGlhfcO9qr7A/OvoANcsMGY3sHuIuiRJQ/ATqpLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUoL7hnuTOJCeTPNzT9s4k30pyqPu6vufYrUmOJnk0ybXjKlyStLBBrtw/DFw3T/t7q2pL9/VpgCSbge3AFd2YO5KsGlWxkqTB9A33qroX+O6A77cN2FdVT1XVY8BRYOsQ9UmSFmGYNfc3J3mwW7a5pGtbBzzR02emaztLkp1JppNMz87ODlGGJOlMiw339wEvBLYAJ4B3d+2Zp2/N9wZVtaeqpqpqamJiYpFlSJLms6hwr6onq+p0VT0NfIAfL73MABt6uq4Hjg9XoiTpfC0q3JOs7dl9DfDMnTQHgO1JLk6yEdgE3D9ciZKk87W6X4ckHwWuBtYkmQFuA65OsoW5JZdjwJsAqupwkv3AI8Ap4JaqOj2WyiVJC+ob7lV10zzNHzpH/93A7mGKkiQNx0+oSlKDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KD+oZ7kjuTnEzycE/bpUnuSfKN7vWSnmO3Jjma5NEk146rcEnSwga5cv8wcN0ZbbuAg1W1CTjY7ZNkM7AduKIbc0eSVSOrVpI0kL7hXlX3At89o3kbsLfb3gvc0NO+r6qeqqrHgKPA1tGUKkka1GLX3C+vqhMA3etlXfs64ImefjNd21mS7EwynWR6dnZ2kWVIkuYz6j+oZp62mq9jVe2pqqmqmpqYmBhxGZJ0YVtsuD+ZZC1A93qya58BNvT0Ww8cX3x5kqTFWGy4HwB2dNs7gLt72rcnuTjJRmATcP9wJUqSztfqfh2SfBS4GliTZAa4Dbgd2J/kZuBx4EaAqjqcZD/wCHAKuKWqTo+pdknSAvqGe1XdtMChaxbovxvYPUxRkqTh+AlVSWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWrQ6mEGJzkG/AA4DZyqqqkklwIfAyaBY8Drqup7w5UpSTofo7hyf3lVbamqqW5/F3CwqjYBB7t9SdISGseyzDZgb7e9F7hhDOeQJJ3DsOFewD8meSDJzq7t8qo6AdC9XjbfwCQ7k0wnmZ6dnR2yDElSr6HW3IGXVdXxJJcB9yT5+qADq2oPsAdgamqqhqxDktRjqCv3qjrevZ4EPglsBZ5Mshagez05bJGSpPOz6HBP8jNJnvfMNvBK4GHgALCj67YDuHvYIiVJ52eYZZnLgU8meeZ9/qaqPpPkK8D+JDcDjwM3Dl+mJOl8LDrcq+qbwJXztH8HuGaYoiRJw/ETqpLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktSgYf6bvZ8Yk7s+tSznPXb7q5flvJLUj1fuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1aGzhnuS6JI8mOZpk17jOI0k621jCPckq4K+AVwGbgZuSbB7HuSRJZxvXlftW4GhVfbOq/gfYB2wb07kkSWcY1+MH1gFP9OzPAL/R2yHJTmBnt/vDJI8Ocb41wLeHGL8oeddSn/H/LMt8l5lzvjBccHPOu4aa8y8sdGBc4Z552ur/7VTtAfaM5GTJdFVNjeK9VoILbb7gnC8Uznl0xrUsMwNs6NlfDxwf07kkSWcYV7h/BdiUZGOSZwPbgQNjOpck6QxjWZapqlNJ3gx8FlgF3FlVh8dxrs5IlndWkAttvuCcLxTOeURSVf17SZJWFD+hKkkNMtwlqUErJtz7Pc4gc/6iO/5gkpcuR52jNMCcf7+b64NJvpjkyuWoc5QGfWxFkl9PcjrJa5eyvnEYZM5Jrk5yKMnhJP+y1DWO2gA/2z+b5O+SfK2b8xuXo85RSXJnkpNJHl7g+Ojzq6p+4r+Y+6PsvwO/CDwb+Bqw+Yw+1wP/wNw99lcB9y133Usw598ELum2X3UhzLmn3z8DnwZeu9x1L8H3+fnAI8ALuv3LlrvuJZjzO4B3ddsTwHeBZy937UPM+XeAlwIPL3B85Pm1Uq7cB3mcwTbgr2vOl4HnJ1m71IWOUN85V9UXq+p73e6Xmfs8wUo26GMr/gj4BHByKYsbk0Hm/HvAXVX1OEBVrfR5DzLnAp6XJMBzmQv3U0tb5uhU1b3MzWEhI8+vlRLu8z3OYN0i+qwk5zufm5n7zb+S9Z1zknXAa4D3L2Fd4zTI9/lFwCVJPp/kgSRvWLLqxmOQOf8l8GLmPvz4EPCWqnp6acpbFiPPr3E9fmDU+j7OYMA+K8nA80nycubC/bfGWtH4DTLnPwfeXlWn5y7qVrxB5rwa+DXgGuA5wJeSfLmq/m3cxY3JIHO+FjgEvAJ4IXBPkn+tqu+PubblMvL8WinhPsjjDFp75MFA80nyq8AHgVdV1XeWqLZxGWTOU8C+LtjXANcnOVVVf7skFY7eoD/b366qHwE/SnIvcCWwUsN9kDm/Ebi95hakjyZ5DPhl4P6lKXHJjTy/VsqyzCCPMzgAvKH7q/NVwH9V1YmlLnSE+s45yQuAu4DXr+CruF5951xVG6tqsqomgY8Df7iCgx0G+9m+G/jtJKuT/DRzT1g9ssR1jtIgc36cuX+pkORy4JeAby5plUtr5Pm1Iq7ca4HHGST5g+74+5m7c+J64Cjw38z95l+xBpzznwA/B9zRXcmeqhX8RL0B59yUQeZcVUeSfAZ4EHga+GBVzXtL3Uow4Pf5T4EPJ3mIuSWLt1fVin0UcJKPAlcDa5LMALcBF8H48svHD0hSg1bKsowk6TwY7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalB/wuEqCkOD10pAwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(eval_results[\"evaluation\"][\"hist_stats\"][\"episode_reward\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144df6d5-e45e-4d6e-a2bf-a0332211105f",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "Histogram of episode _lengths_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62486690-de69-4b8c-94eb-e0290d5b90e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(eval_results[\"evaluation\"][\"hist_stats\"][\"episode_lengths\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0198fc0-5477-4bec-8680-2d6101fac0a8",
   "metadata": {},
   "source": [
    "- The short ones are the failures, since it's impossible to reach the goal in 4 steps.\n",
    "- Most of the time we reach the goal in the minimum number of steps (6)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c8070f-8ac9-4026-b049-ceeffa94775b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### sklearn / RLlib analogies\n",
    "\n",
    "- We've seen some analogies (and departures) between SL and RL, both in concept and syntax. \n",
    "- Let's compare:\n",
    "\n",
    "| SL/sklearn | RL/RLlib | Description  |\n",
    "|-------------|---------|--------------|\n",
    "| `ModelName(**hypers)` | `AlgoName(hypers, env)` | Initialize a model/algorithm |\n",
    "| `.fit(X,y)`  | `.train()` | Training (fully for sklearn, one iteration for RLlib) |\n",
    "| `.predict(x)` | `.compute_single_action(obs)` | Use the trained model once |\n",
    "| `.score(X,y)` | `.evaluate()` | Evaluate the model |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d205da70-1255-437c-8a99-6ea8a5e8d2a0",
   "metadata": {},
   "source": [
    "#### Let's apply what we learned!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd539d44-aa7c-4958-b448-7759f3adc20e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## RLlib algorithm methods\n",
    "<!-- multiple choice -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17580d1a-2245-4390-bfee-449b1bfff570",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "_Which of the following most accurately describes the role of `algorithm.train()` in RLlib?_\n",
    "\n",
    "- [ ] It neither collects a data set of episodes nor learns a policy. | Are you sure?\n",
    "- [ ] It learns a policy from a fixed data set of episodes. | Remember, calling train() causes the agent to play through episodes.\n",
    "- [ ] It creates a data set of episodes but does not learn a policy. | Remember, calling train() learns a policy.\n",
    "- [x] It simultaneously collects a data set of episodes and also learns a policy. | You got it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48245b9-3007-4d9c-a657-97978c4fea1e",
   "metadata": {},
   "source": [
    "#### Passing in the dataset\n",
    "\n",
    "_When using scikit-learn for supervised learning we call `fit(X,y)`, but with RLlib we call `train()` without passing in the dataset. Why?_\n",
    "\n",
    "- [x] Because RLlib was given access to the environment when the algorithm was initialized, and this is all it needs.\n",
    "- [ ] Because reinforcement learning does not involve data.\n",
    "- [ ] Because X and y are passed into a different RLlib method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc34288f-7416-4041-8b70-5182715bfd0e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Predicting\n",
    "\n",
    "_Which of the following RLlib algorithm methods is most analogous to scikit-learn's `.predict()` function?_\n",
    "\n",
    "- [ ] train() | This is more like _fit()_ in scikit-learn.\n",
    "- [x] compute_single_action()\n",
    "- [ ] evaluate() | This is more like _score()_ in scikit-learn. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34d5c5b-a1be-447a-abe6-e034c9487cce",
   "metadata": {},
   "source": [
    "## Slippery Frozen Lake\n",
    "<!-- coding exercise -->\n",
    "\n",
    "In the slides, we trained an agent to reliably reach the goal in the **non-slippery** Frozen Lake environment. Here, try the same thing with the **slippery** Frozen Lake. Train your agent for enough iterations such it reaches the goal at least 20% of the time. Then, answer the multiple choice question below.\n",
    "\n",
    "Note: we will discuss the algorithm config in the next set of slides. For now just note that we're using the slippery version of Frozen Lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2847c21-1709-4039-b77d-d634e43e6096",
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '____' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgorithms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mppo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PPO, PPOConfig\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m slippery_algo_config\n\u001b[0;32m----> 5\u001b[0m ppo \u001b[38;5;241m=\u001b[39m PPO(env\u001b[38;5;241m=\u001b[39m\u001b[43m____\u001b[49m, config\u001b[38;5;241m=\u001b[39mslippery_algo_config)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(____):\n\u001b[1;32m      8\u001b[0m     train_info \u001b[38;5;241m=\u001b[39m ppo\u001b[38;5;241m.\u001b[39m____()\n",
      "\u001b[0;31mNameError\u001b[0m: name '____' is not defined"
     ]
    }
   ],
   "source": [
    "# EXERCISE\n",
    "from ray.rllib.algorithms.ppo import PPO, PPOConfig\n",
    "from utils import slippery_algo_config\n",
    "\n",
    "ppo = PPO(env=____, config=slippery_algo_config)\n",
    "\n",
    "for i in range(____):\n",
    "    train_info = ppo.____()\n",
    "    \n",
    "eval_results = ____.evaluate()\n",
    "\n",
    "print(eval_results[\"evaluation\"][\"episode_reward_mean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e2f2fdf9-ec1a-488d-8864-0a929e42190c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of reaching goal: 48.2%\n",
      "Action performed from top-right: 3\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "from ray.rllib.algorithms.ppo import PPO, PPOConfig\n",
    "# from utils import slippery_algo_config\n",
    "\n",
    "ppo = PPO(env=\"FrozenLake-v1\", config=slippery_algo_config)\n",
    "\n",
    "for i in range(30): # There is randomness here, but 20 should be enough most of the time\n",
    "    train_info = ppo.train()\n",
    "    \n",
    "eval_results = ppo.evaluate()\n",
    "\n",
    "print(\"Frequency of reaching goal: %.1f%%\" % (eval_results[\"evaluation\"][\"episode_reward_mean\"]*100))\n",
    "\n",
    "print(\"Action performed from top-right:\", ppo.compute_single_action(3, explore=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6485b269-1c62-4cd8-a8d2-0a6cea590aeb",
   "metadata": {},
   "source": [
    "#### Action performed from top-right\n",
    "\n",
    "According to the trained policy, what action is performed when the agent is at the top-right of the arena? This is printed out by the code.\n",
    "\n",
    "- [ ] left (0) | When we ran the code, we got something different here.\n",
    "- [ ] down (1) | When we ran the code, we got something different here.\n",
    "- [ ] right (2) | When we ran the code, we got something different here.\n",
    "- [x] up (3) | You got it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb1e37b-ca77-41ba-ba20-6a9558485b80",
   "metadata": {},
   "source": [
    "#### Interpreting the policy\n",
    "\n",
    "Recall that the arena looks like this:\n",
    "\n",
    "```\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "```\n",
    "\n",
    "In this slippery environment, you do your intended action 1/3 of the time, and each of the two perpendicular directions 1/3 of the time. You never go the opposite of the intended direction.\n",
    "\n",
    "In the previous question, we saw that, from the **top-right corner** (observation 3), the agent tries to move up (action 3). Why do you think the agent tries to move up?\n",
    "\n",
    "- [ ] 'Up' is an arbitrary choice because we did not train the agent.\n",
    "- [ ] The agent receives a reward for the 'up' action. \n",
    "- [x] The agent wants to avoid falling into the hole below it, so the 'up' action is the safest choice.\n",
    "- [ ] Moving up brings the agent closer to the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c553472d-69ad-49af-8282-5671d6cb9c15",
   "metadata": {},
   "source": [
    "## Rendering the trained agent\n",
    "<!-- coding exercise -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3362ef16-d73a-47c3-8b1b-737fd6931041",
   "metadata": {},
   "source": [
    "Fill in the blanks in the code below so that the code performs the observation-action-reward loop for one episode. Then, run the code and watch the trained agent navigate the slippery Frozen Lake. Then, answer the multiple choice question below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f2996d2f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ray.rllib.agents.ppo.ppo_torch_policy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m     11\u001b[0m ppo \u001b[38;5;241m=\u001b[39m PPO(env\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrozenLake-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m, config\u001b[38;5;241m=\u001b[39mslippery_algo_config)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mppo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/FrozenLakeSlippery/checkpoint-20\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrozenLake-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m, is_slippery\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m out \u001b[38;5;241m=\u001b[39m Output();\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/tune/trainable/trainable.py:585\u001b[0m, in \u001b[0;36mTrainable.restore\u001b[0;34m(self, checkpoint_path, checkpoint_node_ip)\u001b[0m\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_checkpoint(checkpoint_dict)\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 585\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time_since_restore \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timesteps_since_restore \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/rllib/algorithms/algorithm.py:1444\u001b[0m, in \u001b[0;36mAlgorithm.load_checkpoint\u001b[0;34m(self, checkpoint_path)\u001b[0m\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;129m@override\u001b[39m(Trainable)\n\u001b[1;32m   1442\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_checkpoint\u001b[39m(\u001b[38;5;28mself\u001b[39m, checkpoint_path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1443\u001b[0m     extra_data \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(checkpoint_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 1444\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__setstate__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextra_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/rllib/algorithms/algorithm.py:2199\u001b[0m, in \u001b[0;36mAlgorithm.__setstate__\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m   2197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__setstate__\u001b[39m(\u001b[38;5;28mself\u001b[39m, state: \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m   2198\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworkers\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworker\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m state:\n\u001b[0;32m-> 2199\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_worker\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mworker\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2200\u001b[0m         remote_state \u001b[38;5;241m=\u001b[39m ray\u001b[38;5;241m.\u001b[39mput(state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworker\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   2201\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers\u001b[38;5;241m.\u001b[39mremote_workers():\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/rllib/evaluation/rollout_worker.py:1554\u001b[0m, in \u001b[0;36mRolloutWorker.restore\u001b[0;34m(self, objs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;129m@DeveloperAPI\u001b[39m\n\u001b[1;32m   1540\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrestore\u001b[39m(\u001b[38;5;28mself\u001b[39m, objs: \u001b[38;5;28mbytes\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1541\u001b[0m     \u001b[38;5;124;03m\"\"\"Restores this RolloutWorker's state from a sequence of bytes.\u001b[39;00m\n\u001b[1;32m   1542\u001b[0m \n\u001b[1;32m   1543\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;124;03m        >>> new_worker.restore(state) # doctest: +SKIP\u001b[39;00m\n\u001b[1;32m   1553\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1554\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1555\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msync_filters(objs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilters\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1556\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pid, state \u001b[38;5;129;01min\u001b[39;00m objs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ray.rllib.agents.ppo.ppo_torch_policy'"
     ]
    }
   ],
   "source": [
    "# EXERCISE\n",
    "from ray.rllib.algorithms.ppo import PPO, PPOConfig\n",
    "from utils import slippery_algo_config\n",
    "\n",
    "import gym\n",
    "\n",
    "from ipywidgets import Output\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "ppo = PPO(env=\"FrozenLake-v1\", config=slippery_algo_config)\n",
    "ppo.restore(\"models/FrozenLakeSlippery/checkpoint-20\")\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "\n",
    "out = Output();\n",
    "display.display(out);\n",
    "\n",
    "with out:\n",
    "    obs = env.reset()\n",
    "    env.seed(1)\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = ppo.____(____, explore=False)\n",
    "        obs, rewards, done, _ = env.step(____)\n",
    "        \n",
    "        out.clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5e7f1f-cde4-4a3e-83b9-ab6334c83cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "from ray.rllib.algorithms.ppo import PPO, PPOConfig\n",
    "from utils import slippery_algo_config\n",
    "\n",
    "from ipywidgets import Output\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "ppo = PPO(env=\"FrozenLake-v1\", config=slippery_algo_config)\n",
    "ppo.restore(\"models/FrozenLakeSlippery/checkpoint-20\")\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "\n",
    "out = Output();\n",
    "display.display(out);\n",
    "\n",
    "with out:\n",
    "    obs = env.reset()\n",
    "    env.seed(1)\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = slippery_trainer.compute_single_action(obs, explore=False)\n",
    "        obs, rewards, done, _ = env.step(action)\n",
    "        \n",
    "        out.clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e291218e-1c96-4163-b617-42450992927c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# we may be able to clean this up and remove the whole out/Output thing, and just use display for everything (see cartpole 2.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ef9dd9-99ea-4acf-ad2a-b09f2b534f5e",
   "metadata": {},
   "source": [
    "#### Choose the option below that best describes the agent's treacherous journey to the goal.\n",
    "\n",
    "- [ ] The agent visits the top-right square in this episode.\n",
    "- [ ] The agent never returns to the start state after leaving it initially.\n",
    "- [ ] The agent does not reach the goal during the episode.\n",
    "- [x] The agent sees observation 11 exactly once on its journey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca0f7c4-786f-4105-9121-5f8122357d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ray2beta]",
   "language": "python",
   "name": "conda-env-ray2beta-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
