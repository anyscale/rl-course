{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90c01ab9-1cd0-47d8-a198-e07e36e2c1fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## RLlib continued"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e74417a3-63bc-4000-951a-72ab882b689e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27a0a477-40eb-44e1-b3aa-59e4f6f31fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import ray\n",
    "import logging\n",
    "ray.init(log_to_driver=False, ignore_reinit_error=True, logging_level=logging.ERROR); # logging.FATAL\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08a296a4-e50a-4530-8034-c8bc1e27671c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "from ray.rllib.algorithms.ppo import PPO, PPOConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f67da7-14aa-456a-84bb-5813db493143",
   "metadata": {},
   "source": [
    "#### RLlib features\n",
    "\n",
    "- So far we've seen training, evaluation, and \"prediction\" with RLlib.\n",
    "- Next we'll explore a few more features:\n",
    "   - Algorithm configs\n",
    "   - Saving/restoring models\n",
    "   - Interpreting stochastic policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e07d9e0-cc95-43f0-81b4-2b937070a7f2",
   "metadata": {},
   "source": [
    "#### Algorithm configs\n",
    "\n",
    "Remember this code?\n",
    "\n",
    "```python\n",
    "from ray.rllib.algorithms.ppo import PPO, PPOConfig\n",
    "ppo = PPO(env=\"FrozenLake-v1\", config=ppo_config)\n",
    "```\n",
    "\n",
    "Previously, we hid the `ppo_config`. Now we'll delve into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6e49ba6-315f-42d5-949a-6ae5d98ae758",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_config = PPOConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae2a8631-271c-4f85-8715-6cf1f95393f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HIDDEN - this shows up in module 5 instead\n",
    "# The number of options is way too many to read:\n",
    "# len(ppo_config.to_dict())\n",
    "# # ppo_config.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af4f47b-ef9a-4443-9dad-d5bb2e533b3b",
   "metadata": {},
   "source": [
    "#### Algorithm configs\n",
    "\n",
    "- When we instantiate a `PPOConfig()` we get the default config values.\n",
    "- There are a few values that we changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0308301b-4f37-4218-8360-862b18574c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_config = ppo_config.framework(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78fb6f4-36b4-424d-8535-9473934b7e88",
   "metadata": {},
   "source": [
    "⬆️ Changes the framework from tensorflow (default) to pytorch for the policy model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d746f6bf-d5d0-41b6-b717-796d91a45f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_config = ppo_config.debugging(seed=0, log_level=\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ca478e-2c3b-4c47-beba-94014f376469",
   "metadata": {},
   "source": [
    "⬆️ Sets a random seed for reproducibility of the course, reduces the warnings displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8890eef-bbad-41a7-af8e-7fac509eb04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_config = ppo_config.training(model={\"fcnet_hiddens\" : [32, 32]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4ecc0b-0d85-4aec-8011-28a2adc309f3",
   "metadata": {},
   "source": [
    "⬆️ Sets the policy neural network to have a smaller-than-default architecture, which helps the course materials run faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "098f8aa8-16b3-455e-9a92-4d92063da388",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_config = ppo_config.rollouts(create_env_on_local_worker=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68b5322-8fb4-4d07-9913-6696aa8905cf",
   "metadata": {},
   "source": [
    "⬆️ This relates to Ray, which we will touch on briefly in Module 5, but skip for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e215df8-9e3e-4607-a831-b083cd949cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_config = ppo_config.environment(env_config={\"is_slippery\" : False})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc6380d-b76e-4960-a3c0-9e1618109ebd",
   "metadata": {},
   "source": [
    "⬆️ This is how we set env parameters, in this case specifying the non-slippery Frozen Lake."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322b9bd0-8426-4648-80c0-479f96f6cddc",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "Remember, the policy is a function from observations to actions, and this function is a neural network. That is why we need a deep learning framework like pytorch.\n",
    "\n",
    "The random seed is only for the algorithm itself, e.g. randomness in the neural network optimization. It doesn't set the random seed for the environment's own randomness. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e834f6c3-0b1f-49dc-8c65-136e8b93fa5f",
   "metadata": {},
   "source": [
    "#### Algorithm configs\n",
    "\n",
    "We can also generate this config in one giant line of Python as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "447b397b-a875-417b-8bc7-ef862229ca64",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_config = (\n",
    "    PPOConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(create_env_on_local_worker=True)\\\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\\\n",
    "    .training(model={\"fcnet_hiddens\" : [32, 32]})\\\n",
    "    .environment(env_config={\"is_slippery\" : False})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7254e6af-7f07-4afc-b46d-575214eed645",
   "metadata": {},
   "source": [
    "(The `\\` character just means the line of Python continues below.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cfa9d1-fbfa-455f-a722-950b634d4d45",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "We'll discuss tuning in Module 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30eb0986-9e52-4656-9819-7c43b6d083a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "algo_config = (\n",
    "    PPOConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(create_env_on_local_worker=True)\\\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\\\n",
    "    .training(model={\"fcnet_hiddens\" : [32, 32]})\\\n",
    "    .environment(env_config={\"is_slippery\" : False})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185074d6-c847-4790-94ba-381030616f56",
   "metadata": {},
   "source": [
    "#### Building an algorithm from the config\n",
    "\n",
    "We previously instantiated our PPO algorithm like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f8b977e-e59d-4afd-9d82-2010be65ed40",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo = PPO(env=\"FrozenLake-v1\", config=ppo_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499bf30c-c42a-40d5-957c-477686ee7ee7",
   "metadata": {},
   "source": [
    "The preferred syntax in the latest release of Ray/RLlib is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "217ddfaf-0332-42a8-8178-23c949a80b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo = ppo_config.build(env=\"FrozenLake-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e483661-3f65-4f61-8db4-3ac3a650619d",
   "metadata": {},
   "source": [
    "As a third option, you can also specify the environment in the config rather than as an argument to `build`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2be522c-6b40-4b0d-8cf3-c473b420c325",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_config.environment(env=\"FrozenLake-v1\")\n",
    "\n",
    "ppo = ppo_config.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef84da1-19ef-4300-8555-7e6e65b3e3b7",
   "metadata": {},
   "source": [
    "The end result is the same whichever way you choose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e3069d-b826-447f-ad34-24eb1d912c4a",
   "metadata": {},
   "source": [
    "#### Saving models\n",
    "\n",
    "- We may want to save trained agents for future use.\n",
    "- This is also called _checkpointing_, especially when done during a training loop.\n",
    "- In RLlib, this can be done simply with:\n",
    "\n",
    "```python\n",
    "algo.save(path_to_checkpoint)\n",
    "```\n",
    "\n",
    "It can then be later restored with\n",
    "\n",
    "```python\n",
    "algo.restore(path_to_checkpoint)\n",
    "```\n",
    "\n",
    "Just make sure you create the trainer with the same environment and parameters when restoring from a checkpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb84cad-1bc0-4bd0-9f80-9360e71c9b90",
   "metadata": {},
   "source": [
    "### TODO!!!!\n",
    "\n",
    "TODO\n",
    "\n",
    "show the saving\n",
    "show the loading\n",
    "start with non slippery\n",
    "then compare with the slippery to see how when slippery it now goes up and left sometimes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27092e6c-772e-4940-b140-fa77b8469397",
   "metadata": {},
   "source": [
    "#### Restoring models\n",
    "\n",
    "Let's restore a PPO algorithm checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "517db53e-410c-441f-8823-53b76a3cf825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # HIDDEN\n",
    "# ppo_config = (\n",
    "#     PPOConfig()\\\n",
    "#     .framework(\"torch\")\\\n",
    "#     .rollouts(create_env_on_local_worker=True)\\\n",
    "#     .debugging(seed=0, log_level=\"ERROR\")\\\n",
    "#     .training(model={\"fcnet_hiddens\" : [32, 32]})\n",
    "#     .environment(env_config={\"is_slippery\" : True})\\\n",
    "#     .evaluation(evaluation_config = {\"explore\" : False})\n",
    "# )\n",
    "\n",
    "# ppo = ppo_config.build(\"FrozenLake-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7ec02f5-d9f0-4b67-a69c-83a075b49cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # HIDDEN\n",
    "# for i in range(30):\n",
    "#     ppo.train()\n",
    "\n",
    "# ppo.save(\"models/FrozenLakeSlippery-3232-Ray2-2\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c6d8542-6f88-4b2d-9627-34a16add29e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # HIDDEN\n",
    "# ppo_config = (\n",
    "#     PPOConfig()\\\n",
    "#     .framework(\"torch\")\\\n",
    "#     .rollouts(create_env_on_local_worker=True)\\\n",
    "#     .debugging(seed=0, log_level=\"ERROR\")\\\n",
    "#     .training(model={\"fcnet_hiddens\" : [32, 32]})\n",
    "#     .environment(env_config={\"is_slippery\" : False})\\\n",
    "#     .evaluation(evaluation_config = {\"explore\" : False})\n",
    "# )\n",
    "\n",
    "# ppo = ppo_config.build(\"FrozenLake-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "107266f4-2f49-43c5-a9f8-80053d048017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # HIDDEN\n",
    "# for i in range(30):\n",
    "#     ppo.train()\n",
    "\n",
    "# ppo.save(\"models/FrozenLakeNonSlippery-3232-Ray2-2\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e39804d-b683-404c-9a7a-443569b284d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo.restore(\"models/FrozenLakeSlippery-3232-Ray2/checkpoint_000030\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370d6ffb-b48b-4209-b7b8-ce0cb4d45e8c",
   "metadata": {},
   "source": [
    "#### Viewing stochastic policies\n",
    "\n",
    "- If we want to see what a policy does, we can use `compute_single action`\n",
    "- But, some RL algorithms, including PPO, learn _stochastic policies_.\n",
    "- We may wish to look at these _action probabilities_.\n",
    "- The code can be viewed on GitHub, but is hidden here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc24efa0-bc12-4d99-b9c6-88fc9bf10550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7096f91a-7a14-4b9a-905b-603583d46c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81533301-9caa-4fcc-a354-45ac8f98dabc",
   "metadata": {},
   "source": [
    "We'll use the trainer we just restored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9cd397c9-9182-4391-a997-b1e8f23d1c5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.0708601e-04, 8.6194783e-01, 1.3743131e-01, 3.1362689e-04],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = utils.query_policy(ppo, env, obs=0)\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92923e7-e5a9-40e2-8df3-3870306456c0",
   "metadata": {},
   "source": [
    "Actions: left (0), down (1), right (2), up (3)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dbf19d-6ce9-4410-893e-9d0bd8bc2b3b",
   "metadata": {},
   "source": [
    "#### Viewing stochastic policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc50e996-b713-4fd5-a9ce-62fb24148e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left:   0.03%\n",
      "Down:  86.19%\n",
      "Right: 13.74%\n",
      "Up:     0.03%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Left:  {probs[0]*100:5.2f}%\")\n",
    "print(f\"Down:  {probs[1]*100:5.2f}%\")\n",
    "print(f\"Right: {probs[2]*100:5.2f}%\")\n",
    "print(f\"Up:    {probs[3]*100:5.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43dbb9c-06f8-4e5e-8c7f-5349d39623c4",
   "metadata": {},
   "source": [
    "Arena:\n",
    "\n",
    "```\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e800476-34c2-4fe4-bb1e-7f9c7e77e94a",
   "metadata": {},
   "source": [
    "Notes:\n",
    "    \n",
    "What we see here are the action probabilities. From the top-left, the agent considers moving left down or right, but not up. Presumably left is preferred to up because slipping down is preferred to slipping right.\n",
    "\n",
    "For a continuous action space this would be different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44268a54-4855-434d-ac21-d2801f46a322",
   "metadata": {},
   "source": [
    "#### Viewing stochastic policies\n",
    "\n",
    "Let's view all the probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4a0e388-5ec4-489b-a4cd-53531859fe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8fdedc6-f162-4c92-a674-648f7ae154c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>left</th>\n",
       "      <th>down</th>\n",
       "      <th>right</th>\n",
       "      <th>up</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000307</td>\n",
       "      <td>0.861948</td>\n",
       "      <td>0.137431</td>\n",
       "      <td>0.000314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001545</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.997325</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000614</td>\n",
       "      <td>0.997883</td>\n",
       "      <td>0.000648</td>\n",
       "      <td>0.000855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.021768</td>\n",
       "      <td>0.002684</td>\n",
       "      <td>0.961310</td>\n",
       "      <td>0.014238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.999648</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.008260</td>\n",
       "      <td>0.198169</td>\n",
       "      <td>0.786407</td>\n",
       "      <td>0.007165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.999218</td>\n",
       "      <td>0.000640</td>\n",
       "      <td>0.000084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.041046</td>\n",
       "      <td>0.286452</td>\n",
       "      <td>0.634076</td>\n",
       "      <td>0.038426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.999394</td>\n",
       "      <td>0.000197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.029428</td>\n",
       "      <td>0.970416</td>\n",
       "      <td>0.000063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.999676</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>0.000088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.001793</td>\n",
       "      <td>0.586685</td>\n",
       "      <td>0.409775</td>\n",
       "      <td>0.001747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.003990</td>\n",
       "      <td>0.652142</td>\n",
       "      <td>0.340331</td>\n",
       "      <td>0.003536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.011372</td>\n",
       "      <td>0.988473</td>\n",
       "      <td>0.000064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.001204</td>\n",
       "      <td>0.998647</td>\n",
       "      <td>0.000057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.001608</td>\n",
       "      <td>0.627634</td>\n",
       "      <td>0.369347</td>\n",
       "      <td>0.001412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        left      down     right        up\n",
       "0   0.000307  0.861948  0.137431  0.000314\n",
       "1   0.001545  0.000129  0.997325  0.001000\n",
       "2   0.000614  0.997883  0.000648  0.000855\n",
       "3   0.021768  0.002684  0.961310  0.014238\n",
       "4   0.000100  0.999648  0.000107  0.000145\n",
       "5   0.008260  0.198169  0.786407  0.007165\n",
       "6   0.000059  0.999218  0.000640  0.000084\n",
       "7   0.041046  0.286452  0.634076  0.038426\n",
       "8   0.000334  0.000075  0.999394  0.000197\n",
       "9   0.000094  0.029428  0.970416  0.000063\n",
       "10  0.000061  0.999676  0.000174  0.000088\n",
       "11  0.001793  0.586685  0.409775  0.001747\n",
       "12  0.003990  0.652142  0.340331  0.003536\n",
       "13  0.000092  0.011372  0.988473  0.000064\n",
       "14  0.000092  0.001204  0.998647  0.000057\n",
       "15  0.001608  0.627634  0.369347  0.001412"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_probs = {obs : utils.query_policy(ppo, env, obs) for obs in range(16)}\n",
    "pd.DataFrame(action_probs, index=[\"left\", \"down\", \"right\", \"up\"]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d83f8b3-a489-4da1-b000-402cbf192497",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Viewing stochastic policies\n",
    "\n",
    "We can also view this as an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67a7b13e-82f5-4c51-a3ad-e0b27410aa44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "# df = pd.DataFrame(action_probs, index=[\"left\", \"down\", \"right\", \"up\"]).T\n",
    "# plt.figure(figsize=(8,4))\n",
    "# plt.imshow(df.values.T);\n",
    "# plt.yticks(np.arange(4), labels=[\"left\", \"down\", \"up\", \"right\"]);\n",
    "# plt.xticks(np.arange(16), labels=np.arange(16));\n",
    "# plt.setp(plt.gca().get_xticklabels(), rotation=45, ha=\"right\",\n",
    "#          rotation_mode=\"anchor\");\n",
    "# plt.xlabel(\"observations\");\n",
    "# plt.ylabel(\"actions\");\n",
    "# plt.colorbar(location=\"bottom\");\n",
    "# plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08aed05b-ced6-465f-bc0e-636daf0a0b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils.plot_action_probs(action_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e860de2e-9b73-4968-a99f-07da50e5978c",
   "metadata": {},
   "source": [
    "Notes:\n",
    "    \n",
    "Note how the agent "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97cf962-8c49-47dd-9fd5-f3883f7ca80e",
   "metadata": {},
   "source": [
    "#### Non-slippery Frozen Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d9e3b1-6715-4883-995b-963d9f2e5c3b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Acting stochastically\n",
    "\n",
    "`compute_single_action` will act according to these probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098512e5-5290-4736-99aa-5663d4c3bc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_probs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700ac122-31cc-4718-98e0-215e58190c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = np.bincount([ppo.compute_single_action(0) for _ in range(10_000)])\n",
    "plt.bar([0,1,2,3], counts/10_000)\n",
    "plt.xticks([0,1,2,3]);\n",
    "plt.xlabel(\"Action\");\n",
    "plt.ylabel(\"Empirical dist\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2dc857-33bd-42e4-a002-797cf8beee3f",
   "metadata": {},
   "source": [
    "#### Acting deterministically\n",
    "\n",
    "We can also tell the agent to act deterministically with `explore=False`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c8797d-7eb8-480b-887f-470c46901f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo.compute_single_action(0, explore=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046bbdf5-ca03-46f9-b6be-2547653d7eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([ppo.compute_single_action(0, explore=False) for _ in range(100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df58d2f-8ad0-4b16-b3e8-058016123bf4",
   "metadata": {},
   "source": [
    "Often this is desirable, but **sometimes the optimal policy is stochastic**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d576256-a969-4b77-a204-2bb8219450cd",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "During training, exploration is a fundamental concept in RL. We will discuss it in Module 4!\n",
    "\n",
    "In Module 4 we'll also see an example where the optimal policy is stochastic, so we would definitely not want to set `explore=False` in deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d205da70-1255-437c-8a99-6ea8a5e8d2a0",
   "metadata": {},
   "source": [
    "#### Let's apply what we learned!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd539d44-aa7c-4958-b448-7759f3adc20e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## RLlib config syntax\n",
    "<!-- multiple choice -->\n",
    "\n",
    "Which of the following is **NOT** correct RLlib syntax for instantiating a PPO algorithm instance? Assume the following import has already been run:\n",
    "\n",
    "```python\n",
    "from ray.rllib.algorithms.ppo import PPO, PPOConfig\n",
    "```\n",
    "\n",
    "- [ ] PPOConfig().build(env='FrozenLake-v1')\n",
    "- [ ] PPOConfig().environment(env='FrozenLake-v1').build()\n",
    "- [x] PPOConfig(env='FrozenLake-v1').build()\n",
    "- [ ] PPO(env='FrozenLake-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46809a96-dc1f-4d56-ad40-226c68d70a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # HIDDEN\n",
    "# from ray.rllib.algorithms.ppo import PPO, PPOConfig\n",
    "\n",
    "# PPOConfig().framework(\"torch\").build(env=\"FrozenLake-v1\")\n",
    "# PPOConfig().framework(\"torch\").environment(env=\"FrozenLake-v1\").build()\n",
    "# PPO(env=\"FrozenLake-v1\", config={\"framework\" : \"torch\"})\n",
    "# PPOConfig(env=\"FrozenLake-v1\").framework(\"torch\").build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce5cbd1-2669-4470-abd7-573ac1b8205a",
   "metadata": {},
   "source": [
    "## Saving/restoring models\n",
    "<!-- multiple choice -->\n",
    "\n",
    "Which of the following is **NOT** a plausible use of saving (\"checkpointing\") models in RLlib?\n",
    "\n",
    "- [x] Checkpointing generates cryptocurrency.\n",
    "- [ ] In a course like this, loading a checkpoint means you don't always have to wait for models to train.\n",
    "- [ ] Saved models can be stored, shared, copied and/or moved for deployment.\n",
    "- [ ] Training can be resumed from a checkpoint in case it fails and needs to be restarted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e27b60-beab-4bd8-a978-198920b26d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# now that we have the fancier plotting code from module 5 (DQN), we could use it here also"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a2dd37-444e-4bfc-a7eb-220271154672",
   "metadata": {},
   "source": [
    "## Cartpole environment\n",
    "<!-- coding exercise -->\n",
    "\n",
    "A famous benchmark environment in RL is the _Cartpole_ environment, in which the agent must balance an [inverted pendulum](https://en.wikipedia.org/wiki/Inverted_pendulum) by applying force on the cart (at the bottom) to keep it stable. More information is available in the gym documentation [here](https://www.gymlibrary.ml/environments/classic_control/cart_pole/).\n",
    "\n",
    "Note: the code below imports the env `MyCartPole`. This is identical to gym's `'CartPole-v1'` except that the rendering method has been overridden with something that can be displayed inside of this interactive course platform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47b1f3a-66b1-463f-9e59-2b3c903cb377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython import display\n",
    "\n",
    "from envs import MyCartPole\n",
    "from ray.rllib.algorithms.ppo import PPO, PPOConfig\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(create_env_on_local_worker=True)\\\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\\\n",
    "    .training(model={\"fcnet_hiddens\" : [32,32]})\n",
    ")\n",
    "\n",
    "ppo = config.build(env=MyCartPole);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337c5eba-1698-44d4-9a45-3e38eb6765f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "for i in range(50):\n",
    "    out = ppo.train()\n",
    "    rewards.append(out[\"episode_reward_mean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d102289-3f41-4dce-b6ca-a649820450db",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MyCartPole()\n",
    "obs = env.reset()\n",
    "\n",
    "for i in range(20):\n",
    "    \n",
    "    action = ppo.compute_single_action(obs)\n",
    "    \n",
    "    obs, reward, done, _ = env.step(action)\n",
    "\n",
    "    env.render()\n",
    "    time.sleep(0.2)\n",
    "\n",
    "display.clear_output(wait=True);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c2c576-fac3-4a50-82aa-cf45fc0057f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "e = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61505554-64d7-476a-b996-7f882571c407",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d9e85c-feeb-40d7-a814-4c442e3d4ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "e._elapsed_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42db4a0-8343-4e0a-bfd1-1616f164977a",
   "metadata": {},
   "outputs": [],
   "source": [
    "e._max_episode_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4078d8a3-1fe1-44d8-b9a6-5df8d4b27435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106e3e29-058e-4bac-aa62-59bc84fbef0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = envs.MyCartPole()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddd4861-0807-4b4d-8071-5a96675c5db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0028d87-4e25-4d99-bfec-ace64b7f1b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eb7fd5-a203-40b4-9d46-ae4b04bc07c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea513c10-7f19-485f-941c-9ebc226d1b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import TimeLimit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6acb83-816a-4a8b-b814-a2a0c7d51280",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.classic_control import CartPoleEnv\n",
    "class MyCartPole(CartPoleEnv):\n",
    "    def __init__(self, env_config=None):\n",
    "        if isinstance(env_config, dict):\n",
    "            super().__init__(**env_config)\n",
    "        else:\n",
    "            super().__init__()\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae19b08-2078-4412-9acc-749625fc8acc",
   "metadata": {},
   "source": [
    "need a class that is time limited when instantiated, so it can be registered with RLlib.\n",
    "can wrappers be specified in env config or something?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3540e48-17e6-4e5e-a14d-0bf83dab7e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = MyCartPole()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb7ca53-a570-4c0c-81ae-607803632fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = (\n",
    "    PPOConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(create_env_on_local_worker=True)\\\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\\\n",
    "    .training(model={\"fcnet_hiddens\" : [32,32]})\\\n",
    "    .environment(env=MyCartPole, env_config={\"wrapper\" : \"blah\"})\n",
    ")\n",
    "\n",
    "algo = c.build(env=MyCartPole);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58500f63-d0bc-4520-b2f4-fd4e2e76d9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2dc421-4a02-45e3-974d-49d30b1dfe67",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "need to be able to load a trained model here, which is currently not working. then, split this up into 2 coding exercises, one with the video and one with the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5a7f1a-4fd7-4182-b4f9-2a23fb5618de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import query_policy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "angle_range_deg = np.arange(-15,15,0.1)\n",
    "push_left_probs = 0*angle_range_deg\n",
    "\n",
    "for i, angle_deg in enumerate(angle_range_deg):\n",
    "    angle_rad = angle_deg/180*np.pi\n",
    "    \n",
    "    obs = np.zeros(4)\n",
    "    obs[2] = angle_rad\n",
    "\n",
    "    push_left_probs[i] = query_policy(ppo, env, obs, actions=[0,1])[0]\n",
    "\n",
    "plt.plot(angle_range_deg, push_left_probs);\n",
    "plt.xlabel(\"pole angle (degrees)\")\n",
    "plt.ylabel(\"probability of pushing left\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a14614-d1c5-4319-9559-543ecf432699",
   "metadata": {},
   "source": [
    "#### Interpreting the plot\n",
    "\n",
    "How does the observed pole angle affect the trained agent's probability of pushing left?\n",
    "\n",
    "- [ ] Positive pole angles (leaning right) result in the agent pushing the cart left. | The agent wants to push the cart underneath the top of the pole.\n",
    "- [x] Negative pole angles (leaning left) result in the agent pushing the cart left.\n",
    "- [ ] The agent's probability of pushing left is not significantly affected by the pole angle.\n",
    "- [ ] None of the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2370b6-7abb-4d23-adad-ada45068d3c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ray2beta-rc1]",
   "language": "python",
   "name": "conda-env-ray2beta-rc1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
