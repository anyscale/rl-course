{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90c01ab9-1cd0-47d8-a198-e07e36e2c1fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## RLlib continued"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e74417a3-63bc-4000-951a-72ab882b689e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4215b4c7-9bf6-453a-9f38-30639c2cce33",
   "metadata": {},
   "source": [
    "#### Saving the model\n",
    "\n",
    "- We may want to save our trained model for future use.\n",
    "- This is also called _checkpointing_, especially when done during a training loop.\n",
    "- In RLlib, this can be done simply with:\n",
    "\n",
    "```python\n",
    "trainer.save()\n",
    "```\n",
    "\n",
    "It can then be later restored with\n",
    "\n",
    "```python\n",
    "trainer.restore()\n",
    "```\n",
    "\n",
    "Just make sure you create the trainer with the same environment and parameters when restoring from a checkpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4562a4a-1e69-4fe7-9c50-0a6302ce24d4",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "In RLlib, we can evaluate with `.evaluate()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "827fb5eb-ac68-44bb-b4a1-b2004e4dd3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eadfb34-0130-421c-bd1d-82f797a3ca10",
   "metadata": {},
   "source": [
    "The evaluation output contains a lot of info. Let's look at a couple pieces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a29d4cf4-e5bd-44a4-b5c9-7430570a94fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9810725552050473"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results[\"evaluation\"][\"episode_reward_mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7653739-2225-4f69-a762-d6894a8df931",
   "metadata": {},
   "source": [
    "That is, we reached the goal in 98% of the evaluation episodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38ddb68-39ee-4af9-8dc8-22efbb2d1e38",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "Histogram of episode rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "599bc340-3adf-4d48-9609-35cec8ef2836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(eval_results[\"evaluation\"][\"hist_stats\"][\"episode_reward\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0cc4db-d89c-4089-b4bd-dbfb9998ebf1",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "Histogram of episode _lengths_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95387fc3-4f24-4e20-9b0b-42f959a28e28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(eval_results[\"evaluation\"][\"hist_stats\"][\"episode_lengths\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f663184c-6cf5-4d8c-b500-f47d51319094",
   "metadata": {},
   "source": [
    "- The short ones are the failures, since it's impossible to reach the goal in 4 steps.\n",
    "- Most of the time we reach the goal in the minimum number of steps (6)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72beef15-af71-4a24-8a5e-59ec40fe271a",
   "metadata": {},
   "source": [
    "#### Configuring the trainer\n",
    "\n",
    "```python\n",
    "trainer = rllib.agents.ppo.PPOTrainer(env=\"FrozenLake-v1\", \n",
    "                                      config=trainer_config)\n",
    "```\n",
    "\n",
    "Remember this line? Earlier, we hid the `trainer_config`. Here's the config we hid:\n",
    "\n",
    "```python\n",
    "trainer_config = {\n",
    "    \"framework\"             : \"torch\",\n",
    "    \"create_env_on_driver\"  : True,\n",
    "    \"seed\"                  : 0,\n",
    "    \"model\"                 : {\"fcnet_hiddens\" : [32, 32]},\n",
    "    \"env_config\"            : {\"is_slippery\" : False}}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cfa9d1-fbfa-455f-a722-950b634d4d45",
   "metadata": {},
   "source": [
    "#### Configuring the trainer\n",
    "\n",
    "```python\n",
    "trainer_config = {\n",
    "    \"framework\"             : \"torch\",\n",
    "    \"create_env_on_driver\"  : True,\n",
    "    \"seed\"                  : 0,\n",
    "    \"model\"                 : {\"fcnet_hiddens\" : [32, 32]},\n",
    "    \"env_config\"            : {\"is_slippery\" : False}}\n",
    "```\n",
    "\n",
    "- `config={\"framework\" : \"torch\"}`: RLlib works with tensorflow (`\"tf\"` or `\"tf2\"`) and pytorch (`\"torch\"`)\n",
    "- `\"create_env_on_driver\" : True`: This relates to Ray, and we will touch on it briefly, but not much in this course\n",
    "-  `\"seed\" : 0`: This is used for reproducibility of the teaching materials, and is not normally needed\n",
    "- `\"model\" : {\"fcnet_hiddens\" : [32, 32]}`: this tells RLlib to use a smaller-than-default neural network architecture, which helps this slide deck compile/run faster\n",
    "- `\"env_config\" : {\"is_slippery\" : False}`: this selects the non-slippery Frozen Lake\n",
    "  - All environment hyperparameters go in this sub-dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbaa9ae-0080-4f7c-871e-d98c3b792323",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### sklearn / RLlib analogies\n",
    "\n",
    "- We've seen some analogies (and departures) between SL and RL, both in concept and syntax. \n",
    "- Let's compare:\n",
    "\n",
    "| SL/sklearn | RL/RLlib | Description  |\n",
    "|-------------|---------|--------------|\n",
    "| `ModelName(**hypers)` | `AlgoName(hypers, env)` | Initialize a model/algorithm |\n",
    "| `.fit(X,y)`  | `.train()` | Training (fully for sklearn, one iteration for RLlib) |\n",
    "| `.predict(x)` | `.compute_single_action(obs)` | Use the trained model once |\n",
    "| `.score(X,y)` | `.evaluate()` | Evaluate the model |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5712dfa4-3077-4dc6-b556-5354f8437a36",
   "metadata": {
    "tags": []
   },
   "source": [
    "# TODO\n",
    "\n",
    "querying the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a1371d-a0e3-4e6d-b49e-65db6a18bb33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c783a88f-f4c3-4345-8534-f8d46dd4ae16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba5ead0-573f-401b-b9e9-9a099ad2dce5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "108d1679-416b-4288-bba1-45b2292892cc",
   "metadata": {},
   "source": [
    "The code below loads the agent, seen in the slides, on the Random Lake environment. When you run the code, it prints out a 16x4 array. Each row corresponds to one of the 16 possible observations, and shows the probabilities of the agent taking each of the 4 possible actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d35b97b-219d-4349-a97f-84003a6db13e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'envs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfont.size\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m14\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mppo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PPOTrainer\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01menvs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomLake\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m action_dist\n\u001b[1;32m      8\u001b[0m trainer \u001b[38;5;241m=\u001b[39m PPOTrainer({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframework\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreate_env_on_driver\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;241m0\u001b[39m}, \n\u001b[1;32m      9\u001b[0m                      env\u001b[38;5;241m=\u001b[39mRandomLake)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'envs'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.size'] = 14\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from envs import RandomLake\n",
    "from utils import action_dist\n",
    "\n",
    "trainer = PPOTrainer({\"framework\" : \"torch\", \"create_env_on_driver\" : True, \"seed\" : 0}, \n",
    "                     env=RandomLake)\n",
    "\n",
    "trainer.restore(\"models/RandomLake/checkpoint-8\")\n",
    "\n",
    "lake = RandomLake()\n",
    "\n",
    "plt.figure(figsize=(4,8))\n",
    "plt.imshow(np.array([action_dist(trainer, lake, i) for i in range(16)]));\n",
    "plt.xticks(np.arange(4), labels=[\"left\", \"down\", \"up\", \"right\"]);\n",
    "plt.yticks(np.arange(16), labels=np.arange(16));\n",
    "plt.setp(plt.gca().get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5ecc70-eb4e-4fc5-9553-acb5039fc6be",
   "metadata": {},
   "source": [
    "this was originally from lesson 3.2. the idea is to look at the action distribtuion for each possible observation of frozen lake. here, in the random lake actually, you can see that it's most sure about what to do right before the goal, and gets less sure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce2fa69-ccbf-4560-a9f5-7df396009cce",
   "metadata": {},
   "source": [
    "what might also be cool is to just make a 4x4 map and the colour can be the max probability (or -entropy or whatever, a measure or sureness), so we can see that it gets more sure closer to the goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790ca57b-5a55-44f5-a18a-6869057db075",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84de9cd9-8b5a-46c2-83da-e74f87d5f04d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbd16a0-b8fe-4703-8bae-77816782eb01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9622607a-1d40-433a-8297-61005326046c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d205da70-1255-437c-8a99-6ea8a5e8d2a0",
   "metadata": {},
   "source": [
    "#### Let's apply what we learned!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd539d44-aa7c-4958-b448-7759f3adc20e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## RLlib trainer methods\n",
    "<!-- multiple choice -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17580d1a-2245-4390-bfee-449b1bfff570",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "_Which of the following most accurately describes the role of `trainer.train()` in RLlib?_\n",
    "\n",
    "- [ ] It neither collects a data set of episodes nor learns a policy. | Are you sure?\n",
    "- [ ] It learns a policy from a fixed data set of episodes. | Remember, calling train() causes the agent to play through episodes.\n",
    "- [ ] It creates a data set of episodes but does not learn a policy. | Remember, calling train() learns a policy.\n",
    "- [x] It simultaneously collects a data set of episodes and also learns a policy. | You got it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48245b9-3007-4d9c-a657-97978c4fea1e",
   "metadata": {},
   "source": [
    "#### Passing in the dataset\n",
    "\n",
    "_When using scikit-learn for supervised learning we call `fit(X,y)`, but with RLlib we call `train()` without passing in the dataset. Why?_\n",
    "\n",
    "- [x] Because RLlib was given access to the environment when the trainer was initialized, and this is all it needs.\n",
    "- [ ] Because reinforcement learning does not involve data.\n",
    "- [ ] Because X and y are passed into a different RLlib method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc34288f-7416-4041-8b70-5182715bfd0e",
   "metadata": {},
   "source": [
    "#### Scoring\n",
    "\n",
    "_Which of the following RLlib trainer methods is most analogous to scikit-learn's `.predict()` function?_\n",
    "\n",
    "- [ ] train() | This is more like _fit()_ in scikit-learn.\n",
    "- [x] compute_single_action()\n",
    "- [ ] evaluate() | This is more like _score()_ in scikit-learn. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34d5c5b-a1be-447a-abe6-e034c9487cce",
   "metadata": {},
   "source": [
    "## Slippery Frozen Lake\n",
    "<!-- coding exercise -->\n",
    "\n",
    "In the slides, we trained an agent to reliably reach the goal in the **non-slippery** Frozen Lake environment. Here, try the same thing with the **slippery** Frozen Lake. Train your agent until it reaches the goal at least 20% of the time. Then, answer the multiple choice question below.\n",
    "\n",
    "Note the addition of `\"evaluation_config\"` in the trainer config. This ensures that the agent acts deterministically during evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c2847c21-1709-4039-b77d-d634e43e6096",
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '____' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rllib\n\u001b[1;32m      4\u001b[0m slippery_trainer_config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframework\u001b[39m\u001b[38;5;124m\"\u001b[39m             : \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreate_env_on_driver\u001b[39m\u001b[38;5;124m\"\u001b[39m  : \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv_config\u001b[39m\u001b[38;5;124m\"\u001b[39m            : {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_slippery\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;28;01mTrue\u001b[39;00m},\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation_config\u001b[39m\u001b[38;5;124m\"\u001b[39m     : {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexplore\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;28;01mFalse\u001b[39;00m}}\n\u001b[0;32m---> 12\u001b[0m slippery_trainer \u001b[38;5;241m=\u001b[39m rllib\u001b[38;5;241m.\u001b[39magents\u001b[38;5;241m.\u001b[39mppo\u001b[38;5;241m.\u001b[39mPPOTrainer(\u001b[43m____\u001b[49m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(____):\n\u001b[1;32m     15\u001b[0m     train_info \u001b[38;5;241m=\u001b[39m slippery_trainer\u001b[38;5;241m.\u001b[39m____()\n",
      "\u001b[0;31mNameError\u001b[0m: name '____' is not defined"
     ]
    }
   ],
   "source": [
    "# EXERCISE\n",
    "from ray import rllib\n",
    "\n",
    "slippery_trainer_config = {\n",
    "    \"framework\"             : \"torch\",\n",
    "    \"create_env_on_driver\"  : True,\n",
    "    \"seed\"                  : 0,\n",
    "    \"model\"                 : {\"fcnet_hiddens\" : [32, 32]},\n",
    "    \"env_config\"            : {\"is_slippery\" : True},\n",
    "    \"evaluation_config\"     : {\"explore\" : False}}\n",
    "\n",
    "slippery_trainer = rllib.agents.ppo.PPOTrainer(____)\n",
    "\n",
    "for i in range(____):\n",
    "    train_info = slippery_trainer.____()\n",
    "    \n",
    "eval_results = ____.evaluate()\n",
    "\n",
    "print(eval_results[\"evaluation\"][\"episode_reward_mean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2f2fdf9-ec1a-488d-8864-0a929e42190c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of reaching goal: 3.6%\n",
      "Action performed from top-right: 1\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "from ray import rllib\n",
    "\n",
    "slippery_trainer_config = {\n",
    "    \"framework\"             : \"torch\",\n",
    "    \"create_env_on_driver\"  : True,\n",
    "    \"seed\"                  : 0,\n",
    "    \"model\"                 : {\"fcnet_hiddens\" : [32, 32]},\n",
    "    \"env_config\"            : {\"is_slippery\" : True},\n",
    "    \"evaluation_config\"     : {\"explore\" : False}}\n",
    "\n",
    "slippery_trainer = rllib.agents.ppo.PPOTrainer(env=\"FrozenLake-v1\", config=slippery_trainer_config)\n",
    "\n",
    "for i in range(12): # There is randomness here, but 12 should be enough most of the time\n",
    "    train_info = slippery_trainer.train()\n",
    "    \n",
    "eval_results = slippery_trainer.evaluate()\n",
    "\n",
    "print(\"Frequency of reaching goal: %.1f%%\" % (eval_results[\"evaluation\"][\"episode_reward_mean\"]*100))\n",
    "\n",
    "print(\"Action performed from top-right:\", slippery_trainer.compute_single_action(3, explore=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6485b269-1c62-4cd8-a8d2-0a6cea590aeb",
   "metadata": {},
   "source": [
    "#### Action performed from top-right\n",
    "\n",
    "According to the trained policy, what action is performed when the agent is at the top-right of the arena? This is printed out by the code.\n",
    "\n",
    "- [ ] left (0) | When we ran the code, we got something different here.\n",
    "- [ ] down (1) | When we ran the code, we got something different here.\n",
    "- [ ] right (2) | When we ran the code, we got something different here.\n",
    "- [x] up (3) | You got it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb1e37b-ca77-41ba-ba20-6a9558485b80",
   "metadata": {},
   "source": [
    "#### Interpreting the policy\n",
    "\n",
    "Recall that the arena looks like this:\n",
    "\n",
    "```\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "```\n",
    "\n",
    "In this slippery environment, you do your intended action 1/3 of the time, and each of the two perpendicular directions 1/3 of the time. You never go the opposite of the intended direction.\n",
    "\n",
    "In the previous question, we saw that, from the **top-right corner** (observation 3), the agent tries to move up (action 3). Why do you think the agent tries to move up?\n",
    "\n",
    "- [ ] 'Up' is an arbitrary choice because we did not train the agent.\n",
    "- [ ] The agent receives a reward for the 'up' action. \n",
    "- [x] The agent wants to avoid falling into the hole below it, so the 'up' action is the safest choice.\n",
    "- [ ] Moving up brings the agent closer to the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6572684-0981-4aaf-8744-5baf38df5f1c",
   "metadata": {},
   "source": [
    "## Rendering the trained agent\n",
    "<!-- coding exercise -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b883cf7e-3818-44d0-a5a3-bb3611623d1d",
   "metadata": {},
   "source": [
    "Fill in the blanks in the code below so that the code performs the observation-action-reward loop for one episode. Then, run the code and watch the trained agent navigate the slippery Frozen Lake. Then, answer the multiple choice question below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f2996d2f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m slippery_trainer_config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframework\u001b[39m\u001b[38;5;124m\"\u001b[39m             : \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreate_env_on_driver\u001b[39m\u001b[38;5;124m\"\u001b[39m  : \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv_config\u001b[39m\u001b[38;5;124m\"\u001b[39m            : {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_slippery\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;28;01mTrue\u001b[39;00m},\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation_config\u001b[39m\u001b[38;5;124m\"\u001b[39m     : {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexplore\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;28;01mFalse\u001b[39;00m}}\n\u001b[1;32m     17\u001b[0m slippery_trainer \u001b[38;5;241m=\u001b[39m rllib\u001b[38;5;241m.\u001b[39magents\u001b[38;5;241m.\u001b[39mppo\u001b[38;5;241m.\u001b[39mPPOTrainer(env\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrozenLake-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m, config\u001b[38;5;241m=\u001b[39mslippery_trainer_config)\n\u001b[0;32m---> 19\u001b[0m \u001b[43mslippery_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/FrozenLakeSlippery/checkpoint-20\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrozenLake-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m, is_slippery\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     22\u001b[0m out \u001b[38;5;241m=\u001b[39m Output();\n",
      "File \u001b[0;32m~/miniconda3/envs/rl-course-dev-2/lib/python3.8/site-packages/ray/tune/trainable.py:490\u001b[0m, in \u001b[0;36mTrainable.restore\u001b[0;34m(self, checkpoint_path)\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_checkpoint(checkpoint_dict)\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 490\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time_since_restore \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timesteps_since_restore \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl-course-dev-2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py:1861\u001b[0m, in \u001b[0;36mTrainer.load_checkpoint\u001b[0;34m(self, checkpoint_path)\u001b[0m\n\u001b[1;32m   1858\u001b[0m \u001b[38;5;129m@override\u001b[39m(Trainable)\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_checkpoint\u001b[39m(\u001b[38;5;28mself\u001b[39m, checkpoint_path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1860\u001b[0m     extra_data \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(checkpoint_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 1861\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__setstate__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextra_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl-course-dev-2/lib/python3.8/site-packages/ray/rllib/agents/trainer.py:2509\u001b[0m, in \u001b[0;36mTrainer.__setstate__\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m   2507\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__setstate__\u001b[39m(\u001b[38;5;28mself\u001b[39m, state: \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m   2508\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworkers\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworker\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m state:\n\u001b[0;32m-> 2509\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_worker\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mworker\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2510\u001b[0m         remote_state \u001b[38;5;241m=\u001b[39m ray\u001b[38;5;241m.\u001b[39mput(state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworker\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   2511\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers\u001b[38;5;241m.\u001b[39mremote_workers():\n",
      "File \u001b[0;32m~/miniconda3/envs/rl-course-dev-2/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py:1353\u001b[0m, in \u001b[0;36mRolloutWorker.restore\u001b[0;34m(self, objs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_policy(\n\u001b[1;32m   1346\u001b[0m             policy_id\u001b[38;5;241m=\u001b[39mpid,\n\u001b[1;32m   1347\u001b[0m             policy_cls\u001b[38;5;241m=\u001b[39mpol_spec\u001b[38;5;241m.\u001b[39mpolicy_class,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1350\u001b[0m             config\u001b[38;5;241m=\u001b[39mpol_spec\u001b[38;5;241m.\u001b[39mconfig,\n\u001b[1;32m   1351\u001b[0m         )\n\u001b[1;32m   1352\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1353\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpid\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl-course-dev-2/lib/python3.8/site-packages/ray/rllib/policy/torch_policy.py:715\u001b[0m, in \u001b[0;36mTorchPolicy.set_state\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(optimizer_vars) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizers)\n\u001b[1;32m    714\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m o, s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizers, optimizer_vars):\n\u001b[0;32m--> 715\u001b[0m         optim_state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_torch_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m            \u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    717\u001b[0m         o\u001b[38;5;241m.\u001b[39mload_state_dict(optim_state_dict)\n\u001b[1;32m    718\u001b[0m \u001b[38;5;66;03m# Set exploration's state.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl-course-dev-2/lib/python3.8/site-packages/ray/rllib/utils/torch_utils.py:158\u001b[0m, in \u001b[0;36mconvert_to_torch_tensor\u001b[0;34m(x, device)\u001b[0m\n\u001b[1;32m    155\u001b[0m         tensor \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl-course-dev-2/lib/python3.8/site-packages/tree/__init__.py:430\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structures, **kwargs)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m structures[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m    428\u001b[0m   assert_same_structure(structures[\u001b[38;5;241m0\u001b[39m], other, check_types\u001b[38;5;241m=\u001b[39mcheck_types)\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unflatten_as(structures[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m--> 430\u001b[0m                     [func(\u001b[38;5;241m*\u001b[39margs) \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(flatten, structures))])\n",
      "File \u001b[0;32m~/miniconda3/envs/rl-course-dev-2/lib/python3.8/site-packages/tree/__init__.py:430\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m structures[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m    428\u001b[0m   assert_same_structure(structures[\u001b[38;5;241m0\u001b[39m], other, check_types\u001b[38;5;241m=\u001b[39mcheck_types)\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unflatten_as(structures[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m--> 430\u001b[0m                     [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(flatten, structures))])\n",
      "File \u001b[0;32m~/miniconda3/envs/rl-course-dev-2/lib/python3.8/site-packages/ray/rllib/utils/torch_utils.py:152\u001b[0m, in \u001b[0;36mconvert_to_torch_tensor.<locals>.mapping\u001b[0;34m(item)\u001b[0m\n\u001b[1;32m    149\u001b[0m         tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(item)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# Everything else: Convert to numpy, then wrap as torch tensor.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 152\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# Floatify all float64 tensors.\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mdouble:\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "# EXERCISE\n",
    "from ray import rllib\n",
    "import gym\n",
    "\n",
    "from ipywidgets import Output\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "slippery_trainer_config = {\n",
    "    \"framework\"             : \"torch\",\n",
    "    \"create_env_on_driver\"  : True,\n",
    "    \"seed\"                  : 0,\n",
    "    \"model\"                 : {\"fcnet_hiddens\" : [32, 32]},\n",
    "    \"env_config\"            : {\"is_slippery\" : True},\n",
    "    \"evaluation_config\"     : {\"explore\" : False}}\n",
    "\n",
    "slippery_trainer = rllib.agents.ppo.PPOTrainer(env=\"FrozenLake-v1\", config=slippery_trainer_config)\n",
    "\n",
    "slippery_trainer.restore(\"models/FrozenLakeSlippery/checkpoint-20\")\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "\n",
    "out = Output();\n",
    "display.display(out);\n",
    "\n",
    "with out:\n",
    "    obs = env.reset()\n",
    "    env.seed(1)\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = slippery_trainer.____(____, explore=False)\n",
    "        obs, rewards, done, _ = env.step(____)\n",
    "        \n",
    "        out.clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5e7f1f-cde4-4a3e-83b9-ab6334c83cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "from ray import rllib\n",
    "import gym\n",
    "\n",
    "from ipywidgets import Output\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "slippery_trainer_config = {\n",
    "    \"framework\"             : \"torch\",\n",
    "    \"create_env_on_driver\"  : True,\n",
    "    \"seed\"                  : 0,\n",
    "    \"model\"                 : {\"fcnet_hiddens\" : [32, 32]},\n",
    "    \"env_config\"            : {\"is_slippery\" : True},\n",
    "    \"evaluation_config\"     : {\"explore\" : False}}\n",
    "\n",
    "slippery_trainer = rllib.agents.ppo.PPOTrainer(env=\"FrozenLake-v1\", config=slippery_trainer_config)\n",
    "\n",
    "slippery_trainer.restore(\"models/FrozenLakeSlippery/checkpoint-20\")\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "\n",
    "out = Output();\n",
    "display.display(out);\n",
    "\n",
    "with out:\n",
    "    obs = env.reset()\n",
    "    env.seed(1)\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = slippery_trainer.compute_single_action(obs, explore=False)\n",
    "        obs, rewards, done, _ = env.step(action)\n",
    "        \n",
    "        out.clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c34271-2871-4251-94d9-b7867767a154",
   "metadata": {},
   "source": [
    "#### Choose the option below that best describes the agent's treacherous journey to the goal.\n",
    "\n",
    "- [ ] The agent visits the top-right square in this episode.\n",
    "- [ ] The agent never returns to the start state after leaving it initially.\n",
    "- [ ] The agent does not reach the goal during the episode.\n",
    "- [x] The agent sees observation 11 exactly once on its journey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a00806-8217-4082-bf9d-d61bda30bb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: should this last exercise be moved from 1.4 to 1.3? it's more about policies than RLlib."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rl-course-dev-2]",
   "language": "python",
   "name": "conda-env-rl-course-dev-2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
