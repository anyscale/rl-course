{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5aa21ad-6bb9-4990-ab4d-fb9435b0bc00",
   "metadata": {},
   "source": [
    "## Entorno de recomendación: diseño"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdd0ce7d-4a5b-4d03-8d93-b176de9e4db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd89af26-eb12-45d8-95b1-14fdd7027deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: adapt images from Sven for this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0293f36-0ca4-40d4-8c4f-456c07627269",
   "metadata": {},
   "source": [
    "#### Simular el comportamiento del usuario\n",
    "\n",
    "- Simular el comportamiento del usuario al responder **repetidamente** a las recomendaciones de artículos.\n",
    "- El comportamiento clave a simular:\n",
    "\n",
    "&gt; Recomendar artículos de \"comida basura\" es bueno a corto plazo, pero malo a largo plazo.\n",
    "\n",
    "- Esto es completamente nuestra elección como diseñador del entorno \n",
    "- Puede que quieras simular/captar un tipo diferente de comportamiento del usuario.\n",
    "  - O bien, aprender de los datos del comportamiento del usuario -¡más adelante se hablará de esto!\n",
    "- Pero éste será nuestro ejemplo en marcha por ahora."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbb31ad-3891-4d9a-91be-09854ff73c31",
   "metadata": {},
   "source": [
    "#### Chocolate\n",
    "\n",
    "- Modelaremos cada artículo como si tuviera un nivel de \"dulzura\" \n",
    "- Nos referiremos a los artículos de alto nivel de dulzura como \"dulces\".\n",
    "\n",
    "![](img/candy.jpg)\n",
    "\n",
    "- Pueden ser vídeos cortos y tontos, o baratijas baratas en oferta, etc.\n",
    "- A los usuarios les encantan los caramelos a corto plazo, pero demasiado chocolate provoca insatisfacción a largo plazo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7aee0e-e60a-419b-bdd0-b0680b5d2f0c",
   "metadata": {},
   "source": [
    "#### Verduras\n",
    "\n",
    "- Por otro lado, nos referiremos a los artículos poco dulces como \"vegetales\".\n",
    "\n",
    "![](img/veggies.jpg)\n",
    "\n",
    "- Podrían ser documentales educativos, o artículos aburridos pero útiles, etc.\n",
    "- Los usuarios no disfrutan mucho de las verduras a corto plazo, pero aumentan la satisfacción a largo plazo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58280654-1e51-492c-96da-4764c0ceba37",
   "metadata": {},
   "source": [
    "#### Nivel de azúcar\n",
    "\n",
    "- Modelaremos a nuestros usuarios como si tuvieran un **nivel de azúcar** que mide la cantidad de dulces que han comido recientemente.\n",
    "- El nivel de azúcar del usuario (o la noción de nivel de azúcar) _no será conocido por el agente_\n",
    "- Pero por ahora, estamos diseñando el simulador, así que lo conocemos todo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ddc3a6-98b4-4a61-a229-4b0ae27c62fd",
   "metadata": {},
   "source": [
    "#### Dinámica del nivel de azúcar\n",
    "\n",
    "- Tenemos que decidir cómo cambia el nivel de azúcar con el consumo de artículos.\n",
    "- Un enfoque sencillo es:\n",
    "\n",
    "&gt; Cuando se consume un artículo, el nivel de azúcar se desplaza hacia la dulzura de ese artículo.\n",
    "\n",
    "Ejemplos:\n",
    "\n",
    "- Si tu nivel de azúcar es 0,2 y consumes un elemento con dulzor 0,5, tu nivel de azúcar sube ⬆️\n",
    "- Si tu nivel de azúcar es 0,2 y consumes un artículo con dulzor 0,1, tu nivel de azúcar baja ⬇️"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5fb6ed-8479-40ae-a489-b279f03a61ac",
   "metadata": {},
   "source": [
    "#### Dinámica del nivel de azúcar\n",
    "\n",
    "- ¿Cómo representamos esto matemáticamente?\n",
    "- Podemos intentar esto:\n",
    "\n",
    "&gt; nuevo nivel de azúcar = ⍺ (antiguo nivel de azúcar) + (1 - ⍺) (dulzor del artículo)\n",
    "\n",
    "- Aquí, ⍺ es un número entre 0 y 1 que controla lo \"obstinado\" que es el nivel de azúcar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce8adf56-8b56-4dc7-bb3d-3a2cebfe6f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN \n",
    "# note the slide above and below are partially the same - just want to hide the bottom half at first"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b8ebcf-d522-42e6-8777-a32b516259e4",
   "metadata": {},
   "source": [
    "#### Dinámica del nivel de azúcar\n",
    "\n",
    "- ¿Cómo representamos esto matemáticamente?\n",
    "- Podemos intentar esto:\n",
    "\n",
    "&gt; nuevo nivel de azúcar = ⍺ (antiguo nivel de azúcar) + (1 - ⍺) (dulzor del artículo)\n",
    "\n",
    "- Aquí, ⍺ es un número entre 0 y 1 que controla lo \"obstinado\" que es el nivel de azúcar.\n",
    "- Por ejemplo, si ⍺=1, la ecuación anterior se convierte en\n",
    "\n",
    "&gt; nuevo nivel de azúcar = antiguo nivel de azúcar\n",
    "\n",
    "y el nivel de azúcar nunca cambia. Si ⍺=0, tenemos\n",
    "\n",
    "&gt; nuevo nivel de azúcar = dulzor del artículo\n",
    "\n",
    "lo que significa que un solo artículo puede cambiar completamente el nivel de azúcar del usuario.\n",
    "\n",
    "- Para ⍺ entre 0 y 1, tenemos una combinación del antiguo nivel de azúcar y la dulzura del artículo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fca4d06-9168-41b9-8ab2-fc1fd34ce0d1",
   "metadata": {},
   "source": [
    "#### Dinámica del nivel de azúcar\n",
    "\n",
    "Podemos poner en práctica lo anterior utilizando esta función:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7821a2f-b42f-4c64-a3d7-a02c7658120f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_sugar_level(sugar_level, item_sweetness, alpha=0.9):\n",
    "    return alpha * sugar_level + (1 - alpha) * item_sweetness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796a261f-2cf0-48f4-ab95-c8ff4c7800be",
   "metadata": {},
   "source": [
    "Vamos a probarlo para asegurarnos de que el comportamiento tiene sentido (utilizando el valor por defecto de ⍺=0,9):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b934fd76-bda0-4e6d-ad2e-17de12d3b4e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sugar_level = 0.2\n",
    "sugar_level = update_sugar_level(sugar_level, 0.8)\n",
    "sugar_level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eda542e-c236-4906-bf8f-fa5f7ab5d238",
   "metadata": {},
   "source": [
    "El artículo era dulce (0,8), así que el nivel de azúcar subió bastante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e9e6f2a-ee8a-4458-82eb-afb57e858676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.264"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sugar_level = update_sugar_level(sugar_level, 0.3)\n",
    "sugar_level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d9bda3-2c8f-44f8-8cf0-4e3a31cfb404",
   "metadata": {},
   "source": [
    "La dulzura del artículo estaba ligeramente por encima del nivel de azúcar, por lo que el nivel de azúcar subió ligeramente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27253dfb-a220-4d0f-9816-1fb542f1045b",
   "metadata": {},
   "source": [
    "#### Dinámica del nivel de azúcar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8c863a1-c37e-4c29-8136-0529501118b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2386"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sugar_level = update_sugar_level(sugar_level, 0.01)\n",
    "sugar_level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eeb5116-a9ab-49d9-8748-d751662a994b",
   "metadata": {},
   "source": [
    "El artículo no era dulce, por lo que el nivel de azúcar bajó."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c20efd-9ca1-43be-8529-aacb7244ad4c",
   "metadata": {},
   "source": [
    "#### Efecto de alfa\n",
    "\n",
    "Podemos ver que, con un alfa más pequeño, el nivel de azúcar cambia mucho más rápido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b684f74b-3519-4300-b4f2-5ff68c0199e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1193"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sugar_level = update_sugar_level(sugar_level, 0.0, alpha=0.5)\n",
    "sugar_level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5d8350-5daa-4377-9ec4-79819528d0bb",
   "metadata": {},
   "source": [
    "#### Recompensa\n",
    "\n",
    "- Muy bien, ¡ya tenemos resuelta la dinámica del nivel de azúcar!\n",
    "- La segunda pieza importante del puzzle es la recompensa.\n",
    "- Lo que queremos:\n",
    "\n",
    "1. Un mayor nivel de dulzor en el artículo conlleva una mayor recompensa (¡dulces!)\n",
    "2. Un mayor nivel de azúcar lleva a una menor recompensa (¡ah, demasiado caramelo!)\n",
    "\n",
    "Una forma sencilla de combinar estos efectos es multiplicarlos:\n",
    "\n",
    "&gt; recompensa = dulzura del objeto * (1 - nivel de azúcar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f6d77a-2458-4a51-91c4-070d074c1978",
   "metadata": {},
   "source": [
    "#### Aplicación de la recompensa\n",
    "\n",
    "&gt; recompensa = dulzura del objeto * (1 - nivel de azúcar)\n",
    "\n",
    "Podemos codificar esto como"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "258777c5-b896-4ec4-b999-38c4aadafdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward(sugar_level, item_sweetness):\n",
    "    return item_sweetness * (1 - sugar_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a1eeea-f027-419c-8f80-756eec6be9ea",
   "metadata": {},
   "source": [
    "¡Utilizaremos estas piezas en la próxima sección cuando implementemos nuestro entorno!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c68cf7-4dc6-4ab4-86c5-93b10fced0ae",
   "metadata": {},
   "source": [
    "#### Espacio de observación\n",
    "\n",
    "- A continuación, tendremos que establecer las observaciones \n",
    "- Nuestras observaciones serán las _características de los artículos candidatos_.\n",
    "- Para simplificar, supondremos sólo una característica, la dulzura del artículo.\n",
    "- Así, el agente verá un montón de niveles de dulzura, y elegirá uno de ellos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb5cadd-bbdc-4a19-931c-522c8902407f",
   "metadata": {},
   "source": [
    "#### Espacio de acción\n",
    "\n",
    "- En este entorno, la acción es el elemento elegido para recomendar, dados los canidatos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7ccbd3-57d6-4914-94a0-7fb03fa0b96e",
   "metadata": {},
   "source": [
    "#### ¡Apliquemos lo que hemos aprendido!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bebfec4-9043-498b-b3da-bc91700a74bf",
   "metadata": {},
   "source": [
    "## Panorama general\n",
    "<!-- multiple choice -->\n",
    "\n",
    "¿Cuál de las siguientes opciones es **NO** cierta sobre el entorno de recomendación RL simulado que estamos creando?\n",
    "\n",
    "- [ ] El entorno contiene un modelo muy simplificado del comportamiento de los usuarios, pero un agente entrenado aún puede ser útil para hacer recomendaciones.\n",
    "- [ ] El entorno representa con exactitud cómo se comportan los usuarios reales.\n",
    "- [ ] El entorno es un buen punto de partida, y es posible que queramos añadir complejidad a medida que avanza nuestro trabajo.\n",
    "- [ ] El entorno capta la noción de que los usuarios responderán de forma diferente a los distintos elementos, y esta respuesta puede depender de su historial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe02b60a-f753-4299-9953-2a6109b77f8a",
   "metadata": {},
   "source": [
    "## Recompensas del recomendador\n",
    "<!-- multiple choice -->\n",
    "\n",
    "Recordemos que nuestra función de recompensa es\n",
    "\n",
    "&gt; recompensa = dulzura del artículo * (1 - nivel de azúcar)\n",
    "\n",
    "#### Satisfacción a corto plazo\n",
    "\n",
    "Verdadero o falso: en un momento dado, la recompensa _inmediata_ es _siempre_ mayor para los dulces que para las verduras.\n",
    "\n",
    "- [x] Verdadero | La recompensa inmediata es directamente proporcional a la dulzura del artículo.\n",
    "- [ ] Falso | ¡Mira bien la fórmula de arriba!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e512009-9deb-4c39-a055-3e82a7acdf14",
   "metadata": {},
   "source": [
    "#### Satisfacción a largo plazo\n",
    "\n",
    "Verdadero o falso: en un momento dado, la recompensa total _a largo plazo_ es _siempre_ mayor por recomendar verduras que por dulces.\n",
    "\n",
    "- [] Verdadero: es complicado determinar qué será lo mejor a largo plazo, ¡esto es lo que tiene que aprender nuestro agente!\n",
    "- [x] Falso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2622b06b-90fc-4fc7-9448-a782be431d63",
   "metadata": {},
   "source": [
    "## Choque de azúcar\n",
    "<!-- coding exercise -->\n",
    "\n",
    "Supongamos que tu nivel de azúcar comienza en 0,5, y que en cada paso sólo tienes dos elementos para elegir, megaverduras (dulzura = 0) y megacaramelos (dulzura = 1). Harás 3 recomendaciones seguidas, utilizando alfa = 0,7. Utiliza la ventana de codificación de abajo para jugar con diferentes opciones y encontrar la mejor secuencia de recomendaciones en términos de recompensa _total_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8695dc5-2538-4158-95fa-50a9135b42f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Received reward 0.00000, new sugar level 0.35000\n",
      "  Received reward 0.00000, new sugar level 0.24500\n",
      "  Received reward 0.00000, new sugar level 0.17150\n",
      "Total reward after 5 recommendations: 0.0\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE\n",
    "\n",
    "def update_sugar_level(sugar_level, item_sweetness, alpha=0.9):\n",
    "    return alpha * sugar_level + (1 - alpha) * item_sweetness\n",
    "\n",
    "def reward(sugar_level, item_sweetness):\n",
    "    return item_sweetness * (1 - sugar_level)\n",
    "\n",
    "# MODIFY THIS LIST\n",
    "# But make sure it always contains 3 items, each 0 or 1\n",
    "recommendations = [0, 0, 0]\n",
    "\n",
    "# starting sugar level\n",
    "sugar_level = 0.5\n",
    "\n",
    "total_reward = 0\n",
    "\n",
    "for item_sweetness in recommendations:\n",
    "    \n",
    "    # add reward\n",
    "    immediate_reward = reward(sugar_level, item_sweetness)\n",
    "    total_reward += immediate_reward\n",
    "    \n",
    "    # update sugar level\n",
    "    sugar_level = update_sugar_level(sugar_level, item_sweetness, alpha=0.7)\n",
    "    \n",
    "    print(f\"  Received reward {immediate_reward:.5f}, new sugar level {sugar_level:.5f}\")\n",
    "    \n",
    "print(\"Total reward after 5 recommendations:\", total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2d62233-1f33-47fe-b155-15e12dd9a8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Received reward 0.00000, new sugar level 0.35000\n",
      "  Received reward 0.65000, new sugar level 0.54500\n",
      "  Received reward 0.45500, new sugar level 0.68150\n",
      "Total reward after 5 recommendations 1.105\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "\n",
    "def update_sugar_level(sugar_level, item_sweetness, alpha=0.9):\n",
    "    return alpha * sugar_level + (1 - alpha) * item_sweetness\n",
    "\n",
    "def reward(sugar_level, item_sweetness):\n",
    "    return item_sweetness * (1 - sugar_level)\n",
    "\n",
    "# MODIFY THIS LIST\n",
    "# But make sure it always contains 3 items, each 0 or 1\n",
    "recommendations = [0,1,1]\n",
    "\n",
    "# starting sugar level\n",
    "sugar_level = 0.5\n",
    "\n",
    "total_reward = 0\n",
    "\n",
    "for item_sweetness in recommendations:\n",
    "    \n",
    "    # add reward\n",
    "    immediate_reward = reward(sugar_level, item_sweetness)\n",
    "    total_reward += immediate_reward\n",
    "    \n",
    "    # update sugar level\n",
    "    sugar_level = update_sugar_level(sugar_level, item_sweetness, alpha=0.7)\n",
    "    \n",
    "    print(f\"  Received reward {immediate_reward:.5f}, new sugar level {sugar_level:.5f}\")\n",
    "    \n",
    "print(\"Total reward after 5 recommendations\", total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a377412-aefd-4b85-8b94-de3869d0858a",
   "metadata": {},
   "source": [
    "#### ¿Cuál fue la mejor estrategia en este ejemplo?\n",
    "\n",
    "- [x] 1 verdura para bajar los niveles de azúcar, luego 2 caramelos para esa dulce recompensa.\n",
    "- [ ] Caramelos, luego verduras para tener buena salud, luego más caramelos.\n",
    "- [ ] ¡Vegetales hasta el final!\n",
    "- [ ] ¡Caramelos hasta el final!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080aeba6-d6fd-461c-8ab7-c6420f692f93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ray200]",
   "language": "python",
   "name": "conda-env-ray200-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
