{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90c01ab9-1cd0-47d8-a198-e07e36e2c1fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Políticas de RL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddd909a-c86f-4e5c-9ba0-8b0ef1152730",
   "metadata": {},
   "source": [
    "#### ¿Qué es una política?\n",
    "\n",
    "Volvamos a la \"API\" de RL:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50604c8f-7c62-495e-b9c6-f3b408a440d9",
   "metadata": {},
   "source": [
    "![](img/RL-API.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ac6598-7965-4496-ac1e-6b71cc7e487b",
   "metadata": {},
   "source": [
    "- La política es el resultado de la RL\n",
    "- Asigna las observaciones a las acciones\n",
    "- La política es como el cerebro del agente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a207992-104b-4ecd-b470-341860f09ad6",
   "metadata": {},
   "source": [
    "#### ¿Qué es una póliza? Los detalles.\n",
    "\n",
    "- Una política es una función que asigna observaciones a acciones.\n",
    "- Consideremos de nuevo el entorno de Frozen Lake:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2a869f8-2fb6-4ef5-bd1c-d8c74493e93c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
    "obs = env.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbfda97-e12c-4284-b0b2-78a8f21c7ede",
   "metadata": {},
   "source": [
    "- Recibimos una observación 0. ¿Qué hacemos ahora? \n",
    "- La política nos lo dirá."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a98f1c-a7b6-40bc-9d18-dce492473ea9",
   "metadata": {},
   "source": [
    "#### Ejemplo de política\n",
    "\n",
    "Una política podría tener el siguiente aspecto\n",
    "\n",
    "| Observación: Acción\n",
    "|------|-------|\n",
    "| 0 | 0 |\n",
    "| 1 | 3 |\n",
    "| 2 | 1 |\n",
    "| 3 | 1 |\n",
    "| ... | ... |\n",
    "| 14 | 2 |\n",
    "| 15 | 2 |\n",
    "\n",
    "- A la izquierda, tenemos todas las observaciones posibles (15 para el Lago Helado).\n",
    "- A la derecha, tenemos la acción correspondiente que realizaremos _si vemos esa observación_.\n",
    "- \"Si veo 0, haré 0; si veo 1, haré 3\", etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c510ca5c-a9d5-44e3-9afb-c4a15979f874",
   "metadata": {},
   "source": [
    "#### Objetivo de RL\n",
    "\n",
    "**El objetivo de la RL es aprender una buena política dado un entorno.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dac4894-b44a-42bc-8269-54f8a7d93c8e",
   "metadata": {},
   "source": [
    "#### Políticas no deterministas\n",
    "\n",
    "- Anteriormente aprendimos sobre los entornos deterministas y no deterministas \n",
    "- Análogamente, tenemos _políticas_ deterministas y no deterministas.\n",
    "- Antes vimos una política determinista: una observación dada provoca una acción fija.\n",
    "- Aquí tenemos un ejemplo de política no determinista:\n",
    "\n",
    "| Observación: P(izquierda), P(abajo), P(derecha), P(arriba) \n",
    "|------------|-------|-----------|---------|-------|\n",
    "| 0 | 0 | 0.9 | 0.01 | 0.04 | 0.05\n",
    "| 1 | 3 | 0.05 | 0.05 | 0.05 | 0.85\n",
    "| ... | ... | ... | ...      | ...      | ...\n",
    "| 15 | 2 | 0.0 | 0.0 | 0.99 | 0.01\n",
    "\n",
    "\"Si veo el 0, me moveré hacia la izquierda el 99% de las veces, hacia abajo el 1% de las veces, hacia la derecha el 4% de las veces y hacia arriba el 5% de las veces\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34003758-53d7-4832-8e82-f805d723db9c",
   "metadata": {},
   "source": [
    "#### Espacios de acción continua\n",
    "\n",
    "¿Y si nuestro espacio de acción es continuo? Podemos seguir teniendo una política. Por ejemplo:\n",
    "\n",
    "| Observación | Acción | |\n",
    "|------|-------|\n",
    "| 0 | 0.42 |\n",
    "| 1 | -3.99 |\n",
    "| ... | ... |\n",
    "| 15 | 2.24 |\n",
    "\n",
    "Sin embargo, una política no determinista tendría que extraerse de una distribución de probabilidad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8212fc5-3270-49b9-8e82-44e04ebc892c",
   "metadata": {},
   "source": [
    "#### Espacios de observación continua\n",
    "\n",
    "- ¿Y si nuestro espacio de _observación_ es continuo? \n",
    "- Bueno, ahora ya no podemos dibujar la política como una tabla...\n",
    "- En este caso, nuestra política es una _función_ del valor de observación \n",
    "- Por ejemplo, \"ángulo del acelerador con el suelo (acción) = 1,5 x distancia al obstáculo más cercano (observación)\" \n",
    "- Este ejemplo de juguete dice que si el obstáculo más cercano está lejos, puedes acelerar el coche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed8dfeec-6879-4d40-9c9e-b28e4bb260b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# too much text around here, need more code and/or images\n",
    "# I think we can remove some of this stuff (continuous, beyond scalars) and add it back it when it's paired with a concrete example\n",
    "# it's not very helpful/interesting as just ideas in isolation..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0043038d-d6f4-4364-b348-e968fdb37d3a",
   "metadata": {},
   "source": [
    "#### Más allá de los escalares\n",
    "\n",
    "- Hasta ahora hemos asumido que la observación es un único número y la acción es un único número.\n",
    "- Sin embargo, ambos pueden ser tipos de datos más complejos: imágenes, vectores, etc \n",
    "- Las observaciones reales de un coche autoconducido pueden ser decenas de mediciones, imágenes, etc.\n",
    "- Las acciones reales de un coche autoconducido pueden ser la fijación de múltiples valores en cada paso de tiempo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dc8172-ae79-405f-801f-a2b750a62e31",
   "metadata": {},
   "source": [
    "#### Pensar en las políticas como funciones\n",
    "\n",
    "- En general, ésta es una forma útil de pensar: la política es una función que mapea las observaciones a las acciones.\n",
    "- En el **aprendizaje profundo por refuerzo**, esta función es una red neuronal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a75dce-81a4-4ff6-923c-d5e0b736384c",
   "metadata": {},
   "source": [
    "#### Resumen\n",
    "\n",
    "- El \"agente\" o \"jugador\" son personificaciones de la política\n",
    "- No hay \"inteligencia\" adicional ni toma de decisiones más allá de la política\n",
    "- Por tanto, técnicamente no necesitamos la noción de agente/jugador\n",
    "- La política es el resultado de la RL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7210fa7d-3ebb-4906-83cb-853e8f4d91d3",
   "metadata": {},
   "source": [
    "#### ¡Apliquemos lo que hemos aprendido!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5780af6f-fa34-4b85-af30-32e20755f1c0",
   "metadata": {},
   "source": [
    "## Política del Lago Congelado\n",
    "<!-- multiple choice -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a299f97-00fa-4e31-ad66-5e843b63e1d1",
   "metadata": {},
   "source": [
    "Recuerda el entorno del lago congelado:\n",
    "\n",
    "```\n",
    "P...\n",
    ".O.O\n",
    "...O\n",
    "O..G\n",
    "```\n",
    "\n",
    "con su espacio de observación representado como\n",
    "\n",
    "```\n",
    " 0 1 2 3\n",
    " 4 5 6 7\n",
    " 8 9 10 11\n",
    "12 13 14 15\n",
    "```\n",
    "\n",
    "y las acciones representadas como\n",
    "\n",
    "| Acción | Significado ||\n",
    "|------|------|\n",
    "| 0 izquierda\n",
    "| 1. Abajo\n",
    "| 2. Derecha\n",
    "| 3. Arriba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8eca95c-cc54-42d3-a442-f41fb0c27b5e",
   "metadata": {},
   "source": [
    "#### Pregunta 1\n",
    "\n",
    "En la política que aparece a continuación falta una entrada representada por el símbolo ``.\n",
    "\n",
    "| Observación: Acción\n",
    "|------|-------|\n",
    "| 0 | 0 |\n",
    "| 1 | 2 |\n",
    "| ... | ... |\n",
    "| 13 | ? |\n",
    "| 14 | 2 |\n",
    "| 15 | 0 |\n",
    "\n",
    "Selecciona la mejor opción para rellenar la entrada `?`.\n",
    "\n",
    "- [ ] 0 | ¡Intenta de nuevo!\n",
    "- [ ] 1 | ¡Inténtalo de nuevo!\n",
    "- [x] 2 | ¡Sí! Moverte hacia la derecha te lleva a la meta.\n",
    "- [ ] 3 | ¡Inténtalo de nuevo!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c41f87-143b-409f-a16c-b8b1ac4b0cc7",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Pregunta 2\n",
    "\n",
    "en la versión resbaladiza del Lago Helado, el agente tiene una probabilidad de 1/3 de moverse en la dirección prevista y 1/3 en las dos direcciones perpendiculares\n",
    "\n",
    "¿Es lo anterior una afirmación sobre el entorno o la política?\n",
    "\n",
    "- [x] Entorno | ¡Lo tienes! \n",
    "- [ ] Política | Recuerda que la política describe cómo responde el agente a las observaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefe9bb5-2162-44df-9559-e98566491dff",
   "metadata": {},
   "source": [
    "#### Pregunta 3\n",
    "\n",
    "en la versión resbaladiza del Lago Helado, a veces es mejor no caminar en la dirección que realmente quieres ir, porque es más importante evitar la posibilidad de resbalar en un agujero._\n",
    "\n",
    "¿Es lo anterior una afirmación sobre el medio ambiente o la política?\n",
    "\n",
    "- [ ] Entorno | La afirmación anterior se refiere a la mejor acción a realizar en una situación; ésta viene determinada por la política.\n",
    "- [x] Política | ¡Lo tienes!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2b5b57-e0e8-40c6-858c-34bc20ce5112",
   "metadata": {},
   "source": [
    "## Cálculo de la recompensa esperada\n",
    "<!-- coding exercise -->\n",
    "\n",
    "Completa el código de abajo para que calcule la recompensa esperada en el resbaladizo (no determinista)\n",
    "Entorno del Lago Congelado a lo largo de 1000 episodios. El bucle interior hace un bucle sobre los pasos de un solo episodio.\n",
    "El bucle exterior es sobre los episodios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca010dee-36a5-452c-84db-9bd7059f28b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 0.047\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "\n",
    "rewards = []\n",
    "N = 1000\n",
    "\n",
    "for ____:\n",
    "\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while ____:\n",
    "        action = np.random.randint(low=0, high=4)\n",
    "        obs, reward, done, _ = env.step(____)\n",
    "    \n",
    "    rewards.append(reward)\n",
    "    \n",
    "print(\"Average reward:\", sum(rewards)/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99564a1c-076f-4167-858f-ce8ce47107f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 0.014\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "\n",
    "rewards = []\n",
    "N = 1000\n",
    "\n",
    "for i in range(N): # loop over N episodes\n",
    "\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.random.randint(low=0, high=4)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "    \n",
    "    rewards.append(reward)\n",
    "    \n",
    "print(\"Average reward:\", sum(rewards)/N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8a29b5-f8b4-452c-9cd2-a045ba136d04",
   "metadata": {},
   "source": [
    "## Política artesanal\n",
    "<!-- coding exercise -->\n",
    "\n",
    "El código siguiente carga el entorno (no determinista) de Frozen Lake.\n",
    "Una política (determinista) se define como un diccionario de Python que mapea de observaciones a acciones.\n",
    "El código hace un bucle sobre 1000 episodios. Dentro de cada episodio, itera a través de los pasos de tiempo (observaciones y acciones) hasta que el episodio haya terminado y se haya obtenido una recompensa. A continuación, imprime la recompensa media de los 1000 episodios. Normalmente obtiene una recompensa media de alrededor de 0,05, lo que significa que el objetivo se alcanza alrededor del 5% de las veces.\n",
    "\n",
    "**Tu tarea:** modificar la política para que se consiga una recompensa media de al menos 0,02 (es decir, que el agente alcance el objetivo el 20% de las veces)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1ca754",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "\n",
    "import gym\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d9fcefc-ba10-45a3-9e03-adb283d6c08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 0.034\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "\n",
    "import gym\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "\n",
    "policy = {\n",
    "    0 : 2,\n",
    "    1 : 2,\n",
    "    2 : 2,\n",
    "    3 : 2,\n",
    "    4 : 1,\n",
    "    5 : 1,\n",
    "    6 : 1,\n",
    "    7 : 1,\n",
    "    8 : 2,\n",
    "    9 : 2,\n",
    "    10: 2,\n",
    "    11: 0,\n",
    "    12: 2,\n",
    "    13: 2,\n",
    "    14: 2,\n",
    "    15: 2\n",
    "}\n",
    "\n",
    "rewards = []\n",
    "N = 1000\n",
    "for i in range(N): # loop over N episodes\n",
    "\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = policy[obs]\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "    \n",
    "    rewards.append(reward)\n",
    "    \n",
    "print(\"Average reward:\", sum(rewards)/N)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ray200]",
   "language": "python",
   "name": "conda-env-ray200-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
