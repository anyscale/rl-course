{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c331561e-c62d-4486-9252-b9ff362f8fa4",
   "metadata": {},
   "source": [
    "## Entornos RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "093747d0-0011-4b28-a683-74870bfdbd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de064a2a-0a07-4c94-a9ac-b6f2d978f412",
   "metadata": {},
   "source": [
    "#### ¿Qué es un entorno?\n",
    "\n",
    "- Un entorno puede ser\n",
    "  - un juego, como un videojuego\n",
    "  - una simulación de un escenario del mundo real, como un robot, el comportamiento de un usuario o el mercado de valores\n",
    "  - cualquier otra configuración con un _agente_ que realiza _acciones_, ve _observaciones_ y recibe _recompensas_\n",
    "  \n",
    "Nota sobre la terminología: utilizaremos indistintamente _agente_ y _jugador_ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5748d00-5c07-49db-aa9f-3cc1613bd0a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Ejemplo de carrera: lago congelado\n",
    "\n",
    "Como ejemplo de ejecución de un entorno, utilizaremos el entorno [Frozen Lake](https://www.gymlibrary.ml/environments/toy_text/frozen_lake/) de [OpenAI Gym](https://www.gymlibrary.ml/), que proporciona la interfaz estándar para los problemas de RL. Podemos visualizar el entorno así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f87dba7-b5de-4ba9-b89d-b5ea2d069f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2eef796-f767-4d99-a454-0c3bd227bfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "from utils_01 import fix_frozen_lake_render\n",
    "fix_frozen_lake_render(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8579403b-4adf-4a22-8d27-d7f54fd52dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "P...\n",
      ".O.O\n",
      "...O\n",
      "O..G\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693db547-8f4d-49ae-b759-80308d6803e0",
   "metadata": {},
   "source": [
    "El objetivo es que el jugador (`P`) llegue a la meta (`G`) caminando por los segmentos del lago helado (`.`) sin caer en los agujeros (`O`).\n",
    "\n",
    "**Nota**: el renderizado del entorno se ha modificado con respecto al renderizado del Gimnasio OpenAI para que pueda mostrarse con claridad en esta plataforma de aprendizaje interactivo.\n",
    "\n",
    "Notas:\n",
    "\n",
    "El código no se muestra para la sustitución del renderizado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a34b31-5bbb-45f4-9e7f-e925fc96d0ee",
   "metadata": {},
   "source": [
    "#### Movimiento\n",
    "\n",
    "El jugador puede moverse por el lago helado. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d5d954a-7890-427c-aea1-7daec590c91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "....\n",
      "PO.O\n",
      "...O\n",
      "O..G\n"
     ]
    }
   ],
   "source": [
    "env.step(1) # 1 -> Down\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb8ff83-f2b0-464f-934c-9e6144977c2c",
   "metadata": {},
   "source": [
    "No te preocupes por `paso(1)` por ahora; ya llegaremos a eso \n",
    "\n",
    "Lo que puedes ver es que el jugador (`P`) se movió hacia abajo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c7ab81-0402-4302-a46f-1b4bc115ca63",
   "metadata": {},
   "source": [
    "#### Objetivo\n",
    "\n",
    "Avanza un montón de pasos y habrás completado el puzzle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a799321-538a-4f9c-ae1d-31e15a9e0e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "....\n",
      ".O.O\n",
      "...O\n",
      "O..P\n"
     ]
    }
   ],
   "source": [
    "env.step(1)\n",
    "env.step(2)\n",
    "env.step(1)\n",
    "env.step(2)\n",
    "env.step(1)\n",
    "env.step(2)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98858094-a152-4e3e-bd03-ed5c81af095b",
   "metadata": {},
   "source": [
    "Has alcanzado el objetivo al llegar a la parte inferior derecha."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1635d0e4-7d38-434c-a956-d47646e57b4a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### ¿Qué hace un entorno?\n",
    "\n",
    "Un entorno implica varios componentes clave, que repasaremos en las siguientes diapositivas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73df9894-c37a-4d34-93d9-c4e43ecf0564",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Estados\n",
    "\n",
    "- Utilizaremos el término _estado_ de manera informal para referirnos a todo lo relacionado con el entorno "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a57b3f1-c76e-4b2c-9d84-742326bb827b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "P...\n",
      ".O.O\n",
      "...O\n",
      "O..G\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16f9748-129e-45c7-9bc1-106e0493fdad",
   "metadata": {},
   "source": [
    "- Por ejemplo, este es el estado inicial del entorno.\n",
    "- El jugador está en la parte superior izquierda, hay hielo congelado cerca, etc.\n",
    "- Utilizaremos el concepto de estado para hablar de nuestro entorno, pero no aparecerá en la \"API\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0e72e4-8f86-4c05-9f33-7e888ef44045",
   "metadata": {},
   "source": [
    "#### Acciones\n",
    "\n",
    "- Aquí, el jugador puede elegir entre 4 acciones posibles: izquierda, abajo, derecha, arriba\n",
    "- El espacio de todas las acciones posibles se denomina **espacio de acción**.\n",
    "- En SL distinguimos entre regresión (continua $y$) y clasificación (categórica $y$)\n",
    "- Del mismo modo, en RL el espacio de acción puede ser continuo o discreto\n",
    "- En este caso, es discreto (4 posibilidades)\n",
    "- El código está de acuerdo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "757cfd69-d93d-4e8b-90d0-f3caf079f5a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1907e0-63fe-4c72-af65-f9869b590365",
   "metadata": {},
   "source": [
    "#### Observaciones\n",
    "\n",
    "- Las observaciones son las _partes del estado que el agente puede ver_.\n",
    "- A veces, el agente puede verlo todo; a esto lo llamamos _completamente observable_.\n",
    "- A menudo, tenemos entornos _parcialmente observables_ \n",
    "- En el ejemplo del Lago Helado, el agente sólo puede ver su propia ubicación de las 16 casillas.\n",
    "- Al agente no se le \"dice\" dónde están los agujeros mediante observaciones directas, por lo que tendrá que _aprender_ esto mediante ensayo y error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659ebb9b-4c6a-43f6-96c1-eb5117c50a54",
   "metadata": {},
   "source": [
    "#### Observaciones\n",
    "\n",
    "- El espacio de todas las observaciones posibles se denomina **espacio de observación**.\n",
    "- Puedes pensar en el espacio de acción como análogo al objetivo en el aprendizaje supervisado.\n",
    "- Puedes pensar en el espacio de observación como análogo a las características en el aprendizaje supervisado.\n",
    "\n",
    "\n",
    "Aquí tenemos un espacio de observación discreto formado por las 16 posiciones posibles del jugador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81aa1ad6-e1bb-492d-a5d0-a72c94a33e29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(16)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ee9cb4-8650-4d39-87b7-97202ffca199",
   "metadata": {},
   "source": [
    "#### Recompensas\n",
    "\n",
    "- En el aprendizaje supervisado, el objetivo suele ser hacer buenas predicciones.\n",
    "- Puedes probar diferentes funciones de pérdida según tu objetivo específico, pero el concepto general es el mismo.\n",
    "- En RL, el objetivo puede ser cualquier cosa.\n",
    "- Pero, al igual que en la SL, tendrás que _optimizar_ algo.\n",
    "- En RL, nuestro objetivo es maximizar la **recompensa**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf68c60b-f6fe-47e0-8482-bc51c4410846",
   "metadata": {},
   "source": [
    "#### Recompensas \n",
    "\n",
    "En el ejemplo del Lago Helado, el agente recibe una recompensa cuando alcanza el objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4807bbe7-7525-4772-9ce9-a0e06f4b21fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "P...\n",
      ".O.O\n",
      "...O\n",
      "O..G\n",
      "reward = 0.0\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "obs, reward, done, _ = env.step(0)\n",
    "env.render()\n",
    "print(\"reward =\", reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aec3a5cd-5607-41fb-a932-7e5e10bf6569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward = 0.0\n"
     ]
    }
   ],
   "source": [
    "obs, reward, done, _ = env.step(1)\n",
    "print(\"reward =\", reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceae768d-d87b-4611-afce-e02137b9fd72",
   "metadata": {},
   "source": [
    "Todavía no hay recompensa, sigamos..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4c7004-91a1-4dcc-ad52-943cfd179a97",
   "metadata": {},
   "source": [
    "#### Recompensas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89518a96-7c75-437d-9bb1-da7dbc7038b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "....\n",
      ".O.O\n",
      "...O\n",
      "O..P\n",
      "reward = 1.0\n"
     ]
    }
   ],
   "source": [
    "env.step(1)\n",
    "env.step(2)\n",
    "env.step(1)\n",
    "env.step(2)\n",
    "env.step(1)\n",
    "obs, reward, done, _ = env.step(2)\n",
    "env.render()\n",
    "print(\"reward =\", reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f9cfd3-9e69-4752-9927-ba108685aa83",
   "metadata": {},
   "source": [
    "Obtuvimos una recompensa de 1,0 por alcanzar el objetivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e9f4d6-ef36-415d-905b-ce1fcce042e6",
   "metadata": {},
   "source": [
    "#### Bucle ambiente-agente\n",
    "\n",
    "Observa cómo el agente (en este caso, nosotros) y el entorno se comunican en un bucle de ida y vuelta:\n",
    "\n",
    "![](img/RL-loop.png)\n",
    "\n",
    "Este es el clásico diagrama que verás en todas partes.\n",
    "\n",
    "El objetivo será aprender automáticamente el comportamiento del agente (¡permanece atento!)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e888ae70-a3f2-400b-8fd9-54dd27369beb",
   "metadata": {},
   "source": [
    "#### Representación de acciones\n",
    "\n",
    "- Para utilizar el software de RL, necesitaremos una representación numérica de nuestro espacio de acción y de nuestro espacio de observación.\n",
    "- En este caso, tenemos 4 posibles acciones discretas, por lo que podemos codificarlas como {0,1,2,3} para (izquierda, abajo, derecha, arriba).\n",
    "- Por eso, antes hicimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "420363c1-9385-47e0-abaf-935a5c0fb4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5a3c2f-ec47-47c4-85e8-95945dfa7019",
   "metadata": {},
   "source": [
    "para caminar hacia abajo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6419f73c-8431-4e46-87af-0a99a95a58ba",
   "metadata": {},
   "source": [
    "#### Representar las observaciones\n",
    "\n",
    "- Del mismo modo, necesitaremos una representación numérica de nuestras observaciones.\n",
    "- Aquí hay 16 posiciones posibles del jugador. Éstas se codifican de 0 a 15 de la siguiente manera:\n",
    "\n",
    "```\n",
    " 0 1 2 3\n",
    " 4 5 6 7\n",
    " 8 9 10 11\n",
    "12 13 14 15\n",
    "```\n",
    "\n",
    "Estos detalles del entorno de Frozen Lake también están disponibles en la [documentación](https://www.gymlibrary.ml/environments/toy_text/frozen_lake/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cecba2-2529-4f24-a5ef-d730457ac994",
   "metadata": {},
   "source": [
    "#### Representar las observaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b616b8-5ccf-45bf-a4ac-65ae31f108e4",
   "metadata": {},
   "source": [
    "Inicialmente, observamos \"0\" porque empezamos en la parte superior izquierda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df316735-5a38-488f-80ae-dd97d715cc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "P...\n",
      ".O.O\n",
      "...O\n",
      "O..G\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b5dad3-f8bc-4298-8dc4-4a3f87285600",
   "metadata": {},
   "source": [
    "Después de desplazarnos hacia abajo (acción 1), pasamos a la posición 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d540232-affa-45de-a10f-6f99f755d5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, reward, done, _ = env.step(1)\n",
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b84f58-bf29-4df0-9b85-08190d1815f0",
   "metadata": {},
   "source": [
    "La observación es devuelta por el método `step()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f179e5d-68cb-4d31-a5e0-cd01f5b14124",
   "metadata": {},
   "source": [
    "#### Entornos no deterministas\n",
    "\n",
    "- Hasta ahora, realizar una acción determinada desde un estado concreto siempre daba como resultado el mismo estado nuevo.\n",
    "- En otras palabras, nuestro entorno del Lago Helado era _determinista_.\n",
    "- Algunos entornos son _no deterministas_, lo que significa que el resultado de una acción puede ser aleatorio.\n",
    "- Podemos inicializar un Lago Congelado no determinista así"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9bdde4c3-e7f9-419a-a25b-4f2d6027354e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_slippery = gym.make(\"FrozenLake-v1\", is_slippery=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a7d44ef-2828-4c7f-aae9-17eab6b7d7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "env_slippery.seed(4)\n",
    "fix_frozen_lake_render(env_slippery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3943d88f-a44b-4fba-96f8-20177b6317b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "P...\n",
      ".O.O\n",
      "...O\n",
      "O..G\n"
     ]
    }
   ],
   "source": [
    "env_slippery.reset()\n",
    "env_slippery.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b97800f-5c7a-43a8-9018-f8505bd9eafd",
   "metadata": {},
   "source": [
    "#### Entornos no deterministas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35e5cb48-3e0f-4b5f-8f0a-663ff31e99e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "....\n",
      "PO.O\n",
      "...O\n",
      "O..G\n"
     ]
    }
   ],
   "source": [
    "env_slippery.step(1) # move down\n",
    "env_slippery.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0755c6-0d78-42d0-a9b4-cd4aec36db82",
   "metadata": {},
   "source": [
    "La mudanza hacia abajo no funcionó como estaba previsto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cba5c8e8-a7ab-4854-9c35-ca79584f7b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "....\n",
      ".P.O\n",
      "...O\n",
      "O..G\n"
     ]
    }
   ],
   "source": [
    "env_slippery.step(1) # move down\n",
    "env_slippery.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031a6a4e-f9a1-4965-86b8-d439a82491ad",
   "metadata": {},
   "source": [
    "Esta vez ha funcionado el movimiento hacia abajo.\n",
    "\n",
    "En este entorno \"resbaladizo\" del Lago Helado, el movimiento sólo funciona como se pretende 1/3 de las veces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0388fe5a-b6e4-484c-ac51-d376c353ffaa",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Episodios\n",
    "\n",
    "- Jugar al Lago Helado tiene un final: o bien caes en un agujero o llegas a la meta.\n",
    "- Sin embargo, una sola partida no es suficiente para que el algoritmo de RL aprenda.\n",
    "- Necesitará varias partidas, llamadas **episodios**.\n",
    "- Después de un episodio, el entorno se reinicia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68508dc1-2009-48a7-85e1-3f4bba4b0220",
   "metadata": {},
   "source": [
    "#### Episodios\n",
    "\n",
    "El método `paso()` devuelve una bandera que nos indica si el episodio ha terminado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "534b8516-26ea-4df4-9062-e6fb1f57d3a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, reward, done, _ = env_slippery.step(1)\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0684e7c-b767-4907-965d-6bcc86d64c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "....\n",
      ".P.O\n",
      "...O\n",
      "O..G\n"
     ]
    }
   ],
   "source": [
    "env_slippery.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdbecc7-7aac-432a-9e6e-301af52e7d2a",
   "metadata": {},
   "source": [
    "Aquí el episodio ha terminado porque hemos caído en un agujero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854521ed-7fb7-453b-9eb6-eaf61074f533",
   "metadata": {},
   "source": [
    "#### Episodios\n",
    "\n",
    "- En algunos entornos (como el Lago Helado), las recompensas sólo se reciben al final de un episodio.\n",
    "- En otros entornos, las recompensas pueden recibirse en cualquier **paso de tiempo** (es decir, después de una acción)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea0a22c-c13f-407e-beb1-5fd0a985dc4a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Ponerlo todo junto\n",
    "\n",
    "- Ya hemos hablado de los principales componentes de un entorno RL:\n",
    "  - Estados\n",
    "  - Acciones\n",
    "  - Observaciones\n",
    "  - Recompensas\n",
    "  - Episodios\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8de8cd1-8733-4c90-89d6-9586be21309a",
   "metadata": {},
   "source": [
    "#### conjuntos de datos de SL frente a entornos de RL\n",
    "\n",
    "- En el aprendizaje supervisado, normalmente se te da un conjunto de datos.\n",
    "- En RL, el entorno actúa como un _generador de datos_.\n",
    "  - Cuanto más juegues con el entorno, más \"datos\" generarás y más podrás aprender.\n",
    "- También se puede hacer RL con un conjunto de datos previamente recogidos (lo que se llama _RL offline_), pero eso está fuera de nuestro alcance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be83b356-8218-44dd-9ccd-f0bc44ade74e",
   "metadata": {},
   "source": [
    "#### ¡Apliquemos lo que hemos aprendido!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c511b50-fae2-4764-8eaa-c47ca7dab1d7",
   "metadata": {},
   "source": [
    "## Entorno del coche autodirigido\n",
    "<!-- multiple choice -->\n",
    "\n",
    "Estás utilizando RL para entrenar un coche de autoconducción. La IA del coche utiliza varias cámaras y sensores como entradas y tiene que decidir el ángulo del volante, así como el ángulo de los pedales de gas/freno en el suelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a443612-50ff-4564-bdf4-a4e1ba79be93",
   "metadata": {},
   "source": [
    "#### ¿Es el espacio de observación continuo o discreto?\n",
    "\n",
    "- [x] Continuo\n",
    "- [ ] Discreto | En este caso, las observaciones son las entradas de los sensores, por ejemplo, las estimaciones de profundidad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cace1880-0440-42f1-a087-576821998a4a",
   "metadata": {},
   "source": [
    "#### ¿Es el espacio de acción continuo o discreto?\n",
    "\n",
    "- [x] Continuo\n",
    "- [ ] Discreto | Las acciones son ángulos; no proceden de un conjunto discreto de opciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124e3763-a526-4cd7-84c1-721bb7b9f102",
   "metadata": {},
   "source": [
    "#### ¿Cuál sería la estructura de recompensa más razonable para este entorno?\n",
    "\n",
    "- [] La recompensa es igual a la cantidad de tiempo que el coche fue capaz de conducir sin chocar | ¿Cuál sería la recompensa si el coche no se mueve nunca?\n",
    "- [x] La recompensa es igual a la distancia que el coche fue capaz de recorrer sin chocar | ¡Sí, eso suena bien!\n",
    "- [ ] +1 de recompensa cada vez que el coche se estrelle | Ten en cuenta que queremos maximizar la recompensa, no minimizarla."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcfbbb6-1765-473c-8b7e-c24e914949d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Episodios frente a pasos de tiempo\n",
    "<!-- multiple choice -->\n",
    "\n",
    "Completa los espacios en blanco de la siguiente frase \n",
    "\n",
    "*En un entorno de aprendizaje por refuerzo, se realizan acciones de forma repetida hasta que el \\_\\_\\_\\_termina. Puede tratarse de una sola \\N acción, o de muchas.*\n",
    "\n",
    "- [ ] paso de tiempo / recompensa | ¡Comprueba cuidadosamente el primer espacio en blanco!\n",
    "- [ ] paso de tiempo / recompensa | ¡Inténtalo de nuevo!\n",
    "- [ ] paso de tiempo / episodio | ¡Inténtalo de nuevo!\n",
    "- [x] episodio / paso de tiempo | ¡Lo tienes!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5286ef-84b7-4b04-a74b-d90b0216abae",
   "metadata": {},
   "source": [
    "## Entorno de taxi del gimnasio\n",
    "<!-- coding exercise -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22599988-4849-4a9b-aa83-2b792dedbc19",
   "metadata": {},
   "source": [
    "En este ejercicio veremos uno de los entornos basados en texto incluidos en\n",
    "OpenAI gym, llamado entorno taxi.\n",
    "Documentación [aquí](https://www.gymlibrary.ml/environments/toy_text/taxi/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb32c250-3e05-4040-bad0-5f8f0d7e3123",
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE\n",
    "import gym\n",
    "\n",
    "taxi = gym.make(\"Taxi-v3\")\n",
    "obs = taxi.reset(seed=5)\n",
    "taxi.render()\n",
    "\n",
    "# Add calls to `step` here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413240b7-6df6-48a5-bb24-4f8a518dc81e",
   "metadata": {},
   "source": [
    "En este ejercicio, el taxi está representado por el resaltado amarillo,\n",
    "actualmente en la parte inferior izquierda de la arena. El `:` se puede cruzar, pero el `|` no.\n",
    "El objetivo es recoger pasajeros y dejarlos.\n",
    "\n",
    "Hay 6 acciones posibles:\n",
    "abajo (0), arriba (1), derecha (2), izquierda (3), recoger (4), dejar (5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "99f35e5c-5e97-4122-9ae0-44e849a51c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(6)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5201b9-8041-4044-b172-a736f43451df",
   "metadata": {},
   "source": [
    "El siguiente código imprime la observación en un formato más legible para los humanos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f0083927-ad7d-4e78-b04f-316c394f0139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taxi row: 4\n",
      "Taxi col: 0\n",
      "Passenger loc: 0\n",
      "Destination loc: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Taxi row: %d\\nTaxi col: %d\\nPassenger loc: %d\\nDestination loc: %d\" % tuple(taxi.decode(obs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2553c73a-e6e8-4096-9b67-1806dae7f79f",
   "metadata": {},
   "source": [
    "Las posibles ubicaciones son `R` (0), `G` (1), `Y` (2), `B` (3) y en taxi (4). Así, el pasajero se encuentra actualmente en `R` y se dirige a `Y`.\n",
    "\n",
    "Para responder a la siguiente pregunta, tendrás que modificar el código, ejecutarlo e imprimir cualquier resultado relevante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a4208ce9-efb2-48e7-8e7a-3b6042288309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[42mY\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "import gym\n",
    "\n",
    "taxi = gym.make(\"Taxi-v3\")\n",
    "obs = taxi.reset(seed=5)\n",
    "taxi.render()\n",
    "taxi.step(1)\n",
    "taxi.step(1)\n",
    "taxi.step(1)\n",
    "taxi.step(1)\n",
    "taxi.render()\n",
    "taxi.step(4)\n",
    "taxi.step(0)\n",
    "taxi.step(0)\n",
    "taxi.step(0)\n",
    "taxi.step(0)\n",
    "taxi.render()\n",
    "obs, reward, done, _ = taxi.step(5)\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "598a018b-569e-4378-bb2a-0de521c18881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: in this case there is no testing of the code; the code is only used to explore\n",
    "# this may not work well with the framework - add an automated test to the code as well? \n",
    "# e.g. can check that the state of the env is correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9564fe-af4a-4ded-97d3-4b754df46265",
   "metadata": {},
   "source": [
    "#### ¿Qué recompensa recibe el agente por dejar al pasajero con éxito?\n",
    "\n",
    "- [ ] 0 | Intenta realizar las acciones necesarias con `taxi.step()` e imprimir las recompensas.\n",
    "- [5 Intenta realizar las acciones necesarias con \"taxi.step()` e imprimir las recompensas.\n",
    "- [10 Intenta realizar las acciones necesarias con \"taxi.step()` e imprimir las recompensas.\n",
    "- [x] 20 | ¡Lo tienes!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fffe5b6-36c3-4b2b-9513-a6125f0eaac3",
   "metadata": {},
   "source": [
    "## Observaciones frente a renders\n",
    "<!-- coding exercise -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a7f02e-4367-48d5-b491-8ee5cce4f468",
   "metadata": {},
   "source": [
    "El entorno del lago congelado nos permite crear una representación visual del entorno, que hemos visto antes. Esto es para fines humanos/depuración, y _no_ lo ve el agente/algoritmo. Tu tarea aquí es reproducir un entorno dado de Frozen Lake **sin mirar la representación visual** (¡sin hacer trampas!). Rellena la lista de \"acciones\" para que contenga un conjunto de acciones que lleven correctamente al agente a la meta. ¡Lo que estás experimentando es lo que un algoritmo de RL \"ve\" cuando aprende!\n",
    "\n",
    "Ten en cuenta que el entorno de Frozen Lake dado es de 3x3 en lugar de 4x4. Por tanto, el espacio de observación va de 0 a 8 en lugar de 0 a 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb2ba815",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '____' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m env\u001b[38;5;241m.\u001b[39mrender \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     11\u001b[0m obs \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m---> 12\u001b[0m actions \u001b[38;5;241m=\u001b[39m \u001b[43m____\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m actions:\n\u001b[1;32m     14\u001b[0m     obs, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "\u001b[0;31mNameError\u001b[0m: name '____' is not defined"
     ]
    }
   ],
   "source": [
    "# EXERCISE\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "env = gym.make(\"FrozenLake-v1\", \n",
    "               desc=gym.envs.toy_text.frozen_lake.generate_random_map(size=3, p=0.3), \n",
    "               is_slippery=False)\n",
    "env.render = None\n",
    "\n",
    "obs = env.reset()\n",
    "actions = ____\n",
    "for action in actions:\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    print(\"Obs:\", obs, \"Reward:\", reward, \"Done:\", done)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7a5f827-8aad-4f48-b762-1feb4b51748a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs: 3 Reward: 0.0 Done: False\n",
      "Obs: 4 Reward: 0.0 Done: False\n",
      "Obs: 7 Reward: 0.0 Done: False\n",
      "Obs: 8 Reward: 1.0 Done: True\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "\n",
    "import gym.envs.toy_text\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "env = gym.make(\"FrozenLake-v1\", desc=gym.envs.toy_text.frozen_lake.generate_random_map(size=3, p=0.3), is_slippery=False)\n",
    "env.render = None\n",
    "\n",
    "obs = env.reset()\n",
    "actions = []\n",
    "# BEGIN SOLUTION\n",
    "actions = [1,2,1,2]\n",
    "# END SOLUTION\n",
    "for action in actions:\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    print(\"Obs:\", obs, \"Reward:\", reward, \"Done:\", done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0c3a6fc-637a-46b7-92e0-ff08e576c65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PO.\n",
      "...\n",
      "O.G\n"
     ]
    }
   ],
   "source": [
    "# HIDDEN\n",
    "import numpy as np\n",
    "import gym.envs.toy_text\n",
    "from utils_01 import fix_frozen_lake_render\n",
    "np.random.seed(1)\n",
    "env = gym.make(\"FrozenLake-v1\", desc=gym.envs.toy_text.frozen_lake.generate_random_map(size=3, p=0.3), is_slippery=False)\n",
    "env.reset()\n",
    "fix_frozen_lake_render(env)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e8a4b0-b102-4047-8e67-942ef81b0ed9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### El mejor camino hacia la meta\n",
    "\n",
    "Recordando que las acciones 0, 1, 2, 3 representan izquierda, abajo, derecha, arriba (respectivamente), ¿cuál de las siguientes afirmaciones describe correctamente el mejor camino hacia la meta?\n",
    "\n",
    "- [ ] Empieza moviéndote hacia abajo, y luego hacia abajo de nuevo\n",
    "- [x] Empieza moviéndote hacia abajo, y luego hacia la derecha\n",
    "- [ ] Empieza moviéndote a la derecha, luego a la derecha otra vez\n",
    "- [ ] Empieza moviéndote hacia la derecha y luego hacia abajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8690d35a-5d51-4452-bd9c-8b44161feb81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ray200]",
   "language": "python",
   "name": "conda-env-ray200-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
