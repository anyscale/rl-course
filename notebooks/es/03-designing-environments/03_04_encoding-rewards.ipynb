{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e23df55-837b-4ec9-86ad-3b60c6a7f00d",
   "metadata": {},
   "source": [
    "## Codificación de las recompensas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28d9b458-b46e-48de-ab18-bf884c1eab85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import ray\n",
    "import logging\n",
    "ray.init(log_to_driver=False, ignore_reinit_error=True, logging_level=logging.ERROR); # logging.FATAL\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c8c796-3785-44d1-8aea-78872152b0cb",
   "metadata": {},
   "source": [
    "#### Recompensas de codificación\n",
    "\n",
    "- Ya hemos hablado de la importancia de codificar las observaciones.\n",
    "- También podemos tener alguna opción en el espacio de acción, aunque aquí (y a menudo) es relativamente clara/fija.\n",
    "- Pero, ¿qué pasa con las recompensas? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e790dc-5146-4b03-a77a-24efd69ffabd",
   "metadata": {},
   "source": [
    "#### Configuración actual\n",
    "\n",
    "- Actualmente, obtenemos una recompensa de +1 por alcanzar el objetivo \n",
    "- Esto es parte de lo que hace que RL sea tan difícil (e impresionante):\n",
    "  - Queremos aprender sobre las acciones aunque no sepamos de inmediato si la acción fue \"buena\" \n",
    "  - Contrasta esto con el aprendizaje supervisado, donde cada predicción que hacemos sobre los datos de entrenamiento puede compararse inmediatamente con el valor objetivo conocido.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c10944be-9746-44d6-bb2b-66843ea7e4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: perhaps this next slide can be moved to Module 1, since it's very general?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff6ac9f-6815-4910-b3ce-aa7e0866101c",
   "metadata": {},
   "source": [
    "#### Los agentes no pueden ser simplemente codiciosos\n",
    "\n",
    "- ¿Pueden los agentes aprender simplemente a buscar la mejor recompensa inmediata?\n",
    "- No. Por ejemplo, en un sistema de recomendación de vídeos, mostrar al usuario otro vídeo divertido de gatos puede hacer que haga clic (alta recompensa inmediata) pero provocar una pérdida de interés en el servicio a largo plazo (baja recompensa a largo plazo).\n",
    "- Nuestro Lago Helado es otro ejemplo del problema: a veces no hay ninguna recompensa inmediata de la que aprender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c44cea8c-a28e-4d7c-a0c8-91c1c3f8fc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: perhaps this next section on \"Learned action probabilities\" could be moved much earlier, even as early as Module 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462e6340-cb1f-4eda-b642-d21505d18b52",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Probabilidades de acción aprendidas\n",
    "\n",
    "- RLlib nos permite mirar dentro del modelo la probabilidad de cada acción dada una observación (es decir, la política aprendida).\n",
    "- Carguemos el modelo entrenado con nuestras observaciones codificadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16d9d371-0c26-46a0-90c5-6f26e25880f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from envs_03 import RandomLakeObs\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "ppo_config = (\n",
    "    PPOConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(create_env_on_local_worker=True, horizon=100)\\\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\n",
    ")\n",
    "ppo_RandomLakeObs = ppo_config.build(env=RandomLakeObs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1420e1fd-22e9-474c-aad6-928428976b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # HIDDEN\n",
    "\n",
    "# for i in range(16):\n",
    "#     ppo_RandomLakeObs.train()\n",
    "    \n",
    "# print(ppo_RandomLakeObs.evaluate()[\"evaluation\"][\"episode_reward_mean\"])\n",
    "\n",
    "# ppo_RandomLakeObs.save(\"models/RandomLakeObs-Ray2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a71119d3-e3c9-427c-a697-fe733fc10ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_RandomLakeObs.restore(\"models/RandomLakeObs-Ray2/checkpoint_000016\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fec9d02-7e22-44ee-b171-2d9bf86b8681",
   "metadata": {},
   "source": [
    "#### Probabilidades de acción aprendidas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66648b5d-abd8-45ef-b592-e56bdee1c9c9",
   "metadata": {},
   "source": [
    "Utilizaremos la función `query_Policy` del módulo 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84b56415-177b-414f-8985-1118cabec0b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00902206, 0.5078786 , 0.47434822, 0.00875122], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils_03 import query_policy\n",
    "query_policy(ppo_RandomLakeObs, RandomLakeObs(), [0,0,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0256b6-3174-4719-931f-65342f5ae4a7",
   "metadata": {},
   "source": [
    "- Recuerda la ordenación (izquierda, abajo, derecha, arriba).\n",
    "- Cuando la observación es `[0 0 0]` (sin agujeros ni aristas a la vista), el agente prefiere ir hacia abajo y hacia la derecha."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99a889a-37bd-4a7c-bf6b-c9eb297a1520",
   "metadata": {},
   "source": [
    "¿Y si hay un agujero debajo de ti? Podemos introducir una observación diferente en la política:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc66f33e-8b23-4bcc-9f12-04b47d18aea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01965651, 0.00299437, 0.9645823 , 0.01276694], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_policy(ppo_RandomLakeObs, RandomLakeObs(), [0,1,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6560c87a-c07e-4aec-a172-915cad2b376c",
   "metadata": {},
   "source": [
    "- ¡Ahora es muy poco probable que el agente baje, y muy probable que vaya a la derecha!\n",
    "- De nuevo, todo esto se aprendió por ensayo y error, con una recompensa obtenida sólo cuando se alcanzaba el objetivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffae4393-3058-40ce-821a-fe1e487cf4d4",
   "metadata": {},
   "source": [
    "#### Recompensas de Random Lake\n",
    "\n",
    "- En el ejemplo de Random Lake, ¿no se puede facilitar la vida al agente dando recompensas inmediatas?\n",
    "\n",
    "Este es el código de recompensa actual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bb56001-259b-4b88-8bdd-29746e208042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward(self):\n",
    "    return int(self.player == self.goal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1ee605-1e87-4080-b8ee-274816dd67fd",
   "metadata": {},
   "source": [
    "- El agente tiene que aprender, por ensayo y error a lo largo de _episodios enteros_, que moverse hacia abajo y hacia la derecha es generalmente algo bueno "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0cf72e-d732-4039-90cc-22a17360943f",
   "metadata": {},
   "source": [
    "#### Redefinir las recompensas\n",
    "\n",
    "- Intentemos, en cambio, dar una recompensa _en cada paso, que sea mayor a medida que el agente se acerca al objetivo_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9a0466b-44fd-43ed-a3c0-d93334748213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from envs_03 import RandomLakeObs\n",
    "\n",
    "class RandomLakeObsRew(RandomLakeObs):\n",
    "    def reward(self):\n",
    "        return 6-(abs(self.player[0]-self.goal[0]) + abs(self.player[1]-self.goal[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce59aa5-1f90-457f-a3f9-c970cd8a7b7b",
   "metadata": {},
   "source": [
    "- El método anterior utiliza la [Distancia Manhattan](https://en.wikipedia.org/wiki/Taxicab_geometry) entre el jugador y la meta como recompensa \n",
    "- Cuando el agente llega a la meta, se obtiene la recompensa máxima de 6.\n",
    "- Cuando el agente se aleja más de la meta, se obtiene la recompensa mínima de 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75f7b77-8962-43fe-a82b-647e4f2aee69",
   "metadata": {},
   "source": [
    "#### Redefinir las recompensas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcba334f-f2a3-4cb0-8b8a-f5dd69a3346f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧑🧊🧊🧊\n",
      "🧊🧊🧊🧊\n",
      "🕳🧊🕳🧊\n",
      "🧊🧊🧊⛳️\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = RandomLakeObsRew()\n",
    "env.reset()\n",
    "env.render()\n",
    "env.reward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5208f2-c7e9-4f1e-9ed7-e91daec89d8e",
   "metadata": {},
   "source": [
    "⬆️ la recompensa es 0\n",
    "\n",
    "⬇️ la recompensa es 1 porque nos acercamos al objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "088715f0-3f06-4231-a149-401b6518dcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧊🧊🧊🧊\n",
      "🧑🧊🧊🧊\n",
      "🕳🧊🕳🧊\n",
      "🧊🧊🧊⛳️\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, rew, done, _ = env.step(1)\n",
    "env.render()\n",
    "rew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e607ab59-e697-44b6-99a6-91a7a3a3d2b2",
   "metadata": {},
   "source": [
    "#### Redefinir las recompensas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95ab62b7-e2b5-4f6e-8f81-9381b605a9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧊🧊🧊🧊\n",
      "🧊🧊🧊🧊\n",
      "🕳🧊🕳🧑\n",
      "🧊🧊🧊⛳️\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, rew, done, _ = env.step(2)\n",
    "obs, rew, done, _ = env.step(2)\n",
    "obs, rew, done, _ = env.step(2)\n",
    "obs, rew, done, _ = env.step(1)\n",
    "env.render()\n",
    "rew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1a183d-af4c-4c9d-a6d3-4bc1a7bfecca",
   "metadata": {},
   "source": [
    "Ahora, la recompensa es 5. Después, será 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "904e40cb-55b6-4b45-b063-5b335123e071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧊🧊🧊🧊\n",
      "🧊🧊🧊🧊\n",
      "🕳🧊🕳🧊\n",
      "🧊🧊🧊🧑\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, rew, done, _ = env.step(1)\n",
    "env.render()\n",
    "rew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ac0aec-ddc4-4d44-833e-c2b2e253120f",
   "metadata": {},
   "source": [
    "#### Comparar las recompensas\n",
    "\n",
    "- Así pues, tenemos dos posibles funciones de recompensa. ¿Cuál funciona mejor? \n",
    "- Recuerda que la última vez, después de entrenar durante 8 iteraciones, fuimos capaces de alcanzar el objetivo alrededor del 70% de las veces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5dbcf34-37e8-4025-bcd2-7a308d77046f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8166666666666667"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_RandomLakeObs.evaluate()['evaluation']['episode_reward_mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c75b369-6e96-4192-b7cd-4216f354d901",
   "metadata": {},
   "source": [
    "#### Comparando las recompensas\n",
    "\n",
    "¡Vamos a entrenar con la nueva función de recompensa!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01c1afa7-4452-43e4-ad2d-e3dc33934fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_RandomLakeObsRew = ppo_config.build(env=RandomLakeObsRew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7478ee6-28d8-4e44-932f-1b4f47e6b911",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    ppo_RandomLakeObsRew.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "257cef04-752e-4fe1-9eb6-3e225ea1ba62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101.90566037735849"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_RandomLakeObsRew.evaluate()['evaluation']['episode_reward_mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bccc6f4-1e9a-4abd-af7a-08b4c6103e4b",
   "metadata": {},
   "source": [
    "Un momento, ¿qué está pasando aquí?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874a6979-1882-4bb4-a5cf-a83282ed15f3",
   "metadata": {},
   "source": [
    "#### ¿Comparar las recompensas?\n",
    "\n",
    "- Intentamos mejorar nuestro sistema de RL dando forma a la función de recompensa.\n",
    "- Esto (presumiblemente) afectó al entrenamiento, pero también a nuestra evaluación.\n",
    "- En el aprendizaje supervisado, esto es como cambiar la métrica de puntuación del error cuadrado al error absoluto.\n",
    "- Si el antiguo sistema obtuvo un error medio al cuadrado de 20.000 y el nuevo sistema obtuvo un error medio absoluto de 40, ¿cuál es mejor?\n",
    "- ¡Estamos comparando manzanas y naranjas!\n",
    "- Queremos comparar ambos modelos con la misma métrica, por ejemplo la métrica original \n",
    "- En este caso, queremos ver la frecuencia con la que el agente alcanza el objetivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf05802-842d-4764-b408-72559e7bb8cf",
   "metadata": {},
   "source": [
    "#### ¿Comparar las recompensas?\n",
    "\n",
    "- El código aquí es un poco más avanzado.\n",
    "- Se incluye para completarlo, pero no entraremos en detalles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "261074ea-c6db-4e1b-a35f-a647e8cf2776",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.agents.callbacks import DefaultCallbacks\n",
    "\n",
    "class MyCallbacks(DefaultCallbacks):\n",
    "    def on_episode_end(self, *, worker, base_env, policies, episode, env_index, **kwargs):\n",
    "        info = episode.last_info_for()\n",
    "        episode.custom_metrics[\"goal_reached\"] = info[\"player\"] == info[\"goal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3ab7223-77b0-46f0-af5f-669b18760a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_config_callback = (\n",
    "    PPOConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(create_env_on_local_worker=True, horizon=100)\\\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\\\n",
    "    .callbacks(callbacks_class=MyCallbacks)\\\n",
    "    .evaluation(evaluation_config={\"callbacks\" : MyCallbacks})\n",
    ")\n",
    "\n",
    "ppo_RandomLakeObsRew = ppo_config_callback.build(env=RandomLakeObsRew)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90980e8e-2ef2-479d-9c55-cbde6fcdebe1",
   "metadata": {},
   "source": [
    "El entrenador de arriba utiliza nuestro nuevo esquema de recompensas, pero también informa/mede la tasa de consecución del objetivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f898830-4600-4ddf-8f05-073f7cb8e842",
   "metadata": {},
   "source": [
    "#### ¿Comparar las recompensas?\n",
    "\n",
    "¡Vamos a probarlo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f118473-544e-43a6-9ead-f742d0ee8fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    ppo_RandomLakeObsRew.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f52dd086-12bb-4482-88a2-30df5240573b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101.90566037735849"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HIDDEN\n",
    "ppo_RandomLakeObsRew.evaluate()[\"evaluation\"][\"episode_reward_mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0021f054-0f77-46c8-ac1c-f6e1f40ff509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04081632653061224"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_RandomLakeObsRew.evaluate()[\"evaluation\"][\"custom_metrics\"][\"goal_reached_mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36e3784-b860-49a2-b3d1-b48a760e756c",
   "metadata": {},
   "source": [
    "- Hmm, ¡estos resultados son terribles!\n",
    "- Antes obteníamos un porcentaje de victorias superior al 70%, y ahora estamos cerca de cero.\n",
    "- ¿Qué ha pasado? 🤔"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398ee5c9-7e5c-4370-912c-b90d8864cac4",
   "metadata": {},
   "source": [
    "#### ¿Qué está optimizando realmente el agente?\n",
    "\n",
    "- El agente está optimizando realmente la _recompensa total descontada_.\n",
    "- _Total_: valora todas las recompensas que recoge, no sólo la recompensa final.\n",
    "- _Descontado_: valora más las recompensas anteriores que las posteriores.\n",
    "- Nuestro agente está maximizando con éxito la recompensa total descontada, pero esto no se corresponde con alcanzar el objetivo.\n",
    "- ¿Pero por qué? La meta da una recompensa mayor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cee4522-4906-4a36-b5e8-714761a11b04",
   "metadata": {},
   "source": [
    "#### Exploración frente a explotación\n",
    "\n",
    "- Un concepto fundamental en la RL es _exploración vs. explotación_\n",
    "- Cuando el agente está aprendiendo la política, puede elegir entre\n",
    "\n",
    "1. Hacer cosas que sabe que son bastante buenas (\"explotar\")\n",
    "2. Probar algo totalmente nuevo y loco, por si acaso (\"explorar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f02fef1-3506-4c10-ac66-cd4150b32d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO \n",
    "# diagram for this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954a891d-f16c-43bb-b538-ce246829d6fb",
   "metadata": {},
   "source": [
    "#### Exploración frente a explotación\n",
    "\n",
    "- Con la antigua estructura de recompensa, el agente obtiene una recompensa de 0 a menos que alcance el objetivo.\n",
    "  - Por tanto, sigue intentando encontrar algo mejor.\n",
    "- Con la nueva estructura de recompensa, el agente obtiene mucha recompensa sólo por caminar.\n",
    "  - No está muy motivado para explorar el entorno.\n",
    "- De hecho, como está maximizando la recompensa **total** descontada, ¡encontrar la meta es algo malo!\n",
    "  - Esto hace que el episodio termine, limitando la recompensa total del agente.\n",
    "  - En realidad, el agente aprende a _evitar_ la meta, especialmente al principio del episodio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83b297e-b4a6-4989-b623-5f53b52562c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Diseñar una mejor estructura de recompensas\n",
    "\n",
    "- En su lugar, intentemos penalizar al agente cuando se meta en un agujero o se salga del borde.\n",
    "- Será más fácil ponerlo en práctica directamente en el \"paso\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c41fd832-6144-41cc-9643-e7f3eb8737d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomLakeObsRew2(RandomLakeObs):\n",
    "    def step(self, action):\n",
    "        # (not shown) existing code gets new_loc, where the player is trying to go\n",
    "        \n",
    "        reward = 0\n",
    "        \n",
    "        if self.is_valid_loc(new_loc):\n",
    "            self.player = new_loc\n",
    "        else:\n",
    "            reward -= 0.1 # small penalty\n",
    "            \n",
    "        if self.holes[self.player]:\n",
    "            reward -= 0.1 # small penalty\n",
    "            \n",
    "        if self.player == self.goal:\n",
    "            reward += 1\n",
    "        \n",
    "        # Return observation/reward/done\n",
    "        return self.observation(), reward, self.done(), {\"player\" : self.player, \"goal\" : self.goal}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5622237f-517f-4a1c-99f0-0e7a0ede3e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "from envs_03 import RandomLakeObsRew2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804b515a-f635-4f48-8e0e-7774b003533a",
   "metadata": {},
   "source": [
    "#### Probando, de nuevo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c20f637-e0f8-4ed5-8133-d49d59d504f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "# redefine ppo_RandomLakeObs to include the new callbacks\n",
    "# so that you can measure the custom metric instead of the reward\n",
    "# they will give the same value but this is better for consistency\n",
    "ppo_RandomLakeObs = ppo_config_callback.build(env=RandomLakeObs)\n",
    "\n",
    "for i in range(8):\n",
    "    ppo_RandomLakeObs.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7459cba1-5771-4d26-b215-4f51df7a7ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_RandomLakeObsRew2 = ppo_config_callback.build(env=RandomLakeObsRew2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dbd1ac25-6d5c-4ed9-a49c-c9564b92f08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    ppo_RandomLakeObsRew2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cdbb4747-9b58-487b-9353-1a1e8a2022c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6853448275862069"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_RandomLakeObs.evaluate()[\"evaluation\"][\"custom_metrics\"][\"goal_reached_mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8e32e1de-b153-4fef-a208-a0d574d547c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.734982332155477"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_RandomLakeObsRew2.evaluate()[\"evaluation\"][\"custom_metrics\"][\"goal_reached_mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8ae49b-73af-43b4-92fa-cd3f608b504d",
   "metadata": {},
   "source": [
    "Parece que, esta vez, los dos métodos tienen un rendimiento mucho más similar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3481f6-4505-48f6-a787-33146ff663af",
   "metadata": {},
   "source": [
    "#### Duración del episodio\n",
    "\n",
    "- Además de la tasa de éxito, podemos calcular otras estadísticas del comportamiento del agente.\n",
    "- Una medida interesante es la duración del episodio.\n",
    "- RLlib la registra por defecto, por lo que podemos acceder a ella fácilmente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e8819cf4-bc66-4ce7-a723-ea6127d4c1cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.585470085470085"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_RandomLakeObs.evaluate()[\"evaluation\"][\"episode_len_mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "879d945f-57e6-409e-8f8a-e0d95e6cfe4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.20216606498195"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_RandomLakeObsRew2.evaluate()[\"evaluation\"][\"episode_len_mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de01c11-01b5-4f86-9f9e-9e2e43a3d62d",
   "metadata": {},
   "source": [
    "Aunque los dos agentes tienen el mismo porcentaje de éxito, el nuevo tiende a tener episodios más cortos.\n",
    "\n",
    "Notas \n",
    "\n",
    "- Esto es bastante interesante porque el agente no puede \"ver\" la diferencia entre los agujeros y las aristas.\n",
    "- Podríamos explorar esto más a fondo añadiendo más métricas personalizadas, por ejemplo, el número de baches en la arista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd02d93-0255-4ae2-aa46-5a9eb5111fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "#### disadvantages - loss of generality\n",
    "\n",
    "#- now only works if goal is at bottom-right\n",
    "#give a few real-world examples here -> important"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf32143e-7c32-49d2-9aed-f1081c5b8c35",
   "metadata": {},
   "source": [
    "## Analogía del aprendizaje supervisado: la conformación de la recompensa\n",
    "<!-- multiple choice -->\n",
    "\n",
    "Antes hemos hecho una analogía entre la codificación de observaciones en la LR y el preprocesamiento de características en el aprendizaje supervisado. ¿Qué aspecto del aprendizaje supervisado es la mejor analogía con la conformación de la recompensa en la RL?\n",
    "\n",
    "- [ ] Ingeniería de rasgos \n",
    "- [ ] Selección de modelos | No exactamente. Pero, como veremos, ¡la selección de modelos también tiene cabida en la LR!\n",
    "- [Ajuste de los hiperparámetros: No del todo. Pero, como veremos, ¡el ajuste de hiperparámetros también tiene cabida en la RL!\n",
    "- [x] Selección de una función de pérdida | Cambiar la función de pérdida cambia el \"mejor\" modelo, igual que cambiar las recompensas cambia la \"mejor\" política."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c458b5c7-bc0a-4084-8572-d35bf27ddc60",
   "metadata": {},
   "source": [
    "## Premiar cada paso: pequeñas recompensas negativas\n",
    "<!-- multiple choice -->\n",
    "\n",
    "En entornos de RL como Random Lake, en los que el agente debe alcanzar un objetivo concreto, imagina que asignamos una pequeña recompensa negativa por _cada_ paso que da el agente. ¿Cómo afectaría esto, en general/típicamente, a la cantidad de tiempo que el agente emplea hasta alcanzar el objetivo?\n",
    "\n",
    "- [x] El agente intentará llegar a la meta en el menor número de pasos posible.\n",
    "- [ ] El agente intentará llegar a la meta en el mayor número de pasos posible. | Si estamos penalizando cada paso, dar más pasos supondrá una menor recompensa.\n",
    "- [ ] No hay cambios. | Si estamos penalizando cada paso, dar más pasos dará lugar a una menor recompensa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96265e6-b392-4923-b694-088fb282efb0",
   "metadata": {},
   "source": [
    "## Exploración vs. Explotación\n",
    "<!-- multiple choice -->\n",
    "\n",
    "¿Cuál de las siguientes es una afirmación correcta sobre el compromiso de exploración-explotación en la LR?\n",
    "\n",
    "- [ ] Si un sólo explora, nunca encontrará una buena política. | De hecho, encontrará buenas políticas, sólo que de forma EXTREMADAMENTE lenta.\n",
    "- [x] Si un agente sólo explota, nunca encontrará una buena política. | Puede que siga intentando lo mismo una y otra vez.\n",
    "- [ ] Los agentes siempre encuentran buenas políticas incluso sin exploración/explotación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93d7454-eaff-4c79-8398-19cb541b4b37",
   "metadata": {},
   "source": [
    "## Consecuencias imprevistas\n",
    "<!-- coding exercise -->\n",
    "\n",
    "En este ejercicio, probarás una mala idea: asignar una gran recompensa negativa cada vez que el agente dé un paso. Utilizaremos -1 por paso. El agente sigue recibiendo una recompensa de +1 por alcanzar la meta. Implementa esta recompensa, entrena al agente y observa la duración media de los episodios que imprime el código. Compáralo con la duración media de los episodios de un agente que sólo actúa al azar. Luego, responde a la pregunta de opción múltiple sobre el comportamiento del agente. ¿Qué crees que ocurre aquí? \n",
    "\n",
    "(Para tu información: como se ha comentado anteriormente, este tipo de cambio en un entorno también se puede conseguir con envoltorios de gimnasia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f8434a-7575-4bfb-af7b-9858f580ae7b",
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "from utils_03 import lake_default_config\n",
    "from envs_03 import RandomLakeObs\n",
    "\n",
    "class RandomLakeBadIdea(RandomLakeObs):\n",
    "    def reward(self):\n",
    "        old_reward = int(self.player == self.goal) \n",
    "        return ____\n",
    "    \n",
    "ppo = lake_default_config.build(env=____)\n",
    "\n",
    "for i in range(8):\n",
    "    ppo.train()\n",
    "    \n",
    "print(\"Average episode length for trained agent: %.1f\" % \n",
    "      ppo.evaluate()[\"evaluation\"][____])\n",
    "\n",
    "random_agent_config = (\n",
    "    lake_default_config\\\n",
    "    .exploration(exploration_config={\"type\": \"Random\"})\\\n",
    "    .evaluation(evaluation_config={\"explore\" : True})\n",
    ")\n",
    "random_agent = random_agent_config.build(env=RandomLakeBadIdea)\n",
    "\n",
    "print(\"Average episode length for random agent: %.1f\" % \n",
    "      random_agent.evaluate()[\"evaluation\"][____])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "265575eb-94ef-4899-b7c0-c00c70c155e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "Average episode length for trained agent: 4.4\n",
      "Average episode length for random agent: 12.1\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "from utils_03 import lake_default_config\n",
    "from envs_03 import RandomLakeObs\n",
    "\n",
    "class RandomLakeBadIdea(RandomLakeObs):\n",
    "    def reward(self):\n",
    "        old_reward = int(self.player == self.goal) \n",
    "        return old_reward - 1\n",
    "\n",
    "ppo = lake_default_config.build(env=RandomLakeBadIdea)\n",
    "\n",
    "\n",
    "for i in range(8):\n",
    "    ppo.train()\n",
    "    \n",
    "print(\"Average episode length for trained agent: %.1f\" % \n",
    "      ppo.evaluate()[\"evaluation\"][\"episode_len_mean\"])\n",
    "\n",
    "random_agent_config = (\n",
    "    lake_default_config\\\n",
    "    .exploration(exploration_config={\"type\": \"Random\"})\\\n",
    "    .evaluation(evaluation_config={\"explore\" : True})\n",
    ")\n",
    "random_agent = random_agent_config.build(env=RandomLakeBadIdea)\n",
    "\n",
    "print(\"Average episode length for random agent: %.1f\" % \n",
    "      random_agent.evaluate()[\"evaluation\"][\"episode_len_mean\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dbab96-1590-409b-b684-d6a05c880c1c",
   "metadata": {},
   "source": [
    "#### Comportamiento del agente\n",
    "\n",
    "Cuando se entrena en un entorno con una gran recompensa negativa a cada paso, ¿qué crees que hace este agente, que es indeseable?\n",
    "\n",
    "- [ ] El agente se queda quieto porque se le disuade de moverse. | ¡Inténtalo de nuevo!\n",
    "- [ ] El agente no está interesado en alcanzar el objetivo porque la recompensa es comparativamente pequeña. | ¡Inténtalo de nuevo!\n",
    "- [x] El agente aprende a saltar al lago tan rápido como puede, para evitar la recompensa negativa de moverse. | ¡Caramba! 🥶\n",
    "- [ ] El agente llega a la meta enseguida. | ¡Sin embargo, sería deseable!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ray200]",
   "language": "python",
   "name": "conda-env-ray200-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
