{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e23df55-837b-4ec9-86ad-3b60c6a7f00d",
   "metadata": {},
   "source": [
    "## Codificaci√≥n de las recompensas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28d9b458-b46e-48de-ab18-bf884c1eab85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import ray\n",
    "import logging\n",
    "ray.init(log_to_driver=False, ignore_reinit_error=True, logging_level=logging.ERROR); # logging.FATAL\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c8c796-3785-44d1-8aea-78872152b0cb",
   "metadata": {},
   "source": [
    "#### Recompensas de codificaci√≥n\n",
    "\n",
    "- Ya hemos hablado de la importancia de codificar las observaciones.\n",
    "- Tambi√©n podemos tener alguna opci√≥n en el espacio de acci√≥n, aunque aqu√≠ (y a menudo) es relativamente clara/fija.\n",
    "- Pero, ¬øqu√© pasa con las recompensas? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e790dc-5146-4b03-a77a-24efd69ffabd",
   "metadata": {},
   "source": [
    "#### Configuraci√≥n actual\n",
    "\n",
    "- Actualmente, obtenemos una recompensa de +1 por alcanzar el objetivo \n",
    "- Esto es parte de lo que hace que RL sea tan dif√≠cil (e impresionante):\n",
    "  - Queremos aprender sobre las acciones aunque no sepamos de inmediato si la acci√≥n fue \"buena\" \n",
    "  - Contrasta esto con el aprendizaje supervisado, donde cada predicci√≥n que hacemos sobre los datos de entrenamiento puede compararse inmediatamente con el valor objetivo conocido.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c10944be-9746-44d6-bb2b-66843ea7e4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: perhaps this next slide can be moved to Module 1, since it's very general?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff6ac9f-6815-4910-b3ce-aa7e0866101c",
   "metadata": {},
   "source": [
    "#### Los agentes no pueden ser simplemente codiciosos\n",
    "\n",
    "- ¬øPueden los agentes aprender simplemente a buscar la mejor recompensa inmediata?\n",
    "- No. Por ejemplo, en un sistema de recomendaci√≥n de v√≠deos, mostrar al usuario otro v√≠deo divertido de gatos puede hacer que haga clic (alta recompensa inmediata) pero provocar una p√©rdida de inter√©s en el servicio a largo plazo (baja recompensa a largo plazo).\n",
    "- Nuestro Lago Helado es otro ejemplo del problema: a veces no hay ninguna recompensa inmediata de la que aprender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c44cea8c-a28e-4d7c-a0c8-91c1c3f8fc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: perhaps this next section on \"Learned action probabilities\" could be moved much earlier, even as early as Module 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462e6340-cb1f-4eda-b642-d21505d18b52",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Probabilidades de acci√≥n aprendidas\n",
    "\n",
    "- RLlib nos permite mirar dentro del modelo la probabilidad de cada acci√≥n dada una observaci√≥n (es decir, la pol√≠tica aprendida).\n",
    "- Carguemos el modelo entrenado con nuestras observaciones codificadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16d9d371-0c26-46a0-90c5-6f26e25880f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from envs_03 import RandomLakeObs\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "ppo_config = (\n",
    "    PPOConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(create_env_on_local_worker=True, horizon=100)\\\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\n",
    ")\n",
    "ppo_RandomLakeObs = ppo_config.build(env=RandomLakeObs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1420e1fd-22e9-474c-aad6-928428976b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # HIDDEN\n",
    "\n",
    "# for i in range(16):\n",
    "#     ppo_RandomLakeObs.train()\n",
    "    \n",
    "# print(ppo_RandomLakeObs.evaluate()[\"evaluation\"][\"episode_reward_mean\"])\n",
    "\n",
    "# ppo_RandomLakeObs.save(\"models/RandomLakeObs-Ray2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a71119d3-e3c9-427c-a697-fe733fc10ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_RandomLakeObs.restore(\"models/RandomLakeObs-Ray2/checkpoint_000016\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fec9d02-7e22-44ee-b171-2d9bf86b8681",
   "metadata": {},
   "source": [
    "#### Probabilidades de acci√≥n aprendidas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66648b5d-abd8-45ef-b592-e56bdee1c9c9",
   "metadata": {},
   "source": [
    "Utilizaremos la funci√≥n `query_Policy` del m√≥dulo 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84b56415-177b-414f-8985-1118cabec0b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00902206, 0.5078786 , 0.47434822, 0.00875122], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils_03 import query_policy\n",
    "query_policy(ppo_RandomLakeObs, RandomLakeObs(), [0,0,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0256b6-3174-4719-931f-65342f5ae4a7",
   "metadata": {},
   "source": [
    "- Recuerda la ordenaci√≥n (izquierda, abajo, derecha, arriba).\n",
    "- Cuando la observaci√≥n es `[0 0 0]` (sin agujeros ni aristas a la vista), el agente prefiere ir hacia abajo y hacia la derecha."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99a889a-37bd-4a7c-bf6b-c9eb297a1520",
   "metadata": {},
   "source": [
    "¬øY si hay un agujero debajo de ti? Podemos introducir una observaci√≥n diferente en la pol√≠tica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc66f33e-8b23-4bcc-9f12-04b47d18aea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01965651, 0.00299437, 0.9645823 , 0.01276694], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_policy(ppo_RandomLakeObs, RandomLakeObs(), [0,1,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6560c87a-c07e-4aec-a172-915cad2b376c",
   "metadata": {},
   "source": [
    "- ¬°Ahora es muy poco probable que el agente baje, y muy probable que vaya a la derecha!\n",
    "- De nuevo, todo esto se aprendi√≥ por ensayo y error, con una recompensa obtenida s√≥lo cuando se alcanzaba el objetivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffae4393-3058-40ce-821a-fe1e487cf4d4",
   "metadata": {},
   "source": [
    "#### Recompensas de Random Lake\n",
    "\n",
    "- En el ejemplo de Random Lake, ¬øno se puede facilitar la vida al agente dando recompensas inmediatas?\n",
    "\n",
    "Este es el c√≥digo de recompensa actual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bb56001-259b-4b88-8bdd-29746e208042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward(self):\n",
    "    return int(self.player == self.goal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1ee605-1e87-4080-b8ee-274816dd67fd",
   "metadata": {},
   "source": [
    "- El agente tiene que aprender, por ensayo y error a lo largo de _episodios enteros_, que moverse hacia abajo y hacia la derecha es generalmente algo bueno "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0cf72e-d732-4039-90cc-22a17360943f",
   "metadata": {},
   "source": [
    "#### Redefinir las recompensas\n",
    "\n",
    "- Intentemos, en cambio, dar una recompensa _en cada paso, que sea mayor a medida que el agente se acerca al objetivo_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9a0466b-44fd-43ed-a3c0-d93334748213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from envs_03 import RandomLakeObs\n",
    "\n",
    "class RandomLakeObsRew(RandomLakeObs):\n",
    "    def reward(self):\n",
    "        return 6-(abs(self.player[0]-self.goal[0]) + abs(self.player[1]-self.goal[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce59aa5-1f90-457f-a3f9-c970cd8a7b7b",
   "metadata": {},
   "source": [
    "- El m√©todo anterior utiliza la [Distancia Manhattan](https://en.wikipedia.org/wiki/Taxicab_geometry) entre el jugador y la meta como recompensa \n",
    "- Cuando el agente llega a la meta, se obtiene la recompensa m√°xima de 6.\n",
    "- Cuando el agente se aleja m√°s de la meta, se obtiene la recompensa m√≠nima de 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75f7b77-8962-43fe-a82b-647e4f2aee69",
   "metadata": {},
   "source": [
    "#### Redefinir las recompensas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcba334f-f2a3-4cb0-8b8a-f5dd69a3346f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßëüßäüßäüßä\n",
      "üßäüßäüßäüßä\n",
      "üï≥üßäüï≥üßä\n",
      "üßäüßäüßä‚õ≥Ô∏è\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = RandomLakeObsRew()\n",
    "env.reset()\n",
    "env.render()\n",
    "env.reward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5208f2-c7e9-4f1e-9ed7-e91daec89d8e",
   "metadata": {},
   "source": [
    "‚¨ÜÔ∏è la recompensa es 0\n",
    "\n",
    "‚¨áÔ∏è la recompensa es 1 porque nos acercamos al objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "088715f0-3f06-4231-a149-401b6518dcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßäüßäüßäüßä\n",
      "üßëüßäüßäüßä\n",
      "üï≥üßäüï≥üßä\n",
      "üßäüßäüßä‚õ≥Ô∏è\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, rew, done, _ = env.step(1)\n",
    "env.render()\n",
    "rew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e607ab59-e697-44b6-99a6-91a7a3a3d2b2",
   "metadata": {},
   "source": [
    "#### Redefinir las recompensas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95ab62b7-e2b5-4f6e-8f81-9381b605a9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßäüßäüßäüßä\n",
      "üßäüßäüßäüßä\n",
      "üï≥üßäüï≥üßë\n",
      "üßäüßäüßä‚õ≥Ô∏è\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, rew, done, _ = env.step(2)\n",
    "obs, rew, done, _ = env.step(2)\n",
    "obs, rew, done, _ = env.step(2)\n",
    "obs, rew, done, _ = env.step(1)\n",
    "env.render()\n",
    "rew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1a183d-af4c-4c9d-a6d3-4bc1a7bfecca",
   "metadata": {},
   "source": [
    "Ahora, la recompensa es 5. Despu√©s, ser√° 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "904e40cb-55b6-4b45-b063-5b335123e071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßäüßäüßäüßä\n",
      "üßäüßäüßäüßä\n",
      "üï≥üßäüï≥üßä\n",
      "üßäüßäüßäüßë\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, rew, done, _ = env.step(1)\n",
    "env.render()\n",
    "rew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ac0aec-ddc4-4d44-833e-c2b2e253120f",
   "metadata": {},
   "source": [
    "#### Comparar las recompensas\n",
    "\n",
    "- As√≠ pues, tenemos dos posibles funciones de recompensa. ¬øCu√°l funciona mejor? \n",
    "- Recuerda que la √∫ltima vez, despu√©s de entrenar durante 8 iteraciones, fuimos capaces de alcanzar el objetivo alrededor del 70% de las veces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5dbcf34-37e8-4025-bcd2-7a308d77046f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8166666666666667"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_RandomLakeObs.evaluate()['evaluation']['episode_reward_mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c75b369-6e96-4192-b7cd-4216f354d901",
   "metadata": {},
   "source": [
    "#### Comparando las recompensas\n",
    "\n",
    "¬°Vamos a entrenar con la nueva funci√≥n de recompensa!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01c1afa7-4452-43e4-ad2d-e3dc33934fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_RandomLakeObsRew = ppo_config.build(env=RandomLakeObsRew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7478ee6-28d8-4e44-932f-1b4f47e6b911",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    ppo_RandomLakeObsRew.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "257cef04-752e-4fe1-9eb6-3e225ea1ba62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101.90566037735849"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_RandomLakeObsRew.evaluate()['evaluation']['episode_reward_mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bccc6f4-1e9a-4abd-af7a-08b4c6103e4b",
   "metadata": {},
   "source": [
    "Un momento, ¬øqu√© est√° pasando aqu√≠?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874a6979-1882-4bb4-a5cf-a83282ed15f3",
   "metadata": {},
   "source": [
    "#### ¬øComparar las recompensas?\n",
    "\n",
    "- Intentamos mejorar nuestro sistema de RL dando forma a la funci√≥n de recompensa.\n",
    "- Esto (presumiblemente) afect√≥ al entrenamiento, pero tambi√©n a nuestra evaluaci√≥n.\n",
    "- En el aprendizaje supervisado, esto es como cambiar la m√©trica de puntuaci√≥n del error cuadrado al error absoluto.\n",
    "- Si el antiguo sistema obtuvo un error medio al cuadrado de 20.000 y el nuevo sistema obtuvo un error medio absoluto de 40, ¬øcu√°l es mejor?\n",
    "- ¬°Estamos comparando manzanas y naranjas!\n",
    "- Queremos comparar ambos modelos con la misma m√©trica, por ejemplo la m√©trica original \n",
    "- En este caso, queremos ver la frecuencia con la que el agente alcanza el objetivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf05802-842d-4764-b408-72559e7bb8cf",
   "metadata": {},
   "source": [
    "#### ¬øComparar las recompensas?\n",
    "\n",
    "- El c√≥digo aqu√≠ es un poco m√°s avanzado.\n",
    "- Se incluye para completarlo, pero no entraremos en detalles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "261074ea-c6db-4e1b-a35f-a647e8cf2776",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.agents.callbacks import DefaultCallbacks\n",
    "\n",
    "class MyCallbacks(DefaultCallbacks):\n",
    "    def on_episode_end(self, *, worker, base_env, policies, episode, env_index, **kwargs):\n",
    "        info = episode.last_info_for()\n",
    "        episode.custom_metrics[\"goal_reached\"] = info[\"player\"] == info[\"goal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3ab7223-77b0-46f0-af5f-669b18760a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_config_callback = (\n",
    "    PPOConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(create_env_on_local_worker=True, horizon=100)\\\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\\\n",
    "    .callbacks(callbacks_class=MyCallbacks)\\\n",
    "    .evaluation(evaluation_config={\"callbacks\" : MyCallbacks})\n",
    ")\n",
    "\n",
    "ppo_RandomLakeObsRew = ppo_config_callback.build(env=RandomLakeObsRew)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90980e8e-2ef2-479d-9c55-cbde6fcdebe1",
   "metadata": {},
   "source": [
    "El entrenador de arriba utiliza nuestro nuevo esquema de recompensas, pero tambi√©n informa/mede la tasa de consecuci√≥n del objetivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f898830-4600-4ddf-8f05-073f7cb8e842",
   "metadata": {},
   "source": [
    "#### ¬øComparar las recompensas?\n",
    "\n",
    "¬°Vamos a probarlo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f118473-544e-43a6-9ead-f742d0ee8fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    ppo_RandomLakeObsRew.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f52dd086-12bb-4482-88a2-30df5240573b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101.90566037735849"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HIDDEN\n",
    "ppo_RandomLakeObsRew.evaluate()[\"evaluation\"][\"episode_reward_mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0021f054-0f77-46c8-ac1c-f6e1f40ff509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04081632653061224"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_RandomLakeObsRew.evaluate()[\"evaluation\"][\"custom_metrics\"][\"goal_reached_mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36e3784-b860-49a2-b3d1-b48a760e756c",
   "metadata": {},
   "source": [
    "- Hmm, ¬°estos resultados son terribles!\n",
    "- Antes obten√≠amos un porcentaje de victorias superior al 70%, y ahora estamos cerca de cero.\n",
    "- ¬øQu√© ha pasado? ü§î"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398ee5c9-7e5c-4370-912c-b90d8864cac4",
   "metadata": {},
   "source": [
    "#### ¬øQu√© est√° optimizando realmente el agente?\n",
    "\n",
    "- El agente est√° optimizando realmente la _recompensa total descontada_.\n",
    "- _Total_: valora todas las recompensas que recoge, no s√≥lo la recompensa final.\n",
    "- _Descontado_: valora m√°s las recompensas anteriores que las posteriores.\n",
    "- Nuestro agente est√° maximizando con √©xito la recompensa total descontada, pero esto no se corresponde con alcanzar el objetivo.\n",
    "- ¬øPero por qu√©? La meta da una recompensa mayor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cee4522-4906-4a36-b5e8-714761a11b04",
   "metadata": {},
   "source": [
    "#### Exploraci√≥n frente a explotaci√≥n\n",
    "\n",
    "- Un concepto fundamental en la RL es _exploraci√≥n vs. explotaci√≥n_\n",
    "- Cuando el agente est√° aprendiendo la pol√≠tica, puede elegir entre\n",
    "\n",
    "1. Hacer cosas que sabe que son bastante buenas (\"explotar\")\n",
    "2. Probar algo totalmente nuevo y loco, por si acaso (\"explorar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f02fef1-3506-4c10-ac66-cd4150b32d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO \n",
    "# diagram for this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954a891d-f16c-43bb-b538-ce246829d6fb",
   "metadata": {},
   "source": [
    "#### Exploraci√≥n frente a explotaci√≥n\n",
    "\n",
    "- Con la antigua estructura de recompensa, el agente obtiene una recompensa de 0 a menos que alcance el objetivo.\n",
    "  - Por tanto, sigue intentando encontrar algo mejor.\n",
    "- Con la nueva estructura de recompensa, el agente obtiene mucha recompensa s√≥lo por caminar.\n",
    "  - No est√° muy motivado para explorar el entorno.\n",
    "- De hecho, como est√° maximizando la recompensa **total** descontada, ¬°encontrar la meta es algo malo!\n",
    "  - Esto hace que el episodio termine, limitando la recompensa total del agente.\n",
    "  - En realidad, el agente aprende a _evitar_ la meta, especialmente al principio del episodio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83b297e-b4a6-4989-b623-5f53b52562c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Dise√±ar una mejor estructura de recompensas\n",
    "\n",
    "- En su lugar, intentemos penalizar al agente cuando se meta en un agujero o se salga del borde.\n",
    "- Ser√° m√°s f√°cil ponerlo en pr√°ctica directamente en el \"paso\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c41fd832-6144-41cc-9643-e7f3eb8737d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomLakeObsRew2(RandomLakeObs):\n",
    "    def step(self, action):\n",
    "        # (not shown) existing code gets new_loc, where the player is trying to go\n",
    "        \n",
    "        reward = 0\n",
    "        \n",
    "        if self.is_valid_loc(new_loc):\n",
    "            self.player = new_loc\n",
    "        else:\n",
    "            reward -= 0.1 # small penalty\n",
    "            \n",
    "        if self.holes[self.player]:\n",
    "            reward -= 0.1 # small penalty\n",
    "            \n",
    "        if self.player == self.goal:\n",
    "            reward += 1\n",
    "        \n",
    "        # Return observation/reward/done\n",
    "        return self.observation(), reward, self.done(), {\"player\" : self.player, \"goal\" : self.goal}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5622237f-517f-4a1c-99f0-0e7a0ede3e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "from envs_03 import RandomLakeObsRew2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804b515a-f635-4f48-8e0e-7774b003533a",
   "metadata": {},
   "source": [
    "#### Probando, de nuevo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c20f637-e0f8-4ed5-8133-d49d59d504f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "# redefine ppo_RandomLakeObs to include the new callbacks\n",
    "# so that you can measure the custom metric instead of the reward\n",
    "# they will give the same value but this is better for consistency\n",
    "ppo_RandomLakeObs = ppo_config_callback.build(env=RandomLakeObs)\n",
    "\n",
    "for i in range(8):\n",
    "    ppo_RandomLakeObs.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7459cba1-5771-4d26-b215-4f51df7a7ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_RandomLakeObsRew2 = ppo_config_callback.build(env=RandomLakeObsRew2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dbd1ac25-6d5c-4ed9-a49c-c9564b92f08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    ppo_RandomLakeObsRew2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cdbb4747-9b58-487b-9353-1a1e8a2022c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6853448275862069"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_RandomLakeObs.evaluate()[\"evaluation\"][\"custom_metrics\"][\"goal_reached_mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8e32e1de-b153-4fef-a208-a0d574d547c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.734982332155477"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_RandomLakeObsRew2.evaluate()[\"evaluation\"][\"custom_metrics\"][\"goal_reached_mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8ae49b-73af-43b4-92fa-cd3f608b504d",
   "metadata": {},
   "source": [
    "Parece que, esta vez, los dos m√©todos tienen un rendimiento mucho m√°s similar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3481f6-4505-48f6-a787-33146ff663af",
   "metadata": {},
   "source": [
    "#### Duraci√≥n del episodio\n",
    "\n",
    "- Adem√°s de la tasa de √©xito, podemos calcular otras estad√≠sticas del comportamiento del agente.\n",
    "- Una medida interesante es la duraci√≥n del episodio.\n",
    "- RLlib la registra por defecto, por lo que podemos acceder a ella f√°cilmente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e8819cf4-bc66-4ce7-a723-ea6127d4c1cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.585470085470085"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_RandomLakeObs.evaluate()[\"evaluation\"][\"episode_len_mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "879d945f-57e6-409e-8f8a-e0d95e6cfe4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.20216606498195"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_RandomLakeObsRew2.evaluate()[\"evaluation\"][\"episode_len_mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de01c11-01b5-4f86-9f9e-9e2e43a3d62d",
   "metadata": {},
   "source": [
    "Aunque los dos agentes tienen el mismo porcentaje de √©xito, el nuevo tiende a tener episodios m√°s cortos.\n",
    "\n",
    "Notas \n",
    "\n",
    "- Esto es bastante interesante porque el agente no puede \"ver\" la diferencia entre los agujeros y las aristas.\n",
    "- Podr√≠amos explorar esto m√°s a fondo a√±adiendo m√°s m√©tricas personalizadas, por ejemplo, el n√∫mero de baches en la arista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd02d93-0255-4ae2-aa46-5a9eb5111fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "#### disadvantages - loss of generality\n",
    "\n",
    "#- now only works if goal is at bottom-right\n",
    "#give a few real-world examples here -> important"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf32143e-7c32-49d2-9aed-f1081c5b8c35",
   "metadata": {},
   "source": [
    "## Analog√≠a del aprendizaje supervisado: la conformaci√≥n de la recompensa\n",
    "<!-- multiple choice -->\n",
    "\n",
    "Antes hemos hecho una analog√≠a entre la codificaci√≥n de observaciones en la LR y el preprocesamiento de caracter√≠sticas en el aprendizaje supervisado. ¬øQu√© aspecto del aprendizaje supervisado es la mejor analog√≠a con la conformaci√≥n de la recompensa en la RL?\n",
    "\n",
    "- [ ] Ingenier√≠a de rasgos \n",
    "- [ ] Selecci√≥n de modelos | No exactamente. Pero, como veremos, ¬°la selecci√≥n de modelos tambi√©n tiene cabida en la LR!\n",
    "- [Ajuste de los hiperpar√°metros: No del todo. Pero, como veremos, ¬°el ajuste de hiperpar√°metros tambi√©n tiene cabida en la RL!\n",
    "- [x] Selecci√≥n de una funci√≥n de p√©rdida | Cambiar la funci√≥n de p√©rdida cambia el \"mejor\" modelo, igual que cambiar las recompensas cambia la \"mejor\" pol√≠tica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c458b5c7-bc0a-4084-8572-d35bf27ddc60",
   "metadata": {},
   "source": [
    "## Premiar cada paso: peque√±as recompensas negativas\n",
    "<!-- multiple choice -->\n",
    "\n",
    "En entornos de RL como Random Lake, en los que el agente debe alcanzar un objetivo concreto, imagina que asignamos una peque√±a recompensa negativa por _cada_ paso que da el agente. ¬øC√≥mo afectar√≠a esto, en general/t√≠picamente, a la cantidad de tiempo que el agente emplea hasta alcanzar el objetivo?\n",
    "\n",
    "- [x] El agente intentar√° llegar a la meta en el menor n√∫mero de pasos posible.\n",
    "- [ ] El agente intentar√° llegar a la meta en el mayor n√∫mero de pasos posible. | Si estamos penalizando cada paso, dar m√°s pasos supondr√° una menor recompensa.\n",
    "- [ ] No hay cambios. | Si estamos penalizando cada paso, dar m√°s pasos dar√° lugar a una menor recompensa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96265e6-b392-4923-b694-088fb282efb0",
   "metadata": {},
   "source": [
    "## Exploraci√≥n vs. Explotaci√≥n\n",
    "<!-- multiple choice -->\n",
    "\n",
    "¬øCu√°l de las siguientes es una afirmaci√≥n correcta sobre el compromiso de exploraci√≥n-explotaci√≥n en la LR?\n",
    "\n",
    "- [ ] Si un s√≥lo explora, nunca encontrar√° una buena pol√≠tica. | De hecho, encontrar√° buenas pol√≠ticas, s√≥lo que de forma EXTREMADAMENTE lenta.\n",
    "- [x] Si un agente s√≥lo explota, nunca encontrar√° una buena pol√≠tica. | Puede que siga intentando lo mismo una y otra vez.\n",
    "- [ ] Los agentes siempre encuentran buenas pol√≠ticas incluso sin exploraci√≥n/explotaci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93d7454-eaff-4c79-8398-19cb541b4b37",
   "metadata": {},
   "source": [
    "## Consecuencias imprevistas\n",
    "<!-- coding exercise -->\n",
    "\n",
    "En este ejercicio, probar√°s una mala idea: asignar una gran recompensa negativa cada vez que el agente d√© un paso. Utilizaremos -1 por paso. El agente sigue recibiendo una recompensa de +1 por alcanzar la meta. Implementa esta recompensa, entrena al agente y observa la duraci√≥n media de los episodios que imprime el c√≥digo. Comp√°ralo con la duraci√≥n media de los episodios de un agente que s√≥lo act√∫a al azar. Luego, responde a la pregunta de opci√≥n m√∫ltiple sobre el comportamiento del agente. ¬øQu√© crees que ocurre aqu√≠? \n",
    "\n",
    "(Para tu informaci√≥n: como se ha comentado anteriormente, este tipo de cambio en un entorno tambi√©n se puede conseguir con envoltorios de gimnasia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f8434a-7575-4bfb-af7b-9858f580ae7b",
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "from utils_03 import lake_default_config\n",
    "from envs_03 import RandomLakeObs\n",
    "\n",
    "class RandomLakeBadIdea(RandomLakeObs):\n",
    "    def reward(self):\n",
    "        old_reward = int(self.player == self.goal) \n",
    "        return ____\n",
    "    \n",
    "ppo = lake_default_config.build(env=____)\n",
    "\n",
    "for i in range(8):\n",
    "    ppo.train()\n",
    "    \n",
    "print(\"Average episode length for trained agent: %.1f\" % \n",
    "      ppo.evaluate()[\"evaluation\"][____])\n",
    "\n",
    "random_agent_config = (\n",
    "    lake_default_config\\\n",
    "    .exploration(exploration_config={\"type\": \"Random\"})\\\n",
    "    .evaluation(evaluation_config={\"explore\" : True})\n",
    ")\n",
    "random_agent = random_agent_config.build(env=RandomLakeBadIdea)\n",
    "\n",
    "print(\"Average episode length for random agent: %.1f\" % \n",
    "      random_agent.evaluate()[\"evaluation\"][____])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "265575eb-94ef-4899-b7c0-c00c70c155e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "Average episode length for trained agent: 4.4\n",
      "Average episode length for random agent: 12.1\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "from utils_03 import lake_default_config\n",
    "from envs_03 import RandomLakeObs\n",
    "\n",
    "class RandomLakeBadIdea(RandomLakeObs):\n",
    "    def reward(self):\n",
    "        old_reward = int(self.player == self.goal) \n",
    "        return old_reward - 1\n",
    "\n",
    "ppo = lake_default_config.build(env=RandomLakeBadIdea)\n",
    "\n",
    "\n",
    "for i in range(8):\n",
    "    ppo.train()\n",
    "    \n",
    "print(\"Average episode length for trained agent: %.1f\" % \n",
    "      ppo.evaluate()[\"evaluation\"][\"episode_len_mean\"])\n",
    "\n",
    "random_agent_config = (\n",
    "    lake_default_config\\\n",
    "    .exploration(exploration_config={\"type\": \"Random\"})\\\n",
    "    .evaluation(evaluation_config={\"explore\" : True})\n",
    ")\n",
    "random_agent = random_agent_config.build(env=RandomLakeBadIdea)\n",
    "\n",
    "print(\"Average episode length for random agent: %.1f\" % \n",
    "      random_agent.evaluate()[\"evaluation\"][\"episode_len_mean\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dbab96-1590-409b-b684-d6a05c880c1c",
   "metadata": {},
   "source": [
    "#### Comportamiento del agente\n",
    "\n",
    "Cuando se entrena en un entorno con una gran recompensa negativa a cada paso, ¬øqu√© crees que hace este agente, que es indeseable?\n",
    "\n",
    "- [ ] El agente se queda quieto porque se le disuade de moverse. | ¬°Int√©ntalo de nuevo!\n",
    "- [ ] El agente no est√° interesado en alcanzar el objetivo porque la recompensa es comparativamente peque√±a. | ¬°Int√©ntalo de nuevo!\n",
    "- [x] El agente aprende a saltar al lago tan r√°pido como puede, para evitar la recompensa negativa de moverse. | ¬°Caramba! ü•∂\n",
    "- [ ] El agente llega a la meta enseguida. | ¬°Sin embargo, ser√≠a deseable!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ray200]",
   "language": "python",
   "name": "conda-env-ray200-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
