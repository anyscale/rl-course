{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e23df55-837b-4ec9-86ad-3b60c6a7f00d",
   "metadata": {},
   "source": [
    "## Sintaxis de los entornos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2e57cc-3491-4489-af86-6052b1efcc1b",
   "metadata": {},
   "source": [
    "#### Motivaci√≥n\n",
    "\n",
    "- Hasta ahora hemos utilizado entornos predefinidos como Frozen Like y Google RecSim.\n",
    "- Para utilizar RL en nuestro propio problema, no podemos usar ninguno de estos entornos.\n",
    "- Tendremos que definir nuestro propio entorno con Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0797f5a9-c856-49f2-9a3c-419d816b6ec1",
   "metadata": {},
   "source": [
    "#### Revisi√≥n del Lago Congelado\n",
    "\n",
    "- Recuerda el entorno del Lago Helado, del M√≥dulo 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69171cee-dc7b-433d-85c5-d0d40486a43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"FrozenLake-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a41dffc-8451-4d7e-a40a-23fd1315ffba",
   "metadata": {},
   "source": [
    "#### Revisi√≥n del Lago Congelado\n",
    "\n",
    "- El Gimnasio OpenAI es de c√≥digo abierto, as√≠ que podr√≠amos consultar el [c√≥digo fuente de Frozen Lake](https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py).\n",
    "- Sin embargo, es complicado y contiene mucho m√°s de lo que necesitamos.\n",
    "- Vamos a crear nuestro propio entorno llamado Frozen Pond con los componentes b√°sicos de Frozen Lake."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735eb606-b1dd-4dba-be55-d6105cc47536",
   "metadata": {},
   "source": [
    "#### Componentes de un Env\n",
    "\n",
    "Decisiones conceptuales:\n",
    "\n",
    "- Espacio de observaci√≥n\n",
    "- Espacio de acci√≥n\n",
    "\n",
    "En Python tendremos que implementar, al menos\n",
    "\n",
    "- el constructor\n",
    "- `reset()`\n",
    "- `paso()`\n",
    "\n",
    "En la pr√°ctica, tambi√©n podemos querer otros m√©todos, como `render()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9cf2ed-dc7e-4bbd-a5f8-9b78b8a38710",
   "metadata": {},
   "source": [
    "#### Decisiones conceptuales\n",
    "\n",
    "En este caso, como estamos imitando el Lago Helado, el espacio de observaci√≥n y el espacio de acci√≥n ya est√°n decididos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34c0aa1d-8ab0-462a-8bdd-8ba12484a5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space = gym.spaces.Discrete(16)\n",
    "action_space = gym.spaces.Discrete(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef0fa48-25fb-4360-a1d8-54ca675dc786",
   "metadata": {},
   "source": [
    "M√°s adelante en este curso profundizaremos en estas decisiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c88c68-7f74-490e-beda-9e199d0c3822",
   "metadata": {},
   "source": [
    "#### Codificarla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db413de7-7df3-4d77-9ae8-5c8e95a39068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "class FrozenPond(gym.Env):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b3bbec-72c8-48dd-bdc4-6f537dacda5c",
   "metadata": {},
   "source": [
    "- F√≠jate en que empezamos subclasificando `gimnasio.Env`.\n",
    "- Opcional: Puedes leer sobre los objetos, la herencia y las subclases.\n",
    "- La l√≠nea de golpe: Este es un \"gimnasio.entorno\" b√°sico y podemos sobrescribir sus caracter√≠sticas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b2467a-4906-473f-8207-cde646def666",
   "metadata": {},
   "source": [
    "#### Constructor\n",
    "\n",
    "- El constructor se llama cuando creamos un nuevo objeto `FrozenPond`.\n",
    "- Aqu√≠ es donde definimos el espacio de observaci√≥n y el espacio de acci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2417aa6-93e3-46c9-984b-f98e24ece49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "class FrozenPond(gym.Env):\n",
    "    def __init__(self, env_config=None):\n",
    "        self.observation_space = gym.spaces.Discrete(16)\n",
    "        self.action_space = gym.spaces.Discrete(4)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359ab164-11d8-4614-a4f1-f875fac08071",
   "metadata": {},
   "source": [
    "- Por compatibilidad con RLlib, el constructor debe recibir un `env_config` \n",
    "- De momento, ignoraremos este argumento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285e5eca-3aa8-472a-a6bc-037b411f83a5",
   "metadata": {},
   "source": [
    "#### Reiniciar\n",
    "\n",
    "- El siguiente m√©todo que necesitaremos es reset.\n",
    "- El constructor establece par√°metros permanentes como el espacio de observaci√≥n.\n",
    "- el `reset` establece cada nuevo episodio.\n",
    "- Hay cierta libertad entre ambos, por ejemplo, para establecer la ubicaci√≥n de la meta.\n",
    "- Si algo _podr√≠a_ cambiar, lo pondremos en `reset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce9715c1-8865-4376-97d5-4c0f7b253e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c438cc9b-d6db-4412-8b29-80288a2cf288",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def reset(self):\n",
    "        self.player = (0, 0) # the player starts at the top-left\n",
    "        self.goal = (3, 3)   # goal is at the bottom-right\n",
    "        \n",
    "        self.holes = np.array([\n",
    "            [0,0,0,0], # FFFF \n",
    "            [0,1,0,1], # FHFH\n",
    "            [0,0,0,1], # FFFH\n",
    "            [1,0,0,0]  # HFFF\n",
    "        ])\n",
    "        \n",
    "        return 0 # to be changed to return self.observation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206ed3bf-aa1d-4833-84fa-e9be1f64648b",
   "metadata": {},
   "source": [
    "#### Reiniciar\n",
    "\n",
    "Vamos a probar esto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c1c6161-2b90-47bf-b002-d3ff6fc62464",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = FrozenPond()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50989bc9-5d31-480a-88ac-dbf85f2ff390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edbdb5ee-0af9-43e2-8833-c1c945eae535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0],\n",
       "       [0, 1, 0, 1],\n",
       "       [0, 0, 0, 1],\n",
       "       [1, 0, 0, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp.holes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79c6258-6353-42ac-81d6-51423385972a",
   "metadata": {},
   "source": [
    "Tiene buena pinta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a138d14-cf05-4d44-b315-4e708692e001",
   "metadata": {},
   "source": [
    "#### Paso\n",
    "\n",
    "- El √∫ltimo m√©todo que necesitamos es `paso`.\n",
    "- Este es el m√©todo m√°s complicado que contiene la l√≥gica central.\n",
    "- Recuerda que `step` devuelve 4 cosas:\n",
    "  1. Observaci√≥n\n",
    "  2. Recompensa\n",
    "  3. Bandera hecha\n",
    "  4. Informaci√≥n extra (la ignoraremos)\n",
    "- Para mayor claridad, escribiremos m√©todos de ayuda para la observaci√≥n, la recompensa y el hecho, adem√°s de un m√©todo de ayuda extra "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5e1f9a-6f10-4038-9c02-2a6e966ecac5",
   "metadata": {},
   "source": [
    "#### Paso: observaci√≥n\n",
    "\n",
    "Recuerda que la observaci√≥n es un √≠ndice de 0 a 15:\n",
    "\n",
    "```\n",
    " 0 1 2 3\n",
    " 4 5 6 7\n",
    " 8 9 10 11\n",
    "12 13 14 15\n",
    "```\n",
    "\n",
    "Podemos codificar esto de la siguiente manera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a66c82f8-21a4-403d-8172-2b878488e155",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def observation(self):\n",
    "        return 4*self.player[0] + self.player[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921f3bc3-6200-4167-839b-a068e88240c1",
   "metadata": {},
   "source": [
    "Por ejemplo, si el jugador est√° en (2,1) entonces devolvemos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b95ad09b-a77d-4039-afa6-0695a8427601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4*2 + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae722e0-9d57-414c-ade2-53b361b8e590",
   "metadata": {},
   "source": [
    "Nota: ahora que `self.observation` est√° implementado, deber√≠amos cambiar `reset` por `return self.observation()` en lugar de `return 0` para mejorar la calidad del c√≥digo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b84fcf-c40f-4513-8cd0-3bb2b0a8d32a",
   "metadata": {},
   "source": [
    "#### Paso: recompensa\n",
    "\n",
    "Siguiendo el ejemplo de Frozen Lake, la recompensa ser√° 1 si el agente alcanza el objetivo, y 0 en caso contrario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ee1fd77-5923-4db1-abe3-96c47360366a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def reward(self):\n",
    "        return int(self.player == self.goal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d71262-1a9e-4daf-940e-df02c2c2631d",
   "metadata": {},
   "source": [
    "Modificaremos esta funci√≥n de recompensa m√°s adelante en el m√≥dulo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb7d07c-9153-4933-94f6-c117ca6993ff",
   "metadata": {},
   "source": [
    "#### Paso: hecho\n",
    "\n",
    "- Tambi√©n necesitamos saber cu√°ndo ha terminado un episodio \n",
    "- Siguiendo con Frozen Lake, el episodio est√° terminado cuando el agente llega a la meta o cae en el estanque."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab0ed064-4f50-425e-8f0f-302a44c2d259",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def done(self):\n",
    "        return self.player == self.goal or self.holes[self.player] == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8829aa45-d26c-4707-ab51-38c38353ce00",
   "metadata": {},
   "source": [
    "#### Paso: lugares v√°lidos\n",
    "\n",
    "Por √∫ltimo, para simplificar el m√©todo `paso`, escribiremos un m√©todo de ayuda llamado `es_lugar_v√°lido` que comprueba si una ubicaci√≥n concreta est√° dentro de los l√≠mites (de 0 a 3 en cada dimensi√≥n)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1872595-ab21-4b4e-a431-c15fc1ea7595",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def is_valid_loc(self, location):\n",
    "        if 0 <= location[0] <= 3 and 0 <= location[1] <= 3:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce94f18-ffb6-4f93-ba45-f7084c3805c5",
   "metadata": {},
   "source": [
    "#### Paso: armarlo\n",
    "\n",
    "- Utilizando las piezas anteriores, ahora podemos escribir el m√©todo `paso`.\n",
    "- el m√©todo `step` recibe una _acci√≥n_, actualiza el _estado_ y devuelve la observaci√≥n, la recompensa, la bandera de hecho y la informaci√≥n extra (ignorada).\n",
    "- Recuerda c√≥mo se codifican las acciones: 0 para izquierda, 1 para abajo, 2 para derecha, 3 para arriba.\n",
    "- Implementaremos un estanque congelado **no resbaladizo**; es decir, determinista en lugar de estoc√°stico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c083279a-b950-45e6-9cfc-265885c80c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def step(self, action):\n",
    "        # Compute the new player location\n",
    "        if action == 0:   # left\n",
    "            new_loc = (self.player[0], self.player[1]-1)\n",
    "        elif action == 1: # down\n",
    "            new_loc = (self.player[0]+1, self.player[1])\n",
    "        elif action == 2: # right\n",
    "            new_loc = (self.player[0], self.player[1]+1)\n",
    "        elif action == 3: # up\n",
    "            new_loc = (self.player[0]-1, self.player[1])\n",
    "        else:\n",
    "            raise ValueError(\"Action must be in {0,1,2,3}\")\n",
    "        \n",
    "        # Update the player location only if you stayed in bounds\n",
    "        # (if you try to move out of bounds, the action does nothing)\n",
    "        if self.is_valid_loc(new_loc):\n",
    "            self.player = new_loc\n",
    "        \n",
    "        # Return observation/reward/done\n",
    "        return self.observation(), self.reward(), self.done(), {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a4a4ec-cfe6-4d3e-80a1-a606afc87c0e",
   "metadata": {},
   "source": [
    "#### ¬°√âxito!\n",
    "\n",
    "- ¬°Ya est√°! Hemos implementado las piezas necesarias en Estanque Congelado \n",
    "  - constructor\n",
    "  - `reinicio`\n",
    "  - paso\"\n",
    "- Tambi√©n a√±adiremos una funci√≥n opcional `render` para poder dibujar el estado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dace259a-1e8f-4d77-af39-3f1cb6cdc2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def render(self):\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                if (i,j) == self.goal:\n",
    "                    print(\"‚õ≥Ô∏è\", end=\"\")\n",
    "                elif (i,j) == self.player:\n",
    "                    print(\"üßë\", end=\"\")\n",
    "                elif self.holes[i,j]:\n",
    "                    print(\"üï≥\", end=\"\")\n",
    "                else:\n",
    "                    print(\"üßä\", end=\"\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925146e2-188f-4b78-acb5-df30214dad54",
   "metadata": {},
   "source": [
    "- Para divertirnos, usaremos emojis en nuestro render de cliente.\n",
    "- El jugador es üßë, la porter√≠a es ‚õ≥Ô∏è, el segmento del lago congelado es üßä, el agujero es üï≥."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d03db1-e53b-435b-a0fb-aa67bdd99161",
   "metadata": {},
   "source": [
    "#### Probando nuestra implementaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a079671-3fc3-46eb-aaa3-e0629201de06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "from envs_03 import FrozenPond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c03550a-fde5-4885-883a-030a2530488f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßëüßäüßäüßä\n",
      "üßäüï≥üßäüï≥\n",
      "üßäüßäüßäüï≥\n",
      "üï≥üßäüßä‚õ≥Ô∏è\n"
     ]
    }
   ],
   "source": [
    "env = FrozenPond()\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88bcc59-f6c1-4722-a255-ef30a49c2616",
   "metadata": {},
   "source": [
    "#### Probando nuestra implementaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2301a03f-f40e-492d-ae42-0f99b07ee09f",
   "metadata": {},
   "source": [
    "Vamos a probar el m√©todo \"paso\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1cc81874-c11f-430f-904b-1d108c170589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0, False, {'player': (0, 1), 'goal': (3, 3)})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(2) # 0=left / 1=down / 2=right / 3=up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89d4775d-fe17-4628-a97f-33ab2799573d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßäüßëüßäüßä\n",
      "üßäüï≥üßäüï≥\n",
      "üßäüßäüßäüï≥\n",
      "üï≥üßäüßä‚õ≥Ô∏è\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b46b9c2-6ebe-4d37-b809-c38bd711126c",
   "metadata": {},
   "source": [
    "¬°Tiene buena pinta!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be4064b-3f24-48b1-aa6e-184f655067d0",
   "metadata": {},
   "source": [
    "#### Probando nuestra implementaci√≥n\n",
    "\n",
    "Vamos a comparar directamente los dos entornos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3aff2e55-17f6-48bc-8fb8-eaade93881a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter | gym obs / our obs | gym reward / our reward | gym done / our done\n",
      " 0   |       0 /  0      |          0 / 0        |      False / False\n",
      " 1   |       1 /  1      |          0 / 0        |      False / False\n",
      " 2   |       2 /  2      |          0 / 0        |      False / False\n",
      " 3   |       6 /  6      |          0 / 0        |      False / False\n",
      " 4   |      10 / 10      |          0 / 0        |      False / False\n",
      " 5   |      14 / 14      |          0 / 0        |      False / False\n",
      " 6   |      14 / 14      |          0 / 0        |      False / False\n",
      " 7   |      15 / 15      |          1 / 1        |       True /  True\n"
     ]
    }
   ],
   "source": [
    "lake = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
    "pond = FrozenPond()\n",
    "\n",
    "lake.reset()\n",
    "pond.reset()\n",
    "\n",
    "print(\"Iter | gym obs / our obs | gym reward / our reward | gym done / our done\")\n",
    "for i, a in enumerate([0, 2, 2, 1, 1, 1, 1, 2]):\n",
    "    lake_obs, lake_rew, lake_done, _ = lake.step(a)\n",
    "    pond_obs, pond_rew, pond_done, _ = pond.step(a)\n",
    "    print(\"%2d   |      %2d / %2d      |          %d / %d        |      %5s / %5s\" % \\\n",
    "          (i, lake_obs, pond_obs, lake_rew, pond_rew, lake_done, pond_done))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87185e66-37b4-4337-9046-17a17cc6efb2",
   "metadata": {},
   "source": [
    "¬°Son iguales!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5deb1e8-3de2-42dc-8363-aee8acf534e3",
   "metadata": {},
   "source": [
    "#### Probando nuestra implementaci√≥n\n",
    "\n",
    "- RLlib tambi√©n viene con un comprobador de env\n",
    "- Esto no nos dir√° si nuestra env es id√©ntica a la de Frozen Lake\n",
    "- Pero realizar√° varias comprobaciones √∫tiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57e3cbe2-8354-4533-872d-41bd06ce2948",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.utils.pre_checks.env import check_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f4ebd66-5e52-4724-a9c3-df6efdb66eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-27 16:08:55,501\tWARNING env.py:142 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n"
     ]
    }
   ],
   "source": [
    "check_env(pond)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f674689-9df5-4da7-9379-478c2b802571",
   "metadata": {},
   "source": [
    "- Todas las comprobaciones se han superado, excepto esta advertencia sobre la longitud m√°xima de los episodios.\n",
    "- Podemos/debemos fijarlo para que los episodios no puedan ser arbitrariamente largos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29218be1-9c76-4f04-bffd-e7288098bb74",
   "metadata": {},
   "source": [
    "#### Pasos m√°ximos por episodio\n",
    "\n",
    "- Para establecer un n√∫mero m√°ximo de pasos por episodio, podemos utilizar una \"envoltura\" de \"gimnasia\".\n",
    "- Las envolturas son formas pr√°cticas de modificar los entornos, incluyendo las observaciones, las acciones y las recompensas.\n",
    "- Aqu√≠ usaremos la envoltura `L√≠mite de tiempo` para establecer un l√≠mite de pasos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8edeeaf-5dd3-4eee-9ed0-747880709912",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import TimeLimit\n",
    "\n",
    "pond_5 = TimeLimit(pond, max_episode_steps=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37ac862-4df2-4f76-bbe0-b718e575f0e3",
   "metadata": {},
   "source": [
    "Podemos comprobar que se har√° despu√©s de 5 pasos, aunque no se alcance el objetivo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be71dbd1-7668-4499-85d2-68a5b70fa1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, False, {'player': (0, 0), 'goal': (3, 3)})\n",
      "(0, 0, False, {'player': (0, 0), 'goal': (3, 3)})\n",
      "(0, 0, False, {'player': (0, 0), 'goal': (3, 3)})\n",
      "(0, 0, False, {'player': (0, 0), 'goal': (3, 3)})\n",
      "(0, 0, True, {'player': (0, 0), 'goal': (3, 3), 'TimeLimit.truncated': True})\n"
     ]
    }
   ],
   "source": [
    "pond_5.reset()\n",
    "for i in range(5):\n",
    "    print(pond_5.step(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f54f62a-2022-4bdf-926e-b855dc374ec6",
   "metadata": {},
   "source": [
    "#### Pasos m√°ximos por episodio\n",
    "\n",
    "Un l√≠mite de pasos m√°s razonable podr√≠a ser 50, en lugar de 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff4847c8-ff8e-4e15-96b3-b0eb38781266",
   "metadata": {},
   "outputs": [],
   "source": [
    "pond_50 = TimeLimit(pond, max_episode_steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9eadc6-381d-47ac-82ec-f418b6830e6f",
   "metadata": {},
   "source": [
    "- Para tu informaci√≥n: tambi√©n es posible establecer este l√≠mite en RLlib, s√≥lo con fines de entrenamiento.\n",
    "- Esto se hace con el par√°metro \"horizonte\" en la configuraci√≥n del entrenador."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aed5b6c-a486-4f1c-96e0-e027710461f6",
   "metadata": {},
   "source": [
    "#### ¬°Apliquemos lo que hemos aprendido!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456c51dc-3d2c-4e06-be76-62c279e976b0",
   "metadata": {},
   "source": [
    "## Recompensas de estanques congelados\n",
    "<!-- multiple choice -->\n",
    "\n",
    "En el Lago Congelado (y el Estanque), la recompensa es 1 cuando el agente llega a la meta, y 0 en caso contrario. El agente tiene que aprender a evitar los agujeros, pero en realidad no hay ninguna recompensa negativa por caer en un agujero: ¬°es la misma recompensa cero que por entrar en un trozo seguro de lago congelado! ¬øPor qu√© sigue funcionando esta configuraci√≥n, aunque la recompensa sea la misma por entrar en un agujero o en tierra firme?\n",
    "\n",
    "- [ ] Una vez que el agente cae en un agujero, queda atrapado. Puede realizar m√°s acciones, pero no hacen nada. Por tanto, el agente aprende a evitar los agujeros.\n",
    "- [ ] Una recompensa de 0 es la menor recompensa posible; por tanto, cuando el agente recibe una recompensa de 0 por caer en un agujero, sabe inmediatamente que caer en un agujero es algo malo.\n",
    "- [x] La penalizaci√≥n por caer en un agujero es indirecta, ya que el episodio termina con una recompensa de cero, perdiendo as√≠ la recompensa potencial de 1 por alcanzar el objetivo con √©xito. El agente est√° aprendiendo que al caer en un agujero pierde _recompensas futuras_.\n",
    "- [ ] Los agentes de RL prefieren los episodios m√°s largos. Cuando el agente cae en el agujero, el episodio termina inmediatamente, lo que el agente aprende a evitar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d53229-6adf-4b71-b8dd-40157b9a703d",
   "metadata": {},
   "source": [
    "## Estanque vs. Laberinto\n",
    "<!-- coding exercise -->\n",
    "\n",
    "Supongamos que queremos cambiar el entorno de nuestro estanque por un entorno de _laberinto_. En este caso, tenemos paredes en lugar de agujeros. La √∫nica diferencia entre el estanque y el laberinto es el comportamiento de los agujeros frente a las paredes. En el estanque congelado, entrar en un agujero pone fin al episodio. En el entorno del laberinto, chocar con una pared no hace nada (es decir, la acci√≥n no cambia la ubicaci√≥n del agente, al igual que intentar salirse del borde del mapa). Para convertir nuestro lago congelado en un laberinto, tendremos que modificar dos m√©todos: `hecho` y `es_lugar_valido`.\n",
    "\n",
    "A continuaci√≥n encontrar√°s los m√©todos `done` y `step` que vimos en las diapositivas anteriores. Modif√≠calos para que ahora tengamos un Laberinto con el comportamiento descrito anteriormente: chocar con una pared no hace nada.\n",
    "\n",
    "Ten en cuenta que la clase \"Laberinto\" hereda todos los dem√°s m√©todos de \"Estanque Congelado\", ¬°as√≠ que puedes probarlo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "238c70d1-4cd2-4cb0-ba58-3599ffa0879e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 0, True, {'player': (1, 1), 'goal': (3, 3)})\n",
      "(5, 0, True, {'player': (1, 1), 'goal': (3, 3)})\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE\n",
    "from envs_03 import FrozenPond\n",
    "\n",
    "\n",
    "class Maze(FrozenPond):\n",
    "    def done(self):\n",
    "        return self.player == self.goal or self.holes[self.player] == 1\n",
    "    def is_valid_loc(self, location):\n",
    "        if 0 <= location[0] <= 3 and 0 <= location[1] <= 3:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "pond = FrozenPond()\n",
    "pond.reset()\n",
    "pond.step(1)\n",
    "print(pond.step(2))\n",
    "\n",
    "maze = Maze()\n",
    "maze.reset()\n",
    "maze.step(1)\n",
    "print(maze.step(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a483f55-b956-44c1-bf8e-d534e6ad3ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 0, True, {'player': (1, 1), 'goal': (3, 3)})\n",
      "(4, 0, False, {'player': (1, 0), 'goal': (3, 3)})\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "from envs_03 import FrozenPond\n",
    "\n",
    "\n",
    "class Maze(FrozenPond):   \n",
    "    def done(self):\n",
    "        return self.player == self.goal\n",
    "    def is_valid_loc(self, location):\n",
    "        if 0 <= location[0] <= 3 and 0 <= location[1] <= 3 and not self.holes[location]:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "pond = FrozenPond()\n",
    "pond.reset()\n",
    "pond.step(1)\n",
    "print(pond.step(2))\n",
    "\n",
    "maze = Maze()\n",
    "maze.reset()\n",
    "maze.step(1)\n",
    "print(maze.step(2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ray200]",
   "language": "python",
   "name": "conda-env-ray200-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
