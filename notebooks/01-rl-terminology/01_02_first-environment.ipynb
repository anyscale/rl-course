{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c331561e-c62d-4486-9252-b9ff362f8fa4",
   "metadata": {},
   "source": [
    "## RL Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de064a2a-0a07-4c94-a9ac-b6f2d978f412",
   "metadata": {},
   "source": [
    "#### What is an environment?\n",
    "\n",
    "- An environment could be:\n",
    "  - a game, like a video game.\n",
    "  - a simulation of a real world scenario, like a robot, user behavior, or the stock market\n",
    "  - any other setup with an _agent_ who takes _actions_, views _observations_, and receives _rewards_\n",
    "  \n",
    "TERMINOLOGY NOTICE: we will use _agent_ and _player_ interchangeably. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5748d00-5c07-49db-aa9f-3cc1613bd0a7",
   "metadata": {},
   "source": [
    "#### Running example: frozen lake\n",
    "\n",
    "As a running example of an environment, we will use the [Frozen Lake](https://gym.openai.com/envs/FrozenLake-v0/) environment from [OpenAI Gym](https://gym.openai.com/). We can visualize the environment like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f87dba7-b5de-4ba9-b89d-b5ea2d069f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"FrozenLake-v1\")\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693db547-8f4d-49ae-b759-80308d6803e0",
   "metadata": {},
   "source": [
    "The goal is for the player (red highlight) to reach the goal (`G`) by walking on the frozen lake segments (`F`) without falling in the holes (`H`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a34b31-5bbb-45f4-9e7f-e925fc96d0ee",
   "metadata": {},
   "source": [
    "#### Movement\n",
    "\n",
    "The player can move around the frozen lake. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e787118-3952-48c0-b8d4-22ea69f88eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "env.reset();\n",
    "env.seed(6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d5d954a-7890-427c-aea1-7daec590c91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.step(1); # 1 -> Down\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb8ff83-f2b0-464f-934c-9e6144977c2c",
   "metadata": {},
   "source": [
    "Don't worry about `step(1)` for now; we'll get to that. \n",
    "\n",
    "What you can see is that the player (red highlight) moved downward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c7ab81-0402-4302-a46f-1b4bc115ca63",
   "metadata": {},
   "source": [
    "#### Goal\n",
    "\n",
    "Fast-forward a lot of steps, and you've completed the puzzle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3a799321-538a-4f9c-ae1d-31e15a9e0e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# HIDDEN, OUTPUT SHOWN\n",
    "def step_to_end(env):\n",
    "    env.reset();\n",
    "    env.seed(6);\n",
    "    env.step(1);\n",
    "    env.step(3);\n",
    "    env.step(1);\n",
    "    env.step(3);\n",
    "    env.step(2);\n",
    "    env.step(2);\n",
    "    env.step(2);\n",
    "    env.step(2);\n",
    "    env.step(0);\n",
    "    env.step(0);\n",
    "    env.step(2);\n",
    "    env.step(0);\n",
    "    env.step(1);\n",
    "    return env\n",
    "env = step_to_end(env)\n",
    "env.render()\n",
    "# note to self, whoops, this was silly, could just set is_slippery=False..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98858094-a152-4e3e-bd03-ed5c81af095b",
   "metadata": {},
   "source": [
    "You've achieved the goal by reaching `G`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1635d0e4-7d38-434c-a956-d47646e57b4a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### What makes an environment?\n",
    "\n",
    "An environment involves several key components, that we'll go through in the following slides."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73df9894-c37a-4d34-93d9-c4e43ecf0564",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### States\n",
    "\n",
    "- We'll use the term _state_ to refer to everything about the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a57b3f1-c76e-4b2c-9d84-742326bb827b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16f9748-129e-45c7-9bc1-106e0493fdad",
   "metadata": {},
   "source": [
    "- For example, this is the starting state of the environment.\n",
    "- The player is at the top-left, there's some frozen ice nearby, etc.\n",
    "- We'll use the concept of a state to talk about our environment, but it won't appear in the \"API\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0e72e4-8f86-4c05-9f33-7e888ef44045",
   "metadata": {},
   "source": [
    "#### Actions\n",
    "\n",
    "- Here, the player can choose between 4 possible actions: up, down, left, right\n",
    "- The space of all possible actions is called the **action space**.\n",
    "- In SL we have classification (categorical $y$) and regression (continuous $y$)\n",
    "- Likewise in RL the action space can be discrete or continuous\n",
    "- In this case, it is discrete (4 possibilities)\n",
    "- The code agrees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "757cfd69-d93d-4e8b-90d0-f3caf079f5a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1907e0-63fe-4c72-af65-f9869b590365",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "- The observations are the _parts of the state that the agent can see_.\n",
    "- Sometimes, the agent can see everything; we call this _fully observable_.\n",
    "- Oftentimes, we have _partially observable_ environments. \n",
    "- In the Frozen Lake example, the agent can only see its own location out of the 16 squares.\n",
    "- The agent is not \"told\" where the holes are via direct observations, so it will need to _learn_ this via trial and error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659ebb9b-4c6a-43f6-96c1-eb5117c50a54",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "- The space of all possible observations is called the **observation space**.\n",
    "- Here, we have a discrete observation space consisting of the 16 possible player positions.\n",
    "- You can think of the action space as analogous to the target in supervised learning.\n",
    "- You can think of the observation space as analogous to the features in supervised learning.\n",
    "- We might have a mix of discrete and continuous features; likewise, it is possible to have a mix of discrete and continuous components in the observation space (though less common)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81aa1ad6-e1bb-492d-a5d0-a72c94a33e29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(16)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ee9cb4-8650-4d39-87b7-97202ffca199",
   "metadata": {},
   "source": [
    "#### Rewards\n",
    "\n",
    "- In supervised learning, the goal is usually to make good predictions.\n",
    "- You may still try different loss functions depending on your specific goal, but the general concept is the same.\n",
    "- In RL, the goal could be anything.\n",
    "- But, like in SL, you will need to be _optimizing_ something.\n",
    "- In RL, this quantity we are maximizing is called the **reward**.\n",
    "- In the Frozen Lake example, the agent receives a reward when it reaches the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e888ae70-a3f2-400b-8fd9-54dd27369beb",
   "metadata": {},
   "source": [
    "#### Representing actions\n",
    "\n",
    "- To use RL software, we will need a numerical representation of our action space and our observation space.\n",
    "- In this case, we have 4 possible discrete actions, so we can encode them as {0,1,2,3} for (left, down, right, up).\n",
    "- This is why, earlier, we did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "420363c1-9385-47e0-abaf-935a5c0fb4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5a3c2f-ec47-47c4-85e8-95945dfa7019",
   "metadata": {},
   "source": [
    "instead of \n",
    "\n",
    "```python\n",
    "env.step(\"down\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6419f73c-8431-4e46-87af-0a99a95a58ba",
   "metadata": {},
   "source": [
    "#### Representing observations\n",
    "\n",
    "- Likewise, we will need a numerical representation of our observations.\n",
    "- Here, there are 16 possible positions of the player. These are encoded from 0-15 as follows:\n",
    "\n",
    "```\n",
    "0   4   8  12\n",
    "1   5   9  13\n",
    "2   6  10  14\n",
    "3   7  11  15\n",
    "```\n",
    "\n",
    "These details of the Frozen Lake environment are also available in the [documentation](https://www.gymlibrary.ml/pages/environments/toy_text/frozen_lake)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cecba2-2529-4f24-a5ef-d730457ac994",
   "metadata": {},
   "source": [
    "#### Representing observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "154dd058-8e7d-43a7-a62e-c829e51d9b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "env.seed(6);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b616b8-5ccf-45bf-a4ac-65ae31f108e4",
   "metadata": {},
   "source": [
    "Initially, we observe \"0\" because we start at the upper-left:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df316735-5a38-488f-80ae-dd97d715cc36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b5dad3-f8bc-4298-8dc4-4a3f87285600",
   "metadata": {},
   "source": [
    "After moving to the left (action 0), we move to position 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d540232-affa-45de-a10f-6f99f755d5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, _, _, _ = env.step(0)\n",
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f179e5d-68cb-4d31-a5e0-cd01f5b14124",
   "metadata": {},
   "source": [
    "#### Non-deterministic environments\n",
    "\n",
    "- We've been keeping a secret from you, which is that the ice is slippery.\n",
    "- In fact, when you choose action 1 corresponding to \"down\", the agent doesn't always move down.\n",
    "- Some environments are _deterministic_ meaning that the same action always results in the same change of state.\n",
    "- Some environments (like this one) are _non-deterministic_ meaning the outcome of an action can be random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6a7d44ef-2828-4c7f-aae9-17eab6b7d7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "env.seed(4); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3943d88f-a44b-4fba-96f8-20177b6317b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b97800f-5c7a-43a8-9018-f8505bd9eafd",
   "metadata": {},
   "source": [
    "#### Non-deterministic environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "35e5cb48-3e0f-4b5f-8f0a-663ff31e99e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.step(1) # move down\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0755c6-0d78-42d0-a9b4-cd4aec36db82",
   "metadata": {},
   "source": [
    "Moving down did not work as planned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cba5c8e8-a7ab-4854-9c35-ca79584f7b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.step(1) # move down\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031a6a4e-f9a1-4965-86b8-d439a82491ad",
   "metadata": {},
   "source": [
    "Moving down worked this time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0388fe5a-b6e4-484c-ac51-d376c353ffaa",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Episodes\n",
    "\n",
    "- An **episode** is one run through an RL environment.\n",
    "- After an episode, the environment is reset.\n",
    "- For example, in Frozen Lake an episode ends when you fall in a hole or you reach the goal.\n",
    "- In some environments (like Frozen Lake), rewards are only received at the end of an episode.\n",
    "- In other environments, rewards can be received at any time step (i.e., after an action)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea0a22c-c13f-407e-beb1-5fd0a985dc4a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Putting it all together\n",
    "\n",
    "- We've now talked about the main components of an environment\n",
    "  - States\n",
    "  - Actions\n",
    "  - Observations\n",
    "  - Rewards\n",
    "  - Episodes\n",
    "  \n",
    "When you call `.step()` in the code, you'll see these reflected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "aa37ba8c-a1c5-4fd2-b695-d5301b3a8dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.0, False, {'prob': 0.3333333333333333})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "env.step(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0e5220-b48e-4093-b839-fa13997ca67a",
   "metadata": {},
   "source": [
    "- The `1` in `step(1)` is the action we took\n",
    "- `0` is the observation\n",
    "- `0.0` is the reward\n",
    "- `False` tells us the episode is not over yet\n",
    "- The last part includes optional extra info about the entire state (we can ignore this for now)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8de8cd1-8733-4c90-89d6-9586be21309a",
   "metadata": {},
   "source": [
    "#### SL datasets vs. RL environments\n",
    "\n",
    "- In supervised learning, you are typically given a dataset.\n",
    "- In RL, the environment acts as a _data generator_.\n",
    "  - The more you play through the environment, the more \"data\" you generate and the more you can learn.\n",
    "- One can also do RL on a pre-collected dataset (called _offline RL_), but that is out of scope for us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c511b50-fae2-4764-8eaa-c47ca7dab1d7",
   "metadata": {},
   "source": [
    "## Ex 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcfbbb6-1765-473c-8b7e-c24e914949d3",
   "metadata": {},
   "source": [
    "## Ex 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547ac9b5-b354-44c0-8dbf-f0eaeda58ffd",
   "metadata": {},
   "source": [
    "## Ex 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8165060-f1cc-4b42-956a-de9486777812",
   "metadata": {},
   "source": [
    "## Ex 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "418b2e2c-7b23-41d5-b9c9-1be49d403ca5",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (562311143.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/f5/138zmtfd159g5x_f1t80qr440000gp/T/ipykernel_16966/562311143.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    exercise: would this be a reasonable environment\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "exercise: would this be a reasonable environment\n",
    "reward\n",
    "etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48569193-ffd9-4674-94c3-91a2edad8a8d",
   "metadata": {},
   "source": [
    "exercise: what is the action space in this example, what is the observaiton space, how would it be encoded, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aa1864-a7bb-4cce-aa52-400b8811c46b",
   "metadata": {},
   "source": [
    "use `is_slippery=True` and false, have them say if it's deterministic or not"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rllib]",
   "language": "python",
   "name": "conda-env-rllib-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
