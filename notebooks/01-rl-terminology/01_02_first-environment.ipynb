{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c331561e-c62d-4486-9252-b9ff362f8fa4",
   "metadata": {},
   "source": [
    "## RL Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de064a2a-0a07-4c94-a9ac-b6f2d978f412",
   "metadata": {},
   "source": [
    "#### What is an environment?\n",
    "\n",
    "- An environment could be:\n",
    "  - a game, like a video game.\n",
    "  - a simulation of a real world scenario, like a robot, user behaviour, or the stock market\n",
    "  - any other setup with an _agent_ who takes _actions_, views _observations_, and receives _rewards_\n",
    "  \n",
    "TERMINOLOGY NOTICE: we will use _agent_ and _player_ interchangeably. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5748d00-5c07-49db-aa9f-3cc1613bd0a7",
   "metadata": {},
   "source": [
    "#### Running example: frozen lake\n",
    "\n",
    "As a running example of an environment, we will use the [Frozen Lake](https://gym.openai.com/envs/FrozenLake-v0/) environment from [OpenAI Gym](https://gym.openai.com/). We can visualize the environment like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f87dba7-b5de-4ba9-b89d-b5ea2d069f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"FrozenLake-v1\")\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693db547-8f4d-49ae-b759-80308d6803e0",
   "metadata": {},
   "source": [
    "The goal is for the player (red highlight) to reach the goal (`G`) by walking on the frozen lake segments (`F`) without falling in the holes (`H`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a34b31-5bbb-45f4-9e7f-e925fc96d0ee",
   "metadata": {},
   "source": [
    "#### Movement\n",
    "\n",
    "The player can move around the frozen lake. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e787118-3952-48c0-b8d4-22ea69f88eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "env.reset();\n",
    "env.seed(6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d5d954a-7890-427c-aea1-7daec590c91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.step(1); # 1 -> Down\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb8ff83-f2b0-464f-934c-9e6144977c2c",
   "metadata": {},
   "source": [
    "Don't worry about `step(1)` for now; we'll get to that. \n",
    "\n",
    "What you can see is that the player (red highlight) moved downward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c7ab81-0402-4302-a46f-1b4bc115ca63",
   "metadata": {},
   "source": [
    "#### Goal\n",
    "\n",
    "Fast-forward a lot of steps, and you've completed the puzzle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a799321-538a-4f9c-ae1d-31e15a9e0e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# HIDDEN, OUTPUT SHOWN\n",
    "def step_to_end(env):\n",
    "    env.reset();\n",
    "    env.seed(6);\n",
    "    env.step(1);\n",
    "    env.step(3);\n",
    "    env.step(1);\n",
    "    env.step(3);\n",
    "    env.step(2);\n",
    "    env.step(2);\n",
    "    env.step(2);\n",
    "    env.step(2);\n",
    "    env.step(0);\n",
    "    env.step(0);\n",
    "    env.step(2);\n",
    "    env.step(0);\n",
    "    env.step(1);\n",
    "    return env\n",
    "env = step_to_end(env)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98858094-a152-4e3e-bd03-ed5c81af095b",
   "metadata": {},
   "source": [
    "You've achieved the goal by reaching `G`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1635d0e4-7d38-434c-a956-d47646e57b4a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### What makes an environment?\n",
    "\n",
    "An environment involves several key components, that we'll go through in the following slides."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73df9894-c37a-4d34-93d9-c4e43ecf0564",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### States\n",
    "\n",
    "- We'll use the term _state_ to refer to everything about the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a57b3f1-c76e-4b2c-9d84-742326bb827b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16f9748-129e-45c7-9bc1-106e0493fdad",
   "metadata": {},
   "source": [
    "- For example, this is the starting state of the environment.\n",
    "- The player is at the top-left, there's some frozen ice nearby, etc.\n",
    "- We'll use the concept of a state to talk about our environment, but it won't appear in the \"API\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0e72e4-8f86-4c05-9f33-7e888ef44045",
   "metadata": {},
   "source": [
    "#### Actions\n",
    "\n",
    "- Here, the player can choose between 4 possible actions: up, down, left, right\n",
    "- The space of all possible actions is called the **action space**.\n",
    "- In SL we have classification (categorical $y$) and regression (continuous $y$)\n",
    "- Likewise in RL the action space can be discrete or continuous\n",
    "- In this case, it is discrete (4 possibilities)\n",
    "- The code agrees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "757cfd69-d93d-4e8b-90d0-f3caf079f5a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1907e0-63fe-4c72-af65-f9869b590365",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "- The observations are the _parts of the state that the agent can see_.\n",
    "- Sometimes, the agent can see everything; we call this _fully observable_.\n",
    "- Oftentimes, we have _partially observable_ environments. \n",
    "- In the Frozen Lake example, the agent can only see its own location out of the 16 squares.\n",
    "- The agent is not \"told\" where the holes are via direct observations, so it will need to _learn_ this via trial and error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659ebb9b-4c6a-43f6-96c1-eb5117c50a54",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "- The space of all possible observations is called the **observation space**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "81aa1ad6-e1bb-492d-a5d0-a72c94a33e29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(16)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ee9cb4-8650-4d39-87b7-97202ffca199",
   "metadata": {},
   "source": [
    "#### Rewards\n",
    "\n",
    "- In supervised learning, the goal is usually to make good predictions.\n",
    "- You may still try different loss functions depending on your specific goal, but the general concept is the same.\n",
    "- In RL, the goal could be anything.\n",
    "- But, like in SL, you will need to be _optimizing_ something.\n",
    "- In RL, this quantity we are maximizing is called the **reward**.\n",
    "- In the Frozen Lake example, the agent receives a reward when it reaches the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e888ae70-a3f2-400b-8fd9-54dd27369beb",
   "metadata": {},
   "source": [
    "#### Representing actions\n",
    "\n",
    "- To use RL software, we will need a numerical representation of our action space and our observation space.\n",
    "- In this case, we have 4 possible discrete actions, so we can encode them as {0,1,2,3}.\n",
    "- This is why, earlier, we did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "420363c1-9385-47e0-abaf-935a5c0fb4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5a3c2f-ec47-47c4-85e8-95945dfa7019",
   "metadata": {},
   "source": [
    "instead of \n",
    "\n",
    "```python\n",
    "env.step(\"down\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6419f73c-8431-4e46-87af-0a99a95a58ba",
   "metadata": {},
   "source": [
    "#### Representing observations\n",
    "\n",
    "- Likewise, we will need a numerical representation of our observations.\n",
    "- Here, there are 25 pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "154dd058-8e7d-43a7-a62e-c829e51d9b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "env.seed(6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df316735-5a38-488f-80ae-dd97d715cc36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d540232-affa-45de-a10f-6f99f755d5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b51edb-9419-4f79-bd3d-8df7bab52cc7",
   "metadata": {},
   "source": [
    "The agent moved from position 0 to position 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8de8cd1-8733-4c90-89d6-9586be21309a",
   "metadata": {},
   "source": [
    "#### RL environments vs. SL datasets\n",
    "\n",
    "how does this compare?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99e7834-5d92-4789-8007-f9ce29b80365",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "\n",
    "- States\n",
    "- Actions\n",
    "- Rewards\n",
    "- Observations?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c511b50-fae2-4764-8eaa-c47ca7dab1d7",
   "metadata": {},
   "source": [
    "## Ex 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcfbbb6-1765-473c-8b7e-c24e914949d3",
   "metadata": {},
   "source": [
    "## Ex 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547ac9b5-b354-44c0-8dbf-f0eaeda58ffd",
   "metadata": {},
   "source": [
    "## Ex 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8165060-f1cc-4b42-956a-de9486777812",
   "metadata": {},
   "source": [
    "## Ex 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418b2e2c-7b23-41d5-b9c9-1be49d403ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "exercise: would this be a reasonable environment\n",
    "reward\n",
    "etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48569193-ffd9-4674-94c3-91a2edad8a8d",
   "metadata": {},
   "source": [
    "exercise: what is the action space in this example, what is the observaiton space, how would it be encoded, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183959b5-c748-4957-b26d-5ae503d289b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rllib]",
   "language": "python",
   "name": "conda-env-rllib-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
