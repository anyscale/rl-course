{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf336e3e-20b0-4095-af4e-880907ceac4a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The problem\n",
    "\n",
    "- Walking around in a 2D maze, with walls put in random locations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65babae2-56b5-4fd4-9395-833cb7dcdbdf",
   "metadata": {},
   "source": [
    "NOTE FOR COURSE:\n",
    "\n",
    "randomness at constructor vs. randomness at `reset()`\n",
    "\n",
    "e.g. frozen lake is random at constructor, so the task learns a random lake, but it's the same lake every reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfbaf003-567b-415b-a4ce-2cd8c94fe1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.spaces import Discrete\n",
    "import numpy as np\n",
    "import ray\n",
    "from ray.rllib.agents.dqn import DQNTrainer\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.rllib.models.preprocessors import get_preprocessor \n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81e0db0d-bc87-4c99-8e13-4dffe6f24e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cdcef1b-9377-4a6d-8b3f-41552167e6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_names = {\n",
    "    0 : \"up\",\n",
    "    1 : \"down\",\n",
    "    2 : \"left\",\n",
    "    3 : \"right\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0890080-6686-4876-b100-07bc48b87b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ed4f7a9-fc7c-47b4-9d8f-a40b91f9b61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomMazeEnvironment(gym.Env): \n",
    "    def __init__(self, env_config):\n",
    "        self.ylen = env_config.get(\"ylen\", 5)\n",
    "        self.xlen = env_config.get(\"xlen\", 5)\n",
    "        \n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "        \n",
    "        # show entire state, not just nearby the player\n",
    "        # self.observation_space = gym.spaces.Dict({\n",
    "        #     \"player\" : gym.spaces.Discrete(self.ylen*self.xlen),\n",
    "        #     \"walls\" : gym.spaces.MultiBinary(self.ylen*self.xlen)\n",
    "        # })\n",
    "        # self.observation_space = gym.spaces.MultiBinary([2, self.ylen, self.xlen])\n",
    "\n",
    "        # see https://docs.ray.io/en/latest/rllib/rllib-models.html\n",
    "        # looks like they mainly just preprocess discrete/multidiscrete and atari?\n",
    "        # what about box?? hmm. i guess those go without preproc?\n",
    "        \n",
    "        self.observation_space = gym.spaces.MultiDiscrete([2,2,2,2])\n",
    "        # self.observation_space = gym.spaces.Tuple([Discrete(2), Discrete(2), Discrete(2), Discrete(2)])\n",
    "        # self.observation_space = gym.spaces.MultiBinary(4)\n",
    "        \n",
    "        # so there are two notions of a random seed here,\n",
    "        # there’s the notion of a reset seed - if that is fixed then you get the \n",
    "        # same maze every time you call reset (then no longer a random maze!). \n",
    "        # Then there’s also the notion of a general seed for the initialization \n",
    "        # of the environment - so that a sequence of calls to reset  generates \n",
    "        # the same sequence of random mazes each time. This is what we actually want here. \n",
    "        self.reset_random_seed = env_config.get(\"reset_random_seed\", None)\n",
    "        self.single_random_seed = env_config.get(\"single_random_seed\", None)\n",
    "        \n",
    "        if self.single_random_seed is not None:\n",
    "            np.random.seed(self.single_random_seed)\n",
    "\n",
    "        def seed(self, seed):\n",
    "            pass\n",
    "        # self.random_seed = seed\n",
    "    \n",
    "    def reset(self, random_seed=None):\n",
    "        # print(\"RESET\")\n",
    "        self.player = (0, 0)\n",
    "        self.exit = (self.ylen-1, self.xlen-1)\n",
    "        \n",
    "        # fixed seed every time, not random maze\n",
    "        if self.reset_random_seed is not None: \n",
    "            np.random.seed(self.reset_random_seed)\n",
    "        \n",
    "        self.walls = np.random.rand(self.ylen, self.xlen) < 0.2\n",
    "        self.walls[self.player] = 0\n",
    "        self.walls[self.exit] = 0\n",
    "        \n",
    "        self.num_steps = 0\n",
    "        return self.get_observation()\n",
    "    \n",
    "    def get_observation(self): # SWAPPED FROM THE ABOVE ONE\n",
    "        # make the observation into the entire state??\n",
    "        # playerstate = self.player[1] + self.xlen*self.player[0]\n",
    "        # obsdict = {\n",
    "        #     \"player\" : playerstate,\n",
    "        #     \"walls\" : self.wallsstate\n",
    "        # }\n",
    "        # return obsdict\n",
    "        \n",
    "        # playerstate = 0*self.walls\n",
    "        # playerstate[self.player] = 1\n",
    "        # return np.concatenate((playerstate[None], \n",
    "        #                        self.walls[None]), axis=0).astype(int)\n",
    "        i, j = self.player\n",
    "\n",
    "        obs = []\n",
    "        obs.append(1 if i==0           else self.walls[i-1,j]) # up\n",
    "        obs.append(1 if i==self.ylen-1 else self.walls[i+1,j]) # down\n",
    "        obs.append(1 if j==0           else self.walls[i,j-1]) # left\n",
    "        obs.append(1 if j==self.xlen-1 else self.walls[i,j+1]) # right\n",
    "        return np.array(obs, dtype=int)\n",
    "        # 3x3 context, 8 observations\n",
    "        # wallsexit = self.walls.copy()\n",
    "        # wallspad = np.pad(wallsexit, 1, constant_values=1) # pad with 1 to simulate walls all around\n",
    "        # nearby = wallspad[self.player[0]:self.player[0]+3, self.player[1]:self.player[1]+3]\n",
    "        # nearbyflat = nearby.flatten()\n",
    "        # minus_middle = np.concatenate((nearbyflat[:4], nearbyflat[5:]))\n",
    "        # return minus_middle\n",
    "        # NW N NE W E SW S SE\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.num_steps += 1\n",
    "        \n",
    "        if action == 0: # move up\n",
    "            new_loc = (self.player[0]-1, self.player[1])\n",
    "            \n",
    "        elif action == 1: # move down\n",
    "            new_loc = (self.player[0]+1, self.player[1])\n",
    "                \n",
    "        elif action == 2: # move left\n",
    "            new_loc = (self.player[0], self.player[1]-1)\n",
    "  \n",
    "        elif action == 3: # move right\n",
    "            new_loc = (self.player[0], self.player[1]+1)\n",
    "                \n",
    "        else:\n",
    "            raise Exception(\"Action must be {0,1,2,3}\")\n",
    "            \n",
    "        # update state if you are able to move\n",
    "        move_success = False\n",
    "        if 0 <= new_loc[0] < self.ylen and 0 <= new_loc[1] < self.xlen and not self.walls[new_loc]:\n",
    "            self.player = new_loc\n",
    "            move_success = True\n",
    "                \n",
    "        info = {\"player pos\" : self.player}\n",
    "        \n",
    "        # reward\n",
    "        distance = np.abs(self.player[0]-self.exit[0]) + np.abs(self.player[1]-self.exit[1])\n",
    "        reward = -distance\n",
    "        \n",
    "        # penalize walking into walls\n",
    "        if not move_success:\n",
    "            reward -= 5\n",
    "        \n",
    "        return self.get_observation(), reward, self.is_done(), info\n",
    "\n",
    "    \n",
    "    def is_done(self):\n",
    "        return self.player == self.exit or self.num_steps >= 500\n",
    "\n",
    "    def render(self):\n",
    "        for i in range(self.xlen):\n",
    "            for j in range(self.ylen):\n",
    "                if (i,j) == self.exit:\n",
    "                    print(\"E\", end=\"\")\n",
    "                elif (i,j) == self.player:\n",
    "                    print(\"P\", end=\"\")\n",
    "                elif self.walls[i,j]:\n",
    "                    print(\"X\", end=\"\")\n",
    "                else:\n",
    "                    print(\".\", end=\"\")\n",
    "                # print(\"O\", end=\"\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cbac29c-fa8b-45a4-9081-7768ff025f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "rme = RandomMazeEnvironment({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0053ea09-352e-4f07-86f0-f79d4a7cf216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rme.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bdfef18-af55-4a80-8f80-5acc736028de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PXX..\n",
      "....X\n",
      "...X.\n",
      "...X.\n",
      "...XE\n"
     ]
    }
   ],
   "source": [
    "rme.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b9ba80d-57a5-4bfe-867f-ed488665d16c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 0, 1, 1]), -13, False, {'player pos': (0, 0)})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rme.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10a5c9b7-7fe4-4ba9-98b1-1ecaaba50ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer_config_2 = copy.copy(trainer_config)\n",
    "# trainer_config_2[\"model\"] = {\"fcnet_hiddens\" : [64, 64]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87345888-8305-4eeb-95e2-a1f7d5bdb8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config_rme = {\n",
    "    # \"num_workers\": 0,\n",
    "    \"model\": {\n",
    "        \"fcnet_hiddens\": [64, 64],\n",
    "        \"fcnet_activation\": \"relu\",\n",
    "    },\n",
    "    \"evaluation_num_workers\": 1,\n",
    "    # \"create_env_on_driver\" : True,\n",
    "    \"log_level\" : \"ERROR\",\n",
    "    \"framework\" : \"torch\",\n",
    "    \"seed\" : 42\n",
    "    # \"_disable_preprocessor_api\" : True\n",
    "    # \"vf_loss_coef\" : 1/20_000 # make this small if the value function loss is big, only if vf_share_layers\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c761db6-efcb-4efe-9745-eda1d144e11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-19 10:55:15,353\tWARNING trainer.py:2347 -- You have specified 1 evaluation workers, but your `evaluation_interval` is None! Therefore, evaluation will not occur automatically with each call to `Trainer.train()`. Instead, you will have to call `Trainer.evaluate()` manually in order to trigger an evaluation run.\n"
     ]
    }
   ],
   "source": [
    "trainer = PPOTrainer(trainer_config_rme, env=RandomMazeEnvironment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3906f3cb-5fd3-407b-83af-32cc34f950f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.config[\"train_batch_size\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a95ce2-a4c1-4981-a5ad-219c459944f6",
   "metadata": {},
   "source": [
    "This is measure in individual time steps, per worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d695d0a2-7a98-4723-bc5d-3a858f93d859",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2a7dde1-6392-48ab-b131-8a957d4283bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(5):\n",
    "    print(iteration)\n",
    "    out = trainer.train();\n",
    "    rewards.append(out['episode_reward_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27a8103b-3bcf-48fa-9f55-9729fc0241bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(rewards);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1b6603d-601e-46a2-82e1-73638b502b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-54.0,\n",
       " -179.0,\n",
       " -80.0,\n",
       " -80.0,\n",
       " -1402.0,\n",
       " -113.0,\n",
       " -73.0,\n",
       " -39.0,\n",
       " -184.0,\n",
       " -165.0,\n",
       " -143.0,\n",
       " -342.0,\n",
       " -667.0,\n",
       " -94.0,\n",
       " -310.0,\n",
       " -130.0,\n",
       " -280.0,\n",
       " -333.0,\n",
       " -80.0,\n",
       " -6500.0,\n",
       " -210.0,\n",
       " -1917.0,\n",
       " -116.0,\n",
       " -264.0,\n",
       " -5511.0,\n",
       " -293.0,\n",
       " -3859.0,\n",
       " -167.0,\n",
       " -136.0,\n",
       " -62.0,\n",
       " -235.0,\n",
       " -149.0,\n",
       " -6500.0,\n",
       " -40.0,\n",
       " -277.0,\n",
       " -298.0,\n",
       " -195.0,\n",
       " -294.0,\n",
       " -59.0,\n",
       " -89.0,\n",
       " -517.0,\n",
       " -40.0,\n",
       " -1619.0,\n",
       " -2669.0,\n",
       " -312.0,\n",
       " -113.0,\n",
       " -319.0,\n",
       " -3943.0,\n",
       " -2516.0,\n",
       " -135.0,\n",
       " -6500.0,\n",
       " -6500.0,\n",
       " -258.0,\n",
       " -200.0,\n",
       " -96.0,\n",
       " -513.0,\n",
       " -1087.0,\n",
       " -156.0,\n",
       " -753.0,\n",
       " -678.0,\n",
       " -132.0,\n",
       " -473.0,\n",
       " -49.0,\n",
       " -211.0,\n",
       " -491.0,\n",
       " -46.0,\n",
       " -127.0,\n",
       " -3116.0,\n",
       " -97.0,\n",
       " -3700.0,\n",
       " -119.0,\n",
       " -235.0,\n",
       " -3377.0,\n",
       " -142.0,\n",
       " -71.0,\n",
       " -292.0,\n",
       " -513.0,\n",
       " -1966.0,\n",
       " -150.0,\n",
       " -94.0,\n",
       " -51.0,\n",
       " -185.0,\n",
       " -104.0,\n",
       " -66.0,\n",
       " -70.0,\n",
       " -846.0,\n",
       " -80.0,\n",
       " -2412.0,\n",
       " -127.0,\n",
       " -425.0,\n",
       " -233.0,\n",
       " -99.0,\n",
       " -96.0,\n",
       " -72.0,\n",
       " -154.0,\n",
       " -179.0,\n",
       " -56.0,\n",
       " -427.0,\n",
       " -65.0,\n",
       " -138.0]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[\"hist_stats\"][\"episode_reward\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09fbfb57-e202-4280-8081-1679048df783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out[\"hist_stats\"][\"episode_reward\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cea043e-9dc7-4e80-94cd-764ab2deb153",
   "metadata": {},
   "source": [
    "^ number of episodes in one call to `trainer.train()`\n",
    "\n",
    "Keeping the last 100 in memory for a moving average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94a09a1d-3ad9-45e6-b5e4-b5a46465c83f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'evaluation': {'episode_reward_max': -82.0,\n",
       "  'episode_reward_min': -6500.0,\n",
       "  'episode_reward_mean': -1034.4,\n",
       "  'episode_len_mean': 122.7,\n",
       "  'episode_media': {},\n",
       "  'episodes_this_iter': 10,\n",
       "  'policy_reward_min': {},\n",
       "  'policy_reward_max': {},\n",
       "  'policy_reward_mean': {},\n",
       "  'custom_metrics': {},\n",
       "  'hist_stats': {'episode_reward': [-6500.0,\n",
       "    -84.0,\n",
       "    -248.0,\n",
       "    -2528.0,\n",
       "    -82.0,\n",
       "    -107.0,\n",
       "    -259.0,\n",
       "    -101.0,\n",
       "    -216.0,\n",
       "    -219.0],\n",
       "   'episode_lengths': [500, 21, 39, 500, 13, 20, 43, 16, 37, 38]},\n",
       "  'sampler_perf': {'mean_raw_obs_processing_ms': 0.0456963765892998,\n",
       "   'mean_inference_ms': 0.24199330457258691,\n",
       "   'mean_action_processing_ms': 0.015109292070718075,\n",
       "   'mean_env_wait_ms': 0.017514446270970648,\n",
       "   'mean_env_render_ms': 0.0},\n",
       "  'off_policy_estimator': {},\n",
       "  'timesteps_this_iter': 0}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18ca07c4-22cc-4b83-9acd-2b420e751e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P.X..\n",
      "....X\n",
      "XX.X.\n",
      ".....\n",
      ".X..E\n"
     ]
    }
   ],
   "source": [
    "rme = RandomMazeEnvironment({})\n",
    "obs = rme.reset() \n",
    "rme.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c19f029-2399-4dc2-9b5c-e46843e07721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P....\n",
      "X....\n",
      "...X.\n",
      ".X...\n",
      "X...E\n"
     ]
    }
   ],
   "source": [
    "rme.seed(1)\n",
    "rme.reset()\n",
    "rme.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f5e7c49-db20-442e-b054-a358362d0050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".P...\n",
      "X....\n",
      "...X.\n",
      ".X...\n",
      "X...E\n",
      "\n",
      "action: right\n",
      "reward: -7\n",
      "done: False\n",
      "obs: [1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "action = trainer.compute_single_action(input_dict={\"obs\": obs})#, explore=False)\n",
    "res = rme.step(action)\n",
    "obs = res[0]\n",
    "rme.render()\n",
    "\n",
    "print(\"\\naction:\", action_names[action])\n",
    "print(\"reward:\", res[1])\n",
    "print(\"done:\", res[2])\n",
    "print(\"obs:\", obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53b7253b-f12b-4a89-b865-28e960551329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....\n",
      ".X.X.\n",
      "X...X\n",
      ".....\n",
      "....E\n"
     ]
    }
   ],
   "source": [
    "from ipywidgets import Output\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "output_display = Output()\n",
    "display.display(output_display)\n",
    "\n",
    "rme.reset()\n",
    "\n",
    "with output_display:\n",
    "    r_obs = obs\n",
    "    r_d = False\n",
    "    rme.render()\n",
    "    time.sleep(0.1)\n",
    "    while not r_d:\n",
    "\n",
    "        r_a = trainer.compute_single_action(r_obs)\n",
    "        r_obs, r_r, r_d, _ = rme.step(r_a)\n",
    "\n",
    "        output_display.clear_output(wait=True)\n",
    "        rme.render()\n",
    "        time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0146852-4768-419d-a3fb-966ae7d17978",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = trainer.get_policy()\n",
    "model = policy.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf9a5f9d-f88b-4e55-b117-3812aa217c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.models.preprocessors import get_preprocessor\n",
    "prep = get_preprocessor(rme.observation_space)(rme.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5773fee4-8166-4d2c-82de-80ddee06e336",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = np.array([1,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50c3e584-f28c-4b50-8ff2-e7752568aab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output = model({\"obs\": torch.from_numpy(prep.transform(obs)[None])})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d1b1569-a125-4380-962f-6d6c988bc64c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2000,  0.3905, -0.7778,  0.4380]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "725d0186-654d-4ae3-b804-6f58d869a403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(trainer.get_policy().get_weights().values())[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "79294b81-830a-4e21-a556-c9d5ab2a8982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q_values = model.get_q_value_distributions(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "672dfcfa-252e-4074-a0f9-084e9d41990f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dda28df1-1e40-47c4-a8c1-f32ff54d388d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ray.rllib.models.torch.torch_action_dist.TorchCategorical at 0x14f47b460>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist = policy.dist_class(model_output, model) \n",
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "080e58fe-c5d6-4b61-b186-8058840e3051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c3ba6d-52b5-4a33-9b76-ffe5154f4e6b",
   "metadata": {},
   "source": [
    "Probs that add up to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d6cd717-4886-4c59-9821-9869b3fc9bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.19015753, 0.3432089 , 0.10670878, 0.35992482], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(dist.logp(torch.from_numpy(np.array([0,1,2,3]))).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "92c2c0b3-183d-41bb-bce3-89e25caa6625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Categorical(probs: torch.Size([1, 4]), logits: torch.Size([1, 4]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6033a529-8087-48a9-8336-a29b3adf917b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullyConnectedNetwork(\n",
      "  (_logits): SlimFC(\n",
      "    (_model): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=4, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (_hidden_layers): Sequential(\n",
      "    (0): SlimFC(\n",
      "      (_model): Sequential(\n",
      "        (0): Linear(in_features=8, out_features=64, bias=True)\n",
      "        (1): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (1): SlimFC(\n",
      "      (_model): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (1): ReLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (_value_branch_separate): Sequential(\n",
      "    (0): SlimFC(\n",
      "      (_model): Sequential(\n",
      "        (0): Linear(in_features=8, out_features=64, bias=True)\n",
      "        (1): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (1): SlimFC(\n",
      "      (_model): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (1): ReLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (_value_branch): SlimFC(\n",
      "    (_model): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2be3ba1-aa3b-478e-855b-d300ec19be32",
   "metadata": {},
   "source": [
    "## Questions for Sven\n",
    "\n",
    "- How many episodes are actually getting run during training/evaluation? Only 10?\n",
    "  - It's complicated for training, but the batch size is measured in time steps (rather than episodes) so it also depends on episode length\n",
    "- `compute_action` vs `compute_single_action` \n",
    "  - Use `compute_single_action`\n",
    "- Can we go through the results of `print(model)` and understand why it looks the way it does?\n",
    "  - for Pytorch specifically, the layers are shown alphabetically and not in order (it only gets linked up in the call to `forward`)\n",
    "- How to get q values from `ComplexInputNetwork`?\n",
    "  - Don't use `ComplexInputNetwork`, avoid at all costs\n",
    "- What happens if you don't call `ray.init()` - the code still seems to run?\n",
    "- DQN vs PPO\n",
    "  - DQN more interpretable if you dig into the model, PPO more complicated.\n",
    "  - PPO learns action probabilities, will sample from this distribution by default\n",
    "- Is it fine to call `ray.init(ignore_reinit_error=True)` repeatedly?\n",
    "  - Yes\n",
    "- Why does `episode_reward_mean` not get better with more iterations? Is it because we've reaching the \"ceiling\" given the environment/model?\n",
    "  - Wasn't working at all with `MultiBinary`\n",
    "- What would be your next step to improve performance here? Ideas...\n",
    "  - Reward crafting\n",
    "  - Observation crafting\n",
    "  - Model/algo tuning / ray tune\n",
    "  - More iterations (how do you decide this? look for a plateau? results are generally quite noisy, I guess due to the randomness in the env)\n",
    "  - ...?\n",
    "- Higher-level question: in my google doc I have a list of envs for the tuned examples - which ones would be good for the course? can they be rendered?\n",
    "- Can I reuse recsim material from Sven's tutorial in the course? (And if so, do we think it's a good idea)\n",
    "- Ok to do the whole course with the Python API instead of the CLI?\n",
    "- If we actually wanted to make a maze-winning AI, would we want to consider somehow giving it access to the same maze multiple times and not resetting EVERY time? Would that be advantageous for learning somehow? Or is it fine to randomize at every `reset`?\n",
    "- In terms of the return of `step`, namely observation, reward, `is_done`, info -- is it correct that info is never used by any algorithm?\n",
    "- What if we want the reward to be part of the observation?\n",
    "- Is there a way to set a `random_state` in RLlib so that it performs the same actions every time?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682312f9-ba07-4a99-ad73-96d56bc1fdd2",
   "metadata": {},
   "source": [
    "Things that seem to have magically fixed themselves:\n",
    "\n",
    "- Why does it say episode lengths are always 500 when it clearly seems to work (below)?\n",
    "- When I do `dist.sample()`, why do I get numbers larger than 3?\n",
    "- And, why is `model_output` size 64 instead of 4?\n",
    "\n",
    "I wonder why this was happening before... could it have been with `DQN`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805d5e8d-2de0-40ad-a9e4-55a865b1db60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f6dc77-a789-4cd2-9f86-95c3920c639c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e8d72d-55bc-4954-b429-5b4bcb566407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e4cbb7-ca08-4fe7-98c8-832331434c7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf563cb9-e516-49d4-b022-910c49d1e696",
   "metadata": {},
   "source": [
    "Idea: give the agent an action that says \"give up\", take a reward hit, but move on.\n",
    "So basically, see if they can figure out if it's unsolvable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rl-course]",
   "language": "python",
   "name": "conda-env-rl-course-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
