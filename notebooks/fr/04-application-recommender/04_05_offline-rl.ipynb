{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5aa21ad-6bb9-4990-ab4d-fb9435b0bc00",
   "metadata": {},
   "source": [
    "## Hors ligne RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "204bbdb7-e15f-4e5c-be77-b58b39dd3dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f337b2ca-8719-4938-8983-3a02bc76626f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import ray\n",
    "import logging\n",
    "ray.init(log_to_driver=False, ignore_reinit_error=True, logging_level=logging.ERROR); # logging.FATAL\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fd0b8c-0fe5-4295-b752-061d9e199421",
   "metadata": {},
   "source": [
    "#### Est-ce que c'est réaliste ?\n",
    "\n",
    "- Jusqu'à présent, nous avons construit une simulation du comportement de l'utilisateur\n",
    "- Dans certaines applications, nous pourrions être en mesure de construire des simulations précises :\n",
    "  - simulations de physique (par exemple, les robots)\n",
    "  - jeux\n",
    "  - simulations économiques/financières ?\n",
    "- Cependant, pour le comportement de l'utilisateur, c'est difficile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb08a59-42f4-4fce-b709-1907ea5ef3c9",
   "metadata": {},
   "source": [
    "#### Est-ce que c'est réaliste ? \n",
    "\n",
    "- Le mieux serait de déployer RL en direct, mais ce n'est pas pratique\n",
    "- Une autre possibilité : apprendre à partir des données des utilisateurs ?\n",
    "- Nous pouvons le faire avec **l'apprentissage par renforcement en ligne**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba4b7ef-32c4-4caa-bb22-eace97236370",
   "metadata": {},
   "source": [
    "#### RL hors ligne\n",
    "\n",
    "- Qu'est-ce que le RL hors ligne ?\n",
    "- Rappelle-toi notre boucle RL :\n",
    "\n",
    "[](img/RL-loop-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d89bcc9-17c8-4dbc-afb0-9f9e4810dad0",
   "metadata": {},
   "source": [
    "#### RL hors ligne\n",
    "\n",
    "- Dans la RL hors ligne, nous n'avons pas d'environnement avec lequel interagir dans une boucle de rétroaction :\n",
    "\n",
    "[](img/offline-RL-loop.png)\n",
    "\n",
    "Ces données historiques ont été générées par une/des autre(s) politique(s) inconnue(s).\n",
    "\n",
    "Notes :\n",
    "\n",
    "Peut être généré par des utilisateurs réels, ou par une source différente (aléatoire, ou agent RL !)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d6cd2e-9bff-4b43-a49f-38e3211c4a09",
   "metadata": {},
   "source": [
    "#### Le défi de la RL hors ligne\n",
    "\n",
    "- On ne peut pas répondre aux questions \"et si\"\n",
    "- Nous ne pouvons voir que les résultats des actions tentées dans l'ensemble de données\n",
    "\n",
    "Remarques :\n",
    "\n",
    "Peut-être que cela nous fait apprécier à quel point il est précieux/important d'avoir réellement un env à disposition, ce que nous avons eu pendant tout le reste du cours. Cela nous permet d'essayer n'importe quoi sans aucun coût, sauf le coût de calcul (en supposant qu'il s'agit d'un simulateur et non d'un environnement du monde réel) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f9ff23b-8073-44b0-bb32-320ab7fa0a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # HIDDEN\n",
    "\n",
    "# # generate the offline dataset\n",
    "# env_config = {\n",
    "#     \"num_candidates\" : 2,\n",
    "#     \"alpha\"          : 0.5,\n",
    "#     \"seed\"           : 42\n",
    "# }\n",
    "\n",
    "# from ray.rllib.algorithms.ppo import PPO, PPOConfig\n",
    "\n",
    "# ppo_config = (\n",
    "#     PPOConfig()\\\n",
    "#     .framework(\"torch\")\\\n",
    "#     # need to set num_rollout_workers=1 for now per https://github.com/ray-project/ray/issues/25696\n",
    "#     .rollouts(create_env_on_local_worker=True, num_rollout_workers=1)\\\n",
    "#     .debugging(seed=0, log_level=\"ERROR\")\\\n",
    "#     .training(model={\"fcnet_hiddens\" : [64, 64]}, lr=0.001)\\\n",
    "#     # .environment(env_config=env_config)\\\n",
    "#     .offline_data(output=\"data/recommender2\")\n",
    "# )\n",
    "\n",
    "# from envs_04 import BasicRecommenderWithHistory\n",
    "\n",
    "# ppo_history = ppo_config.build(env=\"CartPole-v1\")\n",
    "\n",
    "# rewards_history = []\n",
    "# for i in range(25):\n",
    "#     result = ppo_history.train()\n",
    "#     rewards_history.append(result[\"episode_reward_mean\"])\n",
    "    \n",
    "# ppo_history.evaluate(duration_fn=1000)[\"evaluation\"][\"episode_reward_mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23be8a47-e338-408d-abac-63bbefe41dac",
   "metadata": {},
   "source": [
    "#### Jeu de données Recommender\n",
    "\n",
    "- Explorons un ensemble de données hors ligne dont nous pouvons tirer des enseignements.\n",
    "- Nous aurons besoin d'un peu de code pour lire tous les objets JSON du fichier :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8610f1a2-b704-4b22-aeba-3eb3826693e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json_dataset_file = \"data/recommender_offline.json\"\n",
    "\n",
    "rollouts = []\n",
    "with open(json_dataset_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        rollouts.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9540859-74d2-4c4f-a06c-1cd6fd848994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rollouts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6f10f9-acb9-4aff-9694-622faf8792e1",
   "metadata": {},
   "source": [
    "Nous avons 50 \"déploiements\" de données.\n",
    "\n",
    "Notes :\n",
    "\n",
    "Le fichier est dans le format à partir duquel RLlib apprend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433484e4-8250-473f-9579-f712e9fa9e17",
   "metadata": {},
   "source": [
    "#### Jeu de données Recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b685e2f5-4381-40ca-bff8-cc1398d76ddb",
   "metadata": {},
   "source": [
    "Chaque rollout est un dict contenant des informations sur le pas de temps :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c1e82cd-f6f4-4076-aa36-2a16dc761447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.rllib.utils.compression import unpack, pack\n",
    "\n",
    "obs = unpack(rollouts[0][\"obs\"])\n",
    "obs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ea21ad-940d-4aa0-8a63-43c527d233bd",
   "metadata": {},
   "source": [
    "- Nous avons 200 étapes temporelles de données dans chaque déploiement\n",
    "- Examinons d'abord les observations\n",
    "\n",
    "Notes :\n",
    "\n",
    "Ce nombre 200 est défini par le paramètre de configuration de l'algorithme \"rollout_fragment_length\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fce3e9-db24-4050-bcd4-fc08f760bf5d",
   "metadata": {},
   "source": [
    "#### Jeu de données Recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a579d766-069c-446c-aa3d-7798335ece38",
   "metadata": {},
   "source": [
    "Voici les 3 premières observations :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7946e442-7f6a-4653-8e84-75bc37fe952a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6545137 , 0.29728338],\n",
       "       [0.5238871 , 0.5144319 ],\n",
       "       [0.6741674 , 0.10163702]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93775945-d5ff-4f9f-96c1-3a04e4ac2a85",
   "metadata": {},
   "source": [
    "\n",
    "Nous pouvons voir que `num_candidats` a été fixé à 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a1b0f3-6394-4afe-b444-37914d84ac89",
   "metadata": {},
   "source": [
    "#### Jeu de données Recommender\n",
    "\n",
    "Nous pouvons également examiner les 3 premières actions, les récompenses et les dons :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ace3caf4-ef70-4393-9846-e68648b41bfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rollouts[0][\"actions\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29ff0ca7-6fe4-4786-a19f-c9bee9bf0b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6545137166976929, 0.3524414300918579, 0.05838315561413765]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rollouts[0][\"rewards\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37cafb64-4837-40c3-aa09-7b9e9871906d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, False, False]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rollouts[0][\"dones\"][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff27284-e91c-4ef1-9b01-7e950eda2cd4",
   "metadata": {},
   "source": [
    "Notes :\n",
    "\n",
    "Donc, l'agent a d'abord vu l'observation [0.65, 0.297] de la diapositive précédente, puis il a fait l'action 0, a obtenu une récompense de 0.65, et l'épisode n'a pas été fait.\n",
    "\n",
    "Il y a plus d'informations stockées dans l'ensemble de données que ce qui précède, mais ce sont les points clés."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a206ba60-13ae-47b3-a406-eb06e701af5a",
   "metadata": {},
   "source": [
    "#### Apprentissage supervisé\n",
    "\n",
    "- Attends, c'est un ensemble de données... ne pouvons-nous pas simplement faire de l'apprentissage supervisé pour faire correspondre les états aux actions ? 🤔\n",
    "- Oui, nous le pouvons, et cela viserait à récupérer la politique qui a généré l'ensemble de données (_apprentissage par imitation_).\n",
    "- Mais il est en fait possible de faire _mieux_... et c'est notre objectif avec le RL hors ligne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59b604b7-484a-4ad8-976d-6f49c941f3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# maybe going overboard here, but for a synthetic example we could actually show it does better than the policy that generated the data,\n",
    "# since we generated the data ourselves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e35709e-45fd-4d70-a111-172580296fea",
   "metadata": {},
   "source": [
    "#### Formation RL hors ligne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8049ef58-4f5e-4116-abd7-4aad06f65ee0",
   "metadata": {},
   "source": [
    "- Beaucoup d'informations sur le RL hors ligne avec RLlib [ici] (https://docs.ray.io/en/latest/rllib/rllib-offline.html).\n",
    "- Tout d'abord, nous avons besoin d'un algorithme.\n",
    "- Pour le RL hors ligne, nous ne pouvons pas utiliser `PPO`.\n",
    "- Nous utiliserons l'algorithme `MARWIL` qui est inclus dans RLlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34d0e449-de74-444c-894e-4d58c6e33c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.marwil import MARWIL, MARWILConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0385a4b2-594b-46e0-bd2f-2854a6935111",
   "metadata": {},
   "source": [
    "#### Formation RL hors ligne\n",
    "\n",
    "Ensuite, nous créons la config, en commençant par `MARWILConfig` au lieu de `PPOConfig`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56e22e98-6ca9-4c34-850e-488c4bb38db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the same as before\n",
    "offline_config = ( \n",
    "    MARWILConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(create_env_on_local_worker=True)\\\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\\\n",
    "    .training(model={\"fcnet_hiddens\" : [64, 64]})\\\n",
    ")\n",
    "\n",
    "# This is new for offline RL\n",
    "num_candidates = 2\n",
    "offline_config = offline_config.environment(\n",
    "    observation_space = gym.spaces.Box(low=0, high=1, shape=(num_candidates,)), \n",
    "    action_space = gym.spaces.Discrete(num_candidates),\n",
    ").offline_data(\n",
    "    input_ = [json_dataset_file],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108dd79c-dddb-4153-a89d-258ee623da8a",
   "metadata": {},
   "source": [
    "Notes :\n",
    "\n",
    "- Les éléments de configuration du haut devraient te sembler familiers. Sur la deuxième moitié, les choses sont un peu différentes :\n",
    "  - Nous devons lui donner le chemin d'accès au fichier de l'ensemble de données\n",
    "  - Comme il n'y a pas d'env, nous devons spécifier manuellement les espaces d'observation et d'action\n",
    "- Nous n'avons pas de config d'environnement car il n'y a pas d'environnement !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c811603-4264-479d-bd0a-b5ec9ff38de2",
   "metadata": {},
   "source": [
    "#### Formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7280c730-eeee-49a2-a84b-7c42388df308",
   "metadata": {},
   "outputs": [],
   "source": [
    "marwil = offline_config.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8172e9d-bc26-4e00-b431-c9438863028d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    marwil.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d7651b-e0cb-4e37-bec3-976ac37af9cb",
   "metadata": {},
   "source": [
    "#### Évaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fe0e89-2c77-4a99-98e7-307e7502693f",
   "metadata": {},
   "source": [
    "_Comment évaluer sans simulateur ?\n",
    "\n",
    "- C'est ce qu'on appelle l'estimation hors politique.\n",
    "- C'est une technique plus sophistiquée qui sort du cadre de ce cours.\n",
    "- Voir la documentation de RLlib [ici] (https://docs.ray.io/en/latest/rllib/rllib-offline.html).\n",
    "- Ce que nous allons faire, c'est évaluer avec notre simulateur, puisque nous en avons un.\n",
    "\n",
    "Notes :\n",
    "\n",
    "Dans un scénario réaliste de RL hors ligne, tu n'as pas de simulateur à ta disposition. Tu dois alors utiliser des techniques d'évaluation plus complexes appelées estimation hors ligne. Tu auras des ensembles de données d'entraînement/de test comme dans l'apprentissage supervisé \n",
    "\n",
    "Pour nos besoins, puisque nous avons un simulateur, nous l'utiliserons pour l'évaluation. RLlib a en fait une option pour permettre cela, car c'est utile pour le débogage et autres. Nous verrons cela sur la prochaine diapositive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202d38ea-9ad6-4d17-9137-720251d1f6a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Évaluation avec notre simulateur\n",
    "\n",
    "Nous pouvons évaluer l'algorithme à l'aide de notre simulateur env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5da21ac0-fcba-4d96-8b23-721f98dc1638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "from envs_04 import BasicRecommender\n",
    "\n",
    "env_config = {\n",
    "    \"num_candidates\" : 2,\n",
    "    \"alpha\"          : 0.5,\n",
    "    \"seed\"           : 42\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b582c0ac-88f4-40e0-8078-31ee041bfe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BasicRecommender(env_config)\n",
    "\n",
    "def get_episode_reward(env, algo):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = algo.compute_single_action(obs)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a8671a-26c8-45f5-a468-944dfa4c01b6",
   "metadata": {},
   "source": [
    "- Ci-dessus : configure une fonction qui exécute un épisode avec le simulateur.\n",
    "- Ci-dessous : exécute cette fonction pour 100 épisodes afin d'obtenir la moyenne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "632fbd40-c2bb-44fe-af47-39b9507530dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.537665635536726"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([get_episode_reward(env, marwil) for i in range(100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8818fa-2e46-4d04-85c7-d55797fc169c",
   "metadata": {},
   "source": [
    "Cela semble faire à peu près la même chose que le hasard à nouveau (25,5)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e507580-77c8-47c1-a9a5-8259ee4bdcd9",
   "metadata": {},
   "source": [
    "#### RLlib pour l'évaluation sur simulateur\n",
    "\n",
    "RLlib propose également cette fonctionnalité :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7468c451-e68b-446b-a0f2-5dc86bb1af11",
   "metadata": {},
   "outputs": [],
   "source": [
    "offline_config = offline_config.evaluation(\n",
    "    off_policy_estimation_methods={},\n",
    "    evaluation_config={\n",
    "        \"input\": \"sampler\",\n",
    "        \"env\": BasicRecommender,\n",
    "        \"env_config\" : env_config\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0db4ec-8023-4ada-83ac-e4f0f42acb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "#marwil.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dfe966-9734-42c7-bf5f-bf8655944f36",
   "metadata": {},
   "source": [
    "Si nous avions mis en place la configuration de cette façon et formé :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5bccaf63-54f6-41da-a034-3da05a1ae5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "marwil = offline_config.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "56152208-587f-4306-8e53-9c0eb9f99f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    marwil.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d980136-8c96-42e9-90fb-9d4a6a564922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.3457144503546"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marwil.evaluate()[\"evaluation\"][\"episode_reward_mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d21639-5531-46b6-8cce-68b3cfb013f0",
   "metadata": {},
   "source": [
    "Ici, nous voyons à nouveau un résultat similaire."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e979de95-c039-4493-b958-e2e5d85098ec",
   "metadata": {},
   "source": [
    "#### Appliquons ce que nous avons appris !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643f5645-5a4d-4fcd-aa28-32894a99ffc2",
   "metadata": {},
   "source": [
    "## Exemple de RL hors ligne\n",
    "<!-- multiple choice -->\n",
    "\n",
    "- [ ] Apprendre à une IA à jouer aux échecs en la faisant jouer contre d'autres IA à plusieurs reprises.\n",
    "- [ ] Apprendre à une IA à jouer aux échecs en se basant sur les parties passées de joueurs d'échecs professionnels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcb97ca-5ccd-4eed-b3ac-b58cee110c78",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Les données hors ligne t'aident-elles ?\n",
    "<!-- multiple choice -->\n",
    "\n",
    "Imagine que tu as un simulateur qui représente parfaitement ton environnement ; par exemple, tu entraînes peut-être une IA à jouer à un jeu solo comme [Atari Breakout](https://en.wikipedia.org/wiki/Breakout_(video_game)). En plus du simulateur, tu as aussi des données hors ligne à ta disposition. Bien que cela ne soit pas abordé dans les diapositives ci-dessus, il est possible de combiner un simulateur avec des données hors ligne pour former conjointement un agent à l'aide de RL (voir [ici](https://docs.ray.io/en/latest/rllib/rllib-offline.html#mixing-simulation-and-offline-data)). Si tu as déjà un simulateur parfait, les données hors ligne pourraient-elles apporter une utilité supplémentaire, si elles sont combinées avec le simulateur pendant la formation ?\n",
    "\n",
    "#### Meilleure politique après l'entraînement\n",
    "\n",
    "Choisis l'affirmation correcte concernant la recherche de la meilleure politique après une formation illimitée. Tu peux supposer que tu as un algorithme RL \"parfait\", ce qui signifie qu'il peut représenter n'importe quelle politique et est capable d'optimiser n'importe quelle fonction.\n",
    "\n",
    "- [Avec un algorithme RL \"parfait\" et un temps de calcul suffisant, il n'y a aucun avantage à utiliser les données historiques car le simulateur contient tout ce qu'il y a à savoir sur l'environnement.\n",
    "- [ ] Avec un algorithme RL \"parfait\" et un temps de calcul illimité, les données historiques peuvent t'aider à trouver une meilleure politique qu'en utilisant uniquement le simulateur. | Avec un algorithme RL \"parfait\", tu devrais finir par trouver la politique optimale. Cela revient à avoir des données infinies, un temps de calcul infini, un modèle arbitrairement complexe et une méthode d'optimisation parfaite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bf6575-837f-458d-be6f-924ee4659254",
   "metadata": {},
   "source": [
    "#### Vitesse d'entraînement\n",
    "\n",
    "Choisis l'affirmation correcte concernant le fait de trouver la meilleure politique après un peu d'entraînement. Tu peux supposer que tu as un algorithme RL \"parfait\", c'est-à-dire qu'il peut représenter n'importe quelle politique et est capable d'optimiser n'importe quelle fonction.\n",
    "\n",
    "- [Avec un algorithme RL \"parfait\" mais seulement un peu de temps de calcul, il n'y a aucun avantage à utiliser les données historiques car le simulateur contient tout ce qu'il y a à savoir sur l'environnement. | Et si les données historiques étaient générées à l'aide de la politique *optimale* ?\n",
    "- [Avec un algorithme RL \"parfait\" mais un temps de calcul limité, les données historiques peuvent t'aider à trouver une meilleure politique qu'en utilisant uniquement le simulateur. | Si les données historiques ont été générées à l'aide d'une très bonne politique, tu pourrais en tirer des leçons rapidement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab3ebb6-c70d-44d8-93e1-ae4d0f7b54a7",
   "metadata": {},
   "source": [
    "## Politique de données historiques\n",
    "<!-- multiple choice -->\n",
    "\n",
    "La RL hors ligne s'appuie sur des données générées par une certaine politique qui interagit avec l'environnement. Lequel des éléments suivants est **NON** une propriété souhaitable de cet ensemble de données / politique historique ?\n",
    " \n",
    "- [x] L'environnement et la politique historique sont tous deux déterministes. | Dans ce cas, nous n'explorerions qu'une seule trajectoire à travers l'environnement \n",
    "- [ ] L'ensemble de données contient un grand nombre d'épisodes.\n",
    "- [ ] La politique historique explore une variété d'états dans l'environnement.\n",
    "- [ ] La politique historique obtient une récompense élevée dans certains épisodes.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06aebc9-1031-4635-bb8a-43417dffcb2c",
   "metadata": {},
   "source": [
    "## RL hors ligne pour Cartpole\n",
    "<!-- coding exercise -->\n",
    "\n",
    "Dans cet exercice, nous allons nous attaquer au célèbre [problème de référence Cartpole] (https://www.gymlibrary.ml/environments/classic_control/cart_pole/) qui est fourni avec la bibliothèque `gym`. Le but est d'empêcher le pendule inversé de tomber en appliquant une force, à gauche ou à droite, à chaque pas de temps \n",
    "\n",
    "Nous entraînerons l'agent en utilisant des données hors ligne contenues dans un fichier `cartpolev1_offline.json` qui est lu par le code.\n",
    "\n",
    "Nous _évaluerons_ l'agent en utilisant le vrai simulateur Cartpole. (Encore une fois, dans la réalité, si nous utilisions RL hors ligne, nous n'aurions probablement pas accès au vrai simulateur, mais nous l'incluons ici afin de pouvoir faire une évaluation de terrain de notre agent)\n",
    "\n",
    "Remplis le code manquant. Ensuite, exécute le code et réponds à la question à choix multiple ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd176dc-9a29-4813-b10e-224a61c37aaa",
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ray.rllib.algorithms.marwil import MARWIL, MARWILConfig\n",
    "\n",
    "offline_trainer_config = {\n",
    "    # These should look familiar:\n",
    "    \"framework\"             : \"torch\",\n",
    "    \"create_env_on_driver\"  : True,\n",
    "    \"seed\"                  : 0,\n",
    "    \"model\"                 : {\n",
    "        \"fcnet_hiddens\"     : [64, 64]\n",
    "    },\n",
    "    \n",
    "    # These are new for offline RL:\n",
    "    ____: [\"data/cartpolev1_offline.json\"],\n",
    "    \"observation_space\": gym.spaces.Box(low=____, \n",
    "                                        high=np.array([4.8,  np.inf,  0.42,  np.inf])),\n",
    "    \"action_space\": gym.spaces.Discrete(2),\n",
    "    \"input_evaluation\" : [\"simulation\", \"is\", \"wis\"],\n",
    "    \"env\" : \"CartPole-v1\" # for evaluation only\n",
    "}\n",
    "\n",
    "algo = ...\n",
    "\n",
    "# Training (and storing results)\n",
    "results_off = []\n",
    "results_sim = []\n",
    "for i in range(200):\n",
    "    r = algo.____()\n",
    "    results_off.append(r[\"off_policy_estimator\"][\"wis\"]['V_gain_est'])\n",
    "    results_sim.append(r[\"episode_reward_mean\"])\n",
    "\n",
    "plt.plot(results_sim);\n",
    "plt.xlabel('iterations') \n",
    "plt.ylabel('simulator reward') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e100ab56-3d05-494e-a151-2e82701842ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# This is new for offline RL\u001b[39;00m\n\u001b[1;32m     17\u001b[0m offline_config \u001b[38;5;241m=\u001b[39m offline_config\u001b[38;5;241m.\u001b[39menvironment(\n\u001b[1;32m     18\u001b[0m     observation_space \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mspaces\u001b[38;5;241m.\u001b[39mBox(low\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4.8\u001b[39m, \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.42\u001b[39m, \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf]), \n\u001b[1;32m     19\u001b[0m                                         high\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m4.8\u001b[39m,  np\u001b[38;5;241m.\u001b[39minf,  \u001b[38;5;241m0.42\u001b[39m,  np\u001b[38;5;241m.\u001b[39minf])),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     off_policy_estimation_methods\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msimulation\u001b[39m\u001b[38;5;124m'\u001b[39m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msimulation\u001b[39m\u001b[38;5;124m'\u001b[39m}, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwis\u001b[39m\u001b[38;5;124m'\u001b[39m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwis\u001b[39m\u001b[38;5;124m'\u001b[39m}}\n\u001b[1;32m     26\u001b[0m )\n\u001b[0;32m---> 28\u001b[0m algo \u001b[38;5;241m=\u001b[39m \u001b[43moffline_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Training (and storing results)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m results_off \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/rllib/algorithms/algorithm_config.py:307\u001b[0m, in \u001b[0;36mAlgorithmConfig.build\u001b[0;34m(self, env, logger_creator)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logger_creator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger_creator \u001b[38;5;241m=\u001b[39m logger_creator\n\u001b[0;32m--> 307\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malgo_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/rllib/algorithms/algorithm.py:308\u001b[0m, in \u001b[0;36mAlgorithm.__init__\u001b[0;34m(self, config, env, logger_creator, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;66;03m# Initialize common evaluation_metrics to nan, before they become\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;66;03m# available. We want to make sure the metrics are always present\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;66;03m# (although their values may be nan), so that Tune does not complain\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;66;03m# when we use these as stopping criteria.\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_metrics \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode_reward_max\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mnan,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m     }\n\u001b[1;32m    306\u001b[0m }\n\u001b[0;32m--> 308\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m# Check, whether `training_iteration` is still a tune.Trainable property\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# and has not been overridden by the user in the attempt to implement the\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;66;03m# algos logic (this should be done now inside `training_step`).\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/tune/trainable/trainable.py:157\u001b[0m, in \u001b[0;36mTrainable.__init__\u001b[0;34m(self, config, logger_creator, remote_checkpoint_dir, custom_syncer)\u001b[0m\n\u001b[1;32m    155\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_local_ip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_current_ip()\n\u001b[0;32m--> 157\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m setup_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m setup_time \u001b[38;5;241m>\u001b[39m SETUP_TIME_THRESHOLD:\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/rllib/algorithms/crr/crr.py:163\u001b[0m, in \u001b[0;36mCRR.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetup\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: PartialAlgorithmConfigDict):\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_network_update_freq\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_network_update_freq\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    166\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_batch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m    167\u001b[0m         )\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/rllib/algorithms/algorithm.py:572\u001b[0m, in \u001b[0;36mAlgorithm.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(method_type, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    571\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to import from string: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m method_type)\n\u001b[0;32m--> 572\u001b[0m     mod, obj \u001b[38;5;241m=\u001b[39m method_type\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    573\u001b[0m     mod \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(mod)\n\u001b[1;32m    574\u001b[0m     method_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(mod, obj)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ray.rllib.algorithms.marwil import MARWIL, MARWILConfig\n",
    "from ray.rllib.algorithms.crr import CRR, CRRConfig\n",
    "\n",
    "# This is the same as before\n",
    "offline_config = ( \n",
    "    CRRConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(create_env_on_local_worker=True)\\\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\\\n",
    "    .training(model={\"fcnet_hiddens\" : [32, 32]})\\\n",
    ")\n",
    "# This is new for offline RL\n",
    "offline_config = offline_config.environment(\n",
    "    observation_space = gym.spaces.Box(low=np.array([-4.8, -np.inf, -0.42, -np.inf]), \n",
    "                                        high=np.array([4.8,  np.inf,  0.42,  np.inf])),\n",
    "    action_space = gym.spaces.Discrete(2),\n",
    "    env = \"CartPole-v1\" # for evaluation only\n",
    ").offline_data(\n",
    "    input_ = [\"data/cartpolev1_offline.json\"]\n",
    ").evaluation(\n",
    "    off_policy_estimation_methods={'simulation': {'type': 'simulation'}}\n",
    ")\n",
    "\n",
    "algo = offline_config.build()\n",
    "\n",
    "# Training (and storing results)\n",
    "results_off = []\n",
    "results_sim = []\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "    r = algo.train()\n",
    "    results_off.append(r[\"\"][\"wis\"]['v_new_mean'])\n",
    "    results_sim.append(r[\"episode_reward_mean\"])\n",
    "\n",
    "plt.plot(results_sim);\n",
    "plt.xlabel('iterations') \n",
    "plt.ylabel('simulator reward') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bcc68e84-014c-46ba-ba70-37bf63f6234a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# see the tuned examples here: https://docs.ray.io/en/master/rllib/rllib-algorithms.html#offline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad6fe1d-dc59-41ff-99cb-c688dc12fd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# - ALSO: this might be a great example to try supervised learning and show why it doesn't work...???\n",
    "#   - in some ways this is a way better example than frozen lake... because there IS a short-term reward, it's just not what you should look at.\n",
    "#   - with frozen lake there is no short-term reward, so RL seems \"obvious\"\n",
    "# Or, that could go in the offline RL section, since we already have a data file there and could do SL directly on it\n",
    "# Yes, that seems cool.\n",
    "# could use scipy/numpy to just do the normal equations if we want to avoid adding a dependency on sklearn \n",
    "\n",
    "# Supervised learning only gives you imitation learning, which can only get as good as the policy that generated the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766cc60b-fb10-411c-a798-18ae6f447369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34ae237-8b75-48b9-a998-2914625e71e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ray200]",
   "language": "python",
   "name": "conda-env-ray200-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
