{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e23df55-837b-4ec9-86ad-3b60c6a7f00d",
   "metadata": {},
   "source": [
    "## Syntaxe des environnements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2e57cc-3491-4489-af86-6052b1efcc1b",
   "metadata": {},
   "source": [
    "#### Motivation\n",
    "\n",
    "- Jusqu'√† pr√©sent, nous avons utilis√© des environnements pr√©d√©finis comme Frozen Like et Google RecSim.\n",
    "- Pour utiliser RL sur notre propre probl√®me, nous ne pouvons utiliser aucun de ces environnements.\n",
    "- Nous devrons d√©finir notre propre environnement avec Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0797f5a9-c856-49f2-9a3c-419d816b6ec1",
   "metadata": {},
   "source": [
    "#### Revue de Frozen Lake\n",
    "\n",
    "- Rappelle-toi l'environnement de Frozen Lake, du module 1 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69171cee-dc7b-433d-85c5-d0d40486a43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"FrozenLake-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a41dffc-8451-4d7e-a40a-23fd1315ffba",
   "metadata": {},
   "source": [
    "#### Revue de Frozen Lake\n",
    "\n",
    "- OpenAI Gym est une source ouverte, nous avons donc pu consulter le [code source de Frozen Lake] (https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py).\n",
    "- Cependant, il est compliqu√© et contient bien plus que ce dont nous avons besoin.\n",
    "- Cr√©ons notre propre environnement appel√© Frozen Pond avec les composants de base de Frozen Lake."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735eb606-b1dd-4dba-be55-d6105cc47536",
   "metadata": {},
   "source": [
    "#### Composants d'une Env\n",
    "\n",
    "D√©cisions conceptuelles :\n",
    "\n",
    "- Espace d'observation\n",
    "- Espace d'action\n",
    "\n",
    "En Python, nous devrons impl√©menter, au moins :\n",
    "\n",
    "- constructeur\n",
    "- `reset()`\n",
    "- `step()`\n",
    "\n",
    "En pratique, nous pouvons aussi vouloir d'autres m√©thodes, comme `render()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9cf2ed-dc7e-4bbd-a5f8-9b78b8a38710",
   "metadata": {},
   "source": [
    "#### D√©cisions conceptuelles\n",
    "\n",
    "Dans ce cas, comme nous imitons le Lac gel√©, l'espace d'observation et l'espace d'action sont d√©j√† d√©cid√©s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34c0aa1d-8ab0-462a-8bdd-8ba12484a5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space = gym.spaces.Discrete(16)\n",
    "action_space = gym.spaces.Discrete(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef0fa48-25fb-4360-a1d8-54ca675dc786",
   "metadata": {},
   "source": [
    "Plus tard dans ce cours, nous nous plongerons plus profond√©ment dans ces d√©cisions !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c88c68-7f74-490e-beda-9e199d0c3822",
   "metadata": {},
   "source": [
    "#### Coding it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db413de7-7df3-4d77-9ae8-5c8e95a39068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "class FrozenPond(gym.Env):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b3bbec-72c8-48dd-bdc4-6f537dacda5c",
   "metadata": {},
   "source": [
    "- Remarque que nous commen√ßons par sous-classer `gym.Env`.\n",
    "- Facultatif : Tu peux lire sur les objets, l'h√©ritage et les sous-classes.\n",
    "- L'essentiel : Il s'agit d'un `gym.Env` de base et nous pouvons en √©craser les caract√©ristiques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b2467a-4906-473f-8207-cde646def666",
   "metadata": {},
   "source": [
    "#### Constructeur\n",
    "\n",
    "- Le constructeur est appel√© lorsque nous cr√©ons un nouvel objet `FrozenPond`.\n",
    "- C'est ici que nous d√©finissons l'espace d'observation et l'espace d'action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2417aa6-93e3-46c9-984b-f98e24ece49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "class FrozenPond(gym.Env):\n",
    "    def __init__(self, env_config=None):\n",
    "        self.observation_space = gym.spaces.Discrete(16)\n",
    "        self.action_space = gym.spaces.Discrete(4)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359ab164-11d8-4614-a4f1-f875fac08071",
   "metadata": {},
   "source": [
    "- Pour la compatibilit√© avec RLlib, le constructeur doit prendre un `env_config` \n",
    "- Nous allons simplement ignorer cet argument pour l'instant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285e5eca-3aa8-472a-a6bc-037b411f83a5",
   "metadata": {},
   "source": [
    "_COPY #### Reset\n",
    "\n",
    "- La prochaine m√©thode dont nous aurons besoin est la r√©initialisation.\n",
    "- Le constructeur d√©finit des param√®tres permanents comme l'espace d'observation.\n",
    "- `reset` configure chaque nouvel √©pisode.\n",
    "- Il y a une certaine libert√© entre les deux, par exemple pour d√©finir l'emplacement du but.\n",
    "- Si quelque chose _pourrait_ changer, nous le mettrons dans `reset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce9715c1-8865-4376-97d5-4c0f7b253e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c438cc9b-d6db-4412-8b29-80288a2cf288",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def reset(self):\n",
    "        self.player = (0, 0) # the player starts at the top-left\n",
    "        self.goal = (3, 3)   # goal is at the bottom-right\n",
    "        \n",
    "        self.holes = np.array([\n",
    "            [0,0,0,0], # FFFF \n",
    "            [0,1,0,1], # FHFH\n",
    "            [0,0,0,1], # FFFH\n",
    "            [1,0,0,0]  # HFFF\n",
    "        ])\n",
    "        \n",
    "        return 0 # to be changed to return self.observation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206ed3bf-aa1d-4833-84fa-e9be1f64648b",
   "metadata": {},
   "source": [
    "_COPY #### Reset\n",
    "\n",
    "Testons cela :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c1c6161-2b90-47bf-b002-d3ff6fc62464",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = FrozenPond()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50989bc9-5d31-480a-88ac-dbf85f2ff390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edbdb5ee-0af9-43e2-8833-c1c945eae535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0],\n",
       "       [0, 1, 0, 1],\n",
       "       [0, 0, 0, 1],\n",
       "       [1, 0, 0, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp.holes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79c6258-6353-42ac-81d6-51423385972a",
   "metadata": {},
   "source": [
    "√áa a l'air bien."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a138d14-cf05-4d44-b315-4e708692e001",
   "metadata": {},
   "source": [
    "#### √âtape\n",
    "\n",
    "- La derni√®re m√©thode dont nous avons besoin est `step`.\n",
    "- C'est la m√©thode la plus compliqu√©e qui contient la logique de base.\n",
    "- Rappelle-toi que `step` renvoie 4 choses :\n",
    "  1. Observation\n",
    "  2. R√©compense\n",
    "  3. Drapeau fait\n",
    "  4. Info suppl√©mentaire (nous l'ignorerons)\n",
    "- Pour plus de clart√©, nous allons √©crire des m√©thodes d'aide pour observation, r√©compense et fait, plus une m√©thode d'aide suppl√©mentaire "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5e1f9a-6f10-4038-9c02-2a6e966ecac5",
   "metadata": {},
   "source": [
    "#### √âtape : observation\n",
    "\n",
    "Rappelle-toi que l'observation est un indice de 0 √† 15 :\n",
    "\n",
    "```\n",
    " 0 1 2 3\n",
    " 4 5 6 7\n",
    " 8 9 10 11\n",
    "12 13 14 15\n",
    "```\n",
    "\n",
    "Nous pouvons coder cela comme suit :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a66c82f8-21a4-403d-8172-2b878488e155",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def observation(self):\n",
    "        return 4*self.player[0] + self.player[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921f3bc3-6200-4167-839b-a068e88240c1",
   "metadata": {},
   "source": [
    "Par exemple, si le joueur se trouve √† (2,1), nous renvoyons le message suivant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b95ad09b-a77d-4039-afa6-0695a8427601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4*2 + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae722e0-9d57-414c-ade2-53b361b8e590",
   "metadata": {},
   "source": [
    "Remarque : maintenant que `self.observation` est impl√©ment√©, nous devrions changer `reset` en `return self.observation()` plut√¥t que `return 0` pour un code de meilleure qualit√©."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b84fcf-c40f-4513-8cd0-3bb2b0a8d32a",
   "metadata": {},
   "source": [
    "#### √âtape : r√©compense\n",
    "\n",
    "En suivant l'exemple de Frozen Lake, la r√©compense sera de 1 si l'agent atteint l'objectif, et de 0 sinon :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ee1fd77-5923-4db1-abe3-96c47360366a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def reward(self):\n",
    "        return int(self.player == self.goal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d71262-1a9e-4daf-940e-df02c2c2631d",
   "metadata": {},
   "source": [
    "Nous modifierons cette fonction de r√©compense plus tard dans le module !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb7d07c-9153-4933-94f6-c117ca6993ff",
   "metadata": {},
   "source": [
    "####. √âtape : faite\n",
    "\n",
    "- Nous avons √©galement besoin de savoir quand un √©pisode est termin√© \n",
    "- Selon Frozen Lake, l'√©pisode est termin√© lorsque l'agent atteint l'objectif ou tombe dans l'√©tang."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab0ed064-4f50-425e-8f0f-302a44c2d259",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def done(self):\n",
    "        return self.player == self.goal or self.holes[self.player] == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8829aa45-d26c-4707-ab51-38c38353ce00",
   "metadata": {},
   "source": [
    "#### √âtape : emplacements valides\n",
    "\n",
    "Enfin, pour simplifier la m√©thode `step`, nous allons √©crire une m√©thode auxiliaire appel√©e `is_valid_loc` qui v√©rifie si un emplacement particulier est dans les limites (de 0 √† 3 dans chaque dimension)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1872595-ab21-4b4e-a431-c15fc1ea7595",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def is_valid_loc(self, location):\n",
    "        if 0 <= location[0] <= 3 and 0 <= location[1] <= 3:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce94f18-ffb6-4f93-ba45-f7084c3805c5",
   "metadata": {},
   "source": [
    "#### √âtape : assembler le tout\n",
    "\n",
    "- En utilisant les √©l√©ments ci-dessus, nous pouvons maintenant √©crire la m√©thode `step`.\n",
    "- `step` prend une _action_, met √† jour l'_√©tat_ et renvoie l'observation, la r√©compense, le drapeau \"done\" et des informations suppl√©mentaires (ignor√©es).\n",
    "- Rappelle-toi comment les actions sont cod√©es : 0 pour gauche, 1 pour bas, 2 pour droite, 3 pour haut.\n",
    "- Nous allons impl√©menter un √©tang gel√© **non glissant** ; en d'autres termes, d√©terministe plut√¥t que stochastique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c083279a-b950-45e6-9cfc-265885c80c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def step(self, action):\n",
    "        # Compute the new player location\n",
    "        if action == 0:   # left\n",
    "            new_loc = (self.player[0], self.player[1]-1)\n",
    "        elif action == 1: # down\n",
    "            new_loc = (self.player[0]+1, self.player[1])\n",
    "        elif action == 2: # right\n",
    "            new_loc = (self.player[0], self.player[1]+1)\n",
    "        elif action == 3: # up\n",
    "            new_loc = (self.player[0]-1, self.player[1])\n",
    "        else:\n",
    "            raise ValueError(\"Action must be in {0,1,2,3}\")\n",
    "        \n",
    "        # Update the player location only if you stayed in bounds\n",
    "        # (if you try to move out of bounds, the action does nothing)\n",
    "        if self.is_valid_loc(new_loc):\n",
    "            self.player = new_loc\n",
    "        \n",
    "        # Return observation/reward/done\n",
    "        return self.observation(), self.reward(), self.done(), {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a4a4ec-cfe6-4d3e-80a1-a606afc87c0e",
   "metadata": {},
   "source": [
    "#### Succ√®s !\n",
    "\n",
    "- C'est fait ! Nous avons impl√©ment√© les pi√®ces n√©cessaires dans Frozen Pond \n",
    "  - constructeur\n",
    "  - `reset`\n",
    "  - `step`\n",
    "- Nous ajouterons √©galement une fonction facultative `render` afin de pouvoir dessiner l'√©tat :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dace259a-1e8f-4d77-af39-3f1cb6cdc2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def render(self):\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                if (i,j) == self.goal:\n",
    "                    print(\"‚õ≥Ô∏è\", end=\"\")\n",
    "                elif (i,j) == self.player:\n",
    "                    print(\"üßë\", end=\"\")\n",
    "                elif self.holes[i,j]:\n",
    "                    print(\"üï≥\", end=\"\")\n",
    "                else:\n",
    "                    print(\"üßä\", end=\"\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925146e2-188f-4b78-acb5-df30214dad54",
   "metadata": {},
   "source": [
    "- Pour le plaisir, nous allons utiliser des √©mojis dans le rendu de notre client.\n",
    "- Le joueur est üßë, le but est ‚õ≥Ô∏è, le segment de lac gel√© est üßä, le trou est üï≥."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d03db1-e53b-435b-a0fb-aa67bdd99161",
   "metadata": {},
   "source": [
    "#### Tester notre impl√©mentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a079671-3fc3-46eb-aaa3-e0629201de06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "from envs_03 import FrozenPond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c03550a-fde5-4885-883a-030a2530488f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßëüßäüßäüßä\n",
      "üßäüï≥üßäüï≥\n",
      "üßäüßäüßäüï≥\n",
      "üï≥üßäüßä‚õ≥Ô∏è\n"
     ]
    }
   ],
   "source": [
    "env = FrozenPond()\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88bcc59-f6c1-4722-a255-ef30a49c2616",
   "metadata": {},
   "source": [
    "#### Tester notre impl√©mentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2301a03f-f40e-492d-ae42-0f99b07ee09f",
   "metadata": {},
   "source": [
    "Testons la m√©thode `step` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1cc81874-c11f-430f-904b-1d108c170589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0, False, {'player': (0, 1), 'goal': (3, 3)})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(2) # 0=left / 1=down / 2=right / 3=up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89d4775d-fe17-4628-a97f-33ab2799573d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßäüßëüßäüßä\n",
      "üßäüï≥üßäüï≥\n",
      "üßäüßäüßäüï≥\n",
      "üï≥üßäüßä‚õ≥Ô∏è\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b46b9c2-6ebe-4d37-b809-c38bd711126c",
   "metadata": {},
   "source": [
    "√áa a l'air bien !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be4064b-3f24-48b1-aa6e-184f655067d0",
   "metadata": {},
   "source": [
    "#### Tester notre impl√©mentation\n",
    "\n",
    "Comparons directement les deux environnements :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3aff2e55-17f6-48bc-8fb8-eaade93881a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter | gym obs / our obs | gym reward / our reward | gym done / our done\n",
      " 0   |       0 /  0      |          0 / 0        |      False / False\n",
      " 1   |       1 /  1      |          0 / 0        |      False / False\n",
      " 2   |       2 /  2      |          0 / 0        |      False / False\n",
      " 3   |       6 /  6      |          0 / 0        |      False / False\n",
      " 4   |      10 / 10      |          0 / 0        |      False / False\n",
      " 5   |      14 / 14      |          0 / 0        |      False / False\n",
      " 6   |      14 / 14      |          0 / 0        |      False / False\n",
      " 7   |      15 / 15      |          1 / 1        |       True /  True\n"
     ]
    }
   ],
   "source": [
    "lake = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
    "pond = FrozenPond()\n",
    "\n",
    "lake.reset()\n",
    "pond.reset()\n",
    "\n",
    "print(\"Iter | gym obs / our obs | gym reward / our reward | gym done / our done\")\n",
    "for i, a in enumerate([0, 2, 2, 1, 1, 1, 1, 2]):\n",
    "    lake_obs, lake_rew, lake_done, _ = lake.step(a)\n",
    "    pond_obs, pond_rew, pond_done, _ = pond.step(a)\n",
    "    print(\"%2d   |      %2d / %2d      |          %d / %d        |      %5s / %5s\" % \\\n",
    "          (i, lake_obs, pond_obs, lake_rew, pond_rew, lake_done, pond_done))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87185e66-37b4-4337-9046-17a17cc6efb2",
   "metadata": {},
   "source": [
    "Ils sont identiques !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5deb1e8-3de2-42dc-8363-aee8acf534e3",
   "metadata": {},
   "source": [
    "#### Tester notre impl√©mentation\n",
    "\n",
    "- RLlib est √©galement livr√© avec un v√©rificateur d'env\n",
    "- Cela ne nous dira pas si notre env est identique √† celle de Frozen Lake\n",
    "- Mais il effectuera plusieurs v√©rifications utiles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57e3cbe2-8354-4533-872d-41bd06ce2948",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.utils.pre_checks.env import check_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f4ebd66-5e52-4724-a9c3-df6efdb66eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-27 16:08:55,501\tWARNING env.py:142 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n"
     ]
    }
   ],
   "source": [
    "check_env(pond)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f674689-9df5-4da7-9379-478c2b802571",
   "metadata": {},
   "source": [
    "- Toutes les v√©rifications ont √©t√© pass√©es, sauf cet avertissement concernant la longueur maximale d'un √©pisode.\n",
    "- Nous pouvons/devrions le d√©finir pour que les √©pisodes ne puissent pas devenir arbitrairement longs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29218be1-9c76-4f04-bffd-e7288098bb74",
   "metadata": {},
   "source": [
    "#### Nombre maximum d'√©tapes par √©pisode\n",
    "\n",
    "- Pour d√©finir un nombre maximum de pas par √©pisode, nous pouvons utiliser un `gym` _wrapper_.\n",
    "- Les wrappers sont des moyens pratiques de modifier les environnements, y compris les observations, les actions et les r√©compenses.\n",
    "- Ici, nous allons utiliser le wrapper `TimeLimit` pour d√©finir une limite de pas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8edeeaf-5dd3-4eee-9ed0-747880709912",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import TimeLimit\n",
    "\n",
    "pond_5 = TimeLimit(pond, max_episode_steps=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37ac862-4df2-4f76-bbe0-b718e575f0e3",
   "metadata": {},
   "source": [
    "Nous pouvons v√©rifier que cela sera fait apr√®s 5 √©tapes, m√™me si le but n'est pas atteint :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be71dbd1-7668-4499-85d2-68a5b70fa1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, False, {'player': (0, 0), 'goal': (3, 3)})\n",
      "(0, 0, False, {'player': (0, 0), 'goal': (3, 3)})\n",
      "(0, 0, False, {'player': (0, 0), 'goal': (3, 3)})\n",
      "(0, 0, False, {'player': (0, 0), 'goal': (3, 3)})\n",
      "(0, 0, True, {'player': (0, 0), 'goal': (3, 3), 'TimeLimit.truncated': True})\n"
     ]
    }
   ],
   "source": [
    "pond_5.reset()\n",
    "for i in range(5):\n",
    "    print(pond_5.step(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f54f62a-2022-4bdf-926e-b855dc374ec6",
   "metadata": {},
   "source": [
    "#### Nombre maximum d'√©tapes par √©pisode\n",
    "\n",
    "Une limite de pas plus raisonnable pourrait √™tre de 50, plut√¥t que de 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff4847c8-ff8e-4e15-96b3-b0eb38781266",
   "metadata": {},
   "outputs": [],
   "source": [
    "pond_50 = TimeLimit(pond, max_episode_steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9eadc6-381d-47ac-82ec-f418b6830e6f",
   "metadata": {},
   "source": [
    "- Pour info : il est √©galement possible de d√©finir cette limite dans RLlib, juste √† des fins d'entra√Ænement.\n",
    "- Cela se fait avec le param√®tre `\"horizon\"` dans la configuration de l'entra√Æneur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aed5b6c-a486-4f1c-96e0-e027710461f6",
   "metadata": {},
   "source": [
    "#### Appliquons ce que nous avons appris !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456c51dc-3d2c-4e06-be76-62c279e976b0",
   "metadata": {},
   "source": [
    "## Les r√©compenses de Frozen Pond\n",
    "<!-- multiple choice -->\n",
    "\n",
    "Dans le lac (et l'√©tang) gel√©, la r√©compense est de 1 lorsque l'agent atteint l'objectif, et de 0 sinon. L'agent doit apprendre √† √©viter les trous, mais il n'y a en fait aucune r√©compense n√©gative √† tomber dans un trou - c'est la m√™me r√©compense nulle que de marcher dans un morceau s√ªr du lac gel√© ! Pourquoi cette configuration fonctionne-t-elle toujours, m√™me si la r√©compense est la m√™me pour marcher dans un trou ou sur la terre ferme ?\n",
    "\n",
    "- [ ] Une fois que l'agent est tomb√© dans un trou, il est coinc√©. Il peut faire d'autres actions, mais elles ne font rien. L'agent apprend donc √† √©viter les trous.\n",
    "- [ ] Une r√©compense de 0 est la plus petite r√©compense possible ; par cons√©quent, lorsque l'agent re√ßoit une r√©compense de 0 en tombant dans un trou, il sait imm√©diatement que tomber dans un trou est une mauvaise chose.\n",
    "- [La p√©nalit√© li√©e au fait de tomber dans un trou est indirecte, car l'√©pisode se termine par une r√©compense de z√©ro, ce qui signifie qu'il renonce √† la r√©compense potentielle de 1 qu'il pourrait obtenir en atteignant son objectif. L'agent apprend qu'en tombant dans un trou, il perd des r√©compenses _futures_.\n",
    "- [Les agents RL pr√©f√®rent les √©pisodes plus longs. Lorsque l'agent tombe dans le trou, l'√©pisode se termine imm√©diatement, ce que l'agent apprend √† √©viter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d53229-6adf-4b71-b8dd-40157b9a703d",
   "metadata": {},
   "source": [
    "## √âtang contre labyrinthe\n",
    "<!-- coding exercise -->\n",
    "\n",
    "Disons que nous voulons transformer notre environnement d'√©tang en un environnement de labyrinthe. Dans ce cas, nous avons des murs au lieu de trous. La seule diff√©rence entre l'√©tang et le labyrinthe est le comportement des trous par rapport aux murs. Dans l'√©tang gel√©, marcher dans un trou met fin √† l'√©pisode. Dans l'environnement du labyrinthe, marcher dans un mur ne fait rien (c'est-√†-dire que l'action ne change pas l'emplacement de l'agent, tout comme essayer de marcher sur le bord de la carte). Pour transformer notre lac gel√© en labyrinthe, nous devrons modifier deux m√©thodes : `done` et `is_valid_loc`.\n",
    "\n",
    "Tu trouveras ci-dessous les m√©thodes `done` et `step` que nous avons vues dans les diapositives ci-dessus. Modifie-les pour que nous ayons maintenant un labyrinthe avec le comportement d√©crit ci-dessus : marcher dans un mur ne fait rien.\n",
    "\n",
    "Note que la classe `Maze` h√©rite de toutes les autres m√©thodes de `FrozenPond`, tu peux donc la tester !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "238c70d1-4cd2-4cb0-ba58-3599ffa0879e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 0, True, {'player': (1, 1), 'goal': (3, 3)})\n",
      "(5, 0, True, {'player': (1, 1), 'goal': (3, 3)})\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE\n",
    "from envs_03 import FrozenPond\n",
    "\n",
    "\n",
    "class Maze(FrozenPond):\n",
    "    def done(self):\n",
    "        return self.player == self.goal or self.holes[self.player] == 1\n",
    "    def is_valid_loc(self, location):\n",
    "        if 0 <= location[0] <= 3 and 0 <= location[1] <= 3:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "pond = FrozenPond()\n",
    "pond.reset()\n",
    "pond.step(1)\n",
    "print(pond.step(2))\n",
    "\n",
    "maze = Maze()\n",
    "maze.reset()\n",
    "maze.step(1)\n",
    "print(maze.step(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a483f55-b956-44c1-bf8e-d534e6ad3ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 0, True, {'player': (1, 1), 'goal': (3, 3)})\n",
      "(4, 0, False, {'player': (1, 0), 'goal': (3, 3)})\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "from envs_03 import FrozenPond\n",
    "\n",
    "\n",
    "class Maze(FrozenPond):   \n",
    "    def done(self):\n",
    "        return self.player == self.goal\n",
    "    def is_valid_loc(self, location):\n",
    "        if 0 <= location[0] <= 3 and 0 <= location[1] <= 3 and not self.holes[location]:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "pond = FrozenPond()\n",
    "pond.reset()\n",
    "pond.step(1)\n",
    "print(pond.step(2))\n",
    "\n",
    "maze = Maze()\n",
    "maze.reset()\n",
    "maze.step(1)\n",
    "print(maze.step(2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ray200]",
   "language": "python",
   "name": "conda-env-ray200-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
