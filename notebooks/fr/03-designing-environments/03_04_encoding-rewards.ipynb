{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e23df55-837b-4ec9-86ad-3b60c6a7f00d",
   "metadata": {},
   "source": [
    "## Encodage des r√©compenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28d9b458-b46e-48de-ab18-bf884c1eab85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import ray\n",
    "import logging\n",
    "ray.init(log_to_driver=False, ignore_reinit_error=True, logging_level=logging.ERROR); # logging.FATAL\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c8c796-3785-44d1-8aea-78872152b0cb",
   "metadata": {},
   "source": [
    "#### R√©compenses pour le codage\n",
    "\n",
    "- Nous avons maintenant discut√© de l'importance de coder les observations.\n",
    "- Nous pouvons aussi avoir un certain choix sur l'espace d'action, bien qu'ici (et souvent) il soit relativement clair/fixe.\n",
    "- Mais qu'en est-il des r√©compenses ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e790dc-5146-4b03-a77a-24efd69ffabd",
   "metadata": {},
   "source": [
    "#### Mise en place du courant\n",
    "\n",
    "- Actuellement, nous recevons une r√©compense de +1 pour avoir atteint l'objectif \n",
    "- C'est une partie de ce qui rend RL si difficile (et impressionnant) :\n",
    "  - Nous voulons apprendre des actions m√™me si nous ne savons pas tout de suite si l'action √©tait \"bonne\" \n",
    "  - Compare cela √† l'apprentissage supervis√©, o√π chaque pr√©diction que nous faisons sur les donn√©es de formation peut imm√©diatement √™tre compar√©e √† la valeur cible connue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c10944be-9746-44d6-bb2b-66843ea7e4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: perhaps this next slide can be moved to Module 1, since it's very general?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff6ac9f-6815-4910-b3ce-aa7e0866101c",
   "metadata": {},
   "source": [
    "#### Les agents ne peuvent pas √™tre simplement gourmands\n",
    "\n",
    "- Les agents peuvent-ils simplement apprendre √† rechercher la meilleure r√©compense imm√©diate ?\n",
    "- Non. Par exemple, dans un syst√®me de recommandation de vid√©os, montrer √† l'utilisateur une autre vid√©o de chat dr√¥le pourrait le faire cliquer (r√©compense imm√©diate √©lev√©e) mais entra√Æner une perte d'int√©r√™t √† long terme pour le service (faible r√©compense √† long terme).\n",
    "- Notre lac gel√© est un autre exemple de ce probl√®me : parfois, il n'y a pas du tout de r√©compense imm√©diate dont on peut tirer un enseignement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c44cea8c-a28e-4d7c-a0c8-91c1c3f8fc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: perhaps this next section on \"Learned action probabilities\" could be moved much earlier, even as early as Module 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462e6340-cb1f-4eda-b642-d21505d18b52",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Probabilit√©s d'action apprises\n",
    "\n",
    "- RLlib nous permet de regarder √† l'int√©rieur du mod√®le la probabilit√© de chaque action compte tenu d'une observation (c'est-√†-dire la politique apprise).\n",
    "- Chargeons le mod√®le form√© avec nos observations cod√©es :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16d9d371-0c26-46a0-90c5-6f26e25880f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from envs_03 import RandomLakeObs\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "ppo_config = (\n",
    "    PPOConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(create_env_on_local_worker=True, horizon=100)\\\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\n",
    ")\n",
    "ppo_RandomLakeObs = ppo_config.build(env=RandomLakeObs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1420e1fd-22e9-474c-aad6-928428976b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # HIDDEN\n",
    "\n",
    "# for i in range(16):\n",
    "#     ppo_RandomLakeObs.train()\n",
    "    \n",
    "# print(ppo_RandomLakeObs.evaluate()[\"evaluation\"][\"episode_reward_mean\"])\n",
    "\n",
    "# ppo_RandomLakeObs.save(\"models/RandomLakeObs-Ray2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a71119d3-e3c9-427c-a697-fe733fc10ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_RandomLakeObs.restore(\"models/RandomLakeObs-Ray2/checkpoint_000016\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fec9d02-7e22-44ee-b171-2d9bf86b8681",
   "metadata": {},
   "source": [
    "#### Probabilit√©s d'action apprises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66648b5d-abd8-45ef-b592-e56bdee1c9c9",
   "metadata": {},
   "source": [
    "Nous utiliserons la fonction `query_Policy` du module 2 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84b56415-177b-414f-8985-1118cabec0b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00902206, 0.5078786 , 0.47434822, 0.00875122], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils_03 import query_policy\n",
    "query_policy(ppo_RandomLakeObs, RandomLakeObs(), [0,0,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0256b6-3174-4719-931f-65342f5ae4a7",
   "metadata": {},
   "source": [
    "- Rappelle-toi l'ordonnancement (gauche, bas, droite, haut).\n",
    "- Lorsque l'observation est `[0 0 0 0]` (pas de trous ou de bords en vue), l'agent pr√©f√®re aller vers le bas et la droite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99a889a-37bd-4a7c-bf6b-c9eb297a1520",
   "metadata": {},
   "source": [
    "Et s'il y a un trou en dessous de toi ? Nous pouvons introduire une observation diff√©rente dans la politique :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc66f33e-8b23-4bcc-9f12-04b47d18aea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01965651, 0.00299437, 0.9645823 , 0.01276694], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_policy(ppo_RandomLakeObs, RandomLakeObs(), [0,1,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6560c87a-c07e-4aec-a172-915cad2b376c",
   "metadata": {},
   "source": [
    "- Maintenant, il est tr√®s peu probable que l'agent descende et tr√®s probable qu'il aille √† droite !\n",
    "- Encore une fois, tout cela a √©t√© appris par essais et erreurs, avec une r√©compense obtenue uniquement lorsque l'objectif a √©t√© atteint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffae4393-3058-40ce-821a-fe1e487cf4d4",
   "metadata": {},
   "source": [
    "#### R√©compenses Random Lake\n",
    "\n",
    "- Dans l'exemple de Random Lake, ne peut-on pas faciliter la vie de l'agent en lui donnant des r√©compenses imm√©diates ?\n",
    "\n",
    "Voici le code de r√©compense actuel :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bb56001-259b-4b88-8bdd-29746e208042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward(self):\n",
    "    return int(self.player == self.goal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1ee605-1e87-4080-b8ee-274816dd67fd",
   "metadata": {},
   "source": [
    "- L'agent doit apprendre, par essais et erreurs au cours de _vastes √©pisodes_, que se d√©placer vers le bas et la droite est g√©n√©ralement une bonne chose "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0cf72e-d732-4039-90cc-22a17360943f",
   "metadata": {},
   "source": [
    "#### Red√©finir les r√©compenses\n",
    "\n",
    "- Essayons plut√¥t de donner une r√©compense _√† chaque √©tape, qui est plus √©lev√©e √† mesure que l'agent se rapproche de l'objectif_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9a0466b-44fd-43ed-a3c0-d93334748213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from envs_03 import RandomLakeObs\n",
    "\n",
    "class RandomLakeObsRew(RandomLakeObs):\n",
    "    def reward(self):\n",
    "        return 6-(abs(self.player[0]-self.goal[0]) + abs(self.player[1]-self.goal[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce59aa5-1f90-457f-a3f9-c970cd8a7b7b",
   "metadata": {},
   "source": [
    "- La m√©thode ci-dessus utilise la [Distance de Manhattan] (https://en.wikipedia.org/wiki/Taxicab_geometry) entre le joueur et l'objectif comme r√©compense \n",
    "- Lorsque l'agent atteint l'objectif, la r√©compense maximale de 6 est obtenue.\n",
    "- Lorsque l'agent est le plus √©loign√© du but, la r√©compense minimale de 0 est donn√©e."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75f7b77-8962-43fe-a82b-647e4f2aee69",
   "metadata": {},
   "source": [
    "#### Red√©finir les r√©compenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcba334f-f2a3-4cb0-8b8a-f5dd69a3346f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßëüßäüßäüßä\n",
      "üßäüßäüßäüßä\n",
      "üï≥üßäüï≥üßä\n",
      "üßäüßäüßä‚õ≥Ô∏è\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = RandomLakeObsRew()\n",
    "env.reset()\n",
    "env.render()\n",
    "env.reward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5208f2-c7e9-4f1e-9ed7-e91daec89d8e",
   "metadata": {},
   "source": [
    "‚¨ÜÔ∏è la r√©compense est de 0\n",
    "\n",
    "‚¨áÔ∏è la r√©compense est de 1 car nous nous sommes rapproch√©s de l'objectif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "088715f0-3f06-4231-a149-401b6518dcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßäüßäüßäüßä\n",
      "üßëüßäüßäüßä\n",
      "üï≥üßäüï≥üßä\n",
      "üßäüßäüßä‚õ≥Ô∏è\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, rew, done, _ = env.step(1)\n",
    "env.render()\n",
    "rew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e607ab59-e697-44b6-99a6-91a7a3a3d2b2",
   "metadata": {},
   "source": [
    "#### Red√©finir les r√©compenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95ab62b7-e2b5-4f6e-8f81-9381b605a9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßäüßäüßäüßä\n",
      "üßäüßäüßäüßä\n",
      "üï≥üßäüï≥üßë\n",
      "üßäüßäüßä‚õ≥Ô∏è\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, rew, done, _ = env.step(2)\n",
    "obs, rew, done, _ = env.step(2)\n",
    "obs, rew, done, _ = env.step(2)\n",
    "obs, rew, done, _ = env.step(1)\n",
    "env.render()\n",
    "rew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1a183d-af4c-4c9d-a6d3-4bc1a7bfecca",
   "metadata": {},
   "source": [
    "Maintenant, la r√©compense est de 5. La prochaine fois, elle sera de 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "904e40cb-55b6-4b45-b063-5b335123e071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßäüßäüßäüßä\n",
      "üßäüßäüßäüßä\n",
      "üï≥üßäüï≥üßä\n",
      "üßäüßäüßäüßë\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, rew, done, _ = env.step(1)\n",
    "env.render()\n",
    "rew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ac0aec-ddc4-4d44-833e-c2b2e253120f",
   "metadata": {},
   "source": [
    "#### Comparer les r√©compenses\n",
    "\n",
    "- Nous avons donc deux fonctions de r√©compense possibles. Laquelle fonctionne le mieux ? \n",
    "- Rappelle-toi que la derni√®re fois, apr√®s un entra√Ænement de 8 it√©rations, nous avons r√©ussi √† atteindre l'objectif environ 70 % du temps :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5dbcf34-37e8-4025-bcd2-7a308d77046f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8166666666666667"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_RandomLakeObs.evaluate()['evaluation']['episode_reward_mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c75b369-6e96-4192-b7cd-4216f354d901",
   "metadata": {},
   "source": [
    "#### Comparer les r√©compenses\n",
    "\n",
    "Entra√Æne-toi avec la nouvelle fonction de r√©compense !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01c1afa7-4452-43e4-ad2d-e3dc33934fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_RandomLakeObsRew = ppo_config.build(env=RandomLakeObsRew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7478ee6-28d8-4e44-932f-1b4f47e6b911",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    ppo_RandomLakeObsRew.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "257cef04-752e-4fe1-9eb6-3e225ea1ba62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101.90566037735849"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_RandomLakeObsRew.evaluate()['evaluation']['episode_reward_mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bccc6f4-1e9a-4abd-af7a-08b4c6103e4b",
   "metadata": {},
   "source": [
    "Attends une minute, qu'est-ce qui se passe ici ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874a6979-1882-4bb4-a5cf-a83282ed15f3",
   "metadata": {},
   "source": [
    "#### Comparer les r√©compenses ?\n",
    "\n",
    "- Nous avons essay√© d'am√©liorer notre syst√®me RL en fa√ßonnant la fonction de r√©compense.\n",
    "- Cela a (vraisemblablement) affect√© la formation, mais aussi notre √©valuation.\n",
    "- Dans l'apprentissage supervis√©, cela revient √† changer la m√©trique de notation de l'erreur quadratique √† l'erreur absolue.\n",
    "- Si l'ancien syst√®me a obtenu une erreur quadratique moyenne de 20 000 et le nouveau syst√®me une erreur absolue moyenne de 40, lequel est le meilleur ?\n",
    "- Nous comparons des pommes et des oranges ici !\n",
    "- Nous voulons comparer les deux mod√®les sur la m√™me m√©trique, par exemple la m√©trique originale \n",
    "- Ici, nous voulons voir √† quelle fr√©quence l'agent atteint l'objectif."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf05802-842d-4764-b408-72559e7bb8cf",
   "metadata": {},
   "source": [
    "#### Comparer les r√©compenses ?\n",
    "\n",
    "- Le code ici est un peu plus avanc√©.\n",
    "- Il est inclus par souci d'exhaustivit√©, mais nous n'entrerons pas dans les d√©tails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "261074ea-c6db-4e1b-a35f-a647e8cf2776",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.agents.callbacks import DefaultCallbacks\n",
    "\n",
    "class MyCallbacks(DefaultCallbacks):\n",
    "    def on_episode_end(self, *, worker, base_env, policies, episode, env_index, **kwargs):\n",
    "        info = episode.last_info_for()\n",
    "        episode.custom_metrics[\"goal_reached\"] = info[\"player\"] == info[\"goal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3ab7223-77b0-46f0-af5f-669b18760a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_config_callback = (\n",
    "    PPOConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(create_env_on_local_worker=True, horizon=100)\\\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\\\n",
    "    .callbacks(callbacks_class=MyCallbacks)\\\n",
    "    .evaluation(evaluation_config={\"callbacks\" : MyCallbacks})\n",
    ")\n",
    "\n",
    "ppo_RandomLakeObsRew = ppo_config_callback.build(env=RandomLakeObsRew)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90980e8e-2ef2-479d-9c55-cbde6fcdebe1",
   "metadata": {},
   "source": [
    "Le formateur ci-dessus utilise notre nouveau syst√®me de r√©compense mais signale/mesure √©galement le taux d'atteinte de l'objectif."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f898830-4600-4ddf-8f05-073f7cb8e842",
   "metadata": {},
   "source": [
    "#### Comparer les r√©compenses ?\n",
    "\n",
    "Essayons-le !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f118473-544e-43a6-9ead-f742d0ee8fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    ppo_RandomLakeObsRew.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f52dd086-12bb-4482-88a2-30df5240573b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101.90566037735849"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HIDDEN\n",
    "ppo_RandomLakeObsRew.evaluate()[\"evaluation\"][\"episode_reward_mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0021f054-0f77-46c8-ac1c-f6e1f40ff509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04081632653061224"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_RandomLakeObsRew.evaluate()[\"evaluation\"][\"custom_metrics\"][\"goal_reached_mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36e3784-b860-49a2-b3d1-b48a760e756c",
   "metadata": {},
   "source": [
    "- Hmm, ces r√©sultats sont terribles !\n",
    "- Nous avions l'habitude d'obtenir un taux de victoire de plus de 70%, et maintenant nous sommes proches de z√©ro.\n",
    "- Que s'est-il pass√© ? ü§î"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398ee5c9-7e5c-4370-912c-b90d8864cac4",
   "metadata": {},
   "source": [
    "#### Qu'est-ce que l'agent optimise vraiment ?\n",
    "\n",
    "- L'agent optimise vraiment la _r√©compense totale_.\n",
    "- _Total_ : il valorise toutes les r√©compenses qu'il collecte, pas seulement la r√©compense finale.\n",
    "- _R√©compens√©_ : il valorise davantage les premi√®res r√©compenses que les derni√®res.\n",
    "- Notre agent r√©ussit √† maximiser la r√©compense totale actualis√©e, mais cela ne correspond pas √† l'atteinte de l'objectif.\n",
    "- Mais pourquoi ? L'objectif donne une r√©compense plus √©lev√©e."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cee4522-4906-4a36-b5e8-714761a11b04",
   "metadata": {},
   "source": [
    "#### Exploration vs. exploitation\n",
    "\n",
    "- Un concept fondamental en RL est _exploration vs. exploitation_\n",
    "- Lorsque l'agent apprend la politique, il peut choisir de soit :\n",
    "\n",
    "1. Faire des choses qu'il sait √™tre assez bonnes (\"exploiter\")\n",
    "2. Essayer quelque chose de totalement nouveau et fou, juste au cas o√π (\"explorer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f02fef1-3506-4c10-ac66-cd4150b32d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO \n",
    "# diagram for this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954a891d-f16c-43bb-b538-ce246829d6fb",
   "metadata": {},
   "source": [
    "#### Exploration vs. exploitation\n",
    "\n",
    "- Avec l'ancienne structure de r√©compense, l'agent re√ßoit une r√©compense de 0 √† moins qu'il n'atteigne l'objectif.\n",
    "  - Il continue donc √† essayer de trouver quelque chose de mieux.\n",
    "- Avec la nouvelle structure de r√©compense, l'agent re√ßoit beaucoup de r√©compenses simplement parce qu'il se prom√®ne.\n",
    "  - Il n'est pas tr√®s motiv√© pour explorer l'environnement.\n",
    "- En fait, comme il maximise la r√©compense **totale** actualis√©e, trouver le but est une mauvaise chose !\n",
    "  - Cela entra√Æne la fin de l'√©pisode, ce qui limite la r√©compense totale de l'agent.\n",
    "  - L'agent apprend en fait √† _√©viter_ le but, surtout au d√©but de l'√©pisode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83b297e-b4a6-4989-b623-5f53b52562c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Concevoir une meilleure structure de r√©compense\n",
    "\n",
    "- Essayons plut√¥t de p√©naliser l'agent lorsqu'il entre dans un trou ou sort du bord.\n",
    "- Il sera plus facile d'impl√©menter cela directement dans `step` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c41fd832-6144-41cc-9643-e7f3eb8737d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomLakeObsRew2(RandomLakeObs):\n",
    "    def step(self, action):\n",
    "        # (not shown) existing code gets new_loc, where the player is trying to go\n",
    "        \n",
    "        reward = 0\n",
    "        \n",
    "        if self.is_valid_loc(new_loc):\n",
    "            self.player = new_loc\n",
    "        else:\n",
    "            reward -= 0.1 # small penalty\n",
    "            \n",
    "        if self.holes[self.player]:\n",
    "            reward -= 0.1 # small penalty\n",
    "            \n",
    "        if self.player == self.goal:\n",
    "            reward += 1\n",
    "        \n",
    "        # Return observation/reward/done\n",
    "        return self.observation(), reward, self.done(), {\"player\" : self.player, \"goal\" : self.goal}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5622237f-517f-4a1c-99f0-0e7a0ede3e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "from envs_03 import RandomLakeObsRew2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804b515a-f635-4f48-8e0e-7774b003533a",
   "metadata": {},
   "source": [
    "#### Teste-le, encore une fois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c20f637-e0f8-4ed5-8133-d49d59d504f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "# redefine ppo_RandomLakeObs to include the new callbacks\n",
    "# so that you can measure the custom metric instead of the reward\n",
    "# they will give the same value but this is better for consistency\n",
    "ppo_RandomLakeObs = ppo_config_callback.build(env=RandomLakeObs)\n",
    "\n",
    "for i in range(8):\n",
    "    ppo_RandomLakeObs.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7459cba1-5771-4d26-b215-4f51df7a7ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_RandomLakeObsRew2 = ppo_config_callback.build(env=RandomLakeObsRew2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dbd1ac25-6d5c-4ed9-a49c-c9564b92f08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    ppo_RandomLakeObsRew2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cdbb4747-9b58-487b-9353-1a1e8a2022c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6853448275862069"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_RandomLakeObs.evaluate()[\"evaluation\"][\"custom_metrics\"][\"goal_reached_mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8e32e1de-b153-4fef-a208-a0d574d547c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.734982332155477"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_RandomLakeObsRew2.evaluate()[\"evaluation\"][\"custom_metrics\"][\"goal_reached_mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8ae49b-73af-43b4-92fa-cd3f608b504d",
   "metadata": {},
   "source": [
    "Il semble que, cette fois, les deux m√©thodes ont des performances beaucoup plus similaires."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3481f6-4505-48f6-a787-33146ff663af",
   "metadata": {},
   "source": [
    "#### Dur√©e de l'√©pisode\n",
    "\n",
    "- En plus du taux de r√©ussite, nous pouvons calculer d'autres statistiques sur le comportement de l'agent.\n",
    "- Une mesure int√©ressante est la longueur des √©pisodes.\n",
    "- RLlib l'enregistre par d√©faut, nous pouvons donc y acc√©der facilement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e8819cf4-bc66-4ce7-a723-ea6127d4c1cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.585470085470085"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_RandomLakeObs.evaluate()[\"evaluation\"][\"episode_len_mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "879d945f-57e6-409e-8f8a-e0d95e6cfe4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.20216606498195"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_RandomLakeObsRew2.evaluate()[\"evaluation\"][\"episode_len_mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de01c11-01b5-4f86-9f9e-9e2e43a3d62d",
   "metadata": {},
   "source": [
    "Bien que les deux agents aient le m√™me taux de r√©ussite, le nouveau tend vers des √©pisodes plus courts.\n",
    "\n",
    "Notes \n",
    "\n",
    "- C'est tr√®s int√©ressant car l'agent ne peut pas \"voir\" la diff√©rence entre les trous et les bords.\n",
    "- Nous pourrions approfondir cette question en ajoutant d'autres mesures personnalis√©es, par exemple le nombre de bosses dans l'ar√™te."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd02d93-0255-4ae2-aa46-5a9eb5111fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "#### disadvantages - loss of generality\n",
    "\n",
    "#- now only works if goal is at bottom-right\n",
    "#give a few real-world examples here -> important"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf32143e-7c32-49d2-9aed-f1081c5b8c35",
   "metadata": {},
   "source": [
    "## Analogie de l'apprentissage supervis√© : fa√ßonnage de la r√©compense\n",
    "<!-- multiple choice -->\n",
    "\n",
    "Plus t√¥t, nous avons fait une analogie entre l'encodage des observations en RL et le pr√©traitement des caract√©ristiques en apprentissage supervis√©. Quel aspect de l'apprentissage supervis√© est la meilleure analogie avec la mise en forme des r√©compenses en RL ?\n",
    "\n",
    "- [ ] Ing√©nierie des caract√©ristiques \n",
    "- [ ] S√©lection de mod√®les [ ] Pas tout √† fait. Mais, comme nous le verrons, la s√©lection de mod√®les a aussi sa place dans l'apprentissage supervis√© !\n",
    "- [ ] R√©glage des hyperparam√®tres [ ] Pas tout √† fait. Mais, comme nous le verrons, l'ajustement des hyperparam√®tres a √©galement sa place dans RL !\n",
    "- [S√©lection d'une fonction de perte | Changer la fonction de perte change le \"meilleur\" mod√®le, tout comme changer les r√©compenses change la \"meilleure\" politique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c458b5c7-bc0a-4084-8572-d35bf27ddc60",
   "metadata": {},
   "source": [
    "## R√©compenser chaque √©tape : petites r√©compenses n√©gatives\n",
    "<!-- multiple choice -->\n",
    "\n",
    "Dans des environnements RL comme Random Lake o√π l'agent doit atteindre un objectif sp√©cifique, imagine que nous attribuions une minuscule r√©compense n√©gative pour _chaque_ √©tape effectu√©e par l'agent. Comment cela affecterait-il g√©n√©ralement/typiquement le temps que l'agent passe jusqu'√† ce qu'il atteigne l'objectif ?\n",
    "\n",
    "- [ ] L'agent essaiera d'atteindre l'objectif en faisant le moins d'√©tapes possible.\n",
    "- [ ] L'agent essaiera d'atteindre l'objectif en autant d'√©tapes que possible. | [ ] Si nous p√©nalisons chaque √©tape, le fait de faire plus d'√©tapes entra√Ænera une r√©compense moindre.\n",
    "- [ ] Aucun changement. | Si nous p√©nalisons chaque √©tape, le fait de faire plus d'√©tapes entra√Ænera une r√©compense moindre."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96265e6-b392-4923-b694-088fb282efb0",
   "metadata": {},
   "source": [
    "## Exploration vs. exploitation\n",
    "<!-- multiple choice -->\n",
    "\n",
    "Laquelle des affirmations suivantes est correcte concernant le compromis exploration-exploitation dans RL ?\n",
    "\n",
    "- [ ] Si une personne n'explore que, elle ne trouvera jamais une bonne politique. | Il trouvera de bonnes politiques en fait, mais EXTR√äMEMENT lentement.\n",
    "- [ ] Si un agent ne fait qu'exploiter, il ne trouvera jamais de bonne politique. | Il peut simplement continuer √† essayer la m√™me chose encore et encore.\n",
    "- [ ] Les agents trouvent toujours de bonnes politiques m√™me sans exploration/exploitation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93d7454-eaff-4c79-8398-19cb541b4b37",
   "metadata": {},
   "source": [
    "## Cons√©quences involontaires\n",
    "<!-- coding exercise -->\n",
    "\n",
    "Dans cet exercice, tu vas essayer une mauvaise id√©e : attribuer une grande r√©compense n√©gative chaque fois que l'agent fait un pas. Nous utiliserons -1 par √©tape. L'agent re√ßoit quand m√™me une r√©compense de +1 pour avoir atteint l'objectif. Mets en place cette r√©compense, entra√Æne l'agent et regarde la longueur moyenne des √©pisodes imprim√©e par le code. Compare-le √† la dur√©e moyenne des √©pisodes d'un agent qui agit simplement de mani√®re al√©atoire. Ensuite, r√©ponds √† la question √† choix multiple sur le comportement de l'agent. √Ä ton avis, que se passe-t-il ici ? \n",
    "\n",
    "(Pour info : comme nous l'avons vu pr√©c√©demment, ce type de modification d'un environnement peut aussi √™tre r√©alis√© avec des wrappers de gymnastique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f8434a-7575-4bfb-af7b-9858f580ae7b",
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "from utils_03 import lake_default_config\n",
    "from envs_03 import RandomLakeObs\n",
    "\n",
    "class RandomLakeBadIdea(RandomLakeObs):\n",
    "    def reward(self):\n",
    "        old_reward = int(self.player == self.goal) \n",
    "        return ____\n",
    "    \n",
    "ppo = lake_default_config.build(env=____)\n",
    "\n",
    "for i in range(8):\n",
    "    ppo.train()\n",
    "    \n",
    "print(\"Average episode length for trained agent: %.1f\" % \n",
    "      ppo.evaluate()[\"evaluation\"][____])\n",
    "\n",
    "random_agent_config = (\n",
    "    lake_default_config\\\n",
    "    .exploration(exploration_config={\"type\": \"Random\"})\\\n",
    "    .evaluation(evaluation_config={\"explore\" : True})\n",
    ")\n",
    "random_agent = random_agent_config.build(env=RandomLakeBadIdea)\n",
    "\n",
    "print(\"Average episode length for random agent: %.1f\" % \n",
    "      random_agent.evaluate()[\"evaluation\"][____])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "265575eb-94ef-4899-b7c0-c00c70c155e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "Average episode length for trained agent: 4.4\n",
      "Average episode length for random agent: 12.1\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "from utils_03 import lake_default_config\n",
    "from envs_03 import RandomLakeObs\n",
    "\n",
    "class RandomLakeBadIdea(RandomLakeObs):\n",
    "    def reward(self):\n",
    "        old_reward = int(self.player == self.goal) \n",
    "        return old_reward - 1\n",
    "\n",
    "ppo = lake_default_config.build(env=RandomLakeBadIdea)\n",
    "\n",
    "\n",
    "for i in range(8):\n",
    "    ppo.train()\n",
    "    \n",
    "print(\"Average episode length for trained agent: %.1f\" % \n",
    "      ppo.evaluate()[\"evaluation\"][\"episode_len_mean\"])\n",
    "\n",
    "random_agent_config = (\n",
    "    lake_default_config\\\n",
    "    .exploration(exploration_config={\"type\": \"Random\"})\\\n",
    "    .evaluation(evaluation_config={\"explore\" : True})\n",
    ")\n",
    "random_agent = random_agent_config.build(env=RandomLakeBadIdea)\n",
    "\n",
    "print(\"Average episode length for random agent: %.1f\" % \n",
    "      random_agent.evaluate()[\"evaluation\"][\"episode_len_mean\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dbab96-1590-409b-b684-d6a05c880c1c",
   "metadata": {},
   "source": [
    "#### Comportement de l'agent\n",
    "\n",
    "Lorsqu'il est entra√Æn√© dans un environnement avec une grande r√©compense n√©gative √† chaque √©tape, que crois-tu que cet agent fait, qui est ind√©sirable ?\n",
    "\n",
    "- [L'agent reste immobile car on le d√©courage de bouger. | Essaie encore !\n",
    "- [ ] L'agent n'est pas int√©ress√© par l'atteinte de l'objectif car la r√©compense est relativement faible. | Essaie encore !\n",
    "- [ ] L'agent apprend √† sauter dans le lac aussi vite qu'il le peut, pour √©viter la r√©compense n√©gative du mouvement. | Yikes ! ü•∂\n",
    "- [ ] L'agent atteint l'objectif tout de suite. | Ce serait pourtant souhaitable !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ray200]",
   "language": "python",
   "name": "conda-env-ray200-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
