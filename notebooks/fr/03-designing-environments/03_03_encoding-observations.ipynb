{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e23df55-837b-4ec9-86ad-3b60c6a7f00d",
   "metadata": {},
   "source": [
    "## Encodage des observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58bc4b75-8b99-4450-9b25-6af748253913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import ray\n",
    "import logging\n",
    "ray.init(log_to_driver=False, ignore_reinit_error=True, logging_level=logging.ERROR); # logging.FATAL\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95a9faf-9b2c-45ec-a347-3aadca399516",
   "metadata": {},
   "source": [
    "#### Révision : qu'est-ce qu'une politique ?\n",
    "\n",
    "- Dans RL, nous essayons d'apprendre une politique, qu'est-ce que c'est déjà, exactement ?\n",
    "- Une politique fait correspondre des **observations** à des **actions**.\n",
    "- En d'autres termes, les observations sont tout ce que la politique \"voit\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacebed5-729c-4ea8-8570-3ac70481723b",
   "metadata": {},
   "source": [
    "#### Politiques du lac aléatoire\n",
    "\n",
    "- Quelles sont les observations dans le lac aléatoire ?\n",
    "- Elles sont l'emplacement du joueur, représenté par un nombre entier de 0 à 15  \n",
    "- En guise de rappel du module 1, une politique déterministe pourrait ressembler à ceci :\n",
    "\n",
    "| Observation | Action |\n",
    "|------|-------|\n",
    "| 0 | 0 |\n",
    "| 1 | 3 |\n",
    "| 2 | 1 |\n",
    "| 3 | 1 |\n",
    "| ... | ... |\n",
    "| 14 | 2 |\n",
    "| 15 | 2 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0d4278-ce03-4e94-9f2a-667b88bf1e85",
   "metadata": {},
   "source": [
    "#### Politiques du lac aléatoire\n",
    "\n",
    "Une politique non déterministe pourrait ressembler à ceci :\n",
    "\n",
    "| Observation | P(gauche) | P(bas) | P(droite) | P(haut) | P(haut) | \n",
    "|------------|-------|-----------|---------|-------|\n",
    "| 0 | 0 | 0.9 | 0.01 | 0.04 | 0.05\n",
    "| 1 | 3 | 0.05 | 0.05 | 0.05 | 0.85\n",
    "| ... | ... | ... | ...      | ...      | ...\n",
    "| 15 | 2 | 0.0 | 0.0 | 0.99 | 0.01\n",
    "\n",
    "Cela ne signifie pas que RLlib apprend un tel tableau, d'ailleurs, mais nous pouvons penser à ce tableau de manière conceptuelle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b044dbcb-f7a2-4792-aaf2-2c72bda18af2",
   "metadata": {},
   "source": [
    "#### Politiques du lac aléatoire\n",
    "\n",
    "- Dans le lac aléatoire, toute notre décision doit être basée sur la position du joueur.\n",
    "- Parfois, cela suffit : depuis la position 11, tu devrais descendre.\n",
    "\n",
    "```\n",
    " 0 1 2 3\n",
    " 4 5 6 7\n",
    " 8 9 10 11\n",
    "12 13 14 15\n",
    "```\n",
    "\n",
    "- Mais qu'en est-il de la position 5, que dois-tu faire à partir de là ?\n",
    "- Réponse : _Cela dépend_. S'il y a un trou à la position 9, tu ne veux pas descendre. De même pour la position 6 \n",
    "- Comment puis-je décider _sans savoir où sont les trous_ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fac00c2-6d9b-442f-ad3c-f9212a0dffac",
   "metadata": {},
   "source": [
    "#### State vs. observation, un récapitulatif\n",
    "\n",
    "- Dans le module 1, nous avons défini l'état de manière informelle comme tout ce qui concerne l'environnement.\n",
    "- Ici, cela comprendrait l'emplacement du joueur et des trous.\n",
    "- L'observation, quant à elle, ne code qu'une partie de l'état : dans ce cas, l'emplacement du joueur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbe12b1-eded-4618-a5dc-77dfd9fb75a0",
   "metadata": {},
   "source": [
    "#### Observation = état ? Problème 1.\n",
    "\n",
    "- OK alors, pourquoi ne pas simplement définir l'observation sur l'état ? \n",
    "- Il y a deux problèmes ici.\n",
    "- Problème 1 : Lorsque le système RL est déployé, il se peut que tu ne connaisses pas tout l'état.\n",
    "  - Exemple : dans un système de recommandation, l'agent (le recommandeur) n'a pas accès à l'humeur de l'utilisateur (une partie de l'état qui affecte les résultats)\n",
    "  - Dans l'apprentissage supervisé, nous ne voulons pas nous entraîner sur des caractéristiques auxquelles nous n'aurons pas accès lors du déploiement\n",
    "    - De même ici, l'observation doit être quelque chose à laquelle nous pouvons accéder lors du déploiement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617197ea-fcfe-457e-a567-a1f876b0bb86",
   "metadata": {},
   "source": [
    "#### Observation = état ? Problème 2.\n",
    "\n",
    "- Problème 2 : Il peut être difficile de généraliser à partir d'une observation vraiment complexe.\n",
    "  - Il y a des centaines de milliers d'états possibles dans ce seul petit jeu de lac aléatoire 4x4.\n",
    "  - Trop d'informations pourraient être déroutantes pour l'agent ou nécessiter des quantités déraisonnables de données (simulations) pour avoir du sens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6e363c-9255-442c-a598-1d33199b317b",
   "metadata": {},
   "source": [
    "#### Observations sur le codage\n",
    "\n",
    "- Une partie de notre travail en tant que praticien du RL consiste à choisir une représentation (ou codage) pour l'observation.\n",
    "- À partir des informations que le joueur a permis de connaître, trouve une représentation utile de ce que le joueur doit savoir.\n",
    "- Dans notre cas, nous allons essayer une approche : le joueur a le droit de \"voir\" si les 4 espaces adjacents sont des trous ou non.\n",
    "- Nous coderons cela sous forme de 4 nombres binaires."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb9fb21-9ad8-4800-ab56-fc70eec7b5ec",
   "metadata": {},
   "source": [
    "#### Observations sur le codage\n",
    "\n",
    "```\n",
    "oO.\n",
    "....\n",
    "O.P.\n",
    "...G\n",
    "```\n",
    "\n",
    "- Dans cette situation, il n'y a pas de trous autour du joueur, donc le joueur \"voit\" `[0 0 0 0]` \n",
    "- En d'autres termes, l'observation ici est `[0 0 0 0]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a2fb46-4e64-47d7-9a3c-2af76319760c",
   "metadata": {},
   "source": [
    "#### Observations sur le codage\n",
    "\n",
    "```\n",
    ".OO.\n",
    "..P.\n",
    "O.O.\n",
    "...G\n",
    "```\n",
    "\n",
    "- Ici, le joueur \"voit\" les trous en haut et en bas, donc l'observation est `[0 1 0 1]` (gauche, bas, droite, haut)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114c2ad7-dc3d-45c8-9f59-46e55fd87e07",
   "metadata": {},
   "source": [
    "#### Observations sur le codage\n",
    "\n",
    "Qu'en est-il des bords ?\n",
    "\n",
    "```\n",
    "....\n",
    "..OP\n",
    "O.OO\n",
    "...G\n",
    "```\n",
    "\n",
    "- C'est notre choix lorsque nous concevons l'espace d'observation.\n",
    "- Je vais choisir de représenter \"hors réseau\" comme des trous, ce qui signifie que nous prétendons que le lac ressemble à ceci :\n",
    " \n",
    "```\n",
    "OOOOOO\n",
    "O....O\n",
    "O..OPO\n",
    "OO.OOO\n",
    "O...GO\n",
    "OOOOOO\n",
    "```\n",
    "\n",
    "- Ici, le joueur voit des trous à gauche, en bas et à droite, donc l'observation est `[1 1 1 0]` (gauche, bas, droite, haut)\n",
    "- Il pourrait cependant y avoir de meilleures approches, car tomber dans un trou est pire (l'épisode se termine) que marcher sur le bord (il ne se passe rien)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415a2223-64cb-43ca-befb-2269d33f9639",
   "metadata": {},
   "source": [
    "#### Coder nos observations\n",
    "\n",
    "- Maintenant que nous avons un plan, comment modifier le code ?\n",
    "- Puisque nous avons structuré notre classe pour avoir une méthode `observation`, c'est tout ce que nous devons modifier :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6046cff0-b83f-45b8-96ca-ab092b3c8599",
   "metadata": {},
   "outputs": [],
   "source": [
    "from envs_03 import RandomLake\n",
    "\n",
    "class RandomLakeObs(RandomLake):\n",
    "    def observation(self):\n",
    "        i, j = self.player\n",
    "\n",
    "        obs = []\n",
    "        obs.append(1 if j==0 else self.holes[i,j-1]) # left\n",
    "        obs.append(1 if i==3 else self.holes[i+1,j]) # down\n",
    "        obs.append(1 if j==3 else self.holes[i,j+1]) # right\n",
    "        obs.append(1 if i==0 else self.holes[i-1,j]) # up\n",
    "        \n",
    "        obs = np.array(obs, dtype=int) # cast to numpy array (optional)\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ee30d8-f9f9-4655-817d-84aebd8ca444",
   "metadata": {},
   "source": [
    "- Le code crée une variable `obs` où chaque entrée vaut 1 si cette direction mène au bord **ou** un trou est présent à cet endroit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88f95238-ed09-4582-ab28-b9a85b48d992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53303bf5-5453-4798-92f3-618bb2774151",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Coder nos observations\n",
    "\n",
    "- Un autre changement de code est nécessaire, il s'agit du constructeur où l'espace d'observation est défini.\n",
    "- Nos observations étaient auparavant un nombre entier de 0 à 15, nous avons donc utilisé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0b2ebd3-1775-4bdf-9c93-c5af274dbc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space = gym.spaces.Discrete(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459308c9-a85a-4fcb-a8da-02a91401113f",
   "metadata": {},
   "source": [
    "Et de même pour les actions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3cc9d30-715e-40a6-baa4-03496e91f9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = gym.spaces.Discrete(4)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53434076-3bb8-4fd1-9e8e-e23eb2be2808",
   "metadata": {},
   "source": [
    "- Cependant, nos observations sont maintenant des tableaux de 4 nombres plutôt qu'un seul nombre.\n",
    "- Pour indiquer cela, nous utilisons `gym.spaces.MultiDiscrete` au lieu de `gym.spaces.Discrete`.\n",
    "- Multi, car nous avons plusieurs nombres, mais toujours discret, car chacun des 4 nombres ne peut prendre que 2 valeurs possibles (0 ou 1).\n",
    "- Voici le code :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b119b15e-4d45-4087-8895-0ef3c694f62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomLakeObs(RandomLake):\n",
    "    def __init__(self, env_config=None):\n",
    "        self.observation_space = gym.spaces.MultiDiscrete([2,2,2,2])\n",
    "        self.action_space = gym.spaces.Discrete(4)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47022e95-090c-494b-955e-7e4ccadf088e",
   "metadata": {},
   "source": [
    "(Note que `gym` possède également un type d'espace `MultiBinary`, mais celui-ci n'est actuellement pas pris en charge par RLlib)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f5ebb5-6a8a-448e-a692-335ae56f4377",
   "metadata": {},
   "source": [
    "#### Tester notre nouvel env\n",
    "\n",
    "Testons-le !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "466d94ac-5b1b-4ce5-96ec-2b99b6849c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afccc03d-e0f8-4b8b-9be7-d13aad969084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from envs_03 import RandomLakeObs\n",
    "\n",
    "env = RandomLakeObs()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de7de150-479c-4a5c-98cf-4adf0abd378b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧑🧊🧊🧊\n",
      "🕳🕳🕳🧊\n",
      "🧊🧊🕳🧊\n",
      "🧊🧊🕳⛳️\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b6f296-1805-4467-8b34-fd61ebcdd732",
   "metadata": {},
   "source": [
    "Ici, nous voyons l'observation attendue indiquant des \"trous\" à gauche, en bas et en haut.\n",
    "\n",
    "Notes \n",
    "\n",
    "La gauche et le haut sont les bords de la carte, et le bas est un trou réel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef2d1ac-4700-4dee-8aeb-23d09f18414f",
   "metadata": {},
   "source": [
    "#### Tester notre nouvel env\n",
    "\n",
    "Essayons de faire un pas à droite :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db75ac7f-c4db-4e2e-8dbb-734fb0ff4ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 0, 1]), 0, False, {'player': (0, 1), 'goal': (3, 3)})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24287850-3c51-469c-86cd-17f2faa8cf3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧊🧑🧊🧊\n",
      "🕳🕳🕳🧊\n",
      "🧊🧊🕳🧊\n",
      "🧊🧊🕳⛳️\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bac1dc-f61f-41c5-85d5-095f880079e8",
   "metadata": {},
   "source": [
    "Nous voyons maintenant des trous dans les directions descendante et ascendante, comme prévu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3483468e-5524-4e8b-912e-ed2ab4e5a1ba",
   "metadata": {},
   "source": [
    "#### Formation avec nos nouvelles observations\n",
    "\n",
    "- Nos nouvelles observations semblent fonctionner, mais aident-elles l'agent à apprendre ?\n",
    "- Rappelle-toi qu'avec notre espace d'observation `Discret(16)` nous n'avons pas pu obtenir beaucoup plus qu'un taux de réussite de 30%.\n",
    "- Essayons à nouveau :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83267540-3d6c-43a1-a834-9fb258d0dd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "from utils_03 import lake_default_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "232e7a87-3c60-4ef3-91eb-e3b26db1e257",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo = lake_default_config.build(env=RandomLakeObs)\n",
    "\n",
    "for i in range(8):\n",
    "    ppo.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea82a7de-3556-4f7a-9e78-78ef708e3b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6420454545454546"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo.evaluate()[\"evaluation\"][\"episode_reward_mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c7214d-f48c-4f13-8bb1-a6de0fb92411",
   "metadata": {},
   "source": [
    "- C'est bien mieux que les ~30% que nous obtenions avant !\n",
    "- Ce qui est logique... notre agent peut \"voir\" les trous maintenant, au lieu de marcher à l'aveuglette."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86425828-0bef-4b47-ad0e-29d8afaf465d",
   "metadata": {},
   "source": [
    "#### Appliquons ce que nous avons appris !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4842178d-aa77-46b2-9da3-7deaea3d40f9",
   "metadata": {},
   "source": [
    "## Analogie de l'apprentissage supervisé : espace d'observation\n",
    "<!-- multiple choice -->\n",
    "\n",
    "Dans les diapositives, nous avons modifié l'espace d'observation de notre agent et, par conséquent, obtenu de meilleures récompenses. À quel aspect du processus d'apprentissage supervisé cela ressemble-t-il le plus ?\n",
    "\n",
    "- [L'ingénierie des caractéristiques | Tu as trouvé ! Notre espace d'observation sert d'espace de caractéristiques sur lequel notre politique doit agir.\n",
    "- [ ] Sélection de modèles | Pas tout à fait. Mais, comme nous le verrons, la sélection de modèles a aussi sa place dans l'apprentissage automatique !\n",
    "- [Réglage des hyperparamètres | Pas tout à fait. Mais, comme nous le verrons, l'ajustement des hyperparamètres a également sa place dans RL !\n",
    "- [ ] Sélection d'une fonction de perte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6438bf5-8970-4b51-a1b4-011f067d8903",
   "metadata": {},
   "source": [
    "## Incluant l'emplacement du joueur\n",
    "<!-- multiple choice -->\n",
    "\n",
    "Dans notre nouvelle représentation de l'observation, nous avons en fait _supprimé_ l'emplacement du joueur de l'observation et _seulement_ inclus la présence des trous à proximité. Si nous voulions un espace d'observation qui inclut à la fois les murs proches _et_ l'emplacement du joueur, lequel des espaces de gymnastique suivants pourrions-nous utiliser ?\n",
    "\n",
    "- [ ] `gym.spaces.Discrete(5)` | Essaie encore !\n",
    "- [ ] `gym.spaces.MultiDiscrete([2,2,2,2,16])` | Oui ! Les 4 premiers chiffres représentent les trous, et le dernier chiffre représente l'emplacement du joueur.\n",
    "- [ ] `gym.spaces.MultiDiscrete([2,2,2,2]) + gym.spaces.Discrete(16)` | Réessaie ; malheureusement, nous ne pouvons pas ajouter d'espaces de gym.\n",
    "- [ ] `gym.spaces.MultiDiscrete([32,32,32,32])` | On pourrait faire en sorte que cela fonctionne, mais c'est une représentation confuse/redondante."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03e0f1b-e397-467a-8f62-8537173201c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Manipuler les bords\n",
    "<!-- multiple choice -->\n",
    "\n",
    "Dans les diapositives, nous avons décidé de traiter les bords comme des trous. Rappelle-toi cette image :\n",
    "\n",
    "```\n",
    "OOOOOO\n",
    "O....O\n",
    "O..OPO\n",
    "OO.OOO\n",
    "O...GO\n",
    "OOOOOO\n",
    "```\n",
    "\n",
    "Cependant, les bords et les trous sont en fait différents les uns des autres : marcher dans un bord ne fait rien, alors que marcher dans un trou provoque la fin de l'épisode. Cette distinction pourrait être importante, surtout dans une version \"glissante\" de l'environnement où les résultats des actions sont non déterministes \n",
    "\n",
    "Pour résoudre ce problème, nous décidons de modifier l'espace d'observation. L'agent ne \"voit\" toujours que les quatre carrés qui l'entourent, mais il voit maintenant si chaque carré est un espace vide, un trou ou un bord. Pour cette représentation, lequel des espaces d'observation des gymnases suivants pourrions-nous utiliser ?\n",
    "\n",
    "- [ ] `gym.spaces.MultiDiscrete([2,2,2,2,2,2,2,2,2])` | Essaie encore. Rappelle-toi que l'agent ne \"voit\" toujours que 4 carrés.\n",
    "- [ ] `gym.spaces.MultiDiscrete([3,3,3,3,3,3,3,3,3,3])` | Essaie encore !\n",
    "- [ ] `gym.spaces.MultiDiscrete([2,2,2,2])` | C'est le même espace que le précédent, mais nous avons fait un changement.\n",
    "- [x] `gym.spaces.MultiDiscrete([3,3,3,3])` | Tu as trouvé ! Il y a maintenant 3 options possibles pour ce que l'agent peut \"voir\" à chaque case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "485875e4-6694-4330-838c-1a54ed0499de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO / note to self\n",
    "# query_policy(trainer, RandomLakeObs(), [1,1,1,1])\n",
    "# shows that it wants to go up. this is because the above \"hole\" is probably an edge based on its learning. fascinating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c89bbb1-d5b4-41ef-bdd9-1f9ef08aa1eb",
   "metadata": {},
   "source": [
    "## Mise en œuvre des bords\n",
    "<!-- coding exercise -->\n",
    "\n",
    "Le code ci-dessous montre la fonction `observation` pour l'espace d'observation actuel. Modifie le code pour qu'il utilise le nouvel espace d'observation, où 0 représente un espace vide, 1 représente un trou et 2 représente un bord "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87cf9e9c-550c-43cb-8725-aa60d0cb91ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧑🧊🧊🧊\n",
      "🕳🕳🕳🧊\n",
      "🧊🧊🕳🧊\n",
      "🧊🧊🕳⛳️\n",
      "[1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE\n",
    "\n",
    "from envs_03 import RandomLake\n",
    "\n",
    "class RandomLakeObs2(RandomLakeObs):\n",
    "    def observation(self):\n",
    "        i, j = self.player\n",
    "\n",
    "        obs = []\n",
    "        obs.append(1 if j==0 else self.holes[i,j-1]) # left\n",
    "        obs.append(1 if i==3 else self.holes[i+1,j]) # down\n",
    "        obs.append(1 if j==3 else self.holes[i,j+1]) # right\n",
    "        obs.append(1 if i==0 else self.holes[i-1,j]) # up\n",
    "        \n",
    "        obs = np.array(obs, dtype=int) # cast to numpy array\n",
    "        return obs\n",
    "\n",
    "np.random.seed(42)\n",
    "env = RandomLakeObs2()\n",
    "obs = env.reset()\n",
    "env.render()\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06328a38-f19c-4c71-82b6-31e056f28f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧑🧊🧊🧊\n",
      "🕳🕳🕳🧊\n",
      "🧊🧊🕳🧊\n",
      "🧊🧊🕳⛳️\n",
      "[2 1 0 2]\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "\n",
    "from envs_03 import RandomLake\n",
    "\n",
    "class RandomLakeObs2(RandomLakeObs):\n",
    "    def observation(self):\n",
    "        i, j = self.player\n",
    "\n",
    "        obs = []\n",
    "        obs.append(2 if j==0 else self.holes[i,j-1]) # left\n",
    "        obs.append(2 if i==3 else self.holes[i+1,j]) # down\n",
    "        obs.append(2 if j==3 else self.holes[i,j+1]) # right\n",
    "        obs.append(2 if i==0 else self.holes[i-1,j]) # up\n",
    "        \n",
    "        obs = np.array(obs, dtype=int) # cast to numpy array\n",
    "        return obs\n",
    "\n",
    "np.random.seed(42)\n",
    "env = RandomLakeObs2()\n",
    "obs = env.reset()\n",
    "env.render()\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0f55b9-a660-4a04-8caf-52d13ced50e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Ce que l'agent voit\n",
    "<!-- coding exercise -->\n",
    "\n",
    "Avec notre nouveau codage des espaces d'observation, l'agent ne \"voit\" que les 4 espaces qui l'entourent et ne dispose que de ces informations pour prendre ses décisions. La cellule de code ci-dessous crée un rendu de ce que l'agent \"voit\" pendant qu'il navigue sur le lac aléatoire. Tu peux entrer des actions avec le clavier en tapant les mots \"gauche\", \"bas\", \"droite\" ou \"haut\" (ou \"l\", \"d\", \"r\", \"u\" pour faire court) et la simulation te montrera le résultat. (Tape \"quit\" pour sortir.) Joue le jeu jusqu'à ce que tu atteignes l'objectif. Au fur et à mesure, essaie de cartographier le lac (peut-être en dessinant sur une feuille de papier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d5b24c4-0df7-47e3-8155-895ccbb9e5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO / NOTE:\n",
    "# THIS EXERCISE DOES NOT HAVE A \"solution\"\n",
    "# the code is here ONLY to help them answer the multiple choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8341e39f-8684-4106-94cc-a4a422f476b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation:\n",
      ".O.\n",
      "OP.\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE\n",
    "\n",
    "import numpy as np\n",
    "from envs_03 import RandomLakeObs\n",
    "\n",
    "actions = {\"left\" : 0, \"down\" : 1, \"right\" : 2, \"up\" : 3, \n",
    "           \"l\" : 0, \"d\" : 1, \"r\" : 2, \"u\" : 3}\n",
    "\n",
    "np.random.seed(45)\n",
    "env = RandomLakeObs()\n",
    "obs = env.reset()\n",
    "\n",
    "act = \"start\"\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "   \n",
    "    obs_print = [['.']*3 for i in range(3)]\n",
    "    obs_print[1][1] = \"P\"\n",
    "    if obs[0]:\n",
    "        obs_print[1][0] = \"O\"\n",
    "    if obs[1]:\n",
    "        obs_print[2][1] = \"O\"\n",
    "    if obs[2]:\n",
    "        obs_print[1][2] = \"O\"\n",
    "    if obs[3]:\n",
    "        obs_print[0][1] = \"O\"\n",
    "    print(\"Observation:\")\n",
    "    print(\"\\n\".join(list(map(lambda c: \"\".join(c), obs_print))))\n",
    "    print()\n",
    "    \n",
    "    while act != \"quit\" and act not in actions: \n",
    "        act = input() # gather keyboard input \n",
    "    \n",
    "    if act == \"quit\":\n",
    "        break\n",
    "        \n",
    "    obs, rew, done, _ = env.step(act)\n",
    "    \n",
    "if done:\n",
    "    if rew > 0:\n",
    "        print(\"You win! +1 reward 🎉\")\n",
    "    else:\n",
    "        print(\"You fell into the lake 😢\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c8469b-8040-44db-82cd-174ca76b82c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "import numpy as np\n",
    "from envs_03 import RandomLakeObs\n",
    "\n",
    "actions = {\"left\" : 0, \"down\" : 1, \"right\" : 2, \"up\" : 3, \n",
    "           \"l\" : 0, \"d\" : 1, \"r\" : 2, \"u\" : 3}\n",
    "\n",
    "np.random.seed(45)\n",
    "env = RandomLakeObs()\n",
    "obs = env.reset()\n",
    "\n",
    "act = \"start\"\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "   \n",
    "    obs_print = [['.']*3 for i in range(3)]\n",
    "    obs_print[1][1] = \"P\"\n",
    "    if obs[0]:\n",
    "        obs_print[1][0] = \"O\"\n",
    "    if obs[1]:\n",
    "        obs_print[2][1] = \"O\"\n",
    "    if obs[2]:\n",
    "        obs_print[1][2] = \"O\"\n",
    "    if obs[3]:\n",
    "        obs_print[0][1] = \"O\"\n",
    "    print(\"Observation:\")\n",
    "    print(\"\\n\".join(list(map(lambda c: \"\".join(c), obs_print))))\n",
    "    print()\n",
    "    \n",
    "    while act != \"quit\" and act not in actions: \n",
    "        act = input() # gather keyboard input \n",
    "    \n",
    "    if act == \"quit\":\n",
    "        break\n",
    "        \n",
    "    obs, rew, done, _ = env.step(act)\n",
    "    \n",
    "if done:\n",
    "    if rew > 0:\n",
    "        print(\"You win! +1 reward 🎉\")\n",
    "    else:\n",
    "        print(\"You fell into the lake 😢\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7024e752-4e71-4162-886e-c5be693c9ee5",
   "metadata": {},
   "source": [
    "#### À quoi ressemble le lac ?\n",
    "\n",
    "D'après tes explorations, quelle est la carte correcte du lac dans la question ci-dessus ?\n",
    "\n",
    "```\n",
    " (A) (B) (C) (D)\n",
    "P..O P.OO P..O P.OO\n",
    "..OO .OOO ..OO .OOO\n",
    "O...     O...     O...     O..O\n",
    "...G ...G ..OG ...G\n",
    "```\n",
    "\n",
    "- [x] (A)\n",
    "- [ ] (B)\n",
    "- [ ] (C)\n",
    "- [ ] (D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbad4252-94ef-480a-938d-6e4858ed3675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# could also considering showing a BAD environment encoding to contrast with this reasonable one, as in the next slide deck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ray200]",
   "language": "python",
   "name": "conda-env-ray200-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
