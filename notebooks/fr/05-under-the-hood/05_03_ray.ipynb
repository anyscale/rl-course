{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be76afd1-30c0-4f6a-9107-bf9eef74f904",
   "metadata": {},
   "source": [
    "## Ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fea144b-381f-4fcd-ac3d-e9f1d364d660",
   "metadata": {},
   "source": [
    "#### Qu'est-ce que Ray ?\n",
    "\n",
    "Pendant tout ce cours, nous avons utilis√© le pack Ray :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41754b57-9c36-48f7-843a-4989c90f357b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray.rllib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4357c3f6-bc60-4eb4-b4be-6aac32900960",
   "metadata": {},
   "source": [
    "![](img/ray-logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173eca68-40f8-43ce-9889-a4d3b6f96505",
   "metadata": {},
   "source": [
    "Qu'est-ce que Ray ? Dans les [docs] (https://docs.ray.io/en/latest/) :\n",
    "\n",
    "&gt; Ray est un cadre de calcul distribu√© universel et polyvalent.\n",
    "\n",
    "Ray est aussi :\n",
    "\n",
    "- Un [projet open source actif](https://github.com/ray-project/ray) avec plus de 20k √©toiles sur GitHub ü§©\n",
    "- Soutenu par la startup licorne [Anyscale](https://www.anyscale.com/), qui a produit ce cours ü¶Ñ\n",
    "\n",
    "Notes :\n",
    "\n",
    "Mais, revenons √† l'informatique distribu√©e."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bceac41-d9d5-46e4-8e3a-21d5a914a986",
   "metadata": {},
   "source": [
    "#### Qu'est-ce que l'informatique distribu√©e ?\n",
    "\n",
    "l'informatique distribu√©e est une informatique qui implique plusieurs machines (n≈ìuds) r√©parties sur un r√©seau.\n",
    "\n",
    "[](img/supercomputer.png)\n",
    "\n",
    "Pour :\n",
    "\n",
    "- Capacit√©s massivement am√©lior√©es\n",
    "\n",
    "Contre/d√©fis :\n",
    "\n",
    "- Synchronisation\n",
    "- √âchec\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d345f4-5b01-49f2-9635-130f4d580f21",
   "metadata": {},
   "source": [
    "#### Ray facilite l'informatique distribu√©e\n",
    "\n",
    "- L'objectif de Ray est de rendre l'informatique distribu√©e facile et accessible.\n",
    "- Ray g√®re la plupart des d√©fis pour les utilisateurs.\n",
    "- RLlib, tune et les autres sous-paquets ont √©t√© construits au-dessus de Ray.\n",
    "- Cela signifie que _RLlib et tune ont automatiquement des capacit√©s distribu√©es._\n",
    "\n",
    "Notes :\n",
    "\n",
    "Surprise ! RLlib est facile √† utiliser et impl√©mente de mani√®re pratique de nombreux algorithmes RL de pointe, mais il pr√©sente un autre avantage que nous n'avions pas mentionn√© jusqu'√† pr√©sent : des capacit√©s naturelles de calcul distribu√©. Cela le place loin devant les paquets concurrents en mati√®re de facilit√© de distribution des calculs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275fe8fc-292d-4622-b7c9-5e6d0c0f4022",
   "metadata": {},
   "source": [
    "#### RLlib, distribu√©\n",
    "\n",
    "- Dans ce cours, nous avons configur√© les algorithmes de nombreuses fois.\n",
    "- Mais il y a certains param√®tres que nous n'avons jamais utilis√©s auparavant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a1e8fe8-c52d-4cf6-a846-75f6ca83e381",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68ff95da-eee7-45eb-9694-ac4fd5420c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_config = (\n",
    "    PPOConfig()\n",
    "    .framework(\"torch\")\n",
    "    .rollouts(num_rollout_workers=4, num_envs_per_worker=2)\n",
    "    .resources(num_gpus=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e10ccc-cc94-41a3-bcda-66de6cf68fcc",
   "metadata": {},
   "source": [
    "Tu peux en savoir plus sur la sp√©cification des ressources [ici] (https://docs.ray.io/en/master/rllib/rllib-training.html#specifying-resources) et sur la mise √† l'√©chelle [ici] (https://docs.ray.io/en/master/rllib/rllib-training.html#scaling-guide)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec03fa7b-c78f-446d-9be7-deac2c895ce2",
   "metadata": {},
   "source": [
    "Mais... qu'est-ce qu'un \"ouvrier roulant\" ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ebdf4e-e1c0-41c9-b923-1a3917106f9e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Travailleurs du d√©ploiement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5ea641-4738-4220-8adc-78d5a65141e0",
   "metadata": {},
   "source": [
    "- Les travailleurs du d√©ploiement collectent les donn√©es de l'environnement (simulateur) en parall√®le.\n",
    "- Pour la plupart des environnements de simulateur, on peut r√©pliquer l'environnement dans un cluster.\n",
    "- Par cons√©quent, tu peux collecter les donn√©es beaucoup plus rapidement et √©viter d'engorger la formation.\n",
    "- Quel que soit le cluster auquel Ray est connect√© en backend, `num_rollout_workers=4` fonctionne sans probl√®me.\n",
    "\n",
    "Remarques :\n",
    "\n",
    "Dans l'apprentissage supervis√©, quand tu attends, tu sais que tu attends probablement que le mod√®le s'entra√Æne. En RL, le goulot d'√©tranglement peut √™tre la collecte de donn√©es ou les mises √† jour du mod√®le. Le fait de pouvoir parall√©liser les d√©ploiements att√©nue le goulot d'√©tranglement de la collecte de donn√©es "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925d5abd-b478-43a7-bb71-7eeb9ae4276d",
   "metadata": {},
   "source": [
    "#### L'air de Ray, revisit√©\n",
    "\n",
    "- N'oublie pas que le r√©glage des hyperparam√®tres, comme la recherche sur grille, est √©galement facile √† distribuer.\n",
    "- Heureusement, `tune` fait aussi partie de Ray et, comme RLlib, s'en occupe pour toi ! \n",
    "- Comme tu peux le voir, Ray + tune + RLlib devient une combinaison assez puissante."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac61bd0-31ac-47e7-a51b-6ff1b92cb753",
   "metadata": {},
   "source": [
    "#### Conducteur\n",
    "\n",
    "Dans toutes nos configurations, nous avons eu\n",
    "\n",
    "```python\n",
    "create_env_on_driver = True\n",
    "```\n",
    "\n",
    "Ce que cela signifie, c'est que nous pla√ßons l'env sur le m√™me processus \"pilote\" qui ex√©cute l'entra√Ænement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fac297-c378-4ad3-b58f-65da9042333c",
   "metadata": {},
   "source": [
    "#### R√©sum√©\n",
    "\n",
    "- Ray est incroyablement puissant, et nous n'avons fait qu'effleurer la pointe de l'iceberg.\n",
    "- Quelques autres ressources :\n",
    "  - [ray.io](https://www.ray.io/) (en anglais)\n",
    "  - [Learning Ray](https://www.oreilly.com/library/view/learning-ray/9781098117214/) (livre)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76648d6e-5b58-4241-aeb3-84d4dc6c661f",
   "metadata": {},
   "source": [
    "#### Appliquons ce que nous avons appris !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d0b93e-69ad-4338-b937-62bb19fc506b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Qu'est-ce que Ray ?\n",
    "<!-- multiple choice -->\n",
    "\n",
    "Qu'est-ce que Ray ?\n",
    "\n",
    "- [ ] L'entreprise qui cr√©e RLlib. | Tu penses peut-√™tre √† Anyscale, l'entreprise qui se cache derri√®re Ray !\n",
    "- [ ] Un sous-paquet de RLlib qui traite de l'informatique distribu√©e.\n",
    "- [ ] Un paquetage polyvalent qui inclut RLlib et qui traite du calcul distribu√©.\n",
    "- [ ] Un algorithme d'apprentissage par renforcement "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67749f9a-fc60-436d-878d-ee060772cf9a",
   "metadata": {},
   "source": [
    "## Distribuer RLlib\n",
    "<!-- multiple choice -->\n",
    "\n",
    "Quelle est la principale fa√ßon dont RLlib utilise les capacit√©s de calcul distribu√© ?\n",
    "\n",
    "- [x] Les travailleurs de d√©ploiement distribu√©s g√©n√®rent des donn√©es √† partir des clones env qui sont introduites dans l'algorithme d'apprentissage \n",
    "- [ ] La formation du r√©seau neuronal de politique est r√©partie sur plusieurs n≈ìuds.\n",
    "- [ ] Chaque n≈ìud poss√®de un r√©seau neuronal de politique distinct qui est form√© ind√©pendamment sur son propre n≈ìud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7a1f3a-18b5-4d99-ad1f-409bf87b59ad",
   "metadata": {},
   "source": [
    "## Exp√©rimenter avec les travailleurs du d√©ploiement\n",
    "<!-- coding exercise -->\n",
    "\n",
    "Le code ci-dessous cr√©e deux instances de l'algorithme PPO, l'une qui devrait utiliser deux travailleurs de d√©ploiement avec deux env par travailleur, et l'autre qui n'utilise qu'un seul travailleur de d√©ploiement avec un env par travailleur. Il imprime ensuite le temps √©coul√© pour former chacune d'elles pendant 5 it√©rations. Termine et ex√©cute le code, puis compare les temps \n",
    "\n",
    "Remarque : cette exp√©rience fonctionnera _normalement_. Cependant, ce code s'ex√©cute sur un serveur qui est potentiellement utilis√© par plusieurs apprenants en m√™me temps, les temps d'ex√©cution peuvent donc √™tre influenc√©s par la charge du serveur. De plus, ce serveur n'est pas un v√©ritable cluster, donc tout avantage proviendrait de la disponibilit√© de plusieurs c≈ìurs de CPU pour la parall√©lisation dans une seule machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2689cc8b-4ce7-4b7c-a943-0e937d37ccec",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PPOConfig' object has no attribute '____'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m      6\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mDeprecationWarning\u001b[39;00m) \n\u001b[1;32m      8\u001b[0m ppo_config_many \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m----> 9\u001b[0m     \u001b[43mPPOConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m____\u001b[49m(____)\\\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;241m.\u001b[39mtraining(model\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfcnet_hiddens\u001b[39m\u001b[38;5;124m\"\u001b[39m : [\u001b[38;5;241m32\u001b[39m,\u001b[38;5;241m32\u001b[39m]})\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m ppo_config_single \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     16\u001b[0m     PPOConfig()\\\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;241m.\u001b[39mframework(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;241m.\u001b[39m____(num_rollout_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, num_envs_per_worker\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\\\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;241m.\u001b[39mtraining(model\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfcnet_hiddens\u001b[39m\u001b[38;5;124m\"\u001b[39m : [\u001b[38;5;241m32\u001b[39m,\u001b[38;5;241m32\u001b[39m]})\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     22\u001b[0m ppo_many \u001b[38;5;241m=\u001b[39m ppo_config_many\u001b[38;5;241m.\u001b[39mbuild(env\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrozenLake-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PPOConfig' object has no attribute '____'"
     ]
    }
   ],
   "source": [
    "# EXERCISE\n",
    "\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "ppo_config_many = (\n",
    "    PPOConfig()\n",
    "    .framework(\"torch\")\n",
    "    .____(____)\n",
    "    .training(model={\"fcnet_hiddens\" : [32,32]})\n",
    ")\n",
    "\n",
    "ppo_config_single = (\n",
    "    PPOConfig()\n",
    "    .framework(\"torch\")\n",
    "    .____(num_rollout_workers=1, num_envs_per_worker=1)\n",
    "    .training(model={\"fcnet_hiddens\" : [32,32]})\n",
    ")\n",
    "\n",
    "ppo_many = ppo_config_many.build(env=\"FrozenLake-v1\")\n",
    "t = time.time()\n",
    "for i in range(5):\n",
    "    ppo_many.train()\n",
    "print(f\"Elapsed time with 2 workers, 2 envs each: {time.time()-t:.1f}s.\")\n",
    "ppo_many.stop()\n",
    "\n",
    "ppo_single = ppo_config_single.build(env=\"FrozenLake-v1\")\n",
    "t = time.time()\n",
    "for i in range(5):\n",
    "    ppo_single.train()\n",
    "print(f\"Elapsed time with 1 worker, 1 env: {time.time()-t:.1f}s.\")\n",
    "ppo_single.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2405893d-2dc6-4a91-8d22-bb4db1450b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-27 17:02:42,927\tINFO worker.py:1490 -- Started a local Ray instance.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=70484)\u001b[0m 2022-08-27 17:02:47,263\tWARNING env.py:154 -- Your env reset() method appears to take 'seed' or 'return_info' arguments. Note that these are not yet supported in RLlib. Seeding will take place using 'env.seed()' and the info dict will not be returned from reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=70484)\u001b[0m 2022-08-27 17:02:47,536\tWARNING deprecation.py:47 -- DeprecationWarning: `concat_samples` has been deprecated. Use `concat_samples() from rllib.policy.sample_batch` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time with 2 workers, 2 envs each: 11.3s.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=70561)\u001b[0m 2022-08-27 17:03:00,941\tWARNING env.py:154 -- Your env reset() method appears to take 'seed' or 'return_info' arguments. Note that these are not yet supported in RLlib. Seeding will take place using 'env.seed()' and the info dict will not be returned from reset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time with 1 worker, 1 env: 18.0s.\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "ppo_config_many = (\n",
    "    PPOConfig()\n",
    "    .framework(\"torch\")\n",
    "    .rollouts(num_rollout_workers=2, num_envs_per_worker=2)\n",
    "    .training(model={\"fcnet_hiddens\" : [32,32]})\n",
    ")\n",
    "\n",
    "ppo_config_single = (\n",
    "    PPOConfig()\n",
    "    .framework(\"torch\")\n",
    "    .rollouts(num_rollout_workers=1, num_envs_per_worker=1)\n",
    "    .training(model={\"fcnet_hiddens\" : [32,32]})\n",
    ")\n",
    "\n",
    "ppo_many = ppo_config_many.build(env=\"FrozenLake-v1\")\n",
    "t = time.time()\n",
    "for i in range(5):\n",
    "    ppo_many.train()\n",
    "print(f\"Elapsed time with 2 workers, 2 envs each: {time.time()-t:.1f}s.\")\n",
    "ppo_many.stop()\n",
    "\n",
    "ppo_single = ppo_config_single.build(env=\"FrozenLake-v1\")\n",
    "t = time.time()\n",
    "for i in range(5):\n",
    "    ppo_single.train()\n",
    "print(f\"Elapsed time with 1 worker, 1 env: {time.time()-t:.1f}s.\")\n",
    "ppo_single.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda099fd-12d1-42e3-a529-e757347def40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ray200]",
   "language": "python",
   "name": "conda-env-ray200-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
