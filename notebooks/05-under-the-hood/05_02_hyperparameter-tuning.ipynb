{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b02cdbb-95e8-451d-87d5-8a3f07088797",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a5935760-7836-4a88-bb36-91a2caa600e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7584b00e-80e1-4db0-b5cf-3f06e41b020a",
   "metadata": {},
   "source": [
    "#### Hyperparameters\n",
    "\n",
    "Recall our discussing of RLlib config files in module 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b27c3978-17f5-4e04-b65f-8574b9b7d38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9064d95-d181-4217-a991-a8e790d9b2fb",
   "metadata": {},
   "source": [
    "We wrote code like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eac41c32-da41-4afd-9f6f-1eddae8ab73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_config = (\n",
    "    PPOConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(create_env_on_local_worker=True)\\\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\\\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c53b89b-5554-4cb0-b687-79cc99bd48ca",
   "metadata": {},
   "source": [
    "However, we are only setting a tiny fraction of the config options available in RLlib."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f66b89-8ef6-48b3-aa7e-279df9b985a4",
   "metadata": {},
   "source": [
    "#### Embarrassment of configs\n",
    "\n",
    "Here is the full list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99d28bce-7eb5-4238-9726-a0327bca82b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ppo_config.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1a031774-708a-45d6-8565-d18d9280eafc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'extra_python_environs_for_driver': {},\n",
       " 'extra_python_environs_for_worker': {},\n",
       " 'num_gpus': 0,\n",
       " 'num_cpus_per_worker': 1,\n",
       " 'num_gpus_per_worker': 0,\n",
       " '_fake_gpus': False,\n",
       " 'custom_resources_per_worker': {},\n",
       " 'placement_strategy': 'PACK',\n",
       " 'eager_tracing': False,\n",
       " 'eager_max_retraces': 20,\n",
       " 'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "  'inter_op_parallelism_threads': 2,\n",
       "  'gpu_options': {'allow_growth': True},\n",
       "  'log_device_placement': False,\n",
       "  'device_count': {'CPU': 1},\n",
       "  'allow_soft_placement': True},\n",
       " 'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "  'inter_op_parallelism_threads': 8},\n",
       " 'env': None,\n",
       " 'env_config': {},\n",
       " 'observation_space': None,\n",
       " 'action_space': None,\n",
       " 'env_task_fn': None,\n",
       " 'render_env': False,\n",
       " 'clip_rewards': None,\n",
       " 'normalize_actions': True,\n",
       " 'clip_actions': False,\n",
       " 'disable_env_checking': False,\n",
       " 'num_workers': 2,\n",
       " 'num_envs_per_worker': 1,\n",
       " 'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       " 'sample_async': False,\n",
       " 'rollout_fragment_length': 200,\n",
       " 'batch_mode': 'truncate_episodes',\n",
       " 'remote_worker_envs': False,\n",
       " 'remote_env_batch_wait_ms': 0,\n",
       " 'validate_workers_after_construction': True,\n",
       " 'ignore_worker_failures': False,\n",
       " 'recreate_failed_workers': False,\n",
       " 'restart_failed_sub_environments': False,\n",
       " 'num_consecutive_worker_failures_tolerance': 100,\n",
       " 'horizon': None,\n",
       " 'soft_horizon': False,\n",
       " 'no_done_at_end': False,\n",
       " 'preprocessor_pref': 'deepmind',\n",
       " 'observation_filter': 'NoFilter',\n",
       " 'synchronize_filters': True,\n",
       " 'compress_observations': False,\n",
       " 'gamma': 0.99,\n",
       " 'lr': 5e-05,\n",
       " 'train_batch_size': 4000,\n",
       " 'model': {'_use_default_native_models': False,\n",
       "  '_disable_preprocessor_api': False,\n",
       "  '_disable_action_flattening': False,\n",
       "  'fcnet_hiddens': [256, 256],\n",
       "  'fcnet_activation': 'tanh',\n",
       "  'conv_filters': None,\n",
       "  'conv_activation': 'relu',\n",
       "  'post_fcnet_hiddens': [],\n",
       "  'post_fcnet_activation': 'relu',\n",
       "  'free_log_std': False,\n",
       "  'no_final_linear': False,\n",
       "  'vf_share_layers': False,\n",
       "  'use_lstm': False,\n",
       "  'max_seq_len': 20,\n",
       "  'lstm_cell_size': 256,\n",
       "  'lstm_use_prev_action': False,\n",
       "  'lstm_use_prev_reward': False,\n",
       "  '_time_major': False,\n",
       "  'use_attention': False,\n",
       "  'attention_num_transformer_units': 1,\n",
       "  'attention_dim': 64,\n",
       "  'attention_num_heads': 1,\n",
       "  'attention_head_dim': 32,\n",
       "  'attention_memory_inference': 50,\n",
       "  'attention_memory_training': 50,\n",
       "  'attention_position_wise_mlp_dim': 32,\n",
       "  'attention_init_gru_gate_bias': 2.0,\n",
       "  'attention_use_n_prev_actions': 0,\n",
       "  'attention_use_n_prev_rewards': 0,\n",
       "  'framestack': True,\n",
       "  'dim': 84,\n",
       "  'grayscale': False,\n",
       "  'zero_mean': True,\n",
       "  'custom_model': None,\n",
       "  'custom_model_config': {},\n",
       "  'custom_action_dist': None,\n",
       "  'custom_preprocessor': None,\n",
       "  'lstm_use_prev_action_reward': -1},\n",
       " 'optimizer': {},\n",
       " 'explore': True,\n",
       " 'exploration_config': {'type': 'StochasticSampling'},\n",
       " 'input_config': {},\n",
       " 'actions_in_input_normalized': False,\n",
       " 'off_policy_estimation_methods': {},\n",
       " 'postprocess_inputs': False,\n",
       " 'shuffle_buffer_size': 0,\n",
       " 'output': None,\n",
       " 'output_config': {},\n",
       " 'output_compress_columns': ['obs', 'new_obs'],\n",
       " 'output_max_file_size': 67108864,\n",
       " 'evaluation_interval': None,\n",
       " 'evaluation_duration': 10,\n",
       " 'evaluation_duration_unit': 'episodes',\n",
       " 'evaluation_parallel_to_training': False,\n",
       " 'evaluation_config': {},\n",
       " 'evaluation_num_workers': 0,\n",
       " 'always_attach_evaluation_results': False,\n",
       " 'in_evaluation': False,\n",
       " 'sync_filters_on_rollout_workers_timeout_s': 60.0,\n",
       " 'keep_per_episode_custom_metrics': False,\n",
       " 'metrics_episode_collection_timeout_s': 180,\n",
       " 'metrics_num_episodes_for_smoothing': 100,\n",
       " 'min_time_s_per_iteration': None,\n",
       " 'min_train_timesteps_per_iteration': 0,\n",
       " 'min_sample_timesteps_per_iteration': 0,\n",
       " 'logger_creator': None,\n",
       " 'logger_config': None,\n",
       " 'log_level': 'ERROR',\n",
       " 'log_sys_usage': True,\n",
       " 'fake_sampler': False,\n",
       " 'seed': 0,\n",
       " '_tf_policy_handles_more_than_one_loss': False,\n",
       " '_disable_preprocessor_api': False,\n",
       " '_disable_action_flattening': False,\n",
       " '_disable_execution_plan_api': True,\n",
       " 'simple_optimizer': -1,\n",
       " 'monitor': -1,\n",
       " 'evaluation_num_episodes': -1,\n",
       " 'metrics_smoothing_episodes': -1,\n",
       " 'timesteps_per_iteration': -1,\n",
       " 'min_iter_time_s': -1,\n",
       " 'collect_metrics_timeout': -1,\n",
       " 'buffer_size': -1,\n",
       " 'prioritized_replay': -1,\n",
       " 'learning_starts': -1,\n",
       " 'replay_batch_size': -1,\n",
       " 'replay_sequence_length': None,\n",
       " 'prioritized_replay_alpha': -1,\n",
       " 'prioritized_replay_beta': -1,\n",
       " 'prioritized_replay_eps': -1,\n",
       " 'min_time_s_per_reporting': -1,\n",
       " 'min_train_timesteps_per_reporting': -1,\n",
       " 'min_sample_timesteps_per_reporting': -1,\n",
       " 'input_evaluation': -1,\n",
       " 'lr_schedule': None,\n",
       " 'use_critic': True,\n",
       " 'use_gae': True,\n",
       " 'kl_coeff': 0.2,\n",
       " 'sgd_minibatch_size': 128,\n",
       " 'num_sgd_iter': 30,\n",
       " 'shuffle_sequences': True,\n",
       " 'vf_loss_coeff': 1.0,\n",
       " 'entropy_coeff': 0.0,\n",
       " 'entropy_coeff_schedule': None,\n",
       " 'clip_param': 0.3,\n",
       " 'vf_clip_param': 10.0,\n",
       " 'grad_clip': None,\n",
       " 'kl_target': 0.01,\n",
       " 'vf_share_layers': -1,\n",
       " 'lambda': 1.0,\n",
       " 'input': 'sampler',\n",
       " 'multiagent': {'policies': {},\n",
       "  'policy_map_capacity': 100,\n",
       "  'policy_map_cache': None,\n",
       "  'policy_mapping_fn': None,\n",
       "  'policies_to_train': None,\n",
       "  'observation_fn': None,\n",
       "  'replay_mode': 'independent',\n",
       "  'count_steps_by': 'env_steps'},\n",
       " 'callbacks': ray.rllib.algorithms.callbacks.DefaultCallbacks,\n",
       " 'create_env_on_driver': True,\n",
       " 'custom_eval_function': None,\n",
       " 'framework': 'torch',\n",
       " 'num_cpus_for_driver': 1}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_config.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791e26d9-39ac-4e67-8fbc-08b51005b03f",
   "metadata": {},
   "source": [
    "#### Key hyperparameters\n",
    "\n",
    "We recommend focusing on the following key hyperparameters:\n",
    "\n",
    "- `lr`\n",
    "- `train_batch_size`\n",
    "- `sgd_minibatch_size`\n",
    "- `num_sgd_iter`\n",
    "- `entropy_coeff`\n",
    "- model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a27a37-5d43-4bc1-9258-00ba03c78a66",
   "metadata": {},
   "source": [
    "#### Key hyperparameter interpretations\n",
    "\n",
    "Let's look at the definitions of these key hyperparameters:\n",
    "\n",
    "- `lr`: learning rate\n",
    "- `train_batch_size`: number of iterations of data to be batched together\n",
    "- `sgd_minibatch_size`: minibatch size for SGD\n",
    "- `num_sgd_iter`: epochs of SGD per iteration of PPO\n",
    "- `entropy_coeff`: measure the amount of exploration during training\n",
    "- model architecture: of the policy network\n",
    "\n",
    "Notes:\n",
    "\n",
    "If you are not familiar with deep learning, most of these hyperparameters will not make sense. That is fine, you can skip."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9863bb3e-c5f2-4de5-a6d9-cec0d9163055",
   "metadata": {},
   "source": [
    "#### Key hyperparameter defaults\n",
    "\n",
    "Let's look at the defaults of these key hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b43d9b15-b999-4416-859d-579e084581e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5e-05"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_config.lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "63958eb0-50ad-4692-99cd-42e58703f9f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_config.train_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9af8a557-0383-47d7-94aa-4ef37b619bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_config.sgd_minibatch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "29f3d4b0-e645-4df6-a08b-8e7d82acb19d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_config.num_sgd_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b395eb4-1569-4310-b18a-0676762a4e23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_config.entropy_coeff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01411131-cb8b-4859-8418-f9d047518a6c",
   "metadata": {},
   "source": [
    "#### Key hyperparameter defaults\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c429e78f-e62d-48c4-9286-0c442690aa2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_use_default_native_models': False,\n",
       " '_disable_preprocessor_api': False,\n",
       " '_disable_action_flattening': False,\n",
       " 'fcnet_hiddens': [256, 256],\n",
       " 'fcnet_activation': 'tanh',\n",
       " 'conv_filters': None,\n",
       " 'conv_activation': 'relu',\n",
       " 'post_fcnet_hiddens': [],\n",
       " 'post_fcnet_activation': 'relu',\n",
       " 'free_log_std': False,\n",
       " 'no_final_linear': False,\n",
       " 'vf_share_layers': False,\n",
       " 'use_lstm': False,\n",
       " 'max_seq_len': 20,\n",
       " 'lstm_cell_size': 256,\n",
       " 'lstm_use_prev_action': False,\n",
       " 'lstm_use_prev_reward': False,\n",
       " '_time_major': False,\n",
       " 'use_attention': False,\n",
       " 'attention_num_transformer_units': 1,\n",
       " 'attention_dim': 64,\n",
       " 'attention_num_heads': 1,\n",
       " 'attention_head_dim': 32,\n",
       " 'attention_memory_inference': 50,\n",
       " 'attention_memory_training': 50,\n",
       " 'attention_position_wise_mlp_dim': 32,\n",
       " 'attention_init_gru_gate_bias': 2.0,\n",
       " 'attention_use_n_prev_actions': 0,\n",
       " 'attention_use_n_prev_rewards': 0,\n",
       " 'framestack': True,\n",
       " 'dim': 84,\n",
       " 'grayscale': False,\n",
       " 'zero_mean': True,\n",
       " 'custom_model': None,\n",
       " 'custom_model_config': {},\n",
       " 'custom_action_dist': None,\n",
       " 'custom_preprocessor': None,\n",
       " 'lstm_use_prev_action_reward': -1}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_config.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e796b4de-0256-47a2-9f4b-7e724dd3f0be",
   "metadata": {},
   "source": [
    "#### How do we tune?\n",
    "\n",
    "- Do we resort to tuning by hand?\n",
    "- No!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9a6bb4-1668-4b87-a908-2a41204458f8",
   "metadata": {},
   "source": [
    "#### Introducing Ray tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32a3266-9897-4f02-939d-7c66472d7e1f",
   "metadata": {},
   "source": [
    "![](img/rllib_and_tune.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d7f8f2dd-7ff0-402f-a2d9-274e5b66d740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416e145a-7702-4360-8f99-f6f1f417dd5f",
   "metadata": {},
   "source": [
    "#### Tune usage\n",
    "\n",
    "- Tune is its own sub-package of Ray, like RLlib\n",
    "- It is sophisticated and has its own entire documentation [here](https://docs.ray.io/en/latest/tune/index.html)\n",
    "- For our purposes, we will focus on this syntax:\n",
    "\n",
    "Instead of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "34be5c26-e207-4898-b241-2edd55721bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_config = ppo_config.training(\n",
    "    lr=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c275bda6-7765-42f5-a8dc-f7e10aa5d83a",
   "metadata": {},
   "source": [
    "we do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "96a8ce7f-d282-4076-9658-18204402dbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_config = ppo_config.training(\n",
    "    lr=tune.grid_search([1e-4, 5e-5])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2677f540-e3b4-4d99-a3c9-125719db13a5",
   "metadata": {},
   "source": [
    "We're setting up `tune` to automatically sweep these values!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4c7621-4181-4891-afb6-6eaf05ab3895",
   "metadata": {},
   "source": [
    "#### Running the sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6bcef240-eb0c-4670-8551-bf66e894d186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=6445)\u001b[0m /Users/mike/git/anyscale/ray/python/ray/rllib/utils/debug/deterministic.py:42: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(PPO pid=6445)\u001b[0m   if LooseVersion(torch.__version__) >= LooseVersion(\"1.8.0\"):\n",
      "\u001b[2m\u001b[36m(PPO pid=6445)\u001b[0m 2022-07-30 19:16:27,149\tINFO ppo.py:378 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPO pid=6445)\u001b[0m 2022-07-30 19:16:27,149\tINFO algorithm.py:332 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PPO pid=6444)\u001b[0m /Users/mike/git/anyscale/ray/python/ray/rllib/utils/debug/deterministic.py:42: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(PPO pid=6444)\u001b[0m   if LooseVersion(torch.__version__) >= LooseVersion(\"1.8.0\"):\n",
      "\u001b[2m\u001b[36m(PPO pid=6444)\u001b[0m 2022-07-30 19:16:27,149\tINFO ppo.py:378 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPO pid=6444)\u001b[0m 2022-07-30 19:16:27,149\tINFO algorithm.py:332 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6453)\u001b[0m /Users/mike/git/anyscale/ray/python/ray/rllib/utils/debug/deterministic.py:42: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6453)\u001b[0m   if LooseVersion(torch.__version__) >= LooseVersion(\"1.8.0\"):\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6453)\u001b[0m /Users/mike/miniconda3/envs/ray2beta/lib/python3.9/site-packages/gym/core.py:172: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed) instead.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6453)\u001b[0m   deprecation(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6452)\u001b[0m /Users/mike/git/anyscale/ray/python/ray/rllib/utils/debug/deterministic.py:42: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6452)\u001b[0m   if LooseVersion(torch.__version__) >= LooseVersion(\"1.8.0\"):\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6452)\u001b[0m /Users/mike/miniconda3/envs/ray2beta/lib/python3.9/site-packages/gym/core.py:172: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed) instead.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6452)\u001b[0m   deprecation(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6451)\u001b[0m /Users/mike/git/anyscale/ray/python/ray/rllib/utils/debug/deterministic.py:42: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6451)\u001b[0m   if LooseVersion(torch.__version__) >= LooseVersion(\"1.8.0\"):\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6451)\u001b[0m /Users/mike/miniconda3/envs/ray2beta/lib/python3.9/site-packages/gym/core.py:172: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed) instead.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6451)\u001b[0m   deprecation(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6450)\u001b[0m /Users/mike/git/anyscale/ray/python/ray/rllib/utils/debug/deterministic.py:42: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6450)\u001b[0m   if LooseVersion(torch.__version__) >= LooseVersion(\"1.8.0\"):\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6450)\u001b[0m /Users/mike/miniconda3/envs/ray2beta/lib/python3.9/site-packages/gym/core.py:172: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed) instead.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6450)\u001b[0m   deprecation(\n"
     ]
    }
   ],
   "source": [
    "ppo_config = ppo_config.environment(env=\"FrozenLake-v1\")\n",
    "\n",
    "analysis = tune.run(\n",
    "    \"PPO\",\n",
    "    config            = ppo_config.to_dict(),\n",
    "    stop              = {\"training_iteration\" : 5},\n",
    "    checkpoint_freq   = 1,\n",
    "    verbose           = 0,\n",
    "    metric=\"episode_reward_mean\",\n",
    "    mode=\"max\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2fec4efa-a9a8-4ee5-af55-fe3e0c1a8120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0001"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis.best_config[\"lr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "12dbe1c1-9cf7-435b-9950-e55057c26ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/mike/ray_results/PPO/PPO_FrozenLake-v1_c6bf1_00000_0_lr=0.0001_2022-07-30_19-16-24'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis.get_best_logdir(\"episode_reward_mean\", mode=\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a432bc01-b2dc-4ebd-9c24-6825b5a5e46f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/mike/ray_results/PPO/PPO_FrozenLake-v1_c6bf1_00000_0_lr=0.0001_2022-07-30_19-16-24'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis.best_logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "aaf2cf9e-f2b9-426b-974f-bd69fb842a9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'episode_reward_mean'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [80]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43manalysis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_best_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepisode_reward_mean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/tune/analysis/experiment_analysis.py:469\u001b[0m, in \u001b[0;36mExperimentAnalysis.get_best_checkpoint\u001b[0;34m(self, trial, metric, mode)\u001b[0m\n\u001b[1;32m    466\u001b[0m metric \u001b[38;5;241m=\u001b[39m metric \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_metric \u001b[38;5;129;01mor\u001b[39;00m TRAINING_ITERATION\n\u001b[1;32m    467\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_mode(mode)\n\u001b[0;32m--> 469\u001b[0m checkpoint_paths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_trial_checkpoints_paths\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;66;03m# Filter out nan. Sorting nan values leads to undefined behavior.\u001b[39;00m\n\u001b[1;32m    472\u001b[0m checkpoint_paths \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    473\u001b[0m     (path, metric) \u001b[38;5;28;01mfor\u001b[39;00m path, metric \u001b[38;5;129;01min\u001b[39;00m checkpoint_paths \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_nan(metric)\n\u001b[1;32m    474\u001b[0m ]\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/tune/analysis/experiment_analysis.py:433\u001b[0m, in \u001b[0;36mExperimentAnalysis.get_trial_checkpoints_paths\u001b[0;34m(self, trial, metric)\u001b[0m\n\u001b[1;32m    430\u001b[0m chkpt_df \u001b[38;5;241m=\u001b[39m TrainableUtil\u001b[38;5;241m.\u001b[39mget_checkpoints_paths(trial_dir)\n\u001b[1;32m    432\u001b[0m \u001b[38;5;66;03m# Join with trial dataframe to get metrics.\u001b[39;00m\n\u001b[0;32m--> 433\u001b[0m trial_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrial_dataframes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrial_dir\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    434\u001b[0m path_metric_df \u001b[38;5;241m=\u001b[39m chkpt_df\u001b[38;5;241m.\u001b[39mmerge(\n\u001b[1;32m    435\u001b[0m     trial_df, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_iteration\u001b[39m\u001b[38;5;124m\"\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    436\u001b[0m )\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m path_metric_df[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchkpt_path\u001b[39m\u001b[38;5;124m\"\u001b[39m, metric]]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist()\n",
      "\u001b[0;31mKeyError\u001b[0m: 'episode_reward_mean'"
     ]
    }
   ],
   "source": [
    "analysis.get_best_checkpoint(\"episode_reward_mean\", mode=\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "42fa4b59-c237-4f49-9aad-486d4f916b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PPO_FrozenLake-v1_c6bf1_00000, PPO_FrozenLake-v1_c6bf1_00001]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials = analysis.trials\n",
    "trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e8fcfa94-83b3-42aa-b118-f42842519b53",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Cannot create checkpoint from URI as it is not supported: None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [83]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m best_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43manalysis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_best_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrials\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepisode_reward_mean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/tune/analysis/experiment_analysis.py:487\u001b[0m, in \u001b[0;36mExperimentAnalysis.get_best_checkpoint\u001b[0;34m(self, trial, metric, mode)\u001b[0m\n\u001b[1;32m    484\u001b[0m cloud_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_cloud_path(best_path)\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_legacy_checkpoint:\n\u001b[0;32m--> 487\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTrialCheckpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbest_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcloud_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcloud_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cloud_path:\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;66;03m# Prefer cloud path over local path for downsteam processing\u001b[39;00m\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Checkpoint\u001b[38;5;241m.\u001b[39mfrom_uri(cloud_path)\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/tune/cloud.py:86\u001b[0m, in \u001b[0;36mTrialCheckpoint.__init__\u001b[0;34m(self, local_path, cloud_path)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Checkpoint does not allow empty data, but TrialCheckpoint\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# did. To keep backwards compatibility, we use a placeholder URI\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# here, and manually set self._uri and self._local_dir later.\u001b[39;00m\n\u001b[1;32m     85\u001b[0m PLACEHOLDER \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3://placeholder\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 86\u001b[0m \u001b[43mCheckpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPLACEHOLDER\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Reset local variables\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_uri \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/air/checkpoint.py:184\u001b[0m, in \u001b[0;36mCheckpoint.__init__\u001b[0;34m(self, local_path, data_dict, uri, obj_ref)\u001b[0m\n\u001b[1;32m    182\u001b[0m     resolved \u001b[38;5;241m=\u001b[39m _get_external_path(uri)\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m resolved:\n\u001b[0;32m--> 184\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    185\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot create checkpoint from URI as it is not \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    186\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupported: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    187\u001b[0m         )\n\u001b[1;32m    188\u001b[0m     uri \u001b[38;5;241m=\u001b[39m resolved\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Cannot create checkpoint from URI as it is not supported: None"
     ]
    }
   ],
   "source": [
    "best_checkpoint = analysis.get_best_checkpoint(trial=trials[0], metric=\"episode_reward_mean\", mode=\"max\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0b4bfb1b-1174-4f59-95a5-7d046a94379c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Cannot create checkpoint from URI as it is not supported: None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [72]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43manalysis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_checkpoint\u001b[49m\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/tune/analysis/experiment_analysis.py:264\u001b[0m, in \u001b[0;36mExperimentAnalysis.best_checkpoint\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m best_trial:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo best trial found. Please check if you specified the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorrect default metric (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_metric\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) and mode \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    262\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_mode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    263\u001b[0m     )\n\u001b[0;32m--> 264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_best_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbest_trial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_metric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_mode\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/tune/analysis/experiment_analysis.py:487\u001b[0m, in \u001b[0;36mExperimentAnalysis.get_best_checkpoint\u001b[0;34m(self, trial, metric, mode)\u001b[0m\n\u001b[1;32m    484\u001b[0m cloud_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_cloud_path(best_path)\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_legacy_checkpoint:\n\u001b[0;32m--> 487\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTrialCheckpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbest_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcloud_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcloud_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cloud_path:\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;66;03m# Prefer cloud path over local path for downsteam processing\u001b[39;00m\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Checkpoint\u001b[38;5;241m.\u001b[39mfrom_uri(cloud_path)\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/tune/cloud.py:86\u001b[0m, in \u001b[0;36mTrialCheckpoint.__init__\u001b[0;34m(self, local_path, cloud_path)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Checkpoint does not allow empty data, but TrialCheckpoint\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# did. To keep backwards compatibility, we use a placeholder URI\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# here, and manually set self._uri and self._local_dir later.\u001b[39;00m\n\u001b[1;32m     85\u001b[0m PLACEHOLDER \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3://placeholder\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 86\u001b[0m \u001b[43mCheckpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPLACEHOLDER\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Reset local variables\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_uri \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/air/checkpoint.py:184\u001b[0m, in \u001b[0;36mCheckpoint.__init__\u001b[0;34m(self, local_path, data_dict, uri, obj_ref)\u001b[0m\n\u001b[1;32m    182\u001b[0m     resolved \u001b[38;5;241m=\u001b[39m _get_external_path(uri)\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m resolved:\n\u001b[0;32m--> 184\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    185\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot create checkpoint from URI as it is not \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    186\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupported: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    187\u001b[0m         )\n\u001b[1;32m    188\u001b[0m     uri \u001b[38;5;241m=\u001b[39m resolved\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Cannot create checkpoint from URI as it is not supported: None"
     ]
    }
   ],
   "source": [
    "analysis.best_checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7185c971-e7a2-4cb3-92b8-59ed428fafcd",
   "metadata": {},
   "source": [
    "Gotcha? Tune checkpoints vs. RLlib checkpoints?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1abd8ae-73e9-48f7-8bfe-c2e7b5ab4cb9",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "- `config`: the config file\n",
    "- `stop: the stopping condition. Other possibilities are the total number of timesteps or reaching a certain reward value.\n",
    "- `checkpoint_at_end:\n",
    "- `verbose\n",
    "- `metric`:\n",
    "- `mode`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b327fc30-a3a2-4e47-9ac8-92b4f577d2ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-14 09:51:31 (running for 00:00:00.22)<br>Memory usage on this node: 6.8/8.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/3.05 GiB heap, 0.0/1.53 GiB objects<br>Result logdir: /Users/mike/git/anyscale/rl-course/notebooks/04-application-recommender/PPO<br>Number of trials: 1/1 (1 PENDING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=63452)\u001b[0m 2022-07-14 09:51:34,080\tINFO ppo.py:249 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=63452)\u001b[0m 2022-07-14 09:51:34,090\tINFO trainer.py:779 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-14 09:51:36 (running for 00:00:05.46)<br>Memory usage on this node: 6.8/8.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/0 GPUs, 0.0/3.05 GiB heap, 0.0/1.53 GiB objects<br>Result logdir: /Users/mike/git/anyscale/rl-course/notebooks/04-application-recommender/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial PPO_BasicRecommender_36008_00000 reported episode_reward_max=26.229951468536488,episode_reward_min=24.437207722145967,episode_reward_mean=25.367144631882343,episode_len_mean=100.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.03270105622161454, 'mean_inference_ms': 0.25230732516966003, 'mean_action_processing_ms': 0.015700894078869987, 'mean_env_wait_ms': 0.014201394919453113, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,timesteps_this_iter=4000,agent_timesteps_total=4000,timers={'sample_time_ms': 3011.171, 'sample_throughput': 1328.387, 'load_time_ms': 6.652, 'load_throughput': 601333.907, 'learn_time_ms': 1587.199, 'learn_throughput': 2520.163, 'update_time_ms': 2.732},info={'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.20000000000000004, 'cur_lr': 0.0010000000000000005, 'total_loss': 25.43452578206216, 'policy_loss': -0.014144874419716577, 'vf_loss': 25.447020125645462, 'vf_explained_var': -0.001425929287428497, 'kl': 0.008252872885188042, 'entropy': 0.6851955661850591, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}}}, 'num_steps_sampled': 4000, 'num_agent_steps_sampled': 4000, 'num_steps_trained': 4000, 'num_steps_trained_this_iter': 4000, 'num_agent_steps_trained': 4000},perf={'cpu_util_percent': 34.114285714285714, 'ram_util_percent': 85.31428571428572} with parameters={'framework': 'torch', 'create_env_on_driver': True, 'seed': 0, 'lr': 0.001, 'model': {'fcnet_hiddens': [64, 64]}, 'env': <class 'envs.BasicRecommender'>, 'env_config': {'num_candidates': 2, 'alpha': 0.5, 'seed': 42}}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-14 09:51:42 (running for 00:00:11.04)<br>Memory usage on this node: 6.8/8.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/0 GPUs, 0.0/3.05 GiB heap, 0.0/1.53 GiB objects<br>Result logdir: /Users/mike/git/anyscale/rl-course/notebooks/04-application-recommender/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial PPO_BasicRecommender_36008_00000 reported episode_reward_max=26.52690953227332,episode_reward_min=24.093393864945554,episode_reward_mean=25.429153526172882,episode_len_mean=100.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.036330457471485056, 'mean_inference_ms': 0.2579944025246475, 'mean_action_processing_ms': 0.015710399918824974, 'mean_env_wait_ms': 0.01514268198918096, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,timesteps_this_iter=4000,agent_timesteps_total=16000,timers={'sample_time_ms': 2459.617, 'sample_throughput': 1626.269, 'load_time_ms': 1.859, 'load_throughput': 2151752.725, 'learn_time_ms': 1446.732, 'learn_throughput': 2764.852, 'update_time_ms': 3.082},info={'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.20000000000000004, 'cur_lr': 0.0010000000000000005, 'total_loss': 20.42486534836472, 'policy_loss': -0.012980463006283327, 'vf_loss': 20.436380077690206, 'vf_explained_var': 5.428579545790149e-05, 'kl': 0.007328934054126103, 'entropy': 0.6597061257208547, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}}}, 'num_steps_sampled': 16000, 'num_agent_steps_sampled': 16000, 'num_steps_trained': 16000, 'num_steps_trained_this_iter': 4000, 'num_agent_steps_trained': 16000},perf={'cpu_util_percent': 46.166666666666664, 'ram_util_percent': 86.0} with parameters={'framework': 'torch', 'create_env_on_driver': True, 'seed': 0, 'lr': 0.001, 'model': {'fcnet_hiddens': [64, 64]}, 'env': <class 'envs.BasicRecommender'>, 'env_config': {'num_candidates': 2, 'alpha': 0.5, 'seed': 42}}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-14 09:51:47 (running for 00:00:16.62)<br>Memory usage on this node: 6.8/8.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/0 GPUs, 0.0/3.05 GiB heap, 0.0/1.53 GiB objects<br>Result logdir: /Users/mike/git/anyscale/rl-course/notebooks/04-application-recommender/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial PPO_BasicRecommender_36008_00000 reported episode_reward_max=26.771151646915843,episode_reward_min=24.011689111219024,episode_reward_mean=25.575060131062806,episode_len_mean=100.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.0378616620354775, 'mean_inference_ms': 0.263690299528818, 'mean_action_processing_ms': 0.016213090765989046, 'mean_env_wait_ms': 0.015341564616789033, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,timesteps_this_iter=4000,agent_timesteps_total=28000,timers={'sample_time_ms': 2288.985, 'sample_throughput': 1747.5, 'load_time_ms': 1.285, 'load_throughput': 3112573.534, 'learn_time_ms': 1380.818, 'learn_throughput': 2896.834, 'update_time_ms': 2.364},info={'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.10000000000000002, 'cur_lr': 0.0010000000000000005, 'total_loss': 20.654890163995887, 'policy_loss': -0.01704748478988486, 'vf_loss': 20.6707669391427, 'vf_explained_var': 2.0207640945270496e-05, 'kl': 0.011707536025864792, 'entropy': 0.648529960134978, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}}}, 'num_steps_sampled': 28000, 'num_agent_steps_sampled': 28000, 'num_steps_trained': 28000, 'num_steps_trained_this_iter': 4000, 'num_agent_steps_trained': 28000},perf={'cpu_util_percent': 31.599999999999998, 'ram_util_percent': 85.23333333333333} with parameters={'framework': 'torch', 'create_env_on_driver': True, 'seed': 0, 'lr': 0.001, 'model': {'fcnet_hiddens': [64, 64]}, 'env': <class 'envs.BasicRecommender'>, 'env_config': {'num_candidates': 2, 'alpha': 0.5, 'seed': 42}}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-14 09:51:52 (running for 00:00:21.63)<br>Memory usage on this node: 6.8/8.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/0 GPUs, 0.0/3.05 GiB heap, 0.0/1.53 GiB objects<br>Result logdir: /Users/mike/git/anyscale/rl-course/notebooks/04-application-recommender/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-14 09:51:57 (running for 00:00:26.74)<br>Memory usage on this node: 6.8/8.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/8 CPUs, 0/0 GPUs, 0.0/3.05 GiB heap, 0.0/1.53 GiB objects<br>Result logdir: /Users/mike/git/anyscale/rl-course/notebooks/04-application-recommender/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial PPO_BasicRecommender_36008_00000 reported episode_reward_max=26.6548669958362,episode_reward_min=24.056177303270747,episode_reward_mean=25.540941366961775,episode_len_mean=100.0,episode_media={},episodes_this_iter=40,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.036447590817010475, 'mean_inference_ms': 0.25664676157687394, 'mean_action_processing_ms': 0.015811126708607494, 'mean_env_wait_ms': 0.014775917683812385, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=2,timesteps_this_iter=4000,agent_timesteps_total=40000,timers={'sample_time_ms': 2217.21, 'sample_throughput': 1804.069, 'load_time_ms': 1.0, 'load_throughput': 4000862.307, 'learn_time_ms': 1385.19, 'learn_throughput': 2887.692, 'update_time_ms': 2.919},info={'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.10000000000000002, 'cur_lr': 0.0010000000000000005, 'total_loss': 20.466726658933908, 'policy_loss': -0.015410010112068986, 'vf_loss': 20.48122098368983, 'vf_explained_var': -1.1669948536862609e-05, 'kl': 0.009155829549640001, 'entropy': 0.6179378610144379, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}}}, 'num_steps_sampled': 40000, 'num_agent_steps_sampled': 40000, 'num_steps_trained': 40000, 'num_steps_trained_this_iter': 4000, 'num_agent_steps_trained': 40000},perf={'cpu_util_percent': 45.633333333333326, 'ram_util_percent': 85.39999999999999} with parameters={'framework': 'torch', 'create_env_on_driver': True, 'seed': 0, 'lr': 0.001, 'model': {'fcnet_hiddens': [64, 64]}, 'env': <class 'envs.BasicRecommender'>, 'env_config': {'num_candidates': 2, 'alpha': 0.5, 'seed': 42}}. This trial completed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-14 09:51:57 (running for 00:00:26.83)<br>Memory usage on this node: 6.8/8.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/3.05 GiB heap, 0.0/1.53 GiB objects<br>Result logdir: /Users/mike/git/anyscale/rl-course/notebooks/04-application-recommender/PPO<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BasicRecommender_36008_00000</td><td>TERMINATED</td><td>127.0.0.1:63452</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         23.1234</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\"> 25.5409</td><td style=\"text-align: right;\">             26.6549</td><td style=\"text-align: right;\">             24.0562</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=63452)\u001b[0m 2022-07-14 09:51:57,783\tWARNING deprecation.py:45 -- DeprecationWarning: `clear_buffer` has been deprecated. Use `Filter.reset_buffer()` instead. This will raise an error in the future!\n",
      "2022-07-14 09:51:58,607\tINFO tune.py:639 -- Total run time: 27.73 seconds (26.82 seconds for the tuning loop).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x126f5e7f0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tune.run(\"PPO\", config = tune_config, stop = {\"training_iteration\": 10}, checkpoint_at_end=True, verbose=2, local_dir=\".\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ray2beta]",
   "language": "python",
   "name": "conda-env-ray2beta-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
