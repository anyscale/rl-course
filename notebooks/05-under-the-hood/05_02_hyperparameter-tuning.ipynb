{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b02cdbb-95e8-451d-87d5-8a3f07088797",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5935760-7836-4a88-bb36-91a2caa600e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import ray\n",
    "import logging\n",
    "ray.init(log_to_driver=False, ignore_reinit_error=True, logging_level=logging.ERROR) # logging.FATAL\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7584b00e-80e1-4db0-b5cf-3f06e41b020a",
   "metadata": {},
   "source": [
    "#### Hyperparameters\n",
    "\n",
    "Recall our discussion of RLlib config files in Module 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b27c3978-17f5-4e04-b65f-8574b9b7d38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9064d95-d181-4217-a991-a8e790d9b2fb",
   "metadata": {},
   "source": [
    "We wrote code like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eac41c32-da41-4afd-9f6f-1eddae8ab73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_config = (\n",
    "    PPOConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(create_env_on_local_worker=True)\\\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\\\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c53b89b-5554-4cb0-b687-79cc99bd48ca",
   "metadata": {},
   "source": [
    "However, we are only setting a tiny fraction of the config options available in RLlib."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f66b89-8ef6-48b3-aa7e-279df9b985a4",
   "metadata": {},
   "source": [
    "#### Endless configs!\n",
    "\n",
    "Here is the full list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99d28bce-7eb5-4238-9726-a0327bca82b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ppo_config.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a031774-708a-45d6-8565-d18d9280eafc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'extra_python_environs_for_driver': {},\n",
       " 'extra_python_environs_for_worker': {},\n",
       " 'num_gpus': 0,\n",
       " 'num_cpus_per_worker': 1,\n",
       " 'num_gpus_per_worker': 0,\n",
       " '_fake_gpus': False,\n",
       " 'custom_resources_per_worker': {},\n",
       " 'placement_strategy': 'PACK',\n",
       " 'eager_tracing': False,\n",
       " 'eager_max_retraces': 20,\n",
       " 'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "  'inter_op_parallelism_threads': 2,\n",
       "  'gpu_options': {'allow_growth': True},\n",
       "  'log_device_placement': False,\n",
       "  'device_count': {'CPU': 1},\n",
       "  'allow_soft_placement': True},\n",
       " 'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "  'inter_op_parallelism_threads': 8},\n",
       " 'env': None,\n",
       " 'env_config': {},\n",
       " 'observation_space': None,\n",
       " 'action_space': None,\n",
       " 'env_task_fn': None,\n",
       " 'render_env': False,\n",
       " 'clip_rewards': None,\n",
       " 'normalize_actions': True,\n",
       " 'clip_actions': False,\n",
       " 'disable_env_checking': False,\n",
       " 'num_workers': 2,\n",
       " 'num_envs_per_worker': 1,\n",
       " 'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       " 'sample_async': False,\n",
       " 'enable_connectors': False,\n",
       " 'rollout_fragment_length': 200,\n",
       " 'batch_mode': 'truncate_episodes',\n",
       " 'remote_worker_envs': False,\n",
       " 'remote_env_batch_wait_ms': 0,\n",
       " 'validate_workers_after_construction': True,\n",
       " 'ignore_worker_failures': False,\n",
       " 'recreate_failed_workers': False,\n",
       " 'restart_failed_sub_environments': False,\n",
       " 'num_consecutive_worker_failures_tolerance': 100,\n",
       " 'horizon': None,\n",
       " 'soft_horizon': False,\n",
       " 'no_done_at_end': False,\n",
       " 'preprocessor_pref': 'deepmind',\n",
       " 'observation_filter': 'NoFilter',\n",
       " 'synchronize_filters': True,\n",
       " 'compress_observations': False,\n",
       " 'enable_tf1_exec_eagerly': False,\n",
       " 'sampler_perf_stats_ema_coef': None,\n",
       " 'gamma': 0.99,\n",
       " 'lr': 5e-05,\n",
       " 'train_batch_size': 4000,\n",
       " 'model': {'_use_default_native_models': False,\n",
       "  '_disable_preprocessor_api': False,\n",
       "  '_disable_action_flattening': False,\n",
       "  'fcnet_hiddens': [256, 256],\n",
       "  'fcnet_activation': 'tanh',\n",
       "  'conv_filters': None,\n",
       "  'conv_activation': 'relu',\n",
       "  'post_fcnet_hiddens': [],\n",
       "  'post_fcnet_activation': 'relu',\n",
       "  'free_log_std': False,\n",
       "  'no_final_linear': False,\n",
       "  'vf_share_layers': False,\n",
       "  'use_lstm': False,\n",
       "  'max_seq_len': 20,\n",
       "  'lstm_cell_size': 256,\n",
       "  'lstm_use_prev_action': False,\n",
       "  'lstm_use_prev_reward': False,\n",
       "  '_time_major': False,\n",
       "  'use_attention': False,\n",
       "  'attention_num_transformer_units': 1,\n",
       "  'attention_dim': 64,\n",
       "  'attention_num_heads': 1,\n",
       "  'attention_head_dim': 32,\n",
       "  'attention_memory_inference': 50,\n",
       "  'attention_memory_training': 50,\n",
       "  'attention_position_wise_mlp_dim': 32,\n",
       "  'attention_init_gru_gate_bias': 2.0,\n",
       "  'attention_use_n_prev_actions': 0,\n",
       "  'attention_use_n_prev_rewards': 0,\n",
       "  'framestack': True,\n",
       "  'dim': 84,\n",
       "  'grayscale': False,\n",
       "  'zero_mean': True,\n",
       "  'custom_model': None,\n",
       "  'custom_model_config': {},\n",
       "  'custom_action_dist': None,\n",
       "  'custom_preprocessor': None,\n",
       "  'lstm_use_prev_action_reward': -1},\n",
       " 'optimizer': {},\n",
       " 'explore': True,\n",
       " 'exploration_config': {'type': 'StochasticSampling'},\n",
       " 'input_config': {},\n",
       " 'actions_in_input_normalized': False,\n",
       " 'postprocess_inputs': False,\n",
       " 'shuffle_buffer_size': 0,\n",
       " 'output': None,\n",
       " 'output_config': {},\n",
       " 'output_compress_columns': ['obs', 'new_obs'],\n",
       " 'output_max_file_size': 67108864,\n",
       " 'evaluation_interval': None,\n",
       " 'evaluation_duration': 10,\n",
       " 'evaluation_duration_unit': 'episodes',\n",
       " 'evaluation_sample_timeout_s': 180.0,\n",
       " 'evaluation_parallel_to_training': False,\n",
       " 'evaluation_config': {},\n",
       " 'off_policy_estimation_methods': {},\n",
       " 'evaluation_num_workers': 0,\n",
       " 'always_attach_evaluation_results': False,\n",
       " 'in_evaluation': False,\n",
       " 'sync_filters_on_rollout_workers_timeout_s': 60.0,\n",
       " 'keep_per_episode_custom_metrics': False,\n",
       " 'metrics_episode_collection_timeout_s': 60.0,\n",
       " 'metrics_num_episodes_for_smoothing': 100,\n",
       " 'min_time_s_per_iteration': None,\n",
       " 'min_train_timesteps_per_iteration': 0,\n",
       " 'min_sample_timesteps_per_iteration': 0,\n",
       " 'logger_creator': None,\n",
       " 'logger_config': None,\n",
       " 'log_level': 'ERROR',\n",
       " 'log_sys_usage': True,\n",
       " 'fake_sampler': False,\n",
       " 'seed': 0,\n",
       " '_tf_policy_handles_more_than_one_loss': False,\n",
       " '_disable_preprocessor_api': False,\n",
       " '_disable_action_flattening': False,\n",
       " '_disable_execution_plan_api': True,\n",
       " 'simple_optimizer': -1,\n",
       " 'monitor': -1,\n",
       " 'evaluation_num_episodes': -1,\n",
       " 'metrics_smoothing_episodes': -1,\n",
       " 'timesteps_per_iteration': -1,\n",
       " 'min_iter_time_s': -1,\n",
       " 'collect_metrics_timeout': -1,\n",
       " 'buffer_size': -1,\n",
       " 'prioritized_replay': -1,\n",
       " 'learning_starts': -1,\n",
       " 'replay_batch_size': -1,\n",
       " 'replay_sequence_length': None,\n",
       " 'prioritized_replay_alpha': -1,\n",
       " 'prioritized_replay_beta': -1,\n",
       " 'prioritized_replay_eps': -1,\n",
       " 'min_time_s_per_reporting': -1,\n",
       " 'min_train_timesteps_per_reporting': -1,\n",
       " 'min_sample_timesteps_per_reporting': -1,\n",
       " 'input_evaluation': -1,\n",
       " 'lr_schedule': None,\n",
       " 'use_critic': True,\n",
       " 'use_gae': True,\n",
       " 'kl_coeff': 0.2,\n",
       " 'sgd_minibatch_size': 128,\n",
       " 'num_sgd_iter': 30,\n",
       " 'shuffle_sequences': True,\n",
       " 'vf_loss_coeff': 1.0,\n",
       " 'entropy_coeff': 0.0,\n",
       " 'entropy_coeff_schedule': None,\n",
       " 'clip_param': 0.3,\n",
       " 'vf_clip_param': 10.0,\n",
       " 'grad_clip': None,\n",
       " 'kl_target': 0.01,\n",
       " 'vf_share_layers': -1,\n",
       " 'lambda': 1.0,\n",
       " 'input': 'sampler',\n",
       " 'multiagent': {'policies': {},\n",
       "  'policy_map_capacity': 100,\n",
       "  'policy_map_cache': None,\n",
       "  'policy_mapping_fn': None,\n",
       "  'policies_to_train': None,\n",
       "  'observation_fn': None,\n",
       "  'replay_mode': 'independent',\n",
       "  'count_steps_by': 'env_steps'},\n",
       " 'callbacks': ray.rllib.algorithms.callbacks.DefaultCallbacks,\n",
       " 'create_env_on_driver': True,\n",
       " 'custom_eval_function': None,\n",
       " 'framework': 'torch',\n",
       " 'num_cpus_for_driver': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_config.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791e26d9-39ac-4e67-8fbc-08b51005b03f",
   "metadata": {},
   "source": [
    "#### Key hyperparameters\n",
    "\n",
    "We recommend focusing on the following key hyperparameters:\n",
    "\n",
    "- `lr`\n",
    "- `train_batch_size`\n",
    "- `sgd_minibatch_size`\n",
    "- `num_sgd_iter`\n",
    "- `entropy_coeff`\n",
    "- model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a27a37-5d43-4bc1-9258-00ba03c78a66",
   "metadata": {},
   "source": [
    "#### Key hyperparameter interpretations\n",
    "\n",
    "Let's look at the definitions of these key hyperparameters:\n",
    "\n",
    "- `lr`: learning rate\n",
    "- `train_batch_size`: number of iterations of data to be batched together\n",
    "- `sgd_minibatch_size`: minibatch size for SGD\n",
    "- `num_sgd_iter`: epochs of SGD per iteration of PPO\n",
    "- `entropy_coeff`: measure the amount of exploration during training\n",
    "- model architecture: of the policy network\n",
    "\n",
    "Notes:\n",
    "\n",
    "If you are not familiar with deep learning, most of these hyperparameters will not make sense. That is fine, you can skip."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9863bb3e-c5f2-4de5-a6d9-cec0d9163055",
   "metadata": {},
   "source": [
    "#### Key hyperparameter defaults\n",
    "\n",
    "Let's look at the defaults of these key hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b43d9b15-b999-4416-859d-579e084581e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5e-05"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_config.lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63958eb0-50ad-4692-99cd-42e58703f9f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_config.train_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9af8a557-0383-47d7-94aa-4ef37b619bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_config.sgd_minibatch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29f3d4b0-e645-4df6-a08b-8e7d82acb19d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_config.num_sgd_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b395eb4-1569-4310-b18a-0676762a4e23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_config.entropy_coeff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01411131-cb8b-4859-8418-f9d047518a6c",
   "metadata": {},
   "source": [
    "#### Key hyperparameter defaults\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c429e78f-e62d-48c4-9286-0c442690aa2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_use_default_native_models': False,\n",
       " '_disable_preprocessor_api': False,\n",
       " '_disable_action_flattening': False,\n",
       " 'fcnet_hiddens': [256, 256],\n",
       " 'fcnet_activation': 'tanh',\n",
       " 'conv_filters': None,\n",
       " 'conv_activation': 'relu',\n",
       " 'post_fcnet_hiddens': [],\n",
       " 'post_fcnet_activation': 'relu',\n",
       " 'free_log_std': False,\n",
       " 'no_final_linear': False,\n",
       " 'vf_share_layers': False,\n",
       " 'use_lstm': False,\n",
       " 'max_seq_len': 20,\n",
       " 'lstm_cell_size': 256,\n",
       " 'lstm_use_prev_action': False,\n",
       " 'lstm_use_prev_reward': False,\n",
       " '_time_major': False,\n",
       " 'use_attention': False,\n",
       " 'attention_num_transformer_units': 1,\n",
       " 'attention_dim': 64,\n",
       " 'attention_num_heads': 1,\n",
       " 'attention_head_dim': 32,\n",
       " 'attention_memory_inference': 50,\n",
       " 'attention_memory_training': 50,\n",
       " 'attention_position_wise_mlp_dim': 32,\n",
       " 'attention_init_gru_gate_bias': 2.0,\n",
       " 'attention_use_n_prev_actions': 0,\n",
       " 'attention_use_n_prev_rewards': 0,\n",
       " 'framestack': True,\n",
       " 'dim': 84,\n",
       " 'grayscale': False,\n",
       " 'zero_mean': True,\n",
       " 'custom_model': None,\n",
       " 'custom_model_config': {},\n",
       " 'custom_action_dist': None,\n",
       " 'custom_preprocessor': None,\n",
       " 'lstm_use_prev_action_reward': -1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_config.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e796b4de-0256-47a2-9f4b-7e724dd3f0be",
   "metadata": {},
   "source": [
    "#### How do we tune?\n",
    "\n",
    "- Do we resort to tuning by hand?\n",
    "- No!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9a6bb4-1668-4b87-a908-2a41204458f8",
   "metadata": {},
   "source": [
    "#### Introducing Ray tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32a3266-9897-4f02-939d-7c66472d7e1f",
   "metadata": {},
   "source": [
    "![](img/rllib_and_tune.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7f8f2dd-7ff0-402f-a2d9-274e5b66d740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416e145a-7702-4360-8f99-f6f1f417dd5f",
   "metadata": {},
   "source": [
    "#### Tune usage\n",
    "\n",
    "- Tune is its own sub-package of Ray, like RLlib\n",
    "- It is sophisticated and has its own entire documentation [here](https://docs.ray.io/en/latest/tune/index.html)\n",
    "- For our purposes, we will focus on this syntax:\n",
    "\n",
    "Instead of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34be5c26-e207-4898-b241-2edd55721bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_config = ppo_config.training(\n",
    "    lr=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c275bda6-7765-42f5-a8dc-f7e10aa5d83a",
   "metadata": {},
   "source": [
    "we do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96a8ce7f-d282-4076-9658-18204402dbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_config = ppo_config.training(\n",
    "    lr=tune.grid_search([1e-4, 5e-5])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2677f540-e3b4-4d99-a3c9-125719db13a5",
   "metadata": {},
   "source": [
    "We're setting up `tune` to automatically sweep these values!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4c7621-4181-4891-afb6-6eaf05ab3895",
   "metadata": {},
   "source": [
    "#### Running the sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6bcef240-eb0c-4670-8551-bf66e894d186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mike/git/anyscale/ray/python/ray/util/placement_group.py:78: DeprecationWarning: placement_group parameter is deprecated. Use scheduling_strategy=PlacementGroupSchedulingStrategy(...) instead, see the usage at https://docs.ray.io/en/master/ray-core/package-ref.html#ray-remote.\n",
      "  return bundle_reservation_check.options(\n",
      "/Users/mike/git/anyscale/ray/python/ray/_private/ray_option_utils.py:266: DeprecationWarning: Setting 'object_store_memory' for actors is deprecated since it doesn't actually reserve the required object store memory. Use object spilling that's enabled by default (https://docs.ray.io/en/master/ray-core/objects/object-spilling.html) instead to bypass the object store memory size limitation.\n",
      "  warnings.warn(\n",
      "/Users/mike/git/anyscale/ray/python/ray/actor.py:637: DeprecationWarning: placement_group parameter is deprecated. Use scheduling_strategy=PlacementGroupSchedulingStrategy(...) instead, see the usage at https://docs.ray.io/en/master/ray-core/package-ref.html#ray-remote.\n",
      "  return actor_cls._remote(args=args, kwargs=kwargs, **updated_options)\n",
      "/Users/mike/git/anyscale/ray/python/ray/actor.py:637: DeprecationWarning: placement_group_bundle_index parameter is deprecated. Use scheduling_strategy=PlacementGroupSchedulingStrategy(...) instead, see the usage at https://docs.ray.io/en/master/ray-core/package-ref.html#ray-remote.\n",
      "  return actor_cls._remote(args=args, kwargs=kwargs, **updated_options)\n",
      "/Users/mike/git/anyscale/ray/python/ray/actor.py:637: DeprecationWarning: placement_group_capture_child_tasks parameter is deprecated. Use scheduling_strategy=PlacementGroupSchedulingStrategy(...) instead, see the usage at https://docs.ray.io/en/master/ray-core/package-ref.html#ray-remote.\n",
      "  return actor_cls._remote(args=args, kwargs=kwargs, **updated_options)\n"
     ]
    }
   ],
   "source": [
    "ppo_config = ppo_config.environment(env=\"FrozenLake-v1\")\n",
    "\n",
    "analysis = tune.run(\n",
    "    \"PPO\",\n",
    "    config            = ppo_config.to_dict(),\n",
    "    stop              = {\"training_iteration\" : 5},\n",
    "    checkpoint_freq   = 1,\n",
    "    verbose           = 0,\n",
    "    metric            = \"episode_reward_mean\",\n",
    "    mode              = \"max\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fec4efa-a9a8-4ee5-af55-fe3e0c1a8120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0001"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis.best_config[\"lr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42fa4b59-c237-4f49-9aad-486d4f916b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "analysis.get_best_logdir(\"episode_reward_mean\", mode=\"max\")\n",
    "analysis.best_logdir\n",
    "trials = analysis.trials\n",
    "trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8fcfa94-83b3-42aa-b118-f42842519b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# this should work but it doesn't:\n",
    "# best_checkpoint = analysis.get_best_checkpoint(trial=trials[0], metric=\"episode_reward_mean\", mode=\"max\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1abd8ae-73e9-48f7-8bfe-c2e7b5ab4cb9",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "- `config`: the config file\n",
    "- `stop`: the stopping condition. Other possibilities are the total number of timesteps or reaching a certain reward value.\n",
    "- `checkpoint_freq`: how often to checkpoint\n",
    "- `verbose`: turn off messages\n",
    "- `metric`: the metric we want to maximize\n",
    "- `mode`: min/max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab6ecc1-dcf1-4f0e-bed3-2249bdeb8dd3",
   "metadata": {},
   "source": [
    "#### Larger grid searches\n",
    "\n",
    "- We can also search over multiple hyperparameters simultaneously.\n",
    "- Grid search will search over all combinations, like scikit-learn's `GridSearchCV`.\n",
    "- Tune also includes more sophisticated search methods, see [here](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#tune-search-alg)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e560c23-ddd4-457f-a488-428a7c558408",
   "metadata": {},
   "source": [
    "#### Let's apply what we learned!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a38ed4-9e7d-43bd-aa21-06e7d003547b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Key hyperparameters\n",
    "<!-- multiple choice -->\n",
    "\n",
    "Which of the following RLlib PPO hyperparameters directly controls the exploration/exploitation tradeoff during training?\n",
    "\n",
    "- [ ] lr\n",
    "- [ ] train_batch_size\n",
    "- [ ] num_sgd_iter\n",
    "- [x] entropy_coeff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d8b67d-a3b9-46f1-a180-9e51013a9288",
   "metadata": {},
   "source": [
    "## Grid search\n",
    "<!-- multiple choice -->\n",
    "\n",
    "Given the code below, how many agents are trained by the Ray tune experiment?\n",
    "\n",
    "```python\n",
    "ppo_config = (\n",
    "    PPOConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(create_env_on_local_worker=True)\\\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\\\n",
    "    .training(model={\"fcnet_hiddens\" : [64, 64]}, \n",
    "              lr=tune.grid_search([1e-2, 1e-3, 1e-4]), \n",
    "              train_batch_size=tune.grid_search([400, 4000, 40_000]))\\\n",
    "    .environment(env_config=env_config, env=BasicRecommenderWithHistory)\n",
    ")\n",
    "```\n",
    "\n",
    "- [ ] 1 | With grid search, an agent is trained for every combination of hyperparameters, in this case lr and train_batch_size.\n",
    "- [ ] 3 | With grid search, an agent is trained for every combination of hyperparameters, in this case lr and train_batch_size.\n",
    "- [ ] 6 | With grid search, an agent is trained for every combination of hyperparameters, in this case lr and train_batch_size.\n",
    "- [x] 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e56ba5-ced6-439f-8b20-e37912fb92ca",
   "metadata": {},
   "source": [
    "## Ray tune\n",
    "<!-- multiple choice -->\n",
    "\n",
    "True of false: Ray tune is a library specifically for tuning RLlib algorithms.\n",
    "\n",
    "- [ ] True | Ray tune can tune models beyond RLlib as well!\n",
    "- [x] False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cbb303-fbf4-412c-8e0a-4d94bbb8b140",
   "metadata": {},
   "source": [
    "## Tuning a model\n",
    "<!-- coding exercise -->\n",
    "\n",
    "In Module 4 we claimed that `lr=1e-3` works better than `lr=1e-4` for the recommender environment we created. Complete the code below so that it uses Ray tune to select the best learning rate from the following candidates: `[1e-2, 1e-3, 1e-4]`. Then, answer the multiple choice question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246a703e-5f4f-48c1-8938-96627cf1c679",
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "\n",
    "from envs import BasicRecommenderWithHistory\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray import tune\n",
    "\n",
    "env_config = {\n",
    "    \"num_candidates\" : 2,\n",
    "    \"alpha\"          : 0.5,\n",
    "    \"seed\"           : 42\n",
    "}\n",
    "\n",
    "ppo_config = (\n",
    "    PPOConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(create_env_on_local_worker=True, num_rollout_workers=0)\\\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\\\n",
    "    .training(model={\"fcnet_hiddens\" : [64, 64]}, \n",
    "              lr=____)\\\n",
    "    .environment(env_config=env_config, env=BasicRecommenderWithHistory)\n",
    ")\n",
    "\n",
    "analysis = tune.____(\n",
    "    \"PPO\",\n",
    "    config            = ppo_config.to_dict(),\n",
    "    ____              = {\"training_iteration\" : 10},\n",
    "    checkpoint_freq   = 1,\n",
    "    verbose           = 0,\n",
    "    metric            = \"episode_reward_mean\",\n",
    "    mode              = \"max\"\n",
    ")\n",
    "\n",
    "____.results_df[[\"lr\", \"episode_reward_mean\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d503da-634d-4673-9afb-f916c4117456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "from envs import BasicRecommenderWithHistory\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray import tune\n",
    "\n",
    "env_config = {\n",
    "    \"num_candidates\" : 2,\n",
    "    \"alpha\"          : 0.5,\n",
    "    \"seed\"           : 42\n",
    "}\n",
    "\n",
    "ppo_config = (\n",
    "    PPOConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(create_env_on_local_worker=True, num_rollout_workers=0)\\\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\\\n",
    "    .training(model={\"fcnet_hiddens\" : [64, 64]}, \n",
    "              lr=tune.grid_search([1e-2, 1e-3, 1e-4]))\\\n",
    "    .environment(env_config=env_config, env=BasicRecommenderWithHistory)\n",
    ")\n",
    "\n",
    "analysis = tune.run(\n",
    "    \"PPO\",\n",
    "    config            = ppo_config.to_dict(),\n",
    "    stop              = {\"training_iteration\" : 10},\n",
    "    checkpoint_freq   = 1,\n",
    "    verbose           = 0,\n",
    "    metric            = \"episode_reward_mean\",\n",
    "    mode              = \"max\"\n",
    ")\n",
    "\n",
    "analysis.results_df[[\"config/lr\", \"episode_reward_mean\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1340a44e-fb63-4086-8236-f3ce5b075f1c",
   "metadata": {},
   "source": [
    "#### Did tuning help?\n",
    "\n",
    "Is a learning rate of 0.001 actually better than the default of 0.0001?\n",
    "\n",
    "- [x] Yes\n",
    "- [ ] No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e342ba0-daae-4c14-bac8-efb547244117",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ray200]",
   "language": "python",
   "name": "conda-env-ray200-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
