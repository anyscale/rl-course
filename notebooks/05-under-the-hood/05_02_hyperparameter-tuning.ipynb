{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b02cdbb-95e8-451d-87d5-8a3f07088797",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a5935760-7836-4a88-bb36-91a2caa600e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7584b00e-80e1-4db0-b5cf-3f06e41b020a",
   "metadata": {},
   "source": [
    "#### Hyperparameters\n",
    "\n",
    "Recall our discussing of RLlib config files in module 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b27c3978-17f5-4e04-b65f-8574b9b7d38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9064d95-d181-4217-a991-a8e790d9b2fb",
   "metadata": {},
   "source": [
    "We wrote code like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eac41c32-da41-4afd-9f6f-1eddae8ab73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_config = (\n",
    "    PPOConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(create_env_on_local_worker=True)\\\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\\\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c53b89b-5554-4cb0-b687-79cc99bd48ca",
   "metadata": {},
   "source": [
    "However, we are only setting a tiny fraction of the config options available in RLlib."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f66b89-8ef6-48b3-aa7e-279df9b985a4",
   "metadata": {},
   "source": [
    "#### Embarrassment of configs\n",
    "\n",
    "Here is the full list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99d28bce-7eb5-4238-9726-a0327bca82b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ppo_config.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1a031774-708a-45d6-8565-d18d9280eafc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'extra_python_environs_for_driver': {},\n",
       " 'extra_python_environs_for_worker': {},\n",
       " 'num_gpus': 0,\n",
       " 'num_cpus_per_worker': 1,\n",
       " 'num_gpus_per_worker': 0,\n",
       " '_fake_gpus': False,\n",
       " 'custom_resources_per_worker': {},\n",
       " 'placement_strategy': 'PACK',\n",
       " 'eager_tracing': False,\n",
       " 'eager_max_retraces': 20,\n",
       " 'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "  'inter_op_parallelism_threads': 2,\n",
       "  'gpu_options': {'allow_growth': True},\n",
       "  'log_device_placement': False,\n",
       "  'device_count': {'CPU': 1},\n",
       "  'allow_soft_placement': True},\n",
       " 'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "  'inter_op_parallelism_threads': 8},\n",
       " 'env': None,\n",
       " 'env_config': {},\n",
       " 'observation_space': None,\n",
       " 'action_space': None,\n",
       " 'env_task_fn': None,\n",
       " 'render_env': False,\n",
       " 'clip_rewards': None,\n",
       " 'normalize_actions': True,\n",
       " 'clip_actions': False,\n",
       " 'disable_env_checking': False,\n",
       " 'num_workers': 2,\n",
       " 'num_envs_per_worker': 1,\n",
       " 'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       " 'sample_async': False,\n",
       " 'rollout_fragment_length': 200,\n",
       " 'batch_mode': 'truncate_episodes',\n",
       " 'remote_worker_envs': False,\n",
       " 'remote_env_batch_wait_ms': 0,\n",
       " 'validate_workers_after_construction': True,\n",
       " 'ignore_worker_failures': False,\n",
       " 'recreate_failed_workers': False,\n",
       " 'restart_failed_sub_environments': False,\n",
       " 'num_consecutive_worker_failures_tolerance': 100,\n",
       " 'horizon': None,\n",
       " 'soft_horizon': False,\n",
       " 'no_done_at_end': False,\n",
       " 'preprocessor_pref': 'deepmind',\n",
       " 'observation_filter': 'NoFilter',\n",
       " 'synchronize_filters': True,\n",
       " 'compress_observations': False,\n",
       " 'gamma': 0.99,\n",
       " 'lr': 5e-05,\n",
       " 'train_batch_size': 4000,\n",
       " 'model': {'_use_default_native_models': False,\n",
       "  '_disable_preprocessor_api': False,\n",
       "  '_disable_action_flattening': False,\n",
       "  'fcnet_hiddens': [256, 256],\n",
       "  'fcnet_activation': 'tanh',\n",
       "  'conv_filters': None,\n",
       "  'conv_activation': 'relu',\n",
       "  'post_fcnet_hiddens': [],\n",
       "  'post_fcnet_activation': 'relu',\n",
       "  'free_log_std': False,\n",
       "  'no_final_linear': False,\n",
       "  'vf_share_layers': False,\n",
       "  'use_lstm': False,\n",
       "  'max_seq_len': 20,\n",
       "  'lstm_cell_size': 256,\n",
       "  'lstm_use_prev_action': False,\n",
       "  'lstm_use_prev_reward': False,\n",
       "  '_time_major': False,\n",
       "  'use_attention': False,\n",
       "  'attention_num_transformer_units': 1,\n",
       "  'attention_dim': 64,\n",
       "  'attention_num_heads': 1,\n",
       "  'attention_head_dim': 32,\n",
       "  'attention_memory_inference': 50,\n",
       "  'attention_memory_training': 50,\n",
       "  'attention_position_wise_mlp_dim': 32,\n",
       "  'attention_init_gru_gate_bias': 2.0,\n",
       "  'attention_use_n_prev_actions': 0,\n",
       "  'attention_use_n_prev_rewards': 0,\n",
       "  'framestack': True,\n",
       "  'dim': 84,\n",
       "  'grayscale': False,\n",
       "  'zero_mean': True,\n",
       "  'custom_model': None,\n",
       "  'custom_model_config': {},\n",
       "  'custom_action_dist': None,\n",
       "  'custom_preprocessor': None,\n",
       "  'lstm_use_prev_action_reward': -1},\n",
       " 'optimizer': {},\n",
       " 'explore': True,\n",
       " 'exploration_config': {'type': 'StochasticSampling'},\n",
       " 'input_config': {},\n",
       " 'actions_in_input_normalized': False,\n",
       " 'off_policy_estimation_methods': {},\n",
       " 'postprocess_inputs': False,\n",
       " 'shuffle_buffer_size': 0,\n",
       " 'output': None,\n",
       " 'output_config': {},\n",
       " 'output_compress_columns': ['obs', 'new_obs'],\n",
       " 'output_max_file_size': 67108864,\n",
       " 'evaluation_interval': None,\n",
       " 'evaluation_duration': 10,\n",
       " 'evaluation_duration_unit': 'episodes',\n",
       " 'evaluation_parallel_to_training': False,\n",
       " 'evaluation_config': {},\n",
       " 'evaluation_num_workers': 0,\n",
       " 'always_attach_evaluation_results': False,\n",
       " 'in_evaluation': False,\n",
       " 'sync_filters_on_rollout_workers_timeout_s': 60.0,\n",
       " 'keep_per_episode_custom_metrics': False,\n",
       " 'metrics_episode_collection_timeout_s': 180,\n",
       " 'metrics_num_episodes_for_smoothing': 100,\n",
       " 'min_time_s_per_iteration': None,\n",
       " 'min_train_timesteps_per_iteration': 0,\n",
       " 'min_sample_timesteps_per_iteration': 0,\n",
       " 'logger_creator': None,\n",
       " 'logger_config': None,\n",
       " 'log_level': 'ERROR',\n",
       " 'log_sys_usage': True,\n",
       " 'fake_sampler': False,\n",
       " 'seed': 0,\n",
       " '_tf_policy_handles_more_than_one_loss': False,\n",
       " '_disable_preprocessor_api': False,\n",
       " '_disable_action_flattening': False,\n",
       " '_disable_execution_plan_api': True,\n",
       " 'simple_optimizer': -1,\n",
       " 'monitor': -1,\n",
       " 'evaluation_num_episodes': -1,\n",
       " 'metrics_smoothing_episodes': -1,\n",
       " 'timesteps_per_iteration': -1,\n",
       " 'min_iter_time_s': -1,\n",
       " 'collect_metrics_timeout': -1,\n",
       " 'buffer_size': -1,\n",
       " 'prioritized_replay': -1,\n",
       " 'learning_starts': -1,\n",
       " 'replay_batch_size': -1,\n",
       " 'replay_sequence_length': None,\n",
       " 'prioritized_replay_alpha': -1,\n",
       " 'prioritized_replay_beta': -1,\n",
       " 'prioritized_replay_eps': -1,\n",
       " 'min_time_s_per_reporting': -1,\n",
       " 'min_train_timesteps_per_reporting': -1,\n",
       " 'min_sample_timesteps_per_reporting': -1,\n",
       " 'input_evaluation': -1,\n",
       " 'lr_schedule': None,\n",
       " 'use_critic': True,\n",
       " 'use_gae': True,\n",
       " 'kl_coeff': 0.2,\n",
       " 'sgd_minibatch_size': 128,\n",
       " 'num_sgd_iter': 30,\n",
       " 'shuffle_sequences': True,\n",
       " 'vf_loss_coeff': 1.0,\n",
       " 'entropy_coeff': 0.0,\n",
       " 'entropy_coeff_schedule': None,\n",
       " 'clip_param': 0.3,\n",
       " 'vf_clip_param': 10.0,\n",
       " 'grad_clip': None,\n",
       " 'kl_target': 0.01,\n",
       " 'vf_share_layers': -1,\n",
       " 'lambda': 1.0,\n",
       " 'input': 'sampler',\n",
       " 'multiagent': {'policies': {},\n",
       "  'policy_map_capacity': 100,\n",
       "  'policy_map_cache': None,\n",
       "  'policy_mapping_fn': None,\n",
       "  'policies_to_train': None,\n",
       "  'observation_fn': None,\n",
       "  'replay_mode': 'independent',\n",
       "  'count_steps_by': 'env_steps'},\n",
       " 'callbacks': ray.rllib.algorithms.callbacks.DefaultCallbacks,\n",
       " 'create_env_on_driver': True,\n",
       " 'custom_eval_function': None,\n",
       " 'framework': 'torch',\n",
       " 'num_cpus_for_driver': 1}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_config.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791e26d9-39ac-4e67-8fbc-08b51005b03f",
   "metadata": {},
   "source": [
    "#### Key hyperparameters\n",
    "\n",
    "We recommend focusing on the following key hyperparameters:\n",
    "\n",
    "- `lr`\n",
    "- `train_batch_size`\n",
    "- `sgd_minibatch_size`\n",
    "- `num_sgd_iter`\n",
    "- `entropy_coeff`\n",
    "- model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a27a37-5d43-4bc1-9258-00ba03c78a66",
   "metadata": {},
   "source": [
    "#### Key hyperparameter interpretations\n",
    "\n",
    "Let's look at the definitions of these key hyperparameters:\n",
    "\n",
    "- `lr`: learning rate\n",
    "- `train_batch_size`: number of iterations of data to be batched together\n",
    "- `sgd_minibatch_size`: minibatch size for SGD\n",
    "- `num_sgd_iter`: epochs of SGD per iteration of PPO\n",
    "- `entropy_coeff`: measure the amount of exploration during training\n",
    "- model architecture: of the policy network\n",
    "\n",
    "Notes:\n",
    "\n",
    "If you are not familiar with deep learning, most of these hyperparameters will not make sense. That is fine, you can skip."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9863bb3e-c5f2-4de5-a6d9-cec0d9163055",
   "metadata": {},
   "source": [
    "#### Key hyperparameter defaults\n",
    "\n",
    "Let's look at the defaults of these key hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b43d9b15-b999-4416-859d-579e084581e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5e-05"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_config.lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "63958eb0-50ad-4692-99cd-42e58703f9f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_config.train_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9af8a557-0383-47d7-94aa-4ef37b619bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_config.sgd_minibatch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "29f3d4b0-e645-4df6-a08b-8e7d82acb19d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_config.num_sgd_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b395eb4-1569-4310-b18a-0676762a4e23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_config.entropy_coeff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01411131-cb8b-4859-8418-f9d047518a6c",
   "metadata": {},
   "source": [
    "#### Key hyperparameter defaults\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c429e78f-e62d-48c4-9286-0c442690aa2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_use_default_native_models': False,\n",
       " '_disable_preprocessor_api': False,\n",
       " '_disable_action_flattening': False,\n",
       " 'fcnet_hiddens': [256, 256],\n",
       " 'fcnet_activation': 'tanh',\n",
       " 'conv_filters': None,\n",
       " 'conv_activation': 'relu',\n",
       " 'post_fcnet_hiddens': [],\n",
       " 'post_fcnet_activation': 'relu',\n",
       " 'free_log_std': False,\n",
       " 'no_final_linear': False,\n",
       " 'vf_share_layers': False,\n",
       " 'use_lstm': False,\n",
       " 'max_seq_len': 20,\n",
       " 'lstm_cell_size': 256,\n",
       " 'lstm_use_prev_action': False,\n",
       " 'lstm_use_prev_reward': False,\n",
       " '_time_major': False,\n",
       " 'use_attention': False,\n",
       " 'attention_num_transformer_units': 1,\n",
       " 'attention_dim': 64,\n",
       " 'attention_num_heads': 1,\n",
       " 'attention_head_dim': 32,\n",
       " 'attention_memory_inference': 50,\n",
       " 'attention_memory_training': 50,\n",
       " 'attention_position_wise_mlp_dim': 32,\n",
       " 'attention_init_gru_gate_bias': 2.0,\n",
       " 'attention_use_n_prev_actions': 0,\n",
       " 'attention_use_n_prev_rewards': 0,\n",
       " 'framestack': True,\n",
       " 'dim': 84,\n",
       " 'grayscale': False,\n",
       " 'zero_mean': True,\n",
       " 'custom_model': None,\n",
       " 'custom_model_config': {},\n",
       " 'custom_action_dist': None,\n",
       " 'custom_preprocessor': None,\n",
       " 'lstm_use_prev_action_reward': -1}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_config.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e796b4de-0256-47a2-9f4b-7e724dd3f0be",
   "metadata": {},
   "source": [
    "#### How do we tune?\n",
    "\n",
    "- Do we resort to tuning by hand?\n",
    "- No!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9a6bb4-1668-4b87-a908-2a41204458f8",
   "metadata": {},
   "source": [
    "#### Introducing Ray tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32a3266-9897-4f02-939d-7c66472d7e1f",
   "metadata": {},
   "source": [
    "![](img/rllib_and_tune.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d7f8f2dd-7ff0-402f-a2d9-274e5b66d740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416e145a-7702-4360-8f99-f6f1f417dd5f",
   "metadata": {},
   "source": [
    "#### Tune usage\n",
    "\n",
    "- Tune is its own sub-package of Ray, like RLlib\n",
    "- It is sophisticated and has its own entire documentation [here](https://docs.ray.io/en/latest/tune/index.html)\n",
    "- For our purposes, we will focus on this syntax:\n",
    "\n",
    "Instead of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "34be5c26-e207-4898-b241-2edd55721bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_config = ppo_config.training(\n",
    "    lr=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c275bda6-7765-42f5-a8dc-f7e10aa5d83a",
   "metadata": {},
   "source": [
    "we do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "96a8ce7f-d282-4076-9658-18204402dbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_config = ppo_config.training(\n",
    "    lr=tune.grid_search([1e-4, 5e-5])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2677f540-e3b4-4d99-a3c9-125719db13a5",
   "metadata": {},
   "source": [
    "We're setting up `tune` to automatically sweep these values!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4c7621-4181-4891-afb6-6eaf05ab3895",
   "metadata": {},
   "source": [
    "#### Running the sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6bcef240-eb0c-4670-8551-bf66e894d186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=6445)\u001b[0m /Users/mike/git/anyscale/ray/python/ray/rllib/utils/debug/deterministic.py:42: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(PPO pid=6445)\u001b[0m   if LooseVersion(torch.__version__) >= LooseVersion(\"1.8.0\"):\n",
      "\u001b[2m\u001b[36m(PPO pid=6445)\u001b[0m 2022-07-30 19:16:27,149\tINFO ppo.py:378 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPO pid=6445)\u001b[0m 2022-07-30 19:16:27,149\tINFO algorithm.py:332 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PPO pid=6444)\u001b[0m /Users/mike/git/anyscale/ray/python/ray/rllib/utils/debug/deterministic.py:42: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(PPO pid=6444)\u001b[0m   if LooseVersion(torch.__version__) >= LooseVersion(\"1.8.0\"):\n",
      "\u001b[2m\u001b[36m(PPO pid=6444)\u001b[0m 2022-07-30 19:16:27,149\tINFO ppo.py:378 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPO pid=6444)\u001b[0m 2022-07-30 19:16:27,149\tINFO algorithm.py:332 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6453)\u001b[0m /Users/mike/git/anyscale/ray/python/ray/rllib/utils/debug/deterministic.py:42: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6453)\u001b[0m   if LooseVersion(torch.__version__) >= LooseVersion(\"1.8.0\"):\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6453)\u001b[0m /Users/mike/miniconda3/envs/ray2beta/lib/python3.9/site-packages/gym/core.py:172: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed) instead.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6453)\u001b[0m   deprecation(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6452)\u001b[0m /Users/mike/git/anyscale/ray/python/ray/rllib/utils/debug/deterministic.py:42: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6452)\u001b[0m   if LooseVersion(torch.__version__) >= LooseVersion(\"1.8.0\"):\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6452)\u001b[0m /Users/mike/miniconda3/envs/ray2beta/lib/python3.9/site-packages/gym/core.py:172: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed) instead.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6452)\u001b[0m   deprecation(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6451)\u001b[0m /Users/mike/git/anyscale/ray/python/ray/rllib/utils/debug/deterministic.py:42: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6451)\u001b[0m   if LooseVersion(torch.__version__) >= LooseVersion(\"1.8.0\"):\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6451)\u001b[0m /Users/mike/miniconda3/envs/ray2beta/lib/python3.9/site-packages/gym/core.py:172: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed) instead.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6451)\u001b[0m   deprecation(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6450)\u001b[0m /Users/mike/git/anyscale/ray/python/ray/rllib/utils/debug/deterministic.py:42: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6450)\u001b[0m   if LooseVersion(torch.__version__) >= LooseVersion(\"1.8.0\"):\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6450)\u001b[0m /Users/mike/miniconda3/envs/ray2beta/lib/python3.9/site-packages/gym/core.py:172: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed) instead.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6450)\u001b[0m   deprecation(\n"
     ]
    }
   ],
   "source": [
    "ppo_config = ppo_config.environment(env=\"FrozenLake-v1\")\n",
    "\n",
    "analysis = tune.run(\n",
    "    \"PPO\",\n",
    "    config            = ppo_config.to_dict(),\n",
    "    stop              = {\"training_iteration\" : 5},\n",
    "    checkpoint_freq   = 1,\n",
    "    verbose           = 0,\n",
    "    metric=\"episode_reward_mean\",\n",
    "    mode=\"max\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2fec4efa-a9a8-4ee5-af55-fe3e0c1a8120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0001"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis.best_config[\"lr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "12dbe1c1-9cf7-435b-9950-e55057c26ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/mike/ray_results/PPO/PPO_FrozenLake-v1_c6bf1_00000_0_lr=0.0001_2022-07-30_19-16-24'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis.get_best_logdir(\"episode_reward_mean\", mode=\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a432bc01-b2dc-4ebd-9c24-6825b5a5e46f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/mike/ray_results/PPO/PPO_FrozenLake-v1_c6bf1_00000_0_lr=0.0001_2022-07-30_19-16-24'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis.best_logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "aaf2cf9e-f2b9-426b-974f-bd69fb842a9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'episode_reward_mean'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [80]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43manalysis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_best_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepisode_reward_mean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/tune/analysis/experiment_analysis.py:469\u001b[0m, in \u001b[0;36mExperimentAnalysis.get_best_checkpoint\u001b[0;34m(self, trial, metric, mode)\u001b[0m\n\u001b[1;32m    466\u001b[0m metric \u001b[38;5;241m=\u001b[39m metric \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_metric \u001b[38;5;129;01mor\u001b[39;00m TRAINING_ITERATION\n\u001b[1;32m    467\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_mode(mode)\n\u001b[0;32m--> 469\u001b[0m checkpoint_paths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_trial_checkpoints_paths\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;66;03m# Filter out nan. Sorting nan values leads to undefined behavior.\u001b[39;00m\n\u001b[1;32m    472\u001b[0m checkpoint_paths \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    473\u001b[0m     (path, metric) \u001b[38;5;28;01mfor\u001b[39;00m path, metric \u001b[38;5;129;01min\u001b[39;00m checkpoint_paths \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_nan(metric)\n\u001b[1;32m    474\u001b[0m ]\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/tune/analysis/experiment_analysis.py:433\u001b[0m, in \u001b[0;36mExperimentAnalysis.get_trial_checkpoints_paths\u001b[0;34m(self, trial, metric)\u001b[0m\n\u001b[1;32m    430\u001b[0m chkpt_df \u001b[38;5;241m=\u001b[39m TrainableUtil\u001b[38;5;241m.\u001b[39mget_checkpoints_paths(trial_dir)\n\u001b[1;32m    432\u001b[0m \u001b[38;5;66;03m# Join with trial dataframe to get metrics.\u001b[39;00m\n\u001b[0;32m--> 433\u001b[0m trial_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrial_dataframes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrial_dir\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    434\u001b[0m path_metric_df \u001b[38;5;241m=\u001b[39m chkpt_df\u001b[38;5;241m.\u001b[39mmerge(\n\u001b[1;32m    435\u001b[0m     trial_df, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_iteration\u001b[39m\u001b[38;5;124m\"\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    436\u001b[0m )\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m path_metric_df[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchkpt_path\u001b[39m\u001b[38;5;124m\"\u001b[39m, metric]]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist()\n",
      "\u001b[0;31mKeyError\u001b[0m: 'episode_reward_mean'"
     ]
    }
   ],
   "source": [
    "analysis.get_best_checkpoint(\"episode_reward_mean\", mode=\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "42fa4b59-c237-4f49-9aad-486d4f916b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PPO_FrozenLake-v1_c6bf1_00000, PPO_FrozenLake-v1_c6bf1_00001]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials = analysis.trials\n",
    "trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e8fcfa94-83b3-42aa-b118-f42842519b53",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Cannot create checkpoint from URI as it is not supported: None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [83]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m best_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43manalysis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_best_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrials\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepisode_reward_mean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/tune/analysis/experiment_analysis.py:487\u001b[0m, in \u001b[0;36mExperimentAnalysis.get_best_checkpoint\u001b[0;34m(self, trial, metric, mode)\u001b[0m\n\u001b[1;32m    484\u001b[0m cloud_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_cloud_path(best_path)\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_legacy_checkpoint:\n\u001b[0;32m--> 487\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTrialCheckpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbest_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcloud_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcloud_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cloud_path:\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;66;03m# Prefer cloud path over local path for downsteam processing\u001b[39;00m\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Checkpoint\u001b[38;5;241m.\u001b[39mfrom_uri(cloud_path)\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/tune/cloud.py:86\u001b[0m, in \u001b[0;36mTrialCheckpoint.__init__\u001b[0;34m(self, local_path, cloud_path)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Checkpoint does not allow empty data, but TrialCheckpoint\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# did. To keep backwards compatibility, we use a placeholder URI\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# here, and manually set self._uri and self._local_dir later.\u001b[39;00m\n\u001b[1;32m     85\u001b[0m PLACEHOLDER \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3://placeholder\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 86\u001b[0m \u001b[43mCheckpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPLACEHOLDER\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Reset local variables\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_uri \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/air/checkpoint.py:184\u001b[0m, in \u001b[0;36mCheckpoint.__init__\u001b[0;34m(self, local_path, data_dict, uri, obj_ref)\u001b[0m\n\u001b[1;32m    182\u001b[0m     resolved \u001b[38;5;241m=\u001b[39m _get_external_path(uri)\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m resolved:\n\u001b[0;32m--> 184\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    185\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot create checkpoint from URI as it is not \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    186\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupported: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    187\u001b[0m         )\n\u001b[1;32m    188\u001b[0m     uri \u001b[38;5;241m=\u001b[39m resolved\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Cannot create checkpoint from URI as it is not supported: None"
     ]
    }
   ],
   "source": [
    "best_checkpoint = analysis.get_best_checkpoint(trial=trials[0], metric=\"episode_reward_mean\", mode=\"max\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0b4bfb1b-1174-4f59-95a5-7d046a94379c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Cannot create checkpoint from URI as it is not supported: None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [72]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43manalysis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_checkpoint\u001b[49m\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/tune/analysis/experiment_analysis.py:264\u001b[0m, in \u001b[0;36mExperimentAnalysis.best_checkpoint\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m best_trial:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo best trial found. Please check if you specified the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorrect default metric (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_metric\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) and mode \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    262\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_mode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    263\u001b[0m     )\n\u001b[0;32m--> 264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_best_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbest_trial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_metric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_mode\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/tune/analysis/experiment_analysis.py:487\u001b[0m, in \u001b[0;36mExperimentAnalysis.get_best_checkpoint\u001b[0;34m(self, trial, metric, mode)\u001b[0m\n\u001b[1;32m    484\u001b[0m cloud_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_cloud_path(best_path)\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_legacy_checkpoint:\n\u001b[0;32m--> 487\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTrialCheckpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbest_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcloud_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcloud_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cloud_path:\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;66;03m# Prefer cloud path over local path for downsteam processing\u001b[39;00m\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Checkpoint\u001b[38;5;241m.\u001b[39mfrom_uri(cloud_path)\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/tune/cloud.py:86\u001b[0m, in \u001b[0;36mTrialCheckpoint.__init__\u001b[0;34m(self, local_path, cloud_path)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Checkpoint does not allow empty data, but TrialCheckpoint\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# did. To keep backwards compatibility, we use a placeholder URI\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# here, and manually set self._uri and self._local_dir later.\u001b[39;00m\n\u001b[1;32m     85\u001b[0m PLACEHOLDER \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3://placeholder\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 86\u001b[0m \u001b[43mCheckpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPLACEHOLDER\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Reset local variables\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_uri \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/air/checkpoint.py:184\u001b[0m, in \u001b[0;36mCheckpoint.__init__\u001b[0;34m(self, local_path, data_dict, uri, obj_ref)\u001b[0m\n\u001b[1;32m    182\u001b[0m     resolved \u001b[38;5;241m=\u001b[39m _get_external_path(uri)\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m resolved:\n\u001b[0;32m--> 184\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    185\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot create checkpoint from URI as it is not \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    186\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupported: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    187\u001b[0m         )\n\u001b[1;32m    188\u001b[0m     uri \u001b[38;5;241m=\u001b[39m resolved\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Cannot create checkpoint from URI as it is not supported: None"
     ]
    }
   ],
   "source": [
    "analysis.best_checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7185c971-e7a2-4cb3-92b8-59ed428fafcd",
   "metadata": {},
   "source": [
    "Gotcha? Tune checkpoints vs. RLlib checkpoints?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1abd8ae-73e9-48f7-8bfe-c2e7b5ab4cb9",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "- `config`: the config file\n",
    "- `stop: the stopping condition. Other possibilities are the total number of timesteps or reaching a certain reward value.\n",
    "- `checkpoint_at_end:\n",
    "- `verbose`: \n",
    "- `metric`:\n",
    "- `mode`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab6ecc1-dcf1-4f0e-bed3-2249bdeb8dd3",
   "metadata": {},
   "source": [
    "#### Larger grid searches\n",
    "\n",
    "multiple hypers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa298079-851e-4764-8e42-c38ed6d510d9",
   "metadata": {},
   "source": [
    "#### Other types of searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed341ffc-9b15-4249-8363-fb604f320810",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4906c412-12c8-4a01-87cd-fe8103e988ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e560c23-ddd4-457f-a488-428a7c558408",
   "metadata": {},
   "source": [
    "#### Let's apply what we learned!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a38ed4-9e7d-43bd-aa21-06e7d003547b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Key hyperparameters\n",
    "<!-- multiple choice -->\n",
    "\n",
    "Which of the following RLlib PPO hyperparameters directly controls the exploration/exploitation tradeoff during training?\n",
    "\n",
    "- [ ] lr\n",
    "- [ ] train_batch_size\n",
    "- [ ] num_sgd_iter\n",
    "- [x] entropy_coeff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d8b67d-a3b9-46f1-a180-9e51013a9288",
   "metadata": {},
   "source": [
    "## Grid search\n",
    "<!-- multiple choice -->\n",
    "\n",
    "Given the code below, how many agents are trained by the Ray tune experiment?\n",
    "\n",
    "```python\n",
    "ppo_config = (\n",
    "    PPOConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(create_env_on_local_worker=True)\\\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\\\n",
    "    .training(model={\"fcnet_hiddens\" : [64, 64]}, \n",
    "              lr=tune.grid_search([1e-2, 1e-3, 1e-4]), \n",
    "              train_batch_size=tune.grid_search([400, 4000, 40_000]))\\\n",
    "    .environment(env_config=env_config, env=BasicRecommenderWithHistory)\n",
    ")\n",
    "```\n",
    "\n",
    "- [ ] 1 | With grid search, an agent is trained for every combination of hyperparameters, in this case lr and train_batch_size.\n",
    "- [ ] 3 | With grid search, an agent is trained for every combination of hyperparameters, in this case lr and train_batch_size.\n",
    "- [ ] 6 | With grid search, an agent is trained for every combination of hyperparameters, in this case lr and train_batch_size.\n",
    "- [x] 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e56ba5-ced6-439f-8b20-e37912fb92ca",
   "metadata": {},
   "source": [
    "## Ray tune\n",
    "<!-- multiple choice -->\n",
    "\n",
    "True of false: Ray tune is a library specifically for tuning RLlib algorithms.\n",
    "\n",
    "- [ ] True | Ray tune can tune models beyond RLlib as well!\n",
    "- [x] False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cbb303-fbf4-412c-8e0a-4d94bbb8b140",
   "metadata": {},
   "source": [
    "## Tuning a model\n",
    "<!-- coding exercise -->\n",
    "\n",
    "In Module 4 we claimed that `lr=1e-3` works better than `lr=1e-4` for the recommender environment we created. Complete the code below so that it uses Ray tune to select the best learning rate from the following candidates: `[1e-2, 1e-3, 1e-4]`. Then, answer the multiple choice question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "246a703e-5f4f-48c1-8938-96627cf1c679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "\n",
    "from envs import BasicRecommenderWithHistory\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray import tune\n",
    "\n",
    "env_config = {\n",
    "    \"num_candidates\" : 2,\n",
    "    \"alpha\"          : 0.5,\n",
    "    \"seed\"           : 42\n",
    "}\n",
    "\n",
    "ppo_config = (\n",
    "    PPOConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(create_env_on_local_worker=True)\\\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\\\n",
    "    .training(model={\"fcnet_hiddens\" : [64, 64]}, \n",
    "              lr=____)\\\n",
    "    .environment(env_config=env_config, env=BasicRecommenderWithHistory)\n",
    ")\n",
    "\n",
    "analysis = tune.____(\n",
    "    \"PPO\",\n",
    "    config            = ppo_config.to_dict(),\n",
    "    ____              = {\"training_iteration\" : 10},\n",
    "    checkpoint_freq   = 1,\n",
    "    verbose           = 0\n",
    ")\n",
    "\n",
    "____.results_df[[\"lr\", \"episode_reward_mean\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d503da-634d-4673-9afb-f916c4117456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=12987)\u001b[0m /Users/mike/git/anyscale/ray/python/ray/rllib/utils/debug/deterministic.py:42: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(PPO pid=12987)\u001b[0m   if LooseVersion(torch.__version__) >= LooseVersion(\"1.8.0\"):\n",
      "\u001b[2m\u001b[36m(PPO pid=12987)\u001b[0m 2022-07-31 08:03:27,492\tINFO ppo.py:378 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPO pid=12987)\u001b[0m 2022-07-31 08:03:27,492\tINFO algorithm.py:332 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PPO pid=12986)\u001b[0m /Users/mike/git/anyscale/ray/python/ray/rllib/utils/debug/deterministic.py:42: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(PPO pid=12986)\u001b[0m   if LooseVersion(torch.__version__) >= LooseVersion(\"1.8.0\"):\n",
      "\u001b[2m\u001b[36m(PPO pid=12986)\u001b[0m 2022-07-31 08:03:27,492\tINFO ppo.py:378 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPO pid=12986)\u001b[0m 2022-07-31 08:03:27,492\tINFO algorithm.py:332 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=12991)\u001b[0m /Users/mike/git/anyscale/ray/python/ray/rllib/utils/debug/deterministic.py:42: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=12991)\u001b[0m   if LooseVersion(torch.__version__) >= LooseVersion(\"1.8.0\"):\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=12989)\u001b[0m /Users/mike/git/anyscale/ray/python/ray/rllib/utils/debug/deterministic.py:42: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=12989)\u001b[0m   if LooseVersion(torch.__version__) >= LooseVersion(\"1.8.0\"):\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=12990)\u001b[0m /Users/mike/git/anyscale/ray/python/ray/rllib/utils/debug/deterministic.py:42: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=12990)\u001b[0m   if LooseVersion(torch.__version__) >= LooseVersion(\"1.8.0\"):\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=12992)\u001b[0m /Users/mike/git/anyscale/ray/python/ray/rllib/utils/debug/deterministic.py:42: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=12992)\u001b[0m   if LooseVersion(torch.__version__) >= LooseVersion(\"1.8.0\"):\n",
      "\u001b[2m\u001b[36m(PPO pid=13058)\u001b[0m /Users/mike/git/anyscale/ray/python/ray/rllib/utils/debug/deterministic.py:42: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(PPO pid=13058)\u001b[0m   if LooseVersion(torch.__version__) >= LooseVersion(\"1.8.0\"):\n",
      "\u001b[2m\u001b[36m(PPO pid=13058)\u001b[0m 2022-07-31 08:03:53,732\tINFO ppo.py:378 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPO pid=13058)\u001b[0m 2022-07-31 08:03:53,732\tINFO algorithm.py:332 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13060)\u001b[0m /Users/mike/git/anyscale/ray/python/ray/rllib/utils/debug/deterministic.py:42: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13060)\u001b[0m   if LooseVersion(torch.__version__) >= LooseVersion(\"1.8.0\"):\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13061)\u001b[0m /Users/mike/git/anyscale/ray/python/ray/rllib/utils/debug/deterministic.py:42: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13061)\u001b[0m   if LooseVersion(torch.__version__) >= LooseVersion(\"1.8.0\"):\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "\n",
    "from envs import BasicRecommenderWithHistory\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray import tune\n",
    "\n",
    "env_config = {\n",
    "    \"num_candidates\" : 2,\n",
    "    \"alpha\"          : 0.5,\n",
    "    \"seed\"           : 42\n",
    "}\n",
    "\n",
    "ppo_config = (\n",
    "    PPOConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(create_env_on_local_worker=True)\\\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\\\n",
    "    .training(model={\"fcnet_hiddens\" : [64, 64]}, \n",
    "              lr=tune.grid_search([1e-2, 1e-3, 1e-4]))\\\n",
    "    .environment(env_config=env_config, env=BasicRecommenderWithHistory)\n",
    ")\n",
    "\n",
    "analysis = tune.run(\n",
    "    \"PPO\",\n",
    "    config            = ppo_config.to_dict(),\n",
    "    stop              = {\"training_iteration\" : 10},\n",
    "    checkpoint_freq   = 1,\n",
    "    verbose           = 0\n",
    ")\n",
    "\n",
    "analysis.results_df[[\"lr\", \"episode_reward_mean\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11113a89-87ee-483d-894e-9909360d2194",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.results_df[[\"lr\", \"episode_reward_mean\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d57b8d-d756-450d-a223-c20ad56fe2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1340a44e-fb63-4086-8236-f3ce5b075f1c",
   "metadata": {},
   "source": [
    "#### Is a learning rate of 0.001 actually better than the default of 0.0001\n",
    "\n",
    "- [x] Yes\n",
    "- [ ] No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408f36ac-a998-4079-b34c-0a21cf376b44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ray2beta]",
   "language": "python",
   "name": "conda-env-ray2beta-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
