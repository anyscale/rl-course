{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "734f5ff7-bfbf-4238-8ef4-9a3d6cb20f9a",
   "metadata": {},
   "source": [
    "## RL algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4411aa7c-d51c-4fa7-be20-303ed6daa841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cdfc52-615c-4b7d-9ffb-b3fcd9a9549f",
   "metadata": {},
   "source": [
    "#### RL algorithms\n",
    "\n",
    "- We used the PPO algorithm for almost the entire course (except offline RL)\n",
    "- In fact, RLlib offers many algorithms! See [here](https://docs.ray.io/en/latest/rllib/rllib-algorithms.html).\n",
    "- Let's look at one more algorithm, DQN.\n",
    "\n",
    "Notes:\n",
    "\n",
    "There are over 20 algorithms at the time of recording."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d768d1-373d-49e3-87fb-f510be26745f",
   "metadata": {},
   "source": [
    "#### Deep Q-Network (DQN)\n",
    "\n",
    "- DQN is famous in the RL world for many breakthroughs such as the [Atari results](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf) from 2013.\n",
    "- In RLlib, we only need to make the smallest of swaps:\n",
    "\n",
    "PPO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0732a5c6-4e7e-4d01-9eb4-51fa02a28ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "ppo = PPOConfig().framework(\"torch\").build(env=\"FrozenLake-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cbc410-9e67-42f0-9ada-397fa2e84855",
   "metadata": {},
   "source": [
    "DQN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b67329e9-5c13-48e9-b376-b8c608a5fbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-31 08:14:37,340\tWARNING deprecation.py:47 -- DeprecationWarning: `ray.rllib.algorithms.dqn.dqn.DEFAULT_CONFIG` has been deprecated. Use `ray.rllib.algorithms.dqn.dqn.DQNConfig(...)` instead. This will raise an error in the future!\n",
      "2022-07-31 08:14:37,342\tWARNING deprecation.py:47 -- DeprecationWarning: `config['multiagent']['replay_mode']` has been deprecated. config['replay_buffer_config']['replay_mode'] This will raise an error in the future!\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.dqn import DQNConfig\n",
    "\n",
    "dqn = DQNConfig().framework(\"torch\").build(env=\"FrozenLake-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4989c48f-cd51-4474-9b42-d98ff61bf3ac",
   "metadata": {},
   "source": [
    "And that's it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56e32ec-9000-4ccd-a1fb-92fb596ecc7c",
   "metadata": {},
   "source": [
    "#### Using a DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47559d7a-4b55-41d8-810a-1cdca65852cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8e88dc-093f-4144-a81a-5d9742e80f2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d15933bd-af7d-40da-ae28-18c493e80d0e",
   "metadata": {},
   "source": [
    "#### Deterministic policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc3dcb5-6596-4b83-bee2-4cedba93713b",
   "metadata": {},
   "source": [
    "should show that for the recommender problem, acting deterministically without exploration would not work at all, because the optimal strategy is random. though this effect would be a lot less severe with history available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccbfb16-fd8a-4d47-82aa-d1642f2c3589",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d6424f-c8b7-4226-ba01-925841ac6bab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7fa9f10-dcb6-4f53-b3ca-e7fd162606e0",
   "metadata": {},
   "source": [
    "#### Off-policy algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2bbdd1-e49f-47ca-889e-8980d908f082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5335c92a-b76a-4f51-a1b8-91f37ee841a0",
   "metadata": {},
   "source": [
    "#### Q-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8176da-5a08-4e80-a4f7-f0f6d3b2453a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ebea7dd-10bd-4a2e-9bf1-495e07becf98",
   "metadata": {},
   "source": [
    "#### Q-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1ab140-bc99-4f08-bd4c-57eb37762ad7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5eb4941-bc55-4645-b91a-b9040a0f8e6d",
   "metadata": {},
   "source": [
    "#### Beyond DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce404f4-9150-47af-824c-cef401cdb647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08223a7-6a2c-44b6-b80a-5f990ec96338",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935afbaa-3853-41ac-bfc2-5884c9add93f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55490556-94ca-4451-ba48-616d75e46cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "129769da-7255-4cfc-86e7-0af722124dc3",
   "metadata": {},
   "source": [
    "#### Let's apply what we learned!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ee53af-de7d-47b7-9d3e-ee1917b156f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## MCQ\n",
    "<!-- multiple choice -->\n",
    "\n",
    "Which of the following RLlib PPO hyperparameters directly controls the exploration/exploitation tradeoff during training?\n",
    "\n",
    "- [ ] lr\n",
    "- [ ] train_batch_size\n",
    "- [ ] num_sgd_iter\n",
    "- [x] entropy_coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da827e9-9970-477c-9d55-42f458a088c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ray2beta]",
   "language": "python",
   "name": "conda-env-ray2beta-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
