{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be76afd1-30c0-4f6a-9107-bf9eef74f904",
   "metadata": {},
   "source": [
    "## Ray\n",
    "<!-- video shot=\"/rb-TmEfJIdI\" start=\"16:37\" end=\"22:00\" -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fea144b-381f-4fcd-ac3d-e9f1d364d660",
   "metadata": {},
   "source": [
    "#### What is Ray?\n",
    "\n",
    "This whole course we've been using the Ray package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41754b57-9c36-48f7-843a-4989c90f357b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray.rllib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4357c3f6-bc60-4eb4-b4be-6aac32900960",
   "metadata": {},
   "source": [
    "![](img/ray-logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173eca68-40f8-43ce-9889-a4d3b6f96505",
   "metadata": {},
   "source": [
    "What is Ray? From the [docs](https://docs.ray.io/en/latest/):\n",
    "\n",
    "> Ray is a general-purpose and universal distributed compute framework.\n",
    "\n",
    "Ray is also:\n",
    "\n",
    "- An [active open source project](https://github.com/ray-project/ray) with over 20k stars on GitHub ðŸ¤©\n",
    "- Backed by the unicorn startup [Anyscale](https://www.anyscale.com/), that produced this course ðŸ¦„\n",
    "\n",
    "Notes:\n",
    "\n",
    "But, back to distributed computing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bceac41-d9d5-46e4-8e3a-21d5a914a986",
   "metadata": {},
   "source": [
    "#### What is distributed computing?\n",
    "\n",
    "_Distributed computing_ is computing that involves multiple machines (nodes) distributed across a network.\n",
    "\n",
    "![](img/supercomputer.png)\n",
    "\n",
    "Pros:\n",
    "\n",
    "- Massively improved capabilities\n",
    "\n",
    "Cons/challenges:\n",
    "\n",
    "- Synchronization\n",
    "- Failure\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d345f4-5b01-49f2-9635-130f4d580f21",
   "metadata": {},
   "source": [
    "#### Ray makes distributed computing easy\n",
    "\n",
    "- The goal of Ray is to make distributed computing easy and accessible.\n",
    "- Ray handles most of the challenges for users.\n",
    "- RLlib, tune and the other sub-packages were built on top of Ray.\n",
    "- This means _RLlib and tune automatically have distributed capabilities._\n",
    "\n",
    "Notes:\n",
    "\n",
    "Surprise! RLlib is easy to use and conveniently implements many state-of-the-art RL algorithms, but it has another benefit that we didn't mention until now: natural distributed computing capabilities. This puts it well ahead of competing packages in ease of distributing the computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275fe8fc-292d-4622-b7c9-5e6d0c0f4022",
   "metadata": {},
   "source": [
    "#### RLlib, distributed\n",
    "\n",
    "- In this course we set up algorithm configs many times.\n",
    "- But there are some parameters we haven't used before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a1e8fe8-c52d-4cf6-a846-75f6ca83e381",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68ff95da-eee7-45eb-9694-ac4fd5420c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_config = (\n",
    "    PPOConfig()\n",
    "    .framework(\"torch\")\n",
    "    .rollouts(num_rollout_workers=4, num_envs_per_worker=2)\n",
    "    .resources(num_gpus=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e10ccc-cc94-41a3-bcda-66de6cf68fcc",
   "metadata": {},
   "source": [
    "You can read more about specifying resources [here](https://docs.ray.io/en/master/rllib/rllib-training.html#specifying-resources) and about scaling [here](https://docs.ray.io/en/master/rllib/rllib-training.html#scaling-guide)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec03fa7b-c78f-446d-9be7-deac2c895ce2",
   "metadata": {},
   "source": [
    "But... what is a \"rollout worker\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ebdf4e-e1c0-41c9-b923-1a3917106f9e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Rollout workers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5ea641-4738-4220-8adc-78d5a65141e0",
   "metadata": {},
   "source": [
    "- Rollout workers collect data from the environment (simulator) in parallel.\n",
    "- For most simulator environments, one can replicate the environment in a cluster.\n",
    "- Therefore, you can collect data much faster and avoid bottlenecking the training.\n",
    "- Whatever cluster Ray is connected to on the backend, `num_rollout_workers=4` works seamlessly.\n",
    "\n",
    "Notes:\n",
    "\n",
    "In supervised learning, when you're waiting you know you're probably waiting for the model to train. In RL, the bottleneck could be the data collection or the model updates. Being able to parallelize rollouts alleviates the data collection bottleneck. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925d5abd-b478-43a7-bb71-7eeb9ae4276d",
   "metadata": {},
   "source": [
    "#### Ray tune, revisited\n",
    "\n",
    "- Keep in mind that hyperparameter tuning like grid search is also easily distributed.\n",
    "- Fortunately `tune` is also part of Ray and, like RLlib, takes care of this for you! \n",
    "- As you can see, Ray + tune + RLlib becomes quite a powerful combination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac61bd0-31ac-47e7-a51b-6ff1b92cb753",
   "metadata": {},
   "source": [
    "#### Driver\n",
    "\n",
    "In all our configs we've had\n",
    "\n",
    "```python\n",
    "create_env_on_driver = True\n",
    "```\n",
    "\n",
    "What this means is that we put the env on the same \"driver\" process that's running the training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fac297-c378-4ad3-b58f-65da9042333c",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "\n",
    "- Ray is incredibly powerful, and we've only scratched the tip of the iceberg.\n",
    "- Some other resources:\n",
    "  - [ray.io](https://www.ray.io/)\n",
    "  - [Learning Ray](https://www.oreilly.com/library/view/learning-ray/9781098117214/) (book)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76648d6e-5b58-4241-aeb3-84d4dc6c661f",
   "metadata": {},
   "source": [
    "#### Let's apply what we learned!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d0b93e-69ad-4338-b937-62bb19fc506b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## What is Ray?\n",
    "<!-- multiple choice -->\n",
    "\n",
    "What is Ray?\n",
    "\n",
    "- [ ] The company that creates RLlib. | You might be thinking of Anyscale, the company behind Ray!\n",
    "- [ ] A sub-package within RLlib that deals with distributed computing.\n",
    "- [x] A general-purpose package that includes RLlib, which deals with distributed computing.\n",
    "- [ ] A reinforcement learning algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67749f9a-fc60-436d-878d-ee060772cf9a",
   "metadata": {},
   "source": [
    "## Distributing RLlib\n",
    "<!-- multiple choice -->\n",
    "\n",
    "What is the primary way in which RLlib makes use of distributed computing abilities?\n",
    "\n",
    "- [x] Distributed rollout workers generate data from env clones that is fed into the learning algorithm. \n",
    "- [ ] The policy neural network training is distributed across multiple nodes.\n",
    "- [ ] Each node has a separate policy neural network that is trained independently on its own node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7a1f3a-18b5-4d99-ad1f-409bf87b59ad",
   "metadata": {},
   "source": [
    "## Experimenting with rollout workers\n",
    "<!-- coding exercise -->\n",
    "\n",
    "The code below creates two PPO algorithm instances, one that should use two rollout workers with two envs per worker, and one that uses only one rollout worker with one env per worker. It then prints out the time elapsed to train each one for 5 iterations. Complete and run the code and then compare the times. \n",
    "\n",
    "Note: this experiment will _usually_ work. However, this code us running on a server that is potentially being used my multiple learners at the same time, so the runtimes may be influenced by the server load. Also, this server is not a proper cluster so any benefits would come from multiple cores of CPU being available to parallelize within one machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2689cc8b-4ce7-4b7c-a943-0e937d37ccec",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PPOConfig' object has no attribute '____'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m      6\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mDeprecationWarning\u001b[39;00m) \n\u001b[1;32m      8\u001b[0m ppo_config_many \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m----> 9\u001b[0m     \u001b[43mPPOConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m____\u001b[49m(____)\\\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;241m.\u001b[39mtraining(model\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfcnet_hiddens\u001b[39m\u001b[38;5;124m\"\u001b[39m : [\u001b[38;5;241m32\u001b[39m,\u001b[38;5;241m32\u001b[39m]})\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m ppo_config_single \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     16\u001b[0m     PPOConfig()\\\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;241m.\u001b[39mframework(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;241m.\u001b[39m____(num_rollout_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, num_envs_per_worker\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\\\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;241m.\u001b[39mtraining(model\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfcnet_hiddens\u001b[39m\u001b[38;5;124m\"\u001b[39m : [\u001b[38;5;241m32\u001b[39m,\u001b[38;5;241m32\u001b[39m]})\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     22\u001b[0m ppo_many \u001b[38;5;241m=\u001b[39m ppo_config_many\u001b[38;5;241m.\u001b[39mbuild(env\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrozenLake-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PPOConfig' object has no attribute '____'"
     ]
    }
   ],
   "source": [
    "# EXERCISE\n",
    "\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "ppo_config_many = (\n",
    "    PPOConfig()\n",
    "    .framework(\"torch\")\n",
    "    .____(____)\n",
    "    .training(model={\"fcnet_hiddens\" : [32,32]})\n",
    "    .environment(env=\"FrozenLake-v1\")\n",
    ")\n",
    "\n",
    "ppo_config_single = (\n",
    "    PPOConfig()\n",
    "    .framework(\"torch\")\n",
    "    .____(num_rollout_workers=1, num_envs_per_worker=1)\n",
    "    .training(model={\"fcnet_hiddens\" : [32,32]})\n",
    "    .environment(env=\"FrozenLake-v1\")\n",
    ")\n",
    "\n",
    "ppo_many = ppo_config_many.build()\n",
    "t = time.time()\n",
    "for i in range(5):\n",
    "    ppo_many.train()\n",
    "print(f\"Elapsed time with 2 workers, 2 envs each: {time.time()-t:.1f}s.\")\n",
    "ppo_many.stop()\n",
    "\n",
    "ppo_single = ppo_config_single.build()\n",
    "t = time.time()\n",
    "for i in range(5):\n",
    "    ppo_single.train()\n",
    "print(f\"Elapsed time with 1 worker, 1 env: {time.time()-t:.1f}s.\")\n",
    "ppo_single.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2405893d-2dc6-4a91-8d22-bb4db1450b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-19 15:51:10,871\tINFO worker.py:1518 -- Started a local Ray instance.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=99915)\u001b[0m 2022-11-19 15:51:13,219\tWARNING env.py:154 -- Your env reset() method appears to take 'seed' or 'return_info' arguments. Note that these are not yet supported in RLlib. Seeding will take place using 'env.seed()' and the info dict will not be returned from reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=99915)\u001b[0m 2022-11-19 15:51:13,362\tWARNING deprecation.py:47 -- DeprecationWarning: `concat_samples` has been deprecated. Use `concat_samples() from rllib.policy.sample_batch` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time with 2 workers, 2 envs each: 8.6s.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=99932)\u001b[0m 2022-11-19 15:51:24,204\tWARNING env.py:154 -- Your env reset() method appears to take 'seed' or 'return_info' arguments. Note that these are not yet supported in RLlib. Seeding will take place using 'env.seed()' and the info dict will not be returned from reset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time with 1 worker, 1 env: 13.6s.\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "ppo_config_many = (\n",
    "    PPOConfig()\n",
    "    .framework(\"torch\")\n",
    "    .rollouts(num_rollout_workers=2, num_envs_per_worker=2)\n",
    "    .training(model={\"fcnet_hiddens\" : [32,32]})\n",
    "    .environment(env=\"FrozenLake-v1\")\n",
    ")\n",
    "\n",
    "ppo_config_single = (\n",
    "    PPOConfig()\n",
    "    .framework(\"torch\")\n",
    "    .rollouts(num_rollout_workers=1, num_envs_per_worker=1)\n",
    "    .training(model={\"fcnet_hiddens\" : [32,32]})\n",
    "    .environment(env=\"FrozenLake-v1\")\n",
    ")\n",
    "\n",
    "ppo_many = ppo_config_many.build()\n",
    "t = time.time()\n",
    "for i in range(5):\n",
    "    ppo_many.train()\n",
    "print(f\"Elapsed time with 2 workers, 2 envs each: {time.time()-t:.1f}s.\")\n",
    "ppo_many.stop()\n",
    "\n",
    "ppo_single = ppo_config_single.build()\n",
    "t = time.time()\n",
    "for i in range(5):\n",
    "    ppo_single.train()\n",
    "print(f\"Elapsed time with 1 worker, 1 env: {time.time()-t:.1f}s.\")\n",
    "ppo_single.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda099fd-12d1-42e3-a529-e757347def40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ray200]",
   "language": "python",
   "name": "conda-env-ray200-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
