{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c11f2e-7de2-456f-91fc-97fb9789875a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d60aae9-0e89-4a18-921d-d172429b5b99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "382db561-2bb2-4043-a3d2-138d8fc6ecb8",
   "metadata": {},
   "source": [
    "The code below loads the agent, seen in the slides, on the Random Lake environment. When you run the code, it prints out a 16x4 array. Each row corresponds to one of the 16 possible observations, and shows the probabilities of the agent taking each of the 4 possible actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa5f988d-3a18-4f51-b9fe-80d1270cfa40",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'envs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfont.size\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m14\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mppo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PPOTrainer\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01menvs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomLake\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m action_dist\n\u001b[1;32m      8\u001b[0m trainer \u001b[38;5;241m=\u001b[39m PPOTrainer({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframework\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreate_env_on_driver\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;241m0\u001b[39m}, \n\u001b[1;32m      9\u001b[0m                      env\u001b[38;5;241m=\u001b[39mRandomLake)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'envs'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.size'] = 14\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from envs import RandomLake\n",
    "from utils import action_dist\n",
    "\n",
    "trainer = PPOTrainer({\"framework\" : \"torch\", \"create_env_on_driver\" : True, \"seed\" : 0}, \n",
    "                     env=RandomLake)\n",
    "\n",
    "trainer.restore(\"models/RandomLake/checkpoint-8\")\n",
    "\n",
    "lake = RandomLake()\n",
    "\n",
    "plt.figure(figsize=(4,8))\n",
    "plt.imshow(np.array([action_dist(trainer, lake, i) for i in range(16)]));\n",
    "plt.xticks(np.arange(4), labels=[\"left\", \"down\", \"up\", \"right\"]);\n",
    "plt.yticks(np.arange(16), labels=np.arange(16));\n",
    "plt.setp(plt.gca().get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dc75a9-c0be-412e-a5f3-c0d81030d504",
   "metadata": {},
   "source": [
    "this was originally from lesson 3.2. the idea is to look at the action distribtuion for each possible observation of frozen lake. here, in the random lake actually, you can see that it's most sure about what to do right before the goal, and gets less sure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3ba5e7-5de1-4837-abd6-0c3ee5dbae51",
   "metadata": {},
   "source": [
    "what might also be cool is to just make a 4x4 map and the colour can be the max probability (or -entropy or whatever, a measure or sureness), so we can see that it gets more sure closer to the goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c1a787-b56c-4f3e-be04-9baed74e5222",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rl-course]",
   "language": "python",
   "name": "conda-env-rl-course-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
