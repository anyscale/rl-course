{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e23df55-837b-4ec9-86ad-3b60c6a7f00d",
   "metadata": {},
   "source": [
    "## Environments syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2e57cc-3491-4489-af86-6052b1efcc1b",
   "metadata": {},
   "source": [
    "#### Motivation\n",
    "\n",
    "- So far we've used pre-defined environments like Frozen Like and Google RecSim.\n",
    "- To use RL on our own problem, we can't use any of these environments.\n",
    "- We'll need to define our own environment with Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0797f5a9-c856-49f2-9a3c-419d816b6ec1",
   "metadata": {},
   "source": [
    "#### Frozen Lake Review\n",
    "\n",
    "- Recall the Frozen Lake environment, from Module 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69171cee-dc7b-433d-85c5-d0d40486a43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"FrozenLake-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a41dffc-8451-4d7e-a40a-23fd1315ffba",
   "metadata": {},
   "source": [
    "#### Frozen Lake Review\n",
    "\n",
    "- OpenAI Gym is open source, so we could look at the [Frozen Lake source code](https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py).\n",
    "- However, it's complicated and contains much more than we need.\n",
    "- Let's make our own environment called Frozen Pond with the basic components of Frozen Lake."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735eb606-b1dd-4dba-be55-d6105cc47536",
   "metadata": {},
   "source": [
    "#### Components of an Env\n",
    "\n",
    "Conceptual decisions:\n",
    "\n",
    "- Observation space\n",
    "- Action space\n",
    "\n",
    "In Python we will need to implement, at least:\n",
    "\n",
    "- constructor\n",
    "- `reset()`\n",
    "- `step()`\n",
    "\n",
    "In practice, we may also want other methods, such as `render()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9cf2ed-dc7e-4bbd-a5f8-9b78b8a38710",
   "metadata": {},
   "source": [
    "#### Conceptual decisions\n",
    "\n",
    "- In this case, since we're mimicking the Frozen Lake, the observation space and action space are already decided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c0aa1d-8ab0-462a-8bdd-8ba12484a5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space = gym.spaces.Discrete(16)\n",
    "action_space = gym.spaces.Discrete(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef0fa48-25fb-4360-a1d8-54ca675dc786",
   "metadata": {},
   "source": [
    "Later in this course we'll dive deeper into these decisions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c88c68-7f74-490e-beda-9e199d0c3822",
   "metadata": {},
   "source": [
    "#### Coding it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db413de7-7df3-4d77-9ae8-5c8e95a39068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "class FrozenPond(gym.Env):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b3bbec-72c8-48dd-bdc4-6f537dacda5c",
   "metadata": {},
   "source": [
    "- Notice that we start by subclassing `gym.Env`.\n",
    "- Optional: You can read about objects, inheritance, and subclasses.\n",
    "- Punch line: This is a basic `gym.Env` and we can overwrite features of it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b2467a-4906-473f-8207-cde646def666",
   "metadata": {},
   "source": [
    "#### Constructor\n",
    "\n",
    "- The constructor gets called when we make a new `FrozenPond` object.\n",
    "- Here is where we define the observation space and action space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2417aa6-93e3-46c9-984b-f98e24ece49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "class FrozenPond(gym.Env):\n",
    "    def __init__(self, env_config=None):\n",
    "        self.observation_space = gym.spaces.Discrete(16)\n",
    "        self.action_space = gym.spaces.Discrete(4)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359ab164-11d8-4614-a4f1-f875fac08071",
   "metadata": {},
   "source": [
    "- For RLlib compatibility, the constructor must take in an `env_config`. \n",
    "- We will just ignore this argument for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285e5eca-3aa8-472a-a6bc-037b411f83a5",
   "metadata": {},
   "source": [
    "#### Reset\n",
    "\n",
    "- The next method we'll need is reset.\n",
    "- The constructor sets permanent parameters like the observation space.\n",
    "- `reset` sets up each new episode.\n",
    "- There is some freedom between the two, e.g. setting the goal location.\n",
    "- If something _could_ change, we'll put it in `reset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9715c1-8865-4376-97d5-4c0f7b253e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c438cc9b-d6db-4412-8b29-80288a2cf288",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def reset(self):\n",
    "        self.player = (0, 0) # the player starts at the top-left\n",
    "        self.goal = (3, 3)   # goal is at the bottom-right\n",
    "        \n",
    "        self.holes = np.array([\n",
    "            [0,0,0,0], # FFFF \n",
    "            [0,1,0,1], # FHFH\n",
    "            [0,0,0,1], # FFFH\n",
    "            [1,0,0,0]  # HFFF\n",
    "        ])\n",
    "        \n",
    "        return 0 # to be changed to return self.observation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206ed3bf-aa1d-4833-84fa-e9be1f64648b",
   "metadata": {},
   "source": [
    "#### Reset\n",
    "\n",
    "Let's test this out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1c6161-2b90-47bf-b002-d3ff6fc62464",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = FrozenPond()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50989bc9-5d31-480a-88ac-dbf85f2ff390",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbdb5ee-0af9-43e2-8833-c1c945eae535",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp.holes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79c6258-6353-42ac-81d6-51423385972a",
   "metadata": {},
   "source": [
    "Looks good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a138d14-cf05-4d44-b315-4e708692e001",
   "metadata": {},
   "source": [
    "#### Step\n",
    "\n",
    "- The last method we need is `step`.\n",
    "- This is the most complicated method that contains the core logic.\n",
    "- Recall that `step` returns 4 things:\n",
    "  1. Observation\n",
    "  2. Reward\n",
    "  3. Done flag\n",
    "  4. Extra info (we will ignore)\n",
    "- For clarity, we'll write helper methods for observation, reward and done, plus one extra helper method. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5e1f9a-6f10-4038-9c02-2a6e966ecac5",
   "metadata": {},
   "source": [
    "#### Step: observation\n",
    "\n",
    "Recall the observation is an index from 0 to 15:\n",
    "\n",
    "```\n",
    " 0   1   2   3\n",
    " 4   5   6   7\n",
    " 8   9  10  11\n",
    "12  13  14  15\n",
    "```\n",
    "\n",
    "We can code this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66c82f8-21a4-403d-8172-2b878488e155",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def observation(self):\n",
    "        return 4*self.player[0] + self.player[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921f3bc3-6200-4167-839b-a068e88240c1",
   "metadata": {},
   "source": [
    "For example, if the player is at (2,1) then we return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95ad09b-a77d-4039-afa6-0695a8427601",
   "metadata": {},
   "outputs": [],
   "source": [
    "4*2 + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae722e0-9d57-414c-ade2-53b361b8e590",
   "metadata": {},
   "source": [
    "Note: now that `self.observation` is implemented, we should change `reset` to `return self.observation()` rather than `return 0` for better quality code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b84fcf-c40f-4513-8cd0-3bb2b0a8d32a",
   "metadata": {},
   "source": [
    "#### Step: reward\n",
    "\n",
    "Following the Frozen Lake example, the reward will be 1 if the agent reaches the goal, and 0 otherwise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee1fd77-5923-4db1-abe3-96c47360366a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def reward(self):\n",
    "        return int(self.player == self.goal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d71262-1a9e-4daf-940e-df02c2c2631d",
   "metadata": {},
   "source": [
    "We will modify this reward function later in the module!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb7d07c-9153-4933-94f6-c117ca6993ff",
   "metadata": {},
   "source": [
    "#### Step: done\n",
    "\n",
    "- We also need to know when an episode is done. \n",
    "- Following Frozen Lake, the episode is done when the agent reaches the goal or falls into the pond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0ed064-4f50-425e-8f0f-302a44c2d259",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def done(self):\n",
    "        return self.player == self.goal or self.holes[self.player] == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8829aa45-d26c-4707-ab51-38c38353ce00",
   "metadata": {},
   "source": [
    "#### Step: valid locations\n",
    "\n",
    "Finally, to make the `step` method simpler, we'll write a helper method called `is_valid_loc` that checks whether a particular location is in bounds (from 0 to 3 in each dimension)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1872595-ab21-4b4e-a431-c15fc1ea7595",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def is_valid_loc(self, location):\n",
    "        if 0 <= location[0] <= 3 and 0 <= location[1] <= 3:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce94f18-ffb6-4f93-ba45-f7084c3805c5",
   "metadata": {},
   "source": [
    "#### Step: putting it together\n",
    "\n",
    "- Using the above pieces, we can now write the `step` method.\n",
    "- `step` takes in an _action_, updates the _state_, and returns the observation, reward, done flag, and extra info (ignored).\n",
    "- Recall how actions are encoded: 0 for left, 1 for down, 2 for right, 3 for up.\n",
    "- We will implement a **non-slippery** frozen pond; in other words, deterministic rather than stochastic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c083279a-b950-45e6-9cfc-265885c80c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def step(self, action):\n",
    "        # Compute the new player location\n",
    "        if action == 0:   # left\n",
    "            new_loc = (self.player[0], self.player[1]-1)\n",
    "        elif action == 1: # down\n",
    "            new_loc = (self.player[0]+1, self.player[1])\n",
    "        elif action == 2: # right\n",
    "            new_loc = (self.player[0], self.player[1]+1)\n",
    "        elif action == 3: # up\n",
    "            new_loc = (self.player[0]-1, self.player[1])\n",
    "        else:\n",
    "            raise ValueError(\"Action must be in {0,1,2,3}\")\n",
    "        \n",
    "        # Update the player location only if you stayed in bounds\n",
    "        # (if you try to move out of bounds, the action does nothing)\n",
    "        if self.is_valid_loc(new_loc):\n",
    "            self.player = new_loc\n",
    "        \n",
    "        # Return observation/reward/done\n",
    "        return self.observation(), self.reward(), self.done(), {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a4a4ec-cfe6-4d3e-80a1-a606afc87c0e",
   "metadata": {},
   "source": [
    "#### Success!\n",
    "\n",
    "- That's it! We've implemented the necessary pieces in Frozen Pond: \n",
    "  - constructor\n",
    "  - `reset`\n",
    "  - `step`\n",
    "- We'll also add an optional `render` function so that we can draw the state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dace259a-1e8f-4d77-af39-3f1cb6cdc2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def render(self):\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                if (i,j) == self.goal:\n",
    "                    print(\"G\", end=\"\")\n",
    "                elif (i,j) == self.player:\n",
    "                    print(\"P\", end=\"\")\n",
    "                elif self.holes[i,j]:\n",
    "                    print(\"O\", end=\"\")\n",
    "                else:\n",
    "                    print(\".\", end=\"\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925146e2-188f-4b78-acb5-df30214dad54",
   "metadata": {},
   "source": [
    "- For simplicity, we're using `P` to denote the player, instead of the red highlighting.\n",
    "- We also changed the `F` to `.` and the `H` to `O` as this makes the rendering easier to read."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d03db1-e53b-435b-a0fb-aa67bdd99161",
   "metadata": {},
   "source": [
    "#### Testing our implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a079671-3fc3-46eb-aaa3-e0629201de06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "from envs import FrozenPond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c03550a-fde5-4885-883a-030a2530488f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FrozenPond()\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ead7503-4511-441d-98b5-7d2571d8144e",
   "metadata": {},
   "source": [
    "The holes are denoted by `O` and the safe squares by `.`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88bcc59-f6c1-4722-a255-ef30a49c2616",
   "metadata": {},
   "source": [
    "#### Testing our implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2301a03f-f40e-492d-ae42-0f99b07ee09f",
   "metadata": {},
   "source": [
    "Let's test the `step` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc81874-c11f-430f-904b-1d108c170589",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d4775d-fe17-4628-a97f-33ab2799573d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b46b9c2-6ebe-4d37-b809-c38bd711126c",
   "metadata": {},
   "source": [
    "Looks good!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be4064b-3f24-48b1-aa6e-184f655067d0",
   "metadata": {},
   "source": [
    "#### Testing our implementation\n",
    "\n",
    "Let's directly compare the two environments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aff2e55-17f6-48bc-8fb8-eaade93881a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lake = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
    "pond = FrozenPond()\n",
    "\n",
    "lake.reset()\n",
    "pond.reset()\n",
    "\n",
    "print(\"Iter | gym obs / our obs | gym reward / our reward | gym done / our done\")\n",
    "for i, a in enumerate([0, 2, 2, 1, 1, 1, 1, 2]):\n",
    "    lake_obs, lake_rew, lake_done, _ = lake.step(a)\n",
    "    pond_obs, pond_rew, pond_done, _ = pond.step(a)\n",
    "    print(\"%2d   |      %2d / %2d      |          %d / %d        |      %5s / %5s\" % \\\n",
    "          (i, lake_obs, pond_obs, lake_rew, pond_rew, lake_done, pond_done))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87185e66-37b4-4337-9046-17a17cc6efb2",
   "metadata": {},
   "source": [
    "They look the same to me!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5deb1e8-3de2-42dc-8363-aee8acf534e3",
   "metadata": {},
   "source": [
    "#### Testing our implementation\n",
    "\n",
    "- RLlib also comes with an env checker\n",
    "- This won't tell us if our env is identical to Frozen Lake\n",
    "- But it will run several useful checks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e3cbe2-8354-4533-872d-41bd06ce2948",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.utils.pre_checks.env import check_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4ebd66-5e52-4724-a9c3-df6efdb66eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_env(pond)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f674689-9df5-4da7-9379-478c2b802571",
   "metadata": {},
   "source": [
    "- All checks passed except for this warning about a maximum episode length.\n",
    "- We can set this so that episodes cannot get arbitrarily long, which may improve training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29218be1-9c76-4f04-bffd-e7288098bb74",
   "metadata": {},
   "source": [
    "#### Maximum steps per episode\n",
    "\n",
    "- To set a maximum number of steps per episode, we can use a `gym` _wrapper_.\n",
    "- Wrappers are convenient ways to modify environments, including observations, actions, and rewards.\n",
    "- Here's we'll use the `TimeLimit` wrapper to set a step limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8edeeaf-5dd3-4eee-9ed0-747880709912",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import TimeLimit\n",
    "\n",
    "pond_5 = TimeLimit(pond, max_episode_steps=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37ac862-4df2-4f76-bbe0-b718e575f0e3",
   "metadata": {},
   "source": [
    "We can check that it will be done after 5 steps, even though the goal is not reached:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be71dbd1-7668-4499-85d2-68a5b70fa1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pond_5.reset()\n",
    "for i in range(5):\n",
    "    print(pond_5.step(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f54f62a-2022-4bdf-926e-b855dc374ec6",
   "metadata": {},
   "source": [
    "#### Maximum steps per episode\n",
    "\n",
    "A more reasonable step limit might be 50, rather than 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4847c8-ff8e-4e15-96b3-b0eb38781266",
   "metadata": {},
   "outputs": [],
   "source": [
    "pond_50 = TimeLimit(pond, max_episode_steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9eadc6-381d-47ac-82ec-f418b6830e6f",
   "metadata": {},
   "source": [
    "- FYI: it is also possible to set this limit in RLlib, just for training purposes.\n",
    "- This is done with the `\"horizon\"` parameter in the trainer config."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aed5b6c-a486-4f1c-96e0-e027710461f6",
   "metadata": {},
   "source": [
    "#### Let's apply what we learned!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456c51dc-3d2c-4e06-be76-62c279e976b0",
   "metadata": {},
   "source": [
    "## Frozen Pond rewards\n",
    "<!-- multiple choice -->\n",
    "\n",
    "In the Frozen Lake (and Pond), the reward is 1 when the agent reaches the goal, and 0 otherwise. The agent needs to learn to avoid the holes, but there is actually no negative reward from falling into a hole -- it's the same zero reward as walking into a safe piece of frozen lake! Why does this setup still work, even though the reward is the same for walking into a hole or dry land?\n",
    "\n",
    "- [ ] Once the agent falls into a hole it is stuck. It can take more actions, but they don't do anything. Therefore the agent learns to avoid holes.\n",
    "- [ ] A reward of 0 is the lowest possible reward; therefore, when the agent receives a reward of 0 from falling into a hole, it immediately knows that falling into a hole is a bad thing.\n",
    "- [x] The penalty of falling into a hole is indirect in that the episode ends with a reward of zero, thus forfeiting the potential reward of 1 from reaching the goal successfully. The agent is learning that by falling into a hole it loses _future_ rewards.\n",
    "- [ ] RL agents prefer longer episodes. When the agent falls into the hole, the episode ends immediately, which the agent learns to avoid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d53229-6adf-4b71-b8dd-40157b9a703d",
   "metadata": {},
   "source": [
    "## Pond vs. Maze\n",
    "<!-- coding exercise -->\n",
    "\n",
    "Let's say we wanted to change our pond environment into a _maze_ environment. In this case, we have walls instead of holes. The only difference between the pond and maze is the behavior of holes vs. walls. In the frozen pond, walking into a hole ends the episode. In the maze environment, walking into a wall does nothing (that is, the action does not change the agent's location, just like trying to walk off the edge of the map). To change our Frozen Lake into a Maze, we will need to modify two methods: `done` and `is_valid_loc`.\n",
    "\n",
    "Below you will find the `done` and `step` methods that we saw in the slides above. Modify them so that we now have a Maze with the behavior described above: walking into a wall does nothing.\n",
    "\n",
    "Note that the `Maze` class inherits all other methods from `FrozenPond`, so you can test it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238c70d1-4cd2-4cb0-ba58-3599ffa0879e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "\n",
    "class Maze(FrozenPond):\n",
    "    def done(self):\n",
    "        return self.player == self.goal or self.holes[self.player] == 1\n",
    "    def is_valid_loc(self, location):\n",
    "        if 0 <= location[0] <= 3 and 0 <= location[1] <= 3:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "pond = FrozenPond()\n",
    "pond.reset()\n",
    "pond.step(1)\n",
    "print(pond.step(2))\n",
    "\n",
    "maze = Maze()\n",
    "maze.reset()\n",
    "maze.step(1)\n",
    "print(maze.step(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a483f55-b956-44c1-bf8e-d534e6ad3ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "class Maze(FrozenPond):   \n",
    "    def done(self):\n",
    "        return self.player == self.goal\n",
    "    def is_valid_loc(self, location):\n",
    "        if 0 <= location[0] <= 3 and 0 <= location[1] <= 3 and not self.holes[location]:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "pond = FrozenPond()\n",
    "pond.reset()\n",
    "pond.step(1)\n",
    "print(pond.step(2))\n",
    "\n",
    "maze = Maze()\n",
    "maze.reset()\n",
    "maze.step(1)\n",
    "print(maze.step(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1512d61-1a46-4f42-8f19-fea5176b8fdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b05bd8f-7752-4f4c-ad2b-4512d025ba11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c4d3b0a-e0f0-41ab-a186-46994d8d6299",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from gym.spaces import Discrete, MultiDiscrete\n",
    "import numpy as np\n",
    "\n",
    "class RepeatedPrisonersDilemma(MultiAgentEnv):\n",
    "    def __init__(self, config=None):\n",
    "        config = config or {}\n",
    "        self.timestep_limit = config.get(\"ts\", 50)\n",
    "        self.observation_space = MultiDiscrete([2,2])\n",
    "        self.action_space = Discrete(2)\n",
    "        \n",
    "        self.last_actions = np.array([0,0])\n",
    "        self._spaces_in_preferred_format = False\n",
    "\n",
    "    def reset(self):\n",
    "        self.timesteps = 0\n",
    "        \n",
    "        return self.last_actions\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        if action[\"p1\"] == 0 and action[\"p2\"] == 0: # both cooperate\n",
    "            r1 = 0\n",
    "            r2 = 0\n",
    "        elif action[\"p1\"] == 0 and action[\"p2\"] == 1: # p2 defects\n",
    "            r1 = -2\n",
    "            r2 = 1\n",
    "        elif action[\"p1\"] == 1 and action[\"p2\"] == 0: # p1 defects\n",
    "            r1 = 1\n",
    "            r2 = -2\n",
    "        elif action[\"p1\"] == 1 and action[\"p2\"] == 1: # both defect\n",
    "            r1 = -1\n",
    "            r2 = -1\n",
    "        else:\n",
    "            raise ValueError(\"Unknown action\")\n",
    "            \n",
    "        obs = {\n",
    "            \"p1\" : np.array([action[\"p1\"], action[\"p2\"]]),\n",
    "            \"p2\" : np.array([action[\"p2\"], action[\"p1\"]]),\n",
    "        }\n",
    "        rew = {\"p1\" : r1, \"p2\" : r2}\n",
    "        is_done = self.timesteps >= self.timestep_limit\n",
    "        dones = {\n",
    "            \"p1\": is_done,\n",
    "            \"p2\": is_done,\n",
    "            # special `__all__` key indicates that the episode is done for all agents.\n",
    "            \"__all__\": is_done,\n",
    "        }\n",
    "\n",
    "        return obs, rew, dones, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "944d3697-b95a-4336-ae16-abeab246aaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = RepeatedPrisonersDilemma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b727a5b7-f450-4c61-92f2-89a3628f5ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d387a85-cd71-483d-ac07-02e86239a326",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "ppo_config = (\n",
    "    PPOConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(create_env_on_local_worker=True)\\\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\\\n",
    "    .training(model={\"fcnet_hiddens\" : [64, 64]})\\\n",
    ").multi_agent(\n",
    "    policies=[\"policy1\", \"policy2\"],\n",
    "    policy_mapping_fn=lambda agent_id, episode, worker, **kwargs: \"policy1\" if agent_id == \"p1\" else \"policy2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8980ca28-c3a3-41dd-a05e-78e8464fb8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mike/git/anyscale/ray/python/ray/rllib/utils/debug/deterministic.py:42: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(torch.__version__) >= LooseVersion(\"1.8.0\"):\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23827)\u001b[0m /Users/mike/git/anyscale/ray/python/ray/rllib/utils/debug/deterministic.py:42: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23827)\u001b[0m   if LooseVersion(torch.__version__) >= LooseVersion(\"1.8.0\"):\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23827)\u001b[0m /Users/mike/miniconda3/envs/ray2beta/lib/python3.9/site-packages/gym/core.py:172: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed) instead.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23827)\u001b[0m   deprecation(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23826)\u001b[0m /Users/mike/git/anyscale/ray/python/ray/rllib/utils/debug/deterministic.py:42: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23826)\u001b[0m   if LooseVersion(torch.__version__) >= LooseVersion(\"1.8.0\"):\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23826)\u001b[0m /Users/mike/miniconda3/envs/ray2beta/lib/python3.9/site-packages/gym/core.py:172: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed) instead.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23826)\u001b[0m   deprecation(\n"
     ]
    }
   ],
   "source": [
    "ppo = ppo_config.build(env=RepeatedPrisonersDilemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "390db522-dc2d-43f2-aa85-4251a50216bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "RayTaskError(AttributeError)",
     "evalue": "\u001b[36mray::RolloutWorker.sample()\u001b[39m (pid=23826, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x1253a69d0>)\n  File \"/Users/mike/git/anyscale/ray/python/ray/rllib/evaluation/rollout_worker.py\", line 859, in sample\n    batches = [self.input_reader.next()]\n  File \"/Users/mike/git/anyscale/ray/python/ray/rllib/evaluation/sampler.py\", line 115, in next\n    batches = [self.get_data()]\n  File \"/Users/mike/git/anyscale/ray/python/ray/rllib/evaluation/sampler.py\", line 288, in get_data\n    item = next(self._env_runner)\n  File \"/Users/mike/git/anyscale/ray/python/ray/rllib/evaluation/sampler.py\", line 660, in _env_runner\n    unfiltered_obs, rewards, dones, infos, off_policy_actions = base_env.poll()\n  File \"/Users/mike/git/anyscale/ray/python/ray/rllib/env/multi_agent_env.py\", line 530, in poll\n    obs[i], rewards[i], dones[i], infos[i] = env_state.poll()\n  File \"/Users/mike/git/anyscale/ray/python/ray/rllib/env/multi_agent_env.py\", line 684, in poll\n    for ag in observations.keys():\nAttributeError: 'numpy.ndarray' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayTaskError(AttributeError)\u001b[0m              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[0;32m----> 5\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mppo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     rewards1\u001b[38;5;241m.\u001b[39mappend(result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpolicy_reward_mean\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpolicy1\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      7\u001b[0m     rewards2\u001b[38;5;241m.\u001b[39mappend(result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpolicy_reward_mean\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpolicy2\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/tune/trainable/trainable.py:346\u001b[0m, in \u001b[0;36mTrainable.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warmup_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n\u001b[1;32m    345\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 346\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mdict\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep() needs to return a dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;66;03m# We do not modify internal state nor update this result if duplicate.\u001b[39;00m\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/rllib/algorithms/algorithm.py:591\u001b[0m, in \u001b[0;36mAlgorithm.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    583\u001b[0m     (\n\u001b[1;32m    584\u001b[0m         results,\n\u001b[1;32m    585\u001b[0m         train_iter_ctx,\n\u001b[1;32m    586\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_one_training_iteration_and_evaluation_in_parallel()\n\u001b[1;32m    587\u001b[0m \u001b[38;5;66;03m# - No evaluation necessary, just run the next training iteration.\u001b[39;00m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;66;03m# - We have to evaluate in this training iteration, but no parallelism ->\u001b[39;00m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;66;03m#   evaluate after the training iteration is entirely done.\u001b[39;00m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 591\u001b[0m     results, train_iter_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_one_training_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;66;03m# Sequential: Train (already done above), then evaluate.\u001b[39;00m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evaluate_this_iter \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation_parallel_to_training\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/rllib/algorithms/algorithm.py:2283\u001b[0m, in \u001b[0;36mAlgorithm._run_one_training_iteration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2281\u001b[0m         \u001b[38;5;66;03m# In case of any failures, try to ignore/recover the failed workers.\u001b[39;00m\n\u001b[1;32m   2282\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 2283\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtry_recover_from_step_attempt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2284\u001b[0m \u001b[43m                \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2285\u001b[0m \u001b[43m                \u001b[49m\u001b[43mworker_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2286\u001b[0m \u001b[43m                \u001b[49m\u001b[43mignore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mignore_worker_failures\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2287\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrecreate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrecreate_failed_workers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2288\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2290\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results, train_iter_ctx\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/rllib/algorithms/algorithm.py:2095\u001b[0m, in \u001b[0;36mAlgorithm.try_recover_from_step_attempt\u001b[0;34m(self, error, worker_set, ignore, recreate)\u001b[0m\n\u001b[1;32m   2086\u001b[0m     \u001b[38;5;66;03m# Error out.\u001b[39;00m\n\u001b[1;32m   2087\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2088\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m   2089\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWorker crashed during training or evaluation! \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2090\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo try to continue without failed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2093\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`recreate_failed_workers=True`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2094\u001b[0m         )\n\u001b[0;32m-> 2095\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[1;32m   2096\u001b[0m \u001b[38;5;66;03m# Any other exception.\u001b[39;00m\n\u001b[1;32m   2097\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2098\u001b[0m     \u001b[38;5;66;03m# Allow logs messages to propagate.\u001b[39;00m\n\u001b[1;32m   2099\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.5\u001b[39m)\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/rllib/algorithms/algorithm.py:2278\u001b[0m, in \u001b[0;36mAlgorithm._run_one_training_iteration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2276\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timers[TRAINING_ITERATION_TIMER]:\n\u001b[1;32m   2277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_disable_execution_plan_api\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m-> 2278\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2279\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2280\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_exec_impl)\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/rllib/algorithms/ppo/ppo.py:407\u001b[0m, in \u001b[0;36mPPO.training_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    403\u001b[0m     train_batch \u001b[38;5;241m=\u001b[39m synchronous_parallel_sample(\n\u001b[1;32m    404\u001b[0m         worker_set\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers, max_agent_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_batch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    405\u001b[0m     )\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 407\u001b[0m     train_batch \u001b[38;5;241m=\u001b[39m \u001b[43msynchronous_parallel_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworker_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_env_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_batch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m train_batch \u001b[38;5;241m=\u001b[39m train_batch\u001b[38;5;241m.\u001b[39mas_multi_agent()\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_counters[NUM_AGENT_STEPS_SAMPLED] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m train_batch\u001b[38;5;241m.\u001b[39magent_steps()\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/rllib/execution/rollout_ops.py:99\u001b[0m, in \u001b[0;36msynchronous_parallel_sample\u001b[0;34m(worker_set, max_agent_steps, max_env_steps, concat)\u001b[0m\n\u001b[1;32m     96\u001b[0m     sample_batches \u001b[38;5;241m=\u001b[39m [worker_set\u001b[38;5;241m.\u001b[39mlocal_worker()\u001b[38;5;241m.\u001b[39msample()]\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Loop over remote workers' `sample()` method in parallel.\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 99\u001b[0m     sample_batches \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mworker\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mworker_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Update our counters for the stopping criterion of the while loop.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m sample_batches:\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/_private/client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/_private/worker.py:2178\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   2176\u001b[0m     worker\u001b[38;5;241m.\u001b[39mcore_worker\u001b[38;5;241m.\u001b[39mdump_object_store_memory_usage()\n\u001b[1;32m   2177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, RayTaskError):\n\u001b[0;32m-> 2178\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mas_instanceof_cause()\n\u001b[1;32m   2179\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2180\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "\u001b[0;31mRayTaskError(AttributeError)\u001b[0m: \u001b[36mray::RolloutWorker.sample()\u001b[39m (pid=23826, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x1253a69d0>)\n  File \"/Users/mike/git/anyscale/ray/python/ray/rllib/evaluation/rollout_worker.py\", line 859, in sample\n    batches = [self.input_reader.next()]\n  File \"/Users/mike/git/anyscale/ray/python/ray/rllib/evaluation/sampler.py\", line 115, in next\n    batches = [self.get_data()]\n  File \"/Users/mike/git/anyscale/ray/python/ray/rllib/evaluation/sampler.py\", line 288, in get_data\n    item = next(self._env_runner)\n  File \"/Users/mike/git/anyscale/ray/python/ray/rllib/evaluation/sampler.py\", line 660, in _env_runner\n    unfiltered_obs, rewards, dones, infos, off_policy_actions = base_env.poll()\n  File \"/Users/mike/git/anyscale/ray/python/ray/rllib/env/multi_agent_env.py\", line 530, in poll\n    obs[i], rewards[i], dones[i], infos[i] = env_state.poll()\n  File \"/Users/mike/git/anyscale/ray/python/ray/rllib/env/multi_agent_env.py\", line 684, in poll\n    for ag in observations.keys():\nAttributeError: 'numpy.ndarray' object has no attribute 'keys'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-28 15:40:20,035\tERROR worker.py:396 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::RolloutWorker.sample()\u001b[39m (pid=23827, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x152aa69d0>)\n",
      "  File \"/Users/mike/git/anyscale/ray/python/ray/rllib/evaluation/rollout_worker.py\", line 859, in sample\n",
      "    batches = [self.input_reader.next()]\n",
      "  File \"/Users/mike/git/anyscale/ray/python/ray/rllib/evaluation/sampler.py\", line 115, in next\n",
      "    batches = [self.get_data()]\n",
      "  File \"/Users/mike/git/anyscale/ray/python/ray/rllib/evaluation/sampler.py\", line 288, in get_data\n",
      "    item = next(self._env_runner)\n",
      "  File \"/Users/mike/git/anyscale/ray/python/ray/rllib/evaluation/sampler.py\", line 660, in _env_runner\n",
      "    unfiltered_obs, rewards, dones, infos, off_policy_actions = base_env.poll()\n",
      "  File \"/Users/mike/git/anyscale/ray/python/ray/rllib/env/multi_agent_env.py\", line 530, in poll\n",
      "    obs[i], rewards[i], dones[i], infos[i] = env_state.poll()\n",
      "  File \"/Users/mike/git/anyscale/ray/python/ray/rllib/env/multi_agent_env.py\", line 684, in poll\n",
      "    for ag in observations.keys():\n",
      "AttributeError: 'numpy.ndarray' object has no attribute 'keys'\n"
     ]
    }
   ],
   "source": [
    "rewards1 = []\n",
    "rewards2 = []\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "    result = ppo.train()\n",
    "    rewards1.append(result['policy_reward_mean']['policy1'])\n",
    "    rewards2.append(result['policy_reward_mean']['policy2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cc292e-6c9e-48be-8dad-6092d96e790b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ray2beta]",
   "language": "python",
   "name": "conda-env-ray2beta-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
