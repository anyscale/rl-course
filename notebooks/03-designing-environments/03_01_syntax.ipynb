{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e23df55-837b-4ec9-86ad-3b60c6a7f00d",
   "metadata": {},
   "source": [
    "## Environments syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2e57cc-3491-4489-af86-6052b1efcc1b",
   "metadata": {},
   "source": [
    "#### Motivation\n",
    "\n",
    "- So far we've used pre-defined environments like Frozen Like and Google RecSim.\n",
    "- To use RL on our own problem, we can't use any of these environments.\n",
    "- We'll need to define our own environment with Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0797f5a9-c856-49f2-9a3c-419d816b6ec1",
   "metadata": {},
   "source": [
    "#### Frozen Lake Review\n",
    "\n",
    "- Recall the Frozen Lake environment, from Module 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69171cee-dc7b-433d-85c5-d0d40486a43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"FrozenLake-v1\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a41dffc-8451-4d7e-a40a-23fd1315ffba",
   "metadata": {},
   "source": [
    "#### Frozen Lake Review\n",
    "\n",
    "- OpenAI Gym is open source, so we could look at the [Frozen Lake source code](https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py).\n",
    "- However, it's complicated and contains much more than we need.\n",
    "- Let's make our own environment called Frozen Pond with the basic components of Frozen Lake."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735eb606-b1dd-4dba-be55-d6105cc47536",
   "metadata": {},
   "source": [
    "#### Components of an Env\n",
    "\n",
    "Conceptual decisions:\n",
    "\n",
    "- Observation space\n",
    "- Action space\n",
    "\n",
    "In Python we will need to implement, at least:\n",
    "\n",
    "- constructor\n",
    "- `reset()`\n",
    "- `step()`\n",
    "\n",
    "In practice, we may also want other methods, such as `render()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9cf2ed-dc7e-4bbd-a5f8-9b78b8a38710",
   "metadata": {},
   "source": [
    "#### Conceptual decisions\n",
    "\n",
    "- In this case, since we're mimicking the Frozen Lake, the observation space and action space are already decided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34c0aa1d-8ab0-462a-8bdd-8ba12484a5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space = gym.spaces.Discrete(16)\n",
    "action_space = gym.spaces.Discrete(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef0fa48-25fb-4360-a1d8-54ca675dc786",
   "metadata": {},
   "source": [
    "Later in this course we'll dive deeper into these decisions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c88c68-7f74-490e-beda-9e199d0c3822",
   "metadata": {},
   "source": [
    "#### Coding it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db413de7-7df3-4d77-9ae8-5c8e95a39068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "class FrozenPond(gym.Env):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b3bbec-72c8-48dd-bdc4-6f537dacda5c",
   "metadata": {},
   "source": [
    "- Notice that we start by subclassing `gym.Env`.\n",
    "- Optional: You can read about objects, inheritance, and subclasses.\n",
    "- Punch line: This is a basic `gym.Env` and we can overwrite features of it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b2467a-4906-473f-8207-cde646def666",
   "metadata": {},
   "source": [
    "#### Constructor\n",
    "\n",
    "- The constructor gets called when we make a new `FrozenPond` object.\n",
    "- Here is where we define the observation space and action space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2417aa6-93e3-46c9-984b-f98e24ece49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "class FrozenPond(gym.Env):\n",
    "    def __init__(self, env_config=None):\n",
    "        self.observation_space = gym.spaces.Discrete(16)\n",
    "        self.action_space = gym.spaces.Discrete(4)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359ab164-11d8-4614-a4f1-f875fac08071",
   "metadata": {},
   "source": [
    "- For RLlib compatibility, the constructor must take in an `env_config`. \n",
    "- We will just ignore this argument for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285e5eca-3aa8-472a-a6bc-037b411f83a5",
   "metadata": {},
   "source": [
    "#### Reset\n",
    "\n",
    "- The next method we'll need is reset.\n",
    "- The constructor sets permanent parameters like the observation space.\n",
    "- `reset` sets up each new episode.\n",
    "- There is some freedom between the two, e.g. setting the goal location.\n",
    "- If something _could_ change, we'll put it in `reset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce9715c1-8865-4376-97d5-4c0f7b253e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c438cc9b-d6db-4412-8b29-80288a2cf288",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def reset(self):\n",
    "        self.player = (0, 0) # the player starts at the top-left\n",
    "        self.goal = (3, 3)   # goal is at the bottom-right\n",
    "        \n",
    "        self.holes = np.array([\n",
    "            [0,0,0,0], # FFFF \n",
    "            [0,1,0,1], # FHFH\n",
    "            [0,0,0,1], # FFFH\n",
    "            [1,0,0,0]  # HFFF\n",
    "        ])\n",
    "        \n",
    "        return 0 # to be changed to return self.observation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206ed3bf-aa1d-4833-84fa-e9be1f64648b",
   "metadata": {},
   "source": [
    "#### Reset\n",
    "\n",
    "Let's test this out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c1c6161-2b90-47bf-b002-d3ff6fc62464",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = FrozenPond()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50989bc9-5d31-480a-88ac-dbf85f2ff390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edbdb5ee-0af9-43e2-8833-c1c945eae535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0],\n",
       "       [0, 1, 0, 1],\n",
       "       [0, 0, 0, 1],\n",
       "       [1, 0, 0, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp.holes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79c6258-6353-42ac-81d6-51423385972a",
   "metadata": {},
   "source": [
    "Looks good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a138d14-cf05-4d44-b315-4e708692e001",
   "metadata": {},
   "source": [
    "#### Step\n",
    "\n",
    "- The last method we need is `step`.\n",
    "- This is the most complicated method that contains the core logic.\n",
    "- Recall that `step` returns 4 things:\n",
    "  1. Observation\n",
    "  2. Reward\n",
    "  3. Done flag\n",
    "  4. Extra info (we will ignore)\n",
    "- For clarity, we'll write helper methods for observation, reward and done, plus one extra helper method. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5e1f9a-6f10-4038-9c02-2a6e966ecac5",
   "metadata": {},
   "source": [
    "#### Step: observation\n",
    "\n",
    "Recall the observation is an index from 0 to 15:\n",
    "\n",
    "```\n",
    " 0   1   2   3\n",
    " 4   5   6   7\n",
    " 8   9  10  11\n",
    "12  13  14  15\n",
    "```\n",
    "\n",
    "We can code this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a66c82f8-21a4-403d-8172-2b878488e155",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def observation(self):\n",
    "        return 4*self.player[0] + self.player[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921f3bc3-6200-4167-839b-a068e88240c1",
   "metadata": {},
   "source": [
    "For example, if the player is at (2,1) then we return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b95ad09b-a77d-4039-afa6-0695a8427601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4*2 + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae722e0-9d57-414c-ade2-53b361b8e590",
   "metadata": {},
   "source": [
    "Note: now that `self.observation` is implemented, we should change `reset` to `return self.observation()` rather than `return 0` for better quality code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b84fcf-c40f-4513-8cd0-3bb2b0a8d32a",
   "metadata": {},
   "source": [
    "#### Step: reward\n",
    "\n",
    "Following the Frozen Lake example, the reward will be 1 if the agent reaches the goal, and 0 otherwise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ee1fd77-5923-4db1-abe3-96c47360366a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def reward(self):\n",
    "        return int(self.player == self.goal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d71262-1a9e-4daf-940e-df02c2c2631d",
   "metadata": {},
   "source": [
    "We will modify this reward function later in the module!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb7d07c-9153-4933-94f6-c117ca6993ff",
   "metadata": {},
   "source": [
    "#### Step: done\n",
    "\n",
    "- We also need to know when an episode is done. \n",
    "- Following Frozen Lake, the episode is done when the agent reaches the goal or falls into the pond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab0ed064-4f50-425e-8f0f-302a44c2d259",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def done(self):\n",
    "        return self.player == self.goal or self.holes[self.player] == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8829aa45-d26c-4707-ab51-38c38353ce00",
   "metadata": {},
   "source": [
    "#### Step: valid locations\n",
    "\n",
    "Finally, to make the `step` method simpler, we'll write a helper method called `is_valid_loc` that checks whether a particular location is in bounds (from 0 to 3 in each dimension)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1872595-ab21-4b4e-a431-c15fc1ea7595",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def is_valid_loc(self, location):\n",
    "        if 0 <= location[0] <= 3 and 0 <= location[1] <= 3:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce94f18-ffb6-4f93-ba45-f7084c3805c5",
   "metadata": {},
   "source": [
    "#### Step: putting it together\n",
    "\n",
    "- Using the above pieces, we can now write the `step` method.\n",
    "- `step` takes in an _action_, updates the _state_, and returns the observation, reward, done flag, and extra info (ignored).\n",
    "- Recall how actions are encoded: 0 for left, 1 for down, 2 for right, 3 for up.\n",
    "- We will implement a **non-slippery** frozen pond; in other words, deterministic rather than stochastic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c083279a-b950-45e6-9cfc-265885c80c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def step(self, action):\n",
    "        # Compute the new player location\n",
    "        if action == 0:   # left\n",
    "            new_loc = (self.player[0], self.player[1]-1)\n",
    "        elif action == 1: # down\n",
    "            new_loc = (self.player[0]+1, self.player[1])\n",
    "        elif action == 2: # right\n",
    "            new_loc = (self.player[0], self.player[1]+1)\n",
    "        elif action == 3: # up\n",
    "            new_loc = (self.player[0]-1, self.player[1])\n",
    "        else:\n",
    "            raise ValueError(\"Action must be in {0,1,2,3}\")\n",
    "        \n",
    "        # Update the player location only if you stayed in bounds\n",
    "        # (if you try to move out of bounds, the action does nothing)\n",
    "        if self.is_valid_loc(new_loc):\n",
    "            self.player = new_loc\n",
    "        \n",
    "        # Return observation/reward/done\n",
    "        return self.observation(), self.reward(), self.done(), {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a4a4ec-cfe6-4d3e-80a1-a606afc87c0e",
   "metadata": {},
   "source": [
    "#### Success!\n",
    "\n",
    "- That's it! We've implemented the necessary pieces in Frozen Pond: \n",
    "  - constructor\n",
    "  - `reset`\n",
    "  - `step`\n",
    "- We'll also add an optional `render` function so that we can draw the state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dace259a-1e8f-4d77-af39-3f1cb6cdc2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def render(self):\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                if (i,j) == self.goal:\n",
    "                    print(\"G\", end=\"\")\n",
    "                elif (i,j) == self.player:\n",
    "                    print(\"P\", end=\"\")\n",
    "                elif self.holes[i,j]:\n",
    "                    print(\"O\", end=\"\")\n",
    "                else:\n",
    "                    print(\".\", end=\"\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925146e2-188f-4b78-acb5-df30214dad54",
   "metadata": {},
   "source": [
    "- For simplicity, we're using `P` to denote the player, instead of the red highlighting.\n",
    "- We also changed the `F` to `.` and the `H` to `O` as this makes the rendering easier to read."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d03db1-e53b-435b-a0fb-aa67bdd99161",
   "metadata": {},
   "source": [
    "#### Testing our implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a079671-3fc3-46eb-aaa3-e0629201de06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "from envs import FrozenPond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c03550a-fde5-4885-883a-030a2530488f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P...\n",
      ".O.O\n",
      "...O\n",
      "O..G\n"
     ]
    }
   ],
   "source": [
    "env = FrozenPond()\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ead7503-4511-441d-98b5-7d2571d8144e",
   "metadata": {},
   "source": [
    "The holes are denoted by `O` and the safe squares by `.`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88bcc59-f6c1-4722-a255-ef30a49c2616",
   "metadata": {},
   "source": [
    "#### Testing our implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2301a03f-f40e-492d-ae42-0f99b07ee09f",
   "metadata": {},
   "source": [
    "Let's test the `step` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1cc81874-c11f-430f-904b-1d108c170589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0, False, {})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89d4775d-fe17-4628-a97f-33ab2799573d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".P..\n",
      ".O.O\n",
      "...O\n",
      "O..G\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b46b9c2-6ebe-4d37-b809-c38bd711126c",
   "metadata": {},
   "source": [
    "Looks good!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be4064b-3f24-48b1-aa6e-184f655067d0",
   "metadata": {},
   "source": [
    "#### Testing our implementation\n",
    "\n",
    "Let's directly compare the two environments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3aff2e55-17f6-48bc-8fb8-eaade93881a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0/ 0    0/0    False/False\n",
      " 1/ 1    0/0    False/False\n",
      " 2/ 2    0/0    False/False\n",
      " 6/ 6    0/0    False/False\n",
      "10/10    0/0    False/False\n",
      "14/14    0/0    False/False\n",
      "14/14    0/0    False/False\n",
      "15/15    1/1     True/ True\n"
     ]
    }
   ],
   "source": [
    "pond = FrozenPond()\n",
    "lake = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
    "pond.reset()\n",
    "lake.reset()\n",
    "\n",
    "for a in [0, 2, 2, 1, 1, 1, 1, 2]:\n",
    "    pond_obs, pond_rew, pond_done, _ = pond.step(a)\n",
    "    lake_obs, lake_rew, lake_done, _ = lake.step(a)\n",
    "    print(\"%2d/%2d    %d/%d    %5s/%5s\" % \\\n",
    "          (pond_obs, lake_obs, pond_rew, lake_rew, pond_done, lake_done))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87185e66-37b4-4337-9046-17a17cc6efb2",
   "metadata": {},
   "source": [
    "They look the same to me!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aed5b6c-a486-4f1c-96e0-e027710461f6",
   "metadata": {},
   "source": [
    "#### Let's apply what we learned!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456c51dc-3d2c-4e06-be76-62c279e976b0",
   "metadata": {},
   "source": [
    "## Frozen Pond rewards\n",
    "<!-- multiple choice -->\n",
    "\n",
    "In the Frozen Lake (and Pond), the reward is 1 when the agent reaches the goal, and 0 otherwise. The agent needs to learn to avoid the holes, but there is actually no negative reward from falling into a hole -- it's the same zero reward as walking into a safe piece of frozen lake! Why does this setup still work, even though the reward is the same for walking into a hole or dry land?\n",
    "\n",
    "- [ ] Once the agent falls into a hole it is stuck. It can take more actions, but they don't do anything. Therefore the agent learns to avoid holes.\n",
    "- [ ] A reward of 0 is the lowest possible reward; therefore, when the agent receives a reward of 0 from falling into a hole, it immediately knows that falling into a hole is a bad thing.\n",
    "- [x] The penalty of falling into a hole is indirect in that the episode ends with a reward of zero, thus forfeiting the potential reward of 1 from reaching the goal successfully. The agent is learning that by falling into a hole it loses _future_ rewards.\n",
    "- [ ] RL agents prefer longer episodes. When the agent falls into the hole, the episode ends immediately, which the agent learns to avoid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d53229-6adf-4b71-b8dd-40157b9a703d",
   "metadata": {},
   "source": [
    "## Pond vs. Maze\n",
    "<!-- coding exercise -->\n",
    "\n",
    "Let's say we wanted to change our pond environment into a _maze_ environment. In this case, we have walls instead of holes. The only difference between the pond and maze is the behavior of holes vs. walls. In the frozen pond, walking into a hole ends the episode. In the maze environment, walking into a wall does nothing (that is, the action does not change the agent's location, just like trying to walk off the edge of the map). To change our Frozen Lake into a Maze, we will need to modify two methods: `done` and `is_valid_loc`.\n",
    "\n",
    "Below you will find the `done` and `step` methods that we saw in the slides above. Modify them so that we now have a Maze with the behavior described above: walking into a wall does nothing.\n",
    "\n",
    "Note that the `Maze` class inherits all other methods from `FrozenPond`, so you can test it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238c70d1-4cd2-4cb0-ba58-3599ffa0879e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "\n",
    "class Maze(FrozenPond):\n",
    "    def done(self):\n",
    "        return self.player == self.goal or self.holes[self.player] == 1\n",
    "    def is_valid_loc(self, location):\n",
    "        if 0 <= location[0] <= 3 and 0 <= location[1] <= 3:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "pond = FrozenPond()\n",
    "pond.reset()\n",
    "pond.step(1)\n",
    "print(pond.step(2))\n",
    "\n",
    "maze = Maze()\n",
    "maze.reset()\n",
    "maze.step(1)\n",
    "print(maze.step(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a483f55-b956-44c1-bf8e-d534e6ad3ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 0, True, {})\n",
      "(4, 0, False, {})\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "\n",
    "class Maze(FrozenPond):   \n",
    "    def done(self):\n",
    "        return self.player == self.goal\n",
    "    def is_valid_loc(self, location):\n",
    "        if 0 <= location[0] <= 3 and 0 <= location[1] <= 3 and not self.holes[location]:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "pond = FrozenPond()\n",
    "pond.reset()\n",
    "pond.step(1)\n",
    "print(pond.step(2))\n",
    "\n",
    "maze = Maze()\n",
    "maze.reset()\n",
    "maze.step(1)\n",
    "print(maze.step(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1512d61-1a46-4f42-8f19-fea5176b8fdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rl-course]",
   "language": "python",
   "name": "conda-env-rl-course-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
