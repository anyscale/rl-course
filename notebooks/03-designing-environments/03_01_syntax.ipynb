{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e23df55-837b-4ec9-86ad-3b60c6a7f00d",
   "metadata": {},
   "source": [
    "## Environments syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2e57cc-3491-4489-af86-6052b1efcc1b",
   "metadata": {},
   "source": [
    "#### Motivation\n",
    "\n",
    "- So far we've used pre-defined environments like Frozen Like and Google RecSim.\n",
    "- To use RL on our own problem, we can't use any of these environments.\n",
    "- We'll need to define our own environment with Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0797f5a9-c856-49f2-9a3c-419d816b6ec1",
   "metadata": {},
   "source": [
    "#### Frozen Lake Review\n",
    "\n",
    "- Recall the Frozen Lake environment, from Module 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69171cee-dc7b-433d-85c5-d0d40486a43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"FrozenLake-v1\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a41dffc-8451-4d7e-a40a-23fd1315ffba",
   "metadata": {},
   "source": [
    "#### Frozen Lake Review\n",
    "\n",
    "- OpenAI Gym is open source, so we could look at the [Frozen Lake source code](https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py).\n",
    "- However, it's complicated and contains much more than we need.\n",
    "- Let's make our own environment called Frozen Pond with the basic components of Frozen Lake."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735eb606-b1dd-4dba-be55-d6105cc47536",
   "metadata": {},
   "source": [
    "#### Components of an Env\n",
    "\n",
    "Conceptual decisions:\n",
    "\n",
    "- Observation space\n",
    "- Action space\n",
    "\n",
    "In Python we will need to implement, at least:\n",
    "\n",
    "- constructor\n",
    "- `reset()`\n",
    "- `step()`\n",
    "\n",
    "In practice, we may also want other methods, such as `render()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9cf2ed-dc7e-4bbd-a5f8-9b78b8a38710",
   "metadata": {},
   "source": [
    "#### Conceptual decisions\n",
    "\n",
    "- In this case, since we're mimicking the Frozen Lake, the observation space and action space are already decided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34c0aa1d-8ab0-462a-8bdd-8ba12484a5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space = gym.spaces.Discrete(16)\n",
    "action_space = gym.spaces.Discrete(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef0fa48-25fb-4360-a1d8-54ca675dc786",
   "metadata": {},
   "source": [
    "Later in this course we'll dive deeper into these decisions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c88c68-7f74-490e-beda-9e199d0c3822",
   "metadata": {},
   "source": [
    "#### Coding it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db413de7-7df3-4d77-9ae8-5c8e95a39068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "class FrozenPond(gym.Env):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b3bbec-72c8-48dd-bdc4-6f537dacda5c",
   "metadata": {},
   "source": [
    "- Notice that we start by subclassing `gym.Env`.\n",
    "- Optional: You can read about objects, inheritance, and subclasses.\n",
    "- Punch line: This is a basic `gym.Env` and we can overwrite features of it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b2467a-4906-473f-8207-cde646def666",
   "metadata": {},
   "source": [
    "#### Constructor\n",
    "\n",
    "- The constructor gets called when we make a new `FrozenPond` object.\n",
    "- Here is where we define the observation space and action space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2417aa6-93e3-46c9-984b-f98e24ece49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "class FrozenPond(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.observation_space = gym.spaces.Discrete(16)\n",
    "        self.action_space = gym.spaces.Discrete(4)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285e5eca-3aa8-472a-a6bc-037b411f83a5",
   "metadata": {},
   "source": [
    "#### Reset\n",
    "\n",
    "- The next method we'll need is reset.\n",
    "- The constructor sets permanent parameters like the observation space.\n",
    "- `reset` sets up each new episode.\n",
    "- There is some freedom between the two, e.g. setting the exit location.\n",
    "- If something _could_ change, we'll put it in `reset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce9715c1-8865-4376-97d5-4c0f7b253e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c438cc9b-d6db-4412-8b29-80288a2cf288",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def reset(self):\n",
    "        self.player = (0, 0) # the player starts at the top-left\n",
    "        self.exit = (3, 3)   # exit is at the bottom-right\n",
    "        \n",
    "        self.holes = np.array([\n",
    "            [0,0,0,0], # FFFF \n",
    "            [0,1,0,1], # FHFH\n",
    "            [0,0,0,1], # FFFH\n",
    "            [1,0,0,0]  # HFFF\n",
    "        ])\n",
    "        \n",
    "        return 0 # the observation corresponding to (0,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206ed3bf-aa1d-4833-84fa-e9be1f64648b",
   "metadata": {},
   "source": [
    "#### Reset\n",
    "\n",
    "Let's test this out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0c1c6161-2b90-47bf-b002-d3ff6fc62464",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = FrozenPond()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "50989bc9-5d31-480a-88ac-dbf85f2ff390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "edbdb5ee-0af9-43e2-8833-c1c945eae535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0],\n",
       "       [0, 1, 0, 1],\n",
       "       [0, 0, 0, 1],\n",
       "       [1, 0, 0, 0]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp.holes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae50a93-daf9-49de-b680-e0269e34c657",
   "metadata": {},
   "outputs": [],
   "source": [
    "Looks good"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a138d14-cf05-4d44-b315-4e708692e001",
   "metadata": {},
   "source": [
    "#### Step\n",
    "\n",
    "- The last method we need is `step`.\n",
    "- This is the most complicated method that contains the core logic.\n",
    "- Recall that `step` returns 4 things:\n",
    "  1. Observation\n",
    "  2. Reward\n",
    "  3. Done flag\n",
    "  4. Extra info (we will ignore)\n",
    "- For clarity, let's write these as 3 separate methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5e1f9a-6f10-4038-9c02-2a6e966ecac5",
   "metadata": {},
   "source": [
    "#### Step — observation\n",
    "\n",
    "Recall the observation is an index from 0 to 15:\n",
    "\n",
    "```\n",
    " 0   1   2   3\n",
    " 4   5   6   7\n",
    " 8   9  10  11\n",
    "12  13  14  15\n",
    "```\n",
    "\n",
    "We can code this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a66c82f8-21a4-403d-8172-2b878488e155",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def observation(self):\n",
    "        return 4*self.player[0] + self.player[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921f3bc3-6200-4167-839b-a068e88240c1",
   "metadata": {},
   "source": [
    "For example, if the player is at (2,1) then we return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b95ad09b-a77d-4039-afa6-0695a8427601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4*2 + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b84fcf-c40f-4513-8cd0-3bb2b0a8d32a",
   "metadata": {},
   "source": [
    "#### Step — reward\n",
    "\n",
    "Following the Frozen Lake example, the reward will be 1 if the agent reaches the goal, and 0 otherwise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ee1fd77-5923-4db1-abe3-96c47360366a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def reward(self):\n",
    "        return int(self.player == self.exit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d71262-1a9e-4daf-940e-df02c2c2631d",
   "metadata": {},
   "source": [
    "We will modify this reward function later in the module!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb7d07c-9153-4933-94f6-c117ca6993ff",
   "metadata": {},
   "source": [
    "#### Step — done\n",
    "\n",
    "- Finally, we want to know when an episode is done. \n",
    "- Following Frozen Lake, the episode is done when the agent reaches the goal or falls into the pond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ab0ed064-4f50-425e-8f0f-302a44c2d259",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def done(self):\n",
    "        return self.player == self.exit or self.holes[self.player] == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce94f18-ffb6-4f93-ba45-f7084c3805c5",
   "metadata": {},
   "source": [
    "#### Step — putting it together\n",
    "\n",
    "- Using the above pieces, we can now write the `step` method.\n",
    "- `step` takes in an _action_, updates the _state_, and returns the observation, reward, and done flag.\n",
    "- Recall how actions are encoded: 0 for left, 1 for down, 2 for right, 3 for up.\n",
    "- We will implement a **non-slippery** frozen pond; in other words, deterministic rather than stochastic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c083279a-b950-45e6-9cfc-265885c80c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def step(self, action):\n",
    "        # Compute the new player location\n",
    "        if action == 0:   # left\n",
    "            new_loc = (self.player[0], self.player[1]-1)\n",
    "        elif action == 1: # down\n",
    "            new_loc = (self.player[0]+1, self.player[1])\n",
    "        elif action == 2: # right\n",
    "            new_loc = (self.player[0], self.player[1]+1)\n",
    "        elif action == 3: # up\n",
    "            new_loc = (self.player[0]-1, self.player[1])\n",
    "        else:\n",
    "            raise ValueError(\"Action must be in {0,1,2,3}\")\n",
    "        \n",
    "        # Update the player location only if you stayed in bounds\n",
    "        # (if you try to move out of bounds, the action does nothing)\n",
    "        if 0 <= new_loc[0] <= 3 and 0 <= new_loc[1] <= 3:\n",
    "            self.player = new_loc\n",
    "        \n",
    "        # Return observation/reward/done\n",
    "        return self.observation(), self.reward(), self.done(), None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a4a4ec-cfe6-4d3e-80a1-a606afc87c0e",
   "metadata": {},
   "source": [
    "#### Success!\n",
    "\n",
    "- That's it! We've implemented the necessary pieces in Frozen Pond: \n",
    "  - constructor\n",
    "  - `reset`\n",
    "  - `step`\n",
    "- We'll also add an optional `render` function so that we can draw the state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dace259a-1e8f-4d77-af39-3f1cb6cdc2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenPond(gym.Env):\n",
    "    def render(self):\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                if (i,j) == self.exit:\n",
    "                    print(\"G\", end=\"\")\n",
    "                elif (i,j) == self.player:\n",
    "                    print(\"P\", end=\"\")\n",
    "                elif self.holes[i,j]:\n",
    "                    print(\"H\", end=\"\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925146e2-188f-4b78-acb5-df30214dad54",
   "metadata": {},
   "source": [
    "For simplicity, we're using `P` to denote the player, instead of the red highlighting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d03db1-e53b-435b-a0fb-aa67bdd99161",
   "metadata": {},
   "source": [
    "#### Testing our implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9a079671-3fc3-46eb-aaa3-e0629201de06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "class FrozenPond(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.observation_space = gym.spaces.Discrete(16)\n",
    "        self.action_space = gym.spaces.Discrete(4)      \n",
    "        \n",
    "    def reset(self):\n",
    "        self.player = (0, 0) # the player starts at the top-left\n",
    "        self.exit = (3, 3)   # exit is at the bottom-right\n",
    "        \n",
    "        self.holes = np.array([\n",
    "            [0,0,0,0], # FFFF \n",
    "            [0,1,0,1], # FHFH\n",
    "            [0,0,0,1], # FFFH\n",
    "            [1,0,0,0]  # HFFF\n",
    "        ])\n",
    "        \n",
    "        return 0 # the observation corresponding to (0,0)\n",
    "    \n",
    "    def observation(self):\n",
    "        return 4*self.player[0] + self.player[1]\n",
    "    \n",
    "    def reward(self):\n",
    "        return int(self.player == self.exit)\n",
    "    \n",
    "    def done(self):\n",
    "        return self.player == self.exit or self.holes[self.player] == 1\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Compute the new player location\n",
    "        if action == 0:   # left\n",
    "            new_loc = (self.player[0], self.player[1]-1)\n",
    "        elif action == 1: # down\n",
    "            new_loc = (self.player[0]+1, self.player[1])\n",
    "        elif action == 2: # right\n",
    "            new_loc = (self.player[0], self.player[1]+1)\n",
    "        elif action == 3: # up\n",
    "            new_loc = (self.player[0]-1, self.player[1])\n",
    "        else:\n",
    "            raise ValueError(\"Action must be in {0,1,2,3}\")\n",
    "        \n",
    "        # Update the player location only if you stayed in bounds\n",
    "        # (if you try to move out of bounds, the action does nothing)\n",
    "        if 0 <= new_loc[0] <= 3 and 0 <= new_loc[1] <= 3:\n",
    "            self.player = new_loc\n",
    "        \n",
    "        # Return observation/reward/done\n",
    "        return self.observation(), self.reward(), self.done(), None\n",
    "    \n",
    "    def render(self):\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                if (i,j) == self.exit:\n",
    "                    print(\"G\", end=\"\")\n",
    "                elif (i,j) == self.player:\n",
    "                    print(\"P\", end=\"\")\n",
    "                elif self.holes[i,j]:\n",
    "                    print(\"H\", end=\"\")\n",
    "                else:\n",
    "                    print(\"F\", end=\"\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5c03550a-fde5-4885-883a-030a2530488f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env = FrozenPond()\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1cc81874-c11f-430f-904b-1d108c170589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0, False, None)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "89d4775d-fe17-4628-a97f-33ab2799573d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b46b9c2-6ebe-4d37-b809-c38bd711126c",
   "metadata": {},
   "source": [
    "Looks good!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be4064b-3f24-48b1-aa6e-184f655067d0",
   "metadata": {},
   "source": [
    "#### Testing our implementation\n",
    "\n",
    "Let's directly compare the two environments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3aff2e55-17f6-48bc-8fb8-eaade93881a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0/ 0    0/0    False/False\n",
      " 1/ 1    0/0    False/False\n",
      " 2/ 2    0/0    False/False\n",
      " 6/ 6    0/0    False/False\n",
      "10/10    0/0    False/False\n",
      "14/14    0/0    False/False\n",
      "14/14    0/0    False/False\n",
      "15/15    1/1     True/ True\n"
     ]
    }
   ],
   "source": [
    "pond = FrozenPond()\n",
    "lake = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
    "pond.reset()\n",
    "lake.reset()\n",
    "\n",
    "for a in [0, 2, 2, 1, 1, 1, 1, 2]:\n",
    "    pond_obs, pond_rew, pond_done, _ = pond.step(a)\n",
    "    lake_obs, lake_rew, lake_done, _ = lake.step(a)\n",
    "    print(\"%2d/%2d    %d/%d    %5s/%5s\" % \\\n",
    "          (pond_obs, lake_obs, pond_rew, lake_rew, pond_done, lake_done))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87185e66-37b4-4337-9046-17a17cc6efb2",
   "metadata": {},
   "source": [
    "They look the same to me!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f49c72a-4e1d-442f-a6e9-97be703d52fa",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9714c27-63d5-4559-a544-71a888eab26a",
   "metadata": {},
   "source": [
    "EXERCISE:\n",
    "DIFFERENCE BETWEEN MAZE AND POND\n",
    "difference is whether episode ends when you walk into a wall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7774d3de-7512-4dc5-b914-382b0ee42ea5",
   "metadata": {},
   "source": [
    "exercise: why don't we have a negative reward when we fall?\n",
    "\n",
    "    Note that we don't explicitly penalize falling into the pond here; rather, the penalty is indirect in that the episode ends with a reward of zero, forfeiting the potential reward of 1 from reaching the goal successfully. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1512d61-1a46-4f42-8f19-fea5176b8fdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rl-course]",
   "language": "python",
   "name": "conda-env-rl-course-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
