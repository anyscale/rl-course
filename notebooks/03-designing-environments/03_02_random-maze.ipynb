{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e23df55-837b-4ec9-86ad-3b60c6a7f00d",
   "metadata": {},
   "source": [
    "## Random Maze Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b96688-5597-4709-bd71-552dfec70920",
   "metadata": {},
   "source": [
    "#### Learning the Maze\n",
    "\n",
    "- Previously we created `FrozenPond` and then a `Maze` variant.\n",
    "- Let's train an agent to complete the Maze using RLlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63d101fe-ebcf-43bb-859f-d07d3afc2093",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from envs import Maze # Maze defined in previous slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c31ea0c-5e7b-46ff-b312-c1b846d7b2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = PPOTrainer({\"framework\" : \"torch\", \"create_env_on_driver\" : True}, \n",
    "                     env=Maze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "548763bc-2399-4ac8-9936-2dba7b183467",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_info = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ee6979-e728-454d-bf34-35d1688c748f",
   "metadata": {},
   "source": [
    "#### Learning the Maze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7f6f10-c910-47e8-995c-aac0c33855a8",
   "metadata": {},
   "source": [
    "We see that the agent always receives the reward for finishing the maze:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9199aca4-0be1-47c7-b855-a6f36f9d18e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "print(trainer.evaluate()['evaluation']['hist_stats']['episode_reward'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2514b879-f410-41a1-a052-551851b9385f",
   "metadata": {},
   "source": [
    "But often takes much longer than the optimal number of steps, which is 6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ce5facf6-566a-4319-adc3-dc7e97bc8b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[74, 17, 37, 54, 39, 15, 29, 65, 73, 29, 39, 25, 45, 54, 14, 20, 51, 48, 20, 21, 32, 55, 62, 38, 11, 12, 45, 58, 58, 24, 65, 18, 51, 95, 83, 32, 20, 28, 22, 26, 12, 14, 60, 18, 41, 44, 7, 33, 30, 19, 33, 21, 68, 18]\n"
     ]
    }
   ],
   "source": [
    "print(trainer.evaluate()['evaluation']['hist_stats'][\"episode_lengths\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db523173-21d9-42a4-ab33-51d6f23a4d38",
   "metadata": {},
   "source": [
    "We can improve the agent by training for more iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d709ff68-c2dd-4cf8-acf2-1ed728b44569",
   "metadata": {},
   "source": [
    "#### Learning the Maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b06c345a-3490-4ec8-8ef2-5b311f7db938",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    train_info = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cdc6777d-e77d-4dbc-a9c1-5dbf75c06d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[52, 11, 13, 14, 11, 14, 8, 12, 9, 7, 13, 14, 7, 11, 14, 10, 9, 11, 12, 10, 9, 8, 17, 15, 10, 10, 8, 10, 9, 8, 9, 7, 8, 7, 9, 9, 16, 13, 6, 14, 9, 6, 10, 8, 10, 6, 9, 10, 18, 11, 14, 9, 11, 10, 11, 10, 8, 13, 14, 7, 11, 9, 15, 6, 6, 6, 9, 10, 6, 10, 6, 10, 13, 8, 8, 9, 14, 15, 7, 10, 7, 7, 14, 11, 12, 6, 13, 10, 8, 6, 12, 10, 6, 7, 8, 10, 6, 12, 14, 11, 6, 7, 9, 13, 6, 8, 9, 15, 9, 15, 6, 8, 6, 7, 8, 8, 12, 8, 13, 11, 7, 11, 10, 12, 7, 18, 10, 7, 14, 6, 6, 7, 8, 9, 6, 8, 9, 10, 10, 8, 7, 6, 9, 10, 10, 8, 14, 10, 9, 8, 9, 8, 7, 8, 13, 17, 8, 7, 10, 6, 7, 11, 9, 11, 8, 9, 13, 8, 7, 12, 16, 12, 8, 6, 11, 6, 9, 9, 18, 6, 8, 11, 17, 10, 7, 9, 12, 8, 6, 7, 15, 12, 6, 12, 8, 11, 8, 8, 8, 7, 8, 8, 8, 6, 8, 8, 6]\n"
     ]
    }
   ],
   "source": [
    "print(trainer.evaluate()['evaluation']['hist_stats'][\"episode_lengths\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f416155b-486e-4e4f-958f-103d119cc829",
   "metadata": {},
   "source": [
    "By the end, the number of steps is often less than 10. Progress."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e482d6-850a-4608-932a-ffec45bf52d5",
   "metadata": {},
   "source": [
    "#### Beyond the simple maze\n",
    "\n",
    "We can train an agent to learn this fixed maze:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b9ac3f06-a899-407c-82fe-217d48222aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P...\n",
      ".X.X\n",
      "...X\n",
      "X..G\n"
     ]
    }
   ],
   "source": [
    "maze = Maze()\n",
    "maze.reset()\n",
    "maze.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c23b95-7b31-40c8-819f-eb8f6a3872f8",
   "metadata": {},
   "source": [
    "But this is quite an easy problem:\n",
    "\n",
    "- Small state space\n",
    "- Small action space\n",
    "- No stochasticity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eaa3bd3-4291-4efd-b961-7fe41b71d463",
   "metadata": {},
   "source": [
    "#### Random maze\n",
    "\n",
    "- Let's make the problem harder by looking at a _random_ maze\n",
    "- That is, the wall locations change every episode.\n",
    "- We'll do this by reimplementing the `reset` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4fad2e84-ce76-4c93-a373-74ea955e2a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomMaze(Maze):\n",
    "    def reset(self):\n",
    "        self.player = (0, 0) # the player starts at the top-left\n",
    "        self.exit = (3, 3)   # exit is at the bottom-right\n",
    "        \n",
    "        self.walls = np.random.rand(4, 4) < 0.2\n",
    "        self.walls[self.player] = 0\n",
    "        self.walls[self.exit] = 0\n",
    "        \n",
    "        return 0 # the observation corresponding to (0,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04592876-bd6b-49be-9d0a-41b1ceb68ce0",
   "metadata": {},
   "source": [
    "Now, each square (except the start and end locations) is a wall with probability 20%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3036f7a-9d14-4a6e-916b-490eb9c78d6f",
   "metadata": {},
   "source": [
    "#### Impossible mazes\n",
    "\n",
    "- In this new setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da7c25c-27e8-4d33-995f-825f37e78d83",
   "metadata": {},
   "source": [
    "# IDEA\n",
    "\n",
    "- should we do random pond instead of random maze?\n",
    "- then we don't need to deal with impossible cases because the episode will always end??\n",
    "- ok wait a minute\n",
    "- actually the agent could always go back and forth forever if explore=False\n",
    "- i guess this isn't a concern for training.\n",
    "- ok but we do need some possibility of the done flag. I see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0d6d13a3-487e-4cb2-9b71-f9652953d1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action: 2\n",
      "....\n",
      ".X.X\n",
      "...X\n",
      "X..G\n",
      "episode length: 10\n"
     ]
    }
   ],
   "source": [
    "# from ipywidgets import Output\n",
    "# from IPython import display\n",
    "# import time\n",
    "\n",
    "# env = Maze()\n",
    "\n",
    "# out = Output();\n",
    "# display.display(out);\n",
    "# with out:\n",
    "#     obs = env.reset()\n",
    "#     env.seed(1)\n",
    "#     done = False\n",
    "#     episode_length = 0\n",
    "#     while not done:\n",
    "#         action = trainer.compute_single_action(obs)\n",
    "#         obs, rewards, done, _ = env.step(action)\n",
    "        \n",
    "#         out.clear_output(wait=True)\n",
    "#         print(\"action:\", action)\n",
    "#         env.render()\n",
    "#         time.sleep(0.5)\n",
    "#         episode_length += 1\n",
    "# print(\"episode length:\", episode_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dce3f1b-f371-4237-b2be-74c20f8a7e80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d9ea5a-51ac-4261-b486-ca4c89e77809",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "448b5835-5ec4-4393-bc3a-978d4d2209f1",
   "metadata": {},
   "source": [
    "Then, introduce random maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fdf181-408e-4b32-9f8f-2e8630fddc97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rl-course]",
   "language": "python",
   "name": "conda-env-rl-course-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
