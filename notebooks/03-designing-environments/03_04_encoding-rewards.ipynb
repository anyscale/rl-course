{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e23df55-837b-4ec9-86ad-3b60c6a7f00d",
   "metadata": {},
   "source": [
    "## Encoding Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28d9b458-b46e-48de-ab18-bf884c1eab85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import ray\n",
    "import logging\n",
    "ray.init(log_to_driver=False, ignore_reinit_error=True, logging_level=logging.ERROR); # logging.FATAL\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c8c796-3785-44d1-8aea-78872152b0cb",
   "metadata": {},
   "source": [
    "#### Encoding Rewards\n",
    "\n",
    "- We've now discussed the importance of encoding the observations.\n",
    "- We may also have some choice on the action space, though here (and often) it is relatively clear/fixed.\n",
    "- But what about the rewards? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e790dc-5146-4b03-a77a-24efd69ffabd",
   "metadata": {},
   "source": [
    "#### Current set-up\n",
    "\n",
    "- Currently, we get a reward of +1 for reaching the goal. \n",
    "- This is part of what makes RL so hard (and impressive):\n",
    "  - We want to learn about actions even though we don't know right away whether the action was \"good\". \n",
    "  - Contrast this with supervised learning, where every prediction we make on the training data can immediately be compared with the known target value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff6ac9f-6815-4910-b3ce-aa7e0866101c",
   "metadata": {},
   "source": [
    "#### Agents can't just be greedy\n",
    "\n",
    "- Can agents simply learn to go for the best immediate reward?\n",
    "- No. For example, in a video recommendation system, showing the user another funny cat video might make them click (high immediate reward) but result in long-term loss of interest in the service (low long-term reward).\n",
    "- Our Frozen Lake is another example of the problem here: sometimes there is no immediate reward at all to learn from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c44cea8c-a28e-4d7c-a0c8-91c1c3f8fc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: perhaps this next section on \"Learned action probabilities\" could be moved much earlier, even as early as Module 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462e6340-cb1f-4eda-b642-d21505d18b52",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Learned action probabilities\n",
    "\n",
    "Let's load the trained model with our encoded observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16d9d371-0c26-46a0-90c5-6f26e25880f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from envs import RandomLakeObs\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "ppo_config = (\n",
    "    PPOConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(create_env_on_local_worker=True, horizon=100)\\\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\n",
    ")\n",
    "ppo_RandomLakeObs = ppo_config.build(env=RandomLakeObs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1420e1fd-22e9-474c-aad6-928428976b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # HIDDEN\n",
    "\n",
    "# for i in range(8):\n",
    "#     ppo_RandomLakeObs.train()\n",
    "    \n",
    "# print(ppo_RandomLakeObs.evaluate()[\"evaluation\"][\"episode_reward_mean\"])\n",
    "\n",
    "# ppo_RandomLakeObs.save(\"models/RandomLakeObs-Ray2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a71119d3-e3c9-427c-a697-fe733fc10ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_RandomLakeObs.restore(\"models/RandomLakeObs-Ray2/checkpoint_000008\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fec9d02-7e22-44ee-b171-2d9bf86b8681",
   "metadata": {},
   "source": [
    "#### Learned action probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66648b5d-abd8-45ef-b592-e56bdee1c9c9",
   "metadata": {},
   "source": [
    "We'll use the `query_policy` function from Module 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84b56415-177b-414f-8985-1118cabec0b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02463268, 0.45315894, 0.49981508, 0.02239344], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import query_policy\n",
    "query_policy(ppo_RandomLakeObs, RandomLakeObs(), [0,0,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0256b6-3174-4719-931f-65342f5ae4a7",
   "metadata": {},
   "source": [
    "- Recall the (left, down, right, up) ordering.\n",
    "- When the observation is `[0 0 0 0]` (no holes or edges in sight), the agent prefers to go down and right."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99a889a-37bd-4a7c-bf6b-c9eb297a1520",
   "metadata": {},
   "source": [
    "What if there's a hole below you? We can feed in a different observation to the policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc66f33e-8b23-4bcc-9f12-04b47d18aea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03616549, 0.03877434, 0.9037791 , 0.02128115], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_policy(ppo_RandomLakeObs, RandomLakeObs(), [0,1,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6560c87a-c07e-4aec-a172-915cad2b376c",
   "metadata": {},
   "source": [
    "- Now the agent is very unlikely to go down, and very likely to go right!\n",
    "- Again, all this was learned from trial and error, with a reward earned only when the goal was reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffae4393-3058-40ce-821a-fe1e487cf4d4",
   "metadata": {},
   "source": [
    "#### Random Lake rewards\n",
    "\n",
    "- In the Random Lake example, can't be make life easier for the agent by giving immediate rewards?\n",
    "\n",
    "This is the current reward code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bb56001-259b-4b88-8bdd-29746e208042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward(self):\n",
    "    return int(self.player == self.goal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1ee605-1e87-4080-b8ee-274816dd67fd",
   "metadata": {},
   "source": [
    "- The agent has to learn, through trial and error over _entire episodes_, that moving down and right is generally a good thing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0cf72e-d732-4039-90cc-22a17360943f",
   "metadata": {},
   "source": [
    "#### Redefining rewards\n",
    "\n",
    "- Let's instead try giving a reward _at every step, that is higher as the agent gets closer to the goal_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9a0466b-44fd-43ed-a3c0-d93334748213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from envs import RandomLakeObs\n",
    "\n",
    "class RandomLakeObsRew(RandomLakeObs):\n",
    "    def reward(self):\n",
    "        return 6-(abs(self.player[0]-self.goal[0]) + abs(self.player[1]-self.goal[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce59aa5-1f90-457f-a3f9-c970cd8a7b7b",
   "metadata": {},
   "source": [
    "- The above method uses the [Manhattan Distance](https://en.wikipedia.org/wiki/Taxicab_geometry) between the player and the goal as the reward. \n",
    "- When the agent reaches the goal, the maximum reward of 6 is achieved.\n",
    "- When the agent is furthest from the goal, the minimum reward of 0 is given."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75f7b77-8962-43fe-a82b-647e4f2aee69",
   "metadata": {},
   "source": [
    "#### Redefining rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcba334f-f2a3-4cb0-8b8a-f5dd69a3346f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßëüßäüßäüßä\n",
      "üßäüßäüßäüßä\n",
      "üï≥üßäüï≥üßä\n",
      "üßäüßäüßä‚õ≥Ô∏è\n"
     ]
    }
   ],
   "source": [
    "env = RandomLakeObsRew()\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2944a38-4948-456e-ab34-4e8944234db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5208f2-c7e9-4f1e-9ed7-e91daec89d8e",
   "metadata": {},
   "source": [
    "‚¨ÜÔ∏è the reward is 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f70020a-1a73-4b76-99cd-6339e749198d",
   "metadata": {},
   "source": [
    "#### Redefining rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "088715f0-3f06-4231-a149-401b6518dcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßäüßäüßäüßä\n",
      "üßëüßäüßäüßä\n",
      "üï≥üßäüï≥üßä\n",
      "üßäüßäüßä‚õ≥Ô∏è\n"
     ]
    }
   ],
   "source": [
    "obs, rew, done, _ = env.step(1)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4948b680-c7ee-439c-a638-a5f4ddade218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4b6a57-2f96-4595-9a6b-052ec004aa66",
   "metadata": {},
   "source": [
    "‚¨ÜÔ∏è the reward is 1 because we moved closer to the goal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e607ab59-e697-44b6-99a6-91a7a3a3d2b2",
   "metadata": {},
   "source": [
    "#### Redefining rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95ab62b7-e2b5-4f6e-8f81-9381b605a9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßäüßäüßäüßä\n",
      "üßäüßäüßäüßä\n",
      "üï≥üßäüï≥üßë\n",
      "üßäüßäüßä‚õ≥Ô∏è\n"
     ]
    }
   ],
   "source": [
    "obs, rew, done, _ = env.step(2)\n",
    "obs, rew, done, _ = env.step(2)\n",
    "obs, rew, done, _ = env.step(2)\n",
    "obs, rew, done, _ = env.step(1)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84e00ba0-f659-4822-a369-64ca95e6c321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1a183d-af4c-4c9d-a6d3-4bc1a7bfecca",
   "metadata": {},
   "source": [
    "Now, the reward is 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0bdde7-dd19-4434-ad6e-159bf90ea34b",
   "metadata": {},
   "source": [
    "#### Redefining rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "904e40cb-55b6-4b45-b063-5b335123e071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßäüßäüßäüßä\n",
      "üßäüßäüßäüßä\n",
      "üï≥üßäüï≥üßä\n",
      "üßäüßäüßäüßë\n"
     ]
    }
   ],
   "source": [
    "obs, rew, done, _ = env.step(1)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57574a01-5630-487b-acd6-ccdf60319453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e5e1cc-0907-42b9-87f9-03a12124e089",
   "metadata": {},
   "source": [
    "Now, the reward is 6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ac0aec-ddc4-4d44-833e-c2b2e253120f",
   "metadata": {},
   "source": [
    "#### Comparing rewards\n",
    "\n",
    "- So, we have two possible reward functions. Which one works better? \n",
    "- Recall that last time, after training for 8 iterations, we were able to reach the goal around 70% of the time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5dbcf34-37e8-4025-bcd2-7a308d77046f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7247706422018348"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_RandomLakeObs.evaluate()['evaluation']['episode_reward_mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c75b369-6e96-4192-b7cd-4216f354d901",
   "metadata": {},
   "source": [
    "#### Comparing rewards\n",
    "\n",
    "Let's train with the new reward function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01c1afa7-4452-43e4-ad2d-e3dc33934fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_RandomLakeObsRew = ppo_config.build(env=RandomLakeObsRew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7478ee6-28d8-4e44-932f-1b4f47e6b911",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    ppo_RandomLakeObsRew.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "257cef04-752e-4fe1-9eb6-3e225ea1ba62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101.90566037735849"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_RandomLakeObsRew.evaluate()['evaluation']['episode_reward_mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bccc6f4-1e9a-4abd-af7a-08b4c6103e4b",
   "metadata": {},
   "source": [
    "Wait a minute, what's going on here??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874a6979-1882-4bb4-a5cf-a83282ed15f3",
   "metadata": {},
   "source": [
    "#### Comparing rewards?\n",
    "\n",
    "- We tried to improve our RL system by shaping the reward function.\n",
    "- This (presumably) affected training, but it also affected our evaluation.\n",
    "- In supervised learning, this is like changed the scoring metric from squared error to absolute error.\n",
    "- If the old system got a mean squared error of 20,000 and the new system got a mean absolute error of 40, which is better?\n",
    "- We're comparing apples and oranges here!\n",
    "- We want to compare both models on the same metric, for example the original metric. \n",
    "- Here, we want to see how frequently the agent reaches the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf05802-842d-4764-b408-72559e7bb8cf",
   "metadata": {},
   "source": [
    "#### Comparing rewards?\n",
    "\n",
    "- The code here is a bit more advanced.\n",
    "- It is included for completeness, but we won't go into detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "261074ea-c6db-4e1b-a35f-a647e8cf2776",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.agents.callbacks import DefaultCallbacks\n",
    "\n",
    "class MyCallbacks(DefaultCallbacks):\n",
    "    def on_episode_end(self, *, worker, base_env, policies, episode, env_index, **kwargs):\n",
    "        info = episode.last_info_for()\n",
    "        episode.custom_metrics[\"goal_reached\"] = info[\"player\"] == info[\"goal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3ab7223-77b0-46f0-af5f-669b18760a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_config_callback = (\n",
    "    PPOConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(create_env_on_local_worker=True, horizon=100)\\\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\\\n",
    "    .callbacks(callbacks_class=MyCallbacks)\\\n",
    "    .evaluation(evaluation_config={\"callbacks\" : MyCallbacks})\n",
    ")\n",
    "\n",
    "ppo_RandomLakeObsRew = ppo_config_callback.build(env=RandomLakeObsRew)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90980e8e-2ef2-479d-9c55-cbde6fcdebe1",
   "metadata": {},
   "source": [
    "The trainer above uses our new reward scheme but also reports/measures the rate of reaching the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f898830-4600-4ddf-8f05-073f7cb8e842",
   "metadata": {},
   "source": [
    "#### Comparing rewards?\n",
    "\n",
    "Let's try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f118473-544e-43a6-9ead-f742d0ee8fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    ppo_RandomLakeObsRew.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f52dd086-12bb-4482-88a2-30df5240573b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101.90566037735849"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HIDDEN\n",
    "ppo_RandomLakeObsRew.evaluate()[\"evaluation\"][\"episode_reward_mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0021f054-0f77-46c8-ac1c-f6e1f40ff509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04081632653061224"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_RandomLakeObsRew.evaluate()[\"evaluation\"][\"custom_metrics\"][\"goal_reached_mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36e3784-b860-49a2-b3d1-b48a760e756c",
   "metadata": {},
   "source": [
    "- Hmm, these results are terrible!\n",
    "- We used to get a 70%+ win rate, and now we're close to zero.\n",
    "- What happened? ü§î"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398ee5c9-7e5c-4370-912c-b90d8864cac4",
   "metadata": {},
   "source": [
    "#### What is the agent really optimizing?\n",
    "\n",
    "- The agent is really optimizing _discounted total reward_.\n",
    "- _Total_: it values all the rewards it collects, not just the final reward.\n",
    "- _Discounted_: it values earlier rewards more than later ones.\n",
    "- Our agent is successfully maximizing discounted total reward, but this isn't corresponding to reaching the goal.\n",
    "- But why? The goal gives a higher reward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cee4522-4906-4a36-b5e8-714761a11b04",
   "metadata": {},
   "source": [
    "#### Exploration vs. exploitation\n",
    "\n",
    "- A fundamental concept in RL is _exploration vs. exploitation_\n",
    "- When the agent is learning the policy, it can choose to either:\n",
    "\n",
    "1. Do things that it knows are pretty good (\"exploit\")\n",
    "2. Try something totally new and crazy, just in case (\"explore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f02fef1-3506-4c10-ac66-cd4150b32d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO \n",
    "# diagram for this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954a891d-f16c-43bb-b538-ce246829d6fb",
   "metadata": {},
   "source": [
    "#### Exploration vs. exploitation\n",
    "\n",
    "- With the old reward structure, the agent gets a reward of 0 unless it reaches the goal.\n",
    "  - So, it keeps trying to find something better.\n",
    "- With the new reward structure, the agent is getting lots of reward just for walking around.\n",
    "  - It isn't very motivated to explore the environment.\n",
    "- In fact, because it is maximizing discounted **total** reward, finding the goal is a bad thing!\n",
    "  - This causes the episode to end, limiting the total reward of the agent.\n",
    "  - The agent actually learns to _avoid_ the goal, especially early on in the episode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83b297e-b4a6-4989-b623-5f53b52562c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Designing a better reward structure\n",
    "\n",
    "- Instead, let's try penalizing the agent when it walks into a hole or off the edge.\n",
    "- It will be easier to implement this directly in `step`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c41fd832-6144-41cc-9643-e7f3eb8737d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomLakeObsRew2(RandomLakeObs):\n",
    "    def step(self, action):\n",
    "        # (not shown) existing code gets new_loc, where the player is trying to go\n",
    "        \n",
    "        reward = 0\n",
    "        \n",
    "        if self.is_valid_loc(new_loc):\n",
    "            self.player = new_loc\n",
    "        else:\n",
    "            reward -= 0.1 # small penalty\n",
    "            \n",
    "        if self.holes[self.player]:\n",
    "            reward -= 0.1 # small penalty\n",
    "            \n",
    "        if self.player == self.goal:\n",
    "            reward += 1\n",
    "        \n",
    "        # Return observation/reward/done\n",
    "        return self.observation(), reward, self.done(), {\"player\" : self.player, \"goal\" : self.goal}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5622237f-517f-4a1c-99f0-0e7a0ede3e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "from envs import RandomLakeObsRew2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804b515a-f635-4f48-8e0e-7774b003533a",
   "metadata": {},
   "source": [
    "#### Testing it out, again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c20f637-e0f8-4ed5-8133-d49d59d504f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "# redefine ppo_RandomLakeObs to include the new callbacks\n",
    "# so that you can measure the custom metric instead of the reward\n",
    "# they will give the same value but this is better for consistency\n",
    "ppo_RandomLakeObs = ppo_config_callback.build(env=RandomLakeObs)\n",
    "\n",
    "for i in range(8):\n",
    "    ppo_RandomLakeObs.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7459cba1-5771-4d26-b215-4f51df7a7ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_RandomLakeObsRew2 = ppo_config_callback.build(env=RandomLakeObsRew2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dbd1ac25-6d5c-4ed9-a49c-c9564b92f08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    ppo_RandomLakeObsRew2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cdbb4747-9b58-487b-9353-1a1e8a2022c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6853448275862069"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_RandomLakeObs.evaluate()[\"evaluation\"][\"custom_metrics\"][\"goal_reached_mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8e32e1de-b153-4fef-a208-a0d574d547c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.734982332155477"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_RandomLakeObsRew2.evaluate()[\"evaluation\"][\"custom_metrics\"][\"goal_reached_mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8ae49b-73af-43b4-92fa-cd3f608b504d",
   "metadata": {},
   "source": [
    "It looks like, this time, the two methods perform much more similarly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3481f6-4505-48f6-a787-33146ff663af",
   "metadata": {},
   "source": [
    "#### Episode length\n",
    "\n",
    "- In addition to the success rate, we can compute other statistics of the agent's behavior.\n",
    "- One interesting measure is episode length.\n",
    "- RLlib records this by default, so we can easily access it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e8819cf4-bc66-4ce7-a723-ea6127d4c1cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.585470085470085"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_RandomLakeObs.evaluate()[\"evaluation\"][\"episode_len_mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "879d945f-57e6-409e-8f8a-e0d95e6cfe4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.20216606498195"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_RandomLakeObsRew2.evaluate()[\"evaluation\"][\"episode_len_mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de01c11-01b5-4f86-9f9e-9e2e43a3d62d",
   "metadata": {},
   "source": [
    "Although the two agents have the same success rate, the new one tends toward shorter episodes.\n",
    "\n",
    "Notes: \n",
    "\n",
    "- This is quite interesting because the agent cannot \"see\" the difference between holes and edges.\n",
    "- We could explore this further by adding more custom metrics, e.g. number of bumps into the edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3cd02d93-0255-4ae2-aa46-5a9eb5111fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "#### disadvantages - loss of generality\n",
    "\n",
    "#- now only works if goal is at bottom-right\n",
    "#give a few real-world examples here -> important"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cfb329-ec76-4145-bafa-b53676fb054e",
   "metadata": {},
   "source": [
    "#### Let's apply what we learned!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf32143e-7c32-49d2-9aed-f1081c5b8c35",
   "metadata": {},
   "source": [
    "## Supervised learning analogy: reward shaping\n",
    "<!-- multiple choice -->\n",
    "\n",
    "Earlier, we made an analogy between encoding observations in RL and feature preprocessing in supervised learning. What aspect of supervised learning is the best analogy to reward shaping in RL?\n",
    "\n",
    "- [ ] Feature engineering \n",
    "- [ ] Model selection | Not quite. But, as we'll see, there is a place for model selection in RL as well!\n",
    "- [ ] Hyperparameter tuning | Not quite. But, as we'll see, there is a place for hyperparameter tuning in RL as well!\n",
    "- [x] Selecting a loss function | Changing the loss function changes the \"best\" model, just like changing the rewards changes the \"best\" policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c458b5c7-bc0a-4084-8572-d35bf27ddc60",
   "metadata": {},
   "source": [
    "## Rewarding every step: small negative rewards\n",
    "<!-- multiple choice -->\n",
    "\n",
    "In RL environments like Random Lake where the agent needs to reach a specific goal, imagine we assigned a tiny negative reward for _every_ step taken by the agent. How would this generally/typically affect the amount of time the agent spends until it reaches the goal?\n",
    "\n",
    "- [x] The agent will try to reach the goal in as few steps as possible.\n",
    "- [ ] The agent will try to reach the goal in as many steps as possible. | If we're penalizing each step, taking more steps will result in less reward.\n",
    "- [ ] No change. | If we're penalizing each step, taking more steps will result in less reward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96265e6-b392-4923-b694-088fb282efb0",
   "metadata": {},
   "source": [
    "## Exploration vs. exploitation\n",
    "<!-- multiple choice -->\n",
    "\n",
    "Which of the following is a correct statement about the exploration-exploitation tradeoff in RL?\n",
    "\n",
    "- [ ] If an only explores, it will never find a good policy. | It will find good policies in fact, just EXTREMELY slowly.\n",
    "- [x] If an agent only exploits, it will never find a good policy. | It may just keep trying the same thing over and over again.\n",
    "- [ ] Agents always find good policies even without exploration/exploitation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93d7454-eaff-4c79-8398-19cb541b4b37",
   "metadata": {},
   "source": [
    "## Unintended consequences\n",
    "<!-- coding exercise -->\n",
    "\n",
    "In this exercise, you will try out a bad idea: assigning a large negative reward every time the agent takes a step. We will use -1 per step. The agent still gets a reward of +1 for reaching the goal. Implement this reward, train the agent, and look at the average episode length printed out by the code. Compare this to the average episode length of an agent that just acts randomly. Then, answer the multiple choice question about the agent's behavior. What do you think is going on here? \n",
    "\n",
    "(FYI: as discussed previously, this type of change to an environment can also be achieved with gym wrappers.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "86f8434a-7575-4bfb-af7b-9858f580ae7b",
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '____' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [38]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         old_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplayer \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgoal) \n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ____\n\u001b[0;32m---> 10\u001b[0m ppo \u001b[38;5;241m=\u001b[39m lake_default_config\u001b[38;5;241m.\u001b[39mbuild(env\u001b[38;5;241m=\u001b[39m\u001b[43m____\u001b[49m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m8\u001b[39m):\n\u001b[1;32m     13\u001b[0m     ppo\u001b[38;5;241m.\u001b[39mtrain()\n",
      "\u001b[0;31mNameError\u001b[0m: name '____' is not defined"
     ]
    }
   ],
   "source": [
    "# EXERCISE\n",
    "from utils import lake_default_config\n",
    "from envs import RandomLakeObs\n",
    "\n",
    "class RandomLakeBadIdea(RandomLakeObs):\n",
    "    def reward(self):\n",
    "        old_reward = int(self.player == self.goal) \n",
    "        return ____\n",
    "    \n",
    "ppo = lake_default_config.build(env=____)\n",
    "\n",
    "for i in range(8):\n",
    "    print(i)\n",
    "    ppo.train()\n",
    "    \n",
    "print(\"Average episode length for trained agent: %.1f\" % \n",
    "      ppo.evaluate()[\"evaluation\"][____])\n",
    "\n",
    "random_agent_config = (\n",
    "    lake_default_config\\\n",
    "    .exploration(exploration_config={\"type\": \"Random\"})\\\n",
    "    .evaluation(evaluation_config={\"explore\" : True})\n",
    ")\n",
    "random_agent = random_agent_config.build(env=RandomLakeBadIdea)\n",
    "\n",
    "print(\"Average episode length for random agent: %.1f\" % \n",
    "      random_agent.evaluate()[\"evaluation\"][____])\n",
    "\n",
    "ppo.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "265575eb-94ef-4899-b7c0-c00c70c155e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-03 15:42:39,372\tINFO worker.py:1518 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average episode length for trained agent: 4.4\n",
      "Average episode length for random agent: 13.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "*** SIGTERM received at time=1667515560 ***\n",
      "PC: @        0x1b635f73c  (unknown)  __execve\n",
      "    @        0x12e418298  (unknown)  absl::lts_20211102::WriteFailureInfo()\n",
      "    @        0x12e417fe4  (unknown)  absl::lts_20211102::AbslFailureSignalHandler()\n",
      "    @        0x1b63874a4  (unknown)  _sigtramp\n",
      "    @        0x1b627fc94  (unknown)  execv\n",
      "    @        0x10147f070  (unknown)  child_exec\n",
      "    @        0x10147eb20  (unknown)  subprocess_fork_exec\n",
      "    @        0x100c3d09c  (unknown)  cfunction_call\n",
      "    @        0x100bed080  (unknown)  _PyObject_MakeTpCall\n",
      "    @        0x100ce2748  (unknown)  call_function\n",
      "    @        0x100cdeef0  (unknown)  _PyEval_EvalFrameDefault\n",
      "    @        0x100cd8008  (unknown)  _PyEval_EvalCode\n",
      "    @        0x100bedc48  (unknown)  _PyFunction_Vectorcall\n",
      "    @        0x100ce2620  (unknown)  call_function\n",
      "    @        0x100cdeecc  (unknown)  _PyEval_EvalFrameDefault\n",
      "    @        0x100cd8008  (unknown)  _PyEval_EvalCode\n",
      "    @        0x100bedc48  (unknown)  _PyFunction_Vectorcall\n",
      "    @        0x100bed28c  (unknown)  _PyObject_FastCallDictTstate\n",
      "    @        0x100bee08c  (unknown)  _PyObject_Call_Prepend\n",
      "    @        0x100c60dc8  (unknown)  slot_tp_init\n",
      "    @        0x100c6a8a0  (unknown)  type_call\n",
      "    @        0x100bed080  (unknown)  _PyObject_MakeTpCall\n",
      "    @        0x100ce2748  (unknown)  call_function\n",
      "    @        0x100cdefe8  (unknown)  _PyEval_EvalFrameDefault\n",
      "    @        0x100bedcc8  (unknown)  function_code_fastcall\n",
      "    @        0x100ce2620  (unknown)  call_function\n",
      "    @        0x100cdeef0  (unknown)  _PyEval_EvalFrameDefault\n",
      "    @        0x100bedcc8  (unknown)  function_code_fastcall\n",
      "    @        0x100ce2620  (unknown)  call_function\n",
      "    @        0x100cdeecc  (unknown)  _PyEval_EvalFrameDefault\n",
      "    @        0x100bedcc8  (unknown)  function_code_fastcall\n",
      "    @        0x100ce2620  (unknown)  call_function\n",
      "    @        0x100cdeecc  (unknown)  _PyEval_EvalFrameDefault\n",
      "    @ ... and at least 9 more frames\n",
      "[2022-11-03 15:46:00,886 E 5860 44629] logging.cc:361: *** SIGTERM received at time=1667515560 ***\n",
      "[2022-11-03 15:46:00,886 E 5860 44629] logging.cc:361: PC: @        0x1b635f73c  (unknown)  __execve\n",
      "[2022-11-03 15:46:00,886 E 5860 44629] logging.cc:361:     @        0x12e418298  (unknown)  absl::lts_20211102::WriteFailureInfo()\n",
      "[2022-11-03 15:46:00,886 E 5860 44629] logging.cc:361:     @        0x12e417ffc  (unknown)  absl::lts_20211102::AbslFailureSignalHandler()\n",
      "[2022-11-03 15:46:00,886 E 5860 44629] logging.cc:361:     @        0x1b63874a4  (unknown)  _sigtramp\n",
      "[2022-11-03 15:46:00,886 E 5860 44629] logging.cc:361:     @        0x1b627fc94  (unknown)  execv\n",
      "[2022-11-03 15:46:00,886 E 5860 44629] logging.cc:361:     @        0x10147f070  (unknown)  child_exec\n",
      "[2022-11-03 15:46:00,886 E 5860 44629] logging.cc:361:     @        0x10147eb20  (unknown)  subprocess_fork_exec\n",
      "[2022-11-03 15:46:00,886 E 5860 44629] logging.cc:361:     @        0x100c3d09c  (unknown)  cfunction_call\n",
      "[2022-11-03 15:46:00,886 E 5860 44629] logging.cc:361:     @        0x100bed080  (unknown)  _PyObject_MakeTpCall\n",
      "[2022-11-03 15:46:00,886 E 5860 44629] logging.cc:361:     @        0x100ce2748  (unknown)  call_function\n",
      "[2022-11-03 15:46:00,886 E 5860 44629] logging.cc:361:     @        0x100cdeef0  (unknown)  _PyEval_EvalFrameDefault\n",
      "[2022-11-03 15:46:00,886 E 5860 44629] logging.cc:361:     @        0x100cd8008  (unknown)  _PyEval_EvalCode\n",
      "[2022-11-03 15:46:00,886 E 5860 44629] logging.cc:361:     @        0x100bedc48  (unknown)  _PyFunction_Vectorcall\n",
      "[2022-11-03 15:46:00,886 E 5860 44629] logging.cc:361:     @        0x100ce2620  (unknown)  call_function\n",
      "[2022-11-03 15:46:00,886 E 5860 44629] logging.cc:361:     @        0x100cdeecc  (unknown)  _PyEval_EvalFrameDefault\n",
      "[2022-11-03 15:46:00,886 E 5860 44629] logging.cc:361:     @        0x100cd8008  (unknown)  _PyEval_EvalCode\n",
      "[2022-11-03 15:46:00,886 E 5860 44629] logging.cc:361:     @        0x100bedc48  (unknown)  _PyFunction_Vectorcall\n",
      "[2022-11-03 15:46:00,886 E 5860 44629] logging.cc:361:     @        0x100bed28c  (unknown)  _PyObject_FastCallDictTstate\n",
      "[2022-11-03 15:46:00,886 E 5860 44629] logging.cc:361:     @        0x100bee08c  (unknown)  _PyObject_Call_Prepend\n",
      "[2022-11-03 15:46:00,886 E 5860 44629] logging.cc:361:     @        0x100c60dc8  (unknown)  slot_tp_init\n",
      "[2022-11-03 15:46:00,886 E 5860 44629] logging.cc:361:     @        0x100c6a8a0  (unknown)  type_call\n",
      "[2022-11-03 15:46:00,886 E 5860 44629] logging.cc:361:     @        0x100bed080  (unknown)  _PyObject_MakeTpCall\n",
      "[2022-11-03 15:46:00,886 E 5860 44629] logging.cc:361:     @        0x100ce2748  (unknown)  call_function\n",
      "[2022-11-03 15:46:00,886 E 5860 44629] logging.cc:361:     @        0x100cdefe8  (unknown)  _PyEval_EvalFrameDefault\n",
      "[2022-11-03 15:46:00,886 E 5860 44629] logging.cc:361:     @        0x100bedcc8  (unknown)  function_code_fastcall\n",
      "[2022-11-03 15:46:00,886 E 5860 44629] logging.cc:361:     @        0x100ce2620  (unknown)  call_function\n",
      "[2022-11-03 15:46:00,886 E 5860 44629] logging.cc:361:     @        0x100cdeef0  (unknown)  _PyEval_EvalFrameDefault\n",
      "[2022-11-03 15:46:00,886 E 5860 44629] logging.cc:361:     @        0x100bedcc8  (unknown)  function_code_fastcall\n",
      "[2022-11-03 15:46:00,886 E 5860 44629] logging.cc:361:     @        0x100ce2620  (unknown)  call_function\n",
      "[2022-11-03 15:46:00,886 E 5860 44629] logging.cc:361:     @        0x100cdeecc  (unknown)  _PyEval_EvalFrameDefault\n",
      "[2022-11-03 15:46:00,886 E 5860 44629] logging.cc:361:     @        0x100bedcc8  (unknown)  function_code_fastcall\n",
      "[2022-11-03 15:46:00,886 E 5860 44629] logging.cc:361:     @        0x100ce2620  (unknown)  call_function\n",
      "[2022-11-03 15:46:00,887 E 5860 44629] logging.cc:361:     @        0x100cdeecc  (unknown)  _PyEval_EvalFrameDefault\n",
      "[2022-11-03 15:46:00,887 E 5860 44629] logging.cc:361:     @ ... and at least 9 more frames\n",
      "*** SIGTERM received at time=1667515562 ***\n",
      "PC: @        0x1b6338c20  (unknown)  kevent\n",
      "[2022-11-03 15:46:02,749 E 2910 14297] logging.cc:361: *** SIGTERM received at time=1667515562 ***\n",
      "[2022-11-03 15:46:02,749 E 2910 14297] logging.cc:361: PC: @        0x1b6338c20  (unknown)  kevent\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "from utils import lake_default_config\n",
    "from envs import RandomLakeObs\n",
    "\n",
    "class RandomLakeBadIdea(RandomLakeObs):\n",
    "    def reward(self):\n",
    "        old_reward = int(self.player == self.goal) \n",
    "        return old_reward - 1\n",
    "\n",
    "ppo = lake_default_config.build(env=RandomLakeBadIdea)\n",
    "\n",
    "\n",
    "for i in range(8):\n",
    "    print(i)\n",
    "    ppo.train()\n",
    "    \n",
    "print(\"Average episode length for trained agent: %.1f\" % \n",
    "      ppo.evaluate()[\"evaluation\"][\"episode_len_mean\"])\n",
    "\n",
    "random_agent_config = (\n",
    "    lake_default_config\\\n",
    "    .exploration(exploration_config={\"type\": \"Random\"})\\\n",
    "    .evaluation(evaluation_config={\"explore\" : True})\n",
    ")\n",
    "random_agent = random_agent_config.build(env=RandomLakeBadIdea)\n",
    "\n",
    "print(\"Average episode length for random agent: %.1f\" % \n",
    "      random_agent.evaluate()[\"evaluation\"][\"episode_len_mean\"])\n",
    "\n",
    "ppo.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dbab96-1590-409b-b684-d6a05c880c1c",
   "metadata": {},
   "source": [
    "#### Agent's behavior\n",
    "\n",
    "When trained on an environment with a large negative reward at every step, what do you think this agent is doing, that is undesirable?\n",
    "\n",
    "- [ ] The agent stays still because it is be discouraged from moving. | Try again!\n",
    "- [ ] The agent is not interested in reaching the goal because the reward is comparatively small. | Try again!\n",
    "- [x] The agent learns to jump into the lake as fast as it can, to avoid the negative reward of moving. | Yikes! ü•∂\n",
    "- [ ] The agent reaches the goal right away. | That would be desirable though!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ray200]",
   "language": "python",
   "name": "conda-env-ray200-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
