{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e23df55-837b-4ec9-86ad-3b60c6a7f00d",
   "metadata": {},
   "source": [
    "## Encoding Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28d9b458-b46e-48de-ab18-bf884c1eab85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import ray\n",
    "import logging\n",
    "ray.init(log_to_driver=False, ignore_reinit_error=True, logging_level=logging.ERROR); # logging.FATAL\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c8c796-3785-44d1-8aea-78872152b0cb",
   "metadata": {},
   "source": [
    "#### Encoding Rewards\n",
    "\n",
    "- We've now discussed the importance of encoding the observations.\n",
    "- We may also have some choice on the action space, though here (and often) it is relatively clear/fixed.\n",
    "- But what about the rewards? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e790dc-5146-4b03-a77a-24efd69ffabd",
   "metadata": {},
   "source": [
    "#### Current set-up\n",
    "\n",
    "- Currently, we get a reward of +1 for reaching the goal. \n",
    "- This is part of what makes RL so hard (and impressive):\n",
    "  - We want to learn about actions even though we don't know right away whether the action was \"good\". \n",
    "  - Contrast this with supervised learning, where every prediction we make on the training data can immediately be compared with the known target value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c10944be-9746-44d6-bb2b-66843ea7e4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: perhaps this next slide can be moved to Module 1, since it's very general?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff6ac9f-6815-4910-b3ce-aa7e0866101c",
   "metadata": {},
   "source": [
    "#### Agents can't just be greedy\n",
    "\n",
    "- Can agents simply learn to go for the best immediate reward?\n",
    "- No. For example, in a video recommendation system, showing the user another funny cat video might make them click (high immediate reward) but result in long-term loss of interest in the service (low long-term reward).\n",
    "- Our Frozen Lake is another example of the problem here: sometimes there is no immediate reward at all to learn from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c44cea8c-a28e-4d7c-a0c8-91c1c3f8fc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: perhaps this next section on \"Learned action probabilities\" could be moved much earlier, even as early as Module 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462e6340-cb1f-4eda-b642-d21505d18b52",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Learned action probabilities\n",
    "\n",
    "- RLlib lets us look inside the model at the probability of each action given an observation (i.e., the learned policy).\n",
    "- Let's load the trained model with our encoded observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16d9d371-0c26-46a0-90c5-6f26e25880f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from envs import RandomLakeObs\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "ppo_config = (\n",
    "    PPOConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(create_env_on_local_worker=True, horizon=100)\\\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\n",
    ")\n",
    "ppo_RandomLakeObs = ppo_config.build(env=RandomLakeObs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1420e1fd-22e9-474c-aad6-928428976b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # HIDDEN\n",
    "\n",
    "# for i in range(8):\n",
    "#     ppo_RandomLakeObs.train()\n",
    "    \n",
    "# print(ppo_RandomLakeObs.evaluate()[\"evaluation\"][\"episode_reward_mean\"])\n",
    "\n",
    "# ppo_RandomLakeObs.save(\"models/RandomLakeObs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a71119d3-e3c9-427c-a697-fe733fc10ce8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mppo_RandomLakeObs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/RandomLakeObs/checkpoint-16\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/tune/trainable/trainable.py:585\u001b[0m, in \u001b[0;36mTrainable.restore\u001b[0;34m(self, checkpoint_path, checkpoint_node_ip)\u001b[0m\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_checkpoint(checkpoint_dict)\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 585\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time_since_restore \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timesteps_since_restore \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/rllib/algorithms/algorithm.py:1444\u001b[0m, in \u001b[0;36mAlgorithm.load_checkpoint\u001b[0;34m(self, checkpoint_path)\u001b[0m\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;129m@override\u001b[39m(Trainable)\n\u001b[1;32m   1442\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_checkpoint\u001b[39m(\u001b[38;5;28mself\u001b[39m, checkpoint_path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1443\u001b[0m     extra_data \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(checkpoint_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 1444\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__setstate__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextra_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/rllib/algorithms/algorithm.py:2199\u001b[0m, in \u001b[0;36mAlgorithm.__setstate__\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m   2197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__setstate__\u001b[39m(\u001b[38;5;28mself\u001b[39m, state: \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m   2198\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworkers\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworker\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m state:\n\u001b[0;32m-> 2199\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_worker\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mworker\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2200\u001b[0m         remote_state \u001b[38;5;241m=\u001b[39m ray\u001b[38;5;241m.\u001b[39mput(state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworker\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   2201\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers\u001b[38;5;241m.\u001b[39mremote_workers():\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/rllib/evaluation/rollout_worker.py:1575\u001b[0m, in \u001b[0;36mRolloutWorker.restore\u001b[0;34m(self, objs)\u001b[0m\n\u001b[1;32m   1567\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_policy(\n\u001b[1;32m   1568\u001b[0m             policy_id\u001b[38;5;241m=\u001b[39mpid,\n\u001b[1;32m   1569\u001b[0m             policy_cls\u001b[38;5;241m=\u001b[39mpol_spec\u001b[38;5;241m.\u001b[39mpolicy_class,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1572\u001b[0m             config\u001b[38;5;241m=\u001b[39mpol_spec\u001b[38;5;241m.\u001b[39mconfig,\n\u001b[1;32m   1573\u001b[0m         )\n\u001b[1;32m   1574\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1575\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpid\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/rllib/policy/torch_mixins.py:113\u001b[0m, in \u001b[0;36mKLCoeffMixin.set_state\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkl_coeff \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurrent_kl_coeff\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkl_coeff\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Call super's set_state with rest of the state dict.\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/rllib/policy/torch_policy_v2.py:914\u001b[0m, in \u001b[0;36mTorchPolicyV2.set_state\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    912\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(optimizer_vars) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizers)\n\u001b[1;32m    913\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m o, s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizers, optimizer_vars):\n\u001b[0;32m--> 914\u001b[0m         optim_state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_torch_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    915\u001b[0m         o\u001b[38;5;241m.\u001b[39mload_state_dict(optim_state_dict)\n\u001b[1;32m    916\u001b[0m \u001b[38;5;66;03m# Set exploration's state.\u001b[39;00m\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/rllib/utils/torch_utils.py:178\u001b[0m, in \u001b[0;36mconvert_to_torch_tensor\u001b[0;34m(x, device)\u001b[0m\n\u001b[1;32m    175\u001b[0m         tensor \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ray2beta/lib/python3.9/site-packages/tree/__init__.py:430\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structures, **kwargs)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m structures[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m    428\u001b[0m   assert_same_structure(structures[\u001b[38;5;241m0\u001b[39m], other, check_types\u001b[38;5;241m=\u001b[39mcheck_types)\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unflatten_as(structures[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m--> 430\u001b[0m                     [func(\u001b[38;5;241m*\u001b[39margs) \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(flatten, structures))])\n",
      "File \u001b[0;32m~/miniconda3/envs/ray2beta/lib/python3.9/site-packages/tree/__init__.py:430\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m structures[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m    428\u001b[0m   assert_same_structure(structures[\u001b[38;5;241m0\u001b[39m], other, check_types\u001b[38;5;241m=\u001b[39mcheck_types)\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unflatten_as(structures[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m--> 430\u001b[0m                     [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(flatten, structures))])\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/rllib/utils/torch_utils.py:172\u001b[0m, in \u001b[0;36mconvert_to_torch_tensor.<locals>.mapping\u001b[0;34m(item)\u001b[0m\n\u001b[1;32m    169\u001b[0m         tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(item)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# Everything else: Convert to numpy, then wrap as torch tensor.\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# Floatify all float64 tensors.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mdouble:\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "ppo_RandomLakeObs.restore(\"models/RandomLakeObs/checkpoint-16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fec9d02-7e22-44ee-b171-2d9bf86b8681",
   "metadata": {},
   "source": [
    "#### Learned action probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66648b5d-abd8-45ef-b592-e56bdee1c9c9",
   "metadata": {},
   "source": [
    "The exact RLlib code is hidden inside `utils` for now, but we're mainly interested in the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84b56415-177b-414f-8985-1118cabec0b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02463268, 0.45315894, 0.49981508, 0.02239344], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import query_policy\n",
    "query_policy(ppo_RandomLakeObs, RandomLakeObs(), [0,0,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0256b6-3174-4719-931f-65342f5ae4a7",
   "metadata": {},
   "source": [
    "- Recall the (left, down, right, up) ordering.\n",
    "- When the observation is `[0 0 0 0]` (no holes or edges in sight), the agent prefers to go down and right."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99a889a-37bd-4a7c-bf6b-c9eb297a1520",
   "metadata": {},
   "source": [
    "What if there's a hole below you? We can feed in a different observation to the policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc66f33e-8b23-4bcc-9f12-04b47d18aea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03616549, 0.03877434, 0.9037791 , 0.02128115], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_policy(ppo_RandomLakeObs, RandomLakeObs(), [0,1,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6560c87a-c07e-4aec-a172-915cad2b376c",
   "metadata": {},
   "source": [
    "- Now the agent is very unlikely to go down, and very likely to go right!\n",
    "- Again, all this was learned from trial and error, with a reward earned only when the goal was reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffae4393-3058-40ce-821a-fe1e487cf4d4",
   "metadata": {},
   "source": [
    "#### Random Lake rewards\n",
    "\n",
    "- In the Random Lake example, can't be make life easier for the agent by giving immediate rewards?\n",
    "\n",
    "This is the current reward code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7bb56001-259b-4b88-8bdd-29746e208042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward(self):\n",
    "    return int(self.player == self.goal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1ee605-1e87-4080-b8ee-274816dd67fd",
   "metadata": {},
   "source": [
    "- The agent has to learn, through trial and error over _entire episodes_, that moving down and right is generally a good thing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0cf72e-d732-4039-90cc-22a17360943f",
   "metadata": {},
   "source": [
    "#### Redefining rewards\n",
    "\n",
    "- Let's instead try giving a reward _at every step, that is higher as the agent gets closer to the goal_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9a0466b-44fd-43ed-a3c0-d93334748213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from envs import RandomLakeObs\n",
    "\n",
    "class RandomLakeObsRew(RandomLakeObs):\n",
    "    def reward(self):\n",
    "        return 6-(abs(self.player[0]-self.goal[0]) + abs(self.player[1]-self.goal[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce59aa5-1f90-457f-a3f9-c970cd8a7b7b",
   "metadata": {},
   "source": [
    "- The above method uses the [Manhattan Distance](https://en.wikipedia.org/wiki/Taxicab_geometry) between the player and the goal as the reward. \n",
    "- When the agent reaches the goal, the maximum reward of 6 is achieved.\n",
    "- When the agent is furthest from the goal, the minimum reward of 0 is given."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75f7b77-8962-43fe-a82b-647e4f2aee69",
   "metadata": {},
   "source": [
    "#### Redefining rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dcba334f-f2a3-4cb0-8b8a-f5dd69a3346f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§‘ðŸ•³ðŸ§ŠðŸ•³\n",
      "ðŸ§ŠðŸ§ŠðŸ§ŠðŸ•³\n",
      "ðŸ•³ðŸ•³ðŸ§ŠðŸ§Š\n",
      "ðŸ§ŠðŸ§ŠðŸ•³â›³ï¸\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = RandomLakeObsRew()\n",
    "env.reset()\n",
    "env.render()\n",
    "env.reward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5208f2-c7e9-4f1e-9ed7-e91daec89d8e",
   "metadata": {},
   "source": [
    "â¬†ï¸ the reward is 0\n",
    "\n",
    "â¬‡ï¸ the reward is 1 because we moved closer to the goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "088715f0-3f06-4231-a149-401b6518dcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ŠðŸ•³ðŸ§ŠðŸ•³\n",
      "ðŸ§‘ðŸ§ŠðŸ§ŠðŸ•³\n",
      "ðŸ•³ðŸ•³ðŸ§ŠðŸ§Š\n",
      "ðŸ§ŠðŸ§ŠðŸ•³â›³ï¸\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, rew, done, _ = env.step(1)\n",
    "env.render()\n",
    "rew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e607ab59-e697-44b6-99a6-91a7a3a3d2b2",
   "metadata": {},
   "source": [
    "#### Redefining rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "95ab62b7-e2b5-4f6e-8f81-9381b605a9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ŠðŸ•³ðŸ§ŠðŸ•³\n",
      "ðŸ§ŠðŸ§ŠðŸ§ŠðŸ•³\n",
      "ðŸ•³ðŸ•³ðŸ§ŠðŸ§‘\n",
      "ðŸ§ŠðŸ§ŠðŸ•³â›³ï¸\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, rew, done, _ = env.step(2)\n",
    "obs, rew, done, _ = env.step(2)\n",
    "obs, rew, done, _ = env.step(2)\n",
    "obs, rew, done, _ = env.step(1)\n",
    "env.render()\n",
    "rew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1a183d-af4c-4c9d-a6d3-4bc1a7bfecca",
   "metadata": {},
   "source": [
    "Now, the reward is 5. Next, it will be 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "904e40cb-55b6-4b45-b063-5b335123e071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ŠðŸ•³ðŸ§ŠðŸ•³\n",
      "ðŸ§ŠðŸ§ŠðŸ§ŠðŸ•³\n",
      "ðŸ•³ðŸ•³ðŸ§ŠðŸ§Š\n",
      "ðŸ§ŠðŸ§ŠðŸ•³ðŸ§‘\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, rew, done, _ = env.step(1)\n",
    "env.render()\n",
    "rew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ac0aec-ddc4-4d44-833e-c2b2e253120f",
   "metadata": {},
   "source": [
    "#### Comparing rewards\n",
    "\n",
    "- So, we have two possible reward functions. Which one works better? \n",
    "- Recall that last time, after training for 8 iterations, we were able to reach the goal around 70% of the time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b5dbcf34-37e8-4025-bcd2-7a308d77046f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7130434782608696"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_RandomLakeObs.evaluate()['evaluation']['episode_reward_mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c75b369-6e96-4192-b7cd-4216f354d901",
   "metadata": {},
   "source": [
    "#### Comparing rewards\n",
    "\n",
    "Let's train with the new reward function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01c1afa7-4452-43e4-ad2d-e3dc33934fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_RandomLakeObsRew = ppo_config.build(env=RandomLakeObsRew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f7478ee6-28d8-4e44-932f-1b4f47e6b911",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    ppo_RandomLakeObsRew.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "257cef04-752e-4fe1-9eb6-3e225ea1ba62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101.90566037735849"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_RandomLakeObsRew.evaluate()['evaluation']['episode_reward_mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bccc6f4-1e9a-4abd-af7a-08b4c6103e4b",
   "metadata": {},
   "source": [
    "Wait a minute, what's going on here??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874a6979-1882-4bb4-a5cf-a83282ed15f3",
   "metadata": {},
   "source": [
    "#### Comparing rewards?\n",
    "\n",
    "- We tried to improve our RL system by shaping the reward function.\n",
    "- This (presumably) affected training, but it also affected our evaluation.\n",
    "- In supervised learning, this is like changed the scoring metric from squared error to absolute error.\n",
    "- If the old system got a mean squared error of 20,000 and the new system got a mean absolute error of 40, which is better?\n",
    "- We're comparing apples and oranges here!\n",
    "- We want to compare both models on the same metric, for example the original metric. \n",
    "- Here, we want to see how frequently the agent reaches the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf05802-842d-4764-b408-72559e7bb8cf",
   "metadata": {},
   "source": [
    "#### Comparing rewards?\n",
    "\n",
    "- The code here is a bit more advanced.\n",
    "- It is included for completeness, but we won't go into detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "261074ea-c6db-4e1b-a35f-a647e8cf2776",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.agents.callbacks import DefaultCallbacks\n",
    "\n",
    "class MyCallbacks(DefaultCallbacks):\n",
    "    def on_episode_end(self, *, worker, base_env, policies, episode, env_index, **kwargs):\n",
    "        info = episode.last_info_for()\n",
    "        episode.custom_metrics[\"goal_reached\"] = info[\"player\"] == info[\"goal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d3ab7223-77b0-46f0-af5f-669b18760a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_config_callback = (\n",
    "    PPOConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(create_env_on_local_worker=True, horizon=100)\\\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\\\n",
    "    .callbacks(callbacks_class=MyCallbacks)\\\n",
    "    .evaluation(evaluation_config={\"callbacks\" : MyCallbacks})\n",
    ")\n",
    "\n",
    "ppo_RandomLakeObsRew = ppo_config_callback.build(env=RandomLakeObsRew)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90980e8e-2ef2-479d-9c55-cbde6fcdebe1",
   "metadata": {},
   "source": [
    "The trainer above uses our new reward scheme but also reports/measures the rate of reaching the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f898830-4600-4ddf-8f05-073f7cb8e842",
   "metadata": {},
   "source": [
    "#### Comparing rewards?\n",
    "\n",
    "Let's try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3f118473-544e-43a6-9ead-f742d0ee8fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    ppo_RandomLakeObsRew.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f52dd086-12bb-4482-88a2-30df5240573b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101.90566037735849"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HIDDEN\n",
    "ppo_RandomLakeObsRew.evaluate()[\"evaluation\"][\"episode_reward_mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0021f054-0f77-46c8-ac1c-f6e1f40ff509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04081632653061224"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_RandomLakeObsRew.evaluate()[\"evaluation\"][\"custom_metrics\"][\"goal_reached_mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36e3784-b860-49a2-b3d1-b48a760e756c",
   "metadata": {},
   "source": [
    "- Hmm, these results are terrible!\n",
    "- We used to get a 70%+ win rate, and now we're close to zero.\n",
    "- What happened? ðŸ¤”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398ee5c9-7e5c-4370-912c-b90d8864cac4",
   "metadata": {},
   "source": [
    "#### What is the agent really optimizing?\n",
    "\n",
    "- The agent is really optimizing _discounted total reward_.\n",
    "- _Total_: it values all the rewards it collects, not just the final reward.\n",
    "- _Discounted_: it values earlier rewards more than later ones.\n",
    "- Our agent is successfully maximizing discounted total reward, but this isn't corresponding to reaching the goal.\n",
    "- But why? The goal gives a higher reward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cee4522-4906-4a36-b5e8-714761a11b04",
   "metadata": {},
   "source": [
    "#### Exploration vs. exploitation\n",
    "\n",
    "- A fundamental concept in RL is _exploration vs. exploitation_\n",
    "- When the agent is learning the policy, it can choose to either:\n",
    "\n",
    "1. Do things that it knows are pretty good (\"exploit\")\n",
    "2. Try something totally new and crazy, just in case (\"explore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f02fef1-3506-4c10-ac66-cd4150b32d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO \n",
    "# diagram for this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954a891d-f16c-43bb-b538-ce246829d6fb",
   "metadata": {},
   "source": [
    "#### Exploration vs. exploitation\n",
    "\n",
    "- With the old reward structure, the agent gets a reward of 0 unless it reaches the goal.\n",
    "  - So, it keeps trying to find something better.\n",
    "- With the new reward structure, the agent is getting lots of reward just for walking around.\n",
    "  - It isn't very motivated to explore the environment.\n",
    "- In fact, because it is maximizing discounted **total** reward, finding the goal is a bad thing!\n",
    "  - This causes the episode to end, limiting the total reward of the agent.\n",
    "  - The agent actually learns to _avoid_ the goal, especially early on in the episode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83b297e-b4a6-4989-b623-5f53b52562c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Designing a better reward structure\n",
    "\n",
    "- Instead, let's try penalizing the agent when it walks into a hole or off the edge.\n",
    "- It will be easier to implement this directly in `step`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c41fd832-6144-41cc-9643-e7f3eb8737d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomLakeObsRew2(RandomLakeObs):\n",
    "    def step(self, action):\n",
    "        # (not shown) existing code gets new_loc, where the player is trying to go\n",
    "        \n",
    "        reward = 0\n",
    "        \n",
    "        if self.is_valid_loc(new_loc):\n",
    "            self.player = new_loc\n",
    "        else:\n",
    "            reward -= 0.1 # small penalty\n",
    "            \n",
    "        if self.holes[self.player]:\n",
    "            reward -= 0.1 # small penalty\n",
    "            \n",
    "        if self.player == self.goal:\n",
    "            reward += 1\n",
    "        \n",
    "        # Return observation/reward/done\n",
    "        return self.observation(), reward, self.done(), {\"player\" : self.player, \"goal\" : self.goal}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5622237f-517f-4a1c-99f0-0e7a0ede3e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "from envs import RandomLakeObsRew2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804b515a-f635-4f48-8e0e-7774b003533a",
   "metadata": {},
   "source": [
    "#### Testing it out, again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c20f637-e0f8-4ed5-8133-d49d59d504f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "# redefine ppo_RandomLakeObs to include the new callbacks\n",
    "# so that you can measure the custom metric instead of the reward\n",
    "# they will give the same value but this is better for consistency\n",
    "ppo_RandomLakeObs = ppo_config_callback.build(env=RandomLakeObs)\n",
    "\n",
    "for i in range(8):\n",
    "    ppo_RandomLakeObs.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7459cba1-5771-4d26-b215-4f51df7a7ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_config_callback.build(env=RandomLakeObsRew2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd1ac25-6d5c-4ed9-a49c-c9564b92f08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    ppo_RandomLakeObsRew2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbb4747-9b58-487b-9353-1a1e8a2022c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_RandomLakeObs.evaluate()[\"evaluation\"][\"custom_metrics\"][\"goal_reached_mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e32e1de-b153-4fef-a208-a0d574d547c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_RandomLakeObsRew2.evaluate()[\"evaluation\"][\"custom_metrics\"][\"goal_reached_mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8ae49b-73af-43b4-92fa-cd3f608b504d",
   "metadata": {},
   "source": [
    "It looks like, this time, the two methods perform about the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3481f6-4505-48f6-a787-33146ff663af",
   "metadata": {},
   "source": [
    "#### Episode length\n",
    "\n",
    "- In addition to the success rate, we can compute other statistics of the agent's behavior.\n",
    "- One interesting measure is episode length.\n",
    "- RLlib records this by default, so we can easily access it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8819cf4-bc66-4ce7-a723-ea6127d4c1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_RandomLakeObs.evaluate()[\"evaluation\"][\"episode_len_mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879d945f-57e6-409e-8f8a-e0d95e6cfe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_RandomLakeObsRew2.evaluate()[\"evaluation\"][\"episode_len_mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de01c11-01b5-4f86-9f9e-9e2e43a3d62d",
   "metadata": {},
   "source": [
    "- Although the two agents have the same success rate, the new one tends toward shorter episodes.\n",
    "- This is quite interesting because the agent cannot \"see\" the difference between holes and edges.\n",
    "- We could explore this further by adding more custom metrics, e.g. number of bumps into the edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd02d93-0255-4ae2-aa46-5a9eb5111fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "#### disadvantages - loss of generality\n",
    "\n",
    "#- now only works if goal is at bottom-right\n",
    "#give a few real-world examples here -> important"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf32143e-7c32-49d2-9aed-f1081c5b8c35",
   "metadata": {},
   "source": [
    "## Supervised learning analogy: reward shaping\n",
    "<!-- multiple choice -->\n",
    "\n",
    "Earlier, we made an analogy between encoding observations in RL and feature preprocessing in supervised learning. What aspect of supervised learning is the best analogy to reward shaping in RL?\n",
    "\n",
    "- [ ] Feature engineering \n",
    "- [ ] Model selection | Not quite. But, as we'll see, there is a place for model selection in RL as well!\n",
    "- [ ] Hyperparameter tuning | Not quite. But, as we'll see, there is a place for hyperparameter tuning in RL as well!\n",
    "- [x] Selecting a loss function | Changing the loss function changes the \"best\" model, just like changing the rewards changes the \"best\" policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c458b5c7-bc0a-4084-8572-d35bf27ddc60",
   "metadata": {},
   "source": [
    "## Rewarding every step\n",
    "<!-- multiple choice -->\n",
    "\n",
    "#### Small negative rewards\n",
    "\n",
    "In RL environments like Random Lake where the agent needs to reach a specific goal, imagine we assigned a tiny negative reward for _every_ step taken by the agent. How would this generally/typically affect the amount of time the agent spends until it reaches the goal?\n",
    "\n",
    "- [x] The agent will try to reach the goal in as few steps as possible.\n",
    "- [ ] The agent will try to reach the goal in as many steps as possible. | If we're penalizing each step, taking more steps will result in less reward.\n",
    "- [ ] No change. | If we're penalizing each step, taking more steps will result in less reward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93d7454-eaff-4c79-8398-19cb541b4b37",
   "metadata": {},
   "source": [
    "## Unintended consequences\n",
    "<!-- coding exercise -->\n",
    "\n",
    "In this exercise, you will try out a bad idea: assigning a large negative reward every time the agent takes a step. We will use -1 per step. The agent still gets a reward of +1 for reaching the goal. Implement this reward, train the agent, and look at the average episode length printed out by the code. Compare this to the average episode length of an agent that just acts randomly. Then, answer the multiple choice question about the agent's behavior. What do you think is going on here? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f8434a-7575-4bfb-af7b-9858f580ae7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "from utils import lake_default_config\n",
    "from envs import RandomLakeObs\n",
    "\n",
    "class RandomLakeBadIdea(RandomLakeObs):\n",
    "    def reward(self):\n",
    "        old_reward = int(self.player == self.goal) \n",
    "        return ____\n",
    "    \n",
    "ppo = lake_default_config.build(env=____)\n",
    "\n",
    "for i in range(8):\n",
    "    ppo.train()\n",
    "    \n",
    "print(\"Average episode length for trained agent: %.1f\" % \n",
    "      ppo.evaluate()[\"evaluation\"][____])\n",
    "\n",
    "random_agent_config = (\n",
    "    lake_default_config\\\n",
    "    .exploration(exploration_config={\"type\": \"Random\"})\\\n",
    "    .evaluation(evaluation_config={\"explore\" : True})\n",
    ")\n",
    "random_agent = random_agent_config.build(env=RandomLakeBadIdea)\n",
    "\n",
    "print(\"Average episode length for random agent: %.1f\" % \n",
    "      random_agent.evaluate()[\"evaluation\"][____])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265575eb-94ef-4899-b7c0-c00c70c155e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "from utils import lake_default_config\n",
    "from envs import RandomLakeObs\n",
    "\n",
    "class RandomLakeBadIdea(RandomLakeObs):\n",
    "    def reward(self):\n",
    "        old_reward = int(self.player == self.goal) \n",
    "        return old_reward - 1\n",
    "\n",
    "ppo = lake_default_config.build(env=RandomLakeBadIdea)\n",
    "\n",
    "\n",
    "for i in range(8):\n",
    "    ppo.train()\n",
    "    \n",
    "print(\"Average episode length for trained agent: %.1f\" % \n",
    "      trainer.evaluate()[\"evaluation\"][\"episode_len_mean\"])\n",
    "\n",
    "random_agent_config = (\n",
    "    lake_default_config\\\n",
    "    .exploration(exploration_config={\"type\": \"Random\"})\\\n",
    "    .evaluation(evaluation_config={\"explore\" : True})\n",
    ")\n",
    "random_agent = random_agent_config.build(env=RandomLakeBadIdea)\n",
    "\n",
    "print(\"Average episode length for random agent: %.1f\" % \n",
    "      random_agent.evaluate()[\"evaluation\"][\"episode_len_mean\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dbab96-1590-409b-b684-d6a05c880c1c",
   "metadata": {},
   "source": [
    "#### Agent's behavior\n",
    "\n",
    "When trained on an environment with a large negative reward at every step, what do you think this agent is doing, that is undesirable?\n",
    "\n",
    "- [ ] The agent stays still because it is be discouraged from moving. | Try again!\n",
    "- [ ] The agent is not interested in reaching the goal because the reward is comparatively small. | Try again!\n",
    "- [x] The agent learns to jump into the lake as fast as it can, to avoid the negative reward of moving. | Yikes! ðŸ¥¶\n",
    "- [ ] The agent reaches the goal right away. | That would be desirable though!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d6fd44-922b-43ba-8329-8484a6af957a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ray2beta]",
   "language": "python",
   "name": "conda-env-ray2beta-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
