{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e23df55-837b-4ec9-86ad-3b60c6a7f00d",
   "metadata": {},
   "source": [
    "## Encoding Rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c8c796-3785-44d1-8aea-78872152b0cb",
   "metadata": {},
   "source": [
    "#### Encoding Rewards\n",
    "\n",
    "- We've now discussed the importance of encoding the observations.\n",
    "- We may also have some choice on the action space, though here (and often) it is relatively clear/fixed.\n",
    "- But what about the rewards? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e790dc-5146-4b03-a77a-24efd69ffabd",
   "metadata": {},
   "source": [
    "#### Current set-up\n",
    "\n",
    "- Currently, we get a reward of +1 for reaching the goal. \n",
    "- This is part of what makes RL so hard (and impressive):\n",
    "  - We want to learn about actions even though we don't know right away whether the action was \"good\". \n",
    "  - Contrast this with supervised learning, where every prediction we make on the training data can immediately be compared with the known target value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c10944be-9746-44d6-bb2b-66843ea7e4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: perhaps this next slide can be moved to Module 1, since it's very general?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff6ac9f-6815-4910-b3ce-aa7e0866101c",
   "metadata": {},
   "source": [
    "#### Agents can't just be greedy\n",
    "\n",
    "- Can agents simply learn to go for the best immediate reward?\n",
    "- No. For example, in a video recommendation system, showing the user another funny cat video might make them click (high immediate reward) but result in long-term loss of interest in the service (low long-term reward).\n",
    "- Our Frozen Lake is another example of the problem here: sometimes there is no immediate reward at all to learn from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c44cea8c-a28e-4d7c-a0c8-91c1c3f8fc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: perhaps this next section on \"Learned action probabilities\" could be moved much earlier, even as early as Module 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462e6340-cb1f-4eda-b642-d21505d18b52",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Learned action probabilities\n",
    "\n",
    "- RLlib lets us look inside the model at the probability of each action given an observation (i.e., the learned policy).\n",
    "- Let's load the trained model with our encoded observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "16d9d371-0c26-46a0-90c5-6f26e25880f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from envs import RandomLakeObs\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "\n",
    "trainer_RandomLakeObs = PPOTrainer({\"framework\" : \"torch\", \"create_env_on_driver\" : True, \"seed\" : 0, \"horizon\" : 100}, \n",
    "                       env=RandomLakeObs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a71119d3-e3c9-427c-a697-fe733fc10ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-14 12:22:55,898\tINFO trainable.py:495 -- Restored on 127.0.0.1 from checkpoint: models/RandomLakeObs/checkpoint-8\n",
      "2022-06-14 12:22:55,900\tINFO trainable.py:503 -- Current state after restoring: {'_iteration': 8, '_timesteps_total': 32000, '_time_total': 26.671697854995728, '_episodes_total': 3536}\n"
     ]
    }
   ],
   "source": [
    "trainer_RandomLakeObs.restore(\"models/RandomLakeObs/checkpoint-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fec9d02-7e22-44ee-b171-2d9bf86b8681",
   "metadata": {},
   "source": [
    "#### Learned action probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66648b5d-abd8-45ef-b592-e56bdee1c9c9",
   "metadata": {},
   "source": [
    "The exact RLlib code is hidden inside `utils` for now, but we're mainly interested in the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84b56415-177b-414f-8985-1118cabec0b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04938994, 0.5083452 , 0.4160391 , 0.02622576], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import query_policy\n",
    "query_policy(trainer_RandomLakeObs, RandomLakeObs(), [0,0,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0256b6-3174-4719-931f-65342f5ae4a7",
   "metadata": {},
   "source": [
    "- Recall the (left, down, right, up) ordering.\n",
    "- When the observation is `[0 0 0 0]` (no holes or edges in sight), the agent prefers to go down and right."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99a889a-37bd-4a7c-bf6b-c9eb297a1520",
   "metadata": {},
   "source": [
    "What if there's a hole below you? We can feed in a different observation to the policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc66f33e-8b23-4bcc-9f12-04b47d18aea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03247996, 0.02870463, 0.9153594 , 0.02345585], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_policy(trainer_RandomLakeObs, RandomLakeObs(), [0,1,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6560c87a-c07e-4aec-a172-915cad2b376c",
   "metadata": {},
   "source": [
    "- Now the agent is very unlikely to go down, and very likely to go right!\n",
    "- Again, all this was learned from trial and error, with a reward earned only when the goal was reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffae4393-3058-40ce-821a-fe1e487cf4d4",
   "metadata": {},
   "source": [
    "#### Random Lake rewards\n",
    "\n",
    "- In the Random Lake example, can't be make life easier for the agent by giving immediate rewards?\n",
    "\n",
    "This is the current reward code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bb56001-259b-4b88-8bdd-29746e208042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward(self):\n",
    "    return int(self.player == self.goal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1ee605-1e87-4080-b8ee-274816dd67fd",
   "metadata": {},
   "source": [
    "- The agent has to learn, through trial and error over _entire episodes_, that moving down and right is generally a good thing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0cf72e-d732-4039-90cc-22a17360943f",
   "metadata": {},
   "source": [
    "#### Redefining rewards\n",
    "\n",
    "- Let's instead try giving a reward _at every step, that is higher as the agent gets closer to the goal_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9a0466b-44fd-43ed-a3c0-d93334748213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from envs import RandomLakeObs\n",
    "\n",
    "class RandomLakeObsRew(RandomLakeObs):\n",
    "    def reward(self):\n",
    "        return 6-(abs(self.player[0]-self.goal[0]) + abs(self.player[1]-self.goal[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce59aa5-1f90-457f-a3f9-c970cd8a7b7b",
   "metadata": {},
   "source": [
    "- The above method uses the [Manhattan Distance](https://en.wikipedia.org/wiki/Taxicab_geometry) between the player and the goal as the reward. \n",
    "- When the agent reaches the goal, the maximum reward of 6 is achieved.\n",
    "- When the agent is furthest from the goal, the minimum reward of 0 is given."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75f7b77-8962-43fe-a82b-647e4f2aee69",
   "metadata": {},
   "source": [
    "#### Redefining rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcba334f-f2a3-4cb0-8b8a-f5dd69a3346f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P...\n",
      "....\n",
      "O.O.\n",
      "...G\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = RandomLakeObsRew()\n",
    "env.reset()\n",
    "env.render()\n",
    "env.reward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5208f2-c7e9-4f1e-9ed7-e91daec89d8e",
   "metadata": {},
   "source": [
    "â¬†ï¸ the reward is 0\n",
    "\n",
    "â¬‡ï¸ the reward is 1 because we moved closer to the goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "088715f0-3f06-4231-a149-401b6518dcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....\n",
      "P...\n",
      "O.O.\n",
      "...G\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, rew, done, _ = env.step(1)\n",
    "env.render()\n",
    "rew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e607ab59-e697-44b6-99a6-91a7a3a3d2b2",
   "metadata": {},
   "source": [
    "#### Redefining rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95ab62b7-e2b5-4f6e-8f81-9381b605a9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....\n",
      "....\n",
      "O.OP\n",
      "...G\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, rew, done, _ = env.step(2)\n",
    "obs, rew, done, _ = env.step(2)\n",
    "obs, rew, done, _ = env.step(2)\n",
    "obs, rew, done, _ = env.step(1)\n",
    "env.render()\n",
    "rew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1a183d-af4c-4c9d-a6d3-4bc1a7bfecca",
   "metadata": {},
   "source": [
    "Now, the reward is 5. Next, it will be 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "904e40cb-55b6-4b45-b063-5b335123e071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....\n",
      "....\n",
      "O.O.\n",
      "...P\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, rew, done, _ = env.step(1)\n",
    "env.render()\n",
    "rew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ac0aec-ddc4-4d44-833e-c2b2e253120f",
   "metadata": {},
   "source": [
    "#### Comparing rewards\n",
    "\n",
    "- So, we have two possible reward functions. Which one works better? \n",
    "- Recall that last time, after training for 8 iterations, we were able to reach the goal around 70% of the time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5dbcf34-37e8-4025-bcd2-7a308d77046f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7023809523809523"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_RandomLakeObs.evaluate()['evaluation']['episode_reward_mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c75b369-6e96-4192-b7cd-4216f354d901",
   "metadata": {},
   "source": [
    "#### Comparing rewards\n",
    "\n",
    "Let's train with the new reward function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "01c1afa7-4452-43e4-ad2d-e3dc33934fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_RandomLakeObsRew = PPOTrainer({\"framework\" : \"torch\", \"create_env_on_driver\" : True, \"seed\" : 0, \"horizon\" : 100}, \n",
    "                                      env=RandomLakeObsRew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f7478ee6-28d8-4e44-932f-1b4f47e6b911",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    trainer_RandomLakeObsRew.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "257cef04-752e-4fe1-9eb6-3e225ea1ba62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133.5897435897436"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_RandomLakeObsRew.evaluate()['evaluation']['episode_reward_mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bccc6f4-1e9a-4abd-af7a-08b4c6103e4b",
   "metadata": {},
   "source": [
    "Wait a minute, what's going on here??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874a6979-1882-4bb4-a5cf-a83282ed15f3",
   "metadata": {},
   "source": [
    "#### Comparing rewards?\n",
    "\n",
    "- We tried to improve our RL system by shaping the reward function.\n",
    "- This (presumably) affected training, but it also affected our evaluation.\n",
    "- In supervised learning, this is like changed the scoring metric from squared error to absolute error.\n",
    "- If the old system got a mean squared error of 20,000 and the new system got a mean absolute error of 40, which is better?\n",
    "- We're comparing apples and oranges here!\n",
    "- We want to compare both models on the same metric, for example the original metric. \n",
    "- Here, we want to see how frequently the agent reaches the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf05802-842d-4764-b408-72559e7bb8cf",
   "metadata": {},
   "source": [
    "#### Comparing rewards?\n",
    "\n",
    "- The code here is a bit more advanced.\n",
    "- It is included for completeness, but we won't go into detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "261074ea-c6db-4e1b-a35f-a647e8cf2776",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.agents.callbacks import DefaultCallbacks\n",
    "\n",
    "class MyCallbacks(DefaultCallbacks):\n",
    "    def on_episode_end(self, *, worker, base_env, policies, episode, env_index, **kwargs):\n",
    "        info = episode.last_info_for()\n",
    "        episode.custom_metrics[\"goal_reached\"] = info[\"player\"] == info[\"goal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d3ab7223-77b0-46f0-af5f-669b18760a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_RandomLakeObsRew = PPOTrainer({\"framework\" : \"torch\", \"create_env_on_driver\" : True, \"seed\" : 0, \n",
    "                                      \"callbacks\" : MyCallbacks, \"horizon\" : 100,\n",
    "                                      \"evaluation_config\" : {\"callbacks\" : MyCallbacks}},\n",
    "                                      env=RandomLakeObsRew)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90980e8e-2ef2-479d-9c55-cbde6fcdebe1",
   "metadata": {},
   "source": [
    "The trainer above uses our new reward scheme but also reports/measures the rate of reaching the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f898830-4600-4ddf-8f05-073f7cb8e842",
   "metadata": {},
   "source": [
    "#### Comparing rewards?\n",
    "\n",
    "Let's try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3f118473-544e-43a6-9ead-f742d0ee8fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    trainer_RandomLakeObsRew.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f52dd086-12bb-4482-88a2-30df5240573b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133.5897435897436"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HIDDEN\n",
    "trainer_RandomLakeObsRew.evaluate()[\"evaluation\"][\"episode_reward_mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0021f054-0f77-46c8-ac1c-f6e1f40ff509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.023809523809523808"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_RandomLakeObsRew.evaluate()[\"evaluation\"][\"custom_metrics\"][\"goal_reached_mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36e3784-b860-49a2-b3d1-b48a760e756c",
   "metadata": {},
   "source": [
    "- Hmm, these results are terrible!\n",
    "- We used to get a 70% win rate, and now we're close to zero.\n",
    "- What happened? ðŸ¤”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398ee5c9-7e5c-4370-912c-b90d8864cac4",
   "metadata": {},
   "source": [
    "#### What is the agent really optimizing?\n",
    "\n",
    "- The agent is really optimizing _discounted total reward_.\n",
    "- _Total_: it values all the rewards it collects, not just the final reward.\n",
    "- _Discounted_: it values earlier rewards more than later ones.\n",
    "- Our agent is successfully maximizing discounted total reward, but this isn't corresponding to reaching the goal.\n",
    "- But why? The goal gives a higher reward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cee4522-4906-4a36-b5e8-714761a11b04",
   "metadata": {},
   "source": [
    "#### Exploration vs. exploitation\n",
    "\n",
    "- A fundamental concept in RL is _exploration vs. exploitation_\n",
    "- When the agent is learning the policy, it can choose to either:\n",
    "\n",
    "1. Do things that it knows are pretty good (\"exploit\")\n",
    "2. Try something totally new and crazy, just in case (\"explore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4f02fef1-3506-4c10-ac66-cd4150b32d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO \n",
    "# diagram for this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954a891d-f16c-43bb-b538-ce246829d6fb",
   "metadata": {},
   "source": [
    "#### Exploration vs. exploitation\n",
    "\n",
    "- With the old reward structure, the agent gets a reward of 0 unless it reaches the goal.\n",
    "  - So, it keeps trying to find something better.\n",
    "- With the new reward structure, the agent is getting lots of reward just for walking around.\n",
    "  - It isn't very motivated to explore the environment.\n",
    "- In fact, because it is maximizing discounted **total** reward, finding the goal is a bad thing!\n",
    "  - This causes the episode to end, limiting the total reward of the agent.\n",
    "  - The agent actually learns to _avoid_ the goal, especially early on in the episode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83b297e-b4a6-4989-b623-5f53b52562c5",
   "metadata": {},
   "source": [
    "#### Designing a better reward structure\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3b7b11-b487-4c7f-a7ea-e675c494b112",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ecd38d-2a7c-4dc2-933d-86ae2456fe07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc391dd-331d-4172-ac72-4115401bd210",
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomLakeObs_results = []\n",
    "RandomLakeObsRew_results = []\n",
    "\n",
    "for i in range(8):\n",
    "    RandomLakeObs_results.append(trainer_RandomLakeObs.train())\n",
    "for i in range(8):\n",
    "    RandomLakeObsRew_results.append(trainer_RandomLakeObsRew.train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d56b02-2a36-4b8c-8d58-195bd77e37d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdb7f48-f7f5-431c-867f-49ec5d4e6110",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([x[\"custom_metrics\"][\"goal_reached_mean\"] for x in RandomLakeObs_results], label=\"obs\")\n",
    "plt.plot([x[\"custom_metrics\"][\"goal_reached_mean\"] for x in RandomLakeObsRew_results], label=\"rew\")\n",
    "plt.xlabel(\"training iteration\")\n",
    "plt.ylabel(\"success rate\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c23d956-9fb7-4597-8d35-d329641030e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da40aedd-097e-4548-b00b-b0b308460361",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6017a4c8-f28d-4533-9859-e6c8d8572ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from envs import RandomLakeObsRew, RandomLakeObs, RandomLakeObsTest\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0961f2dc-06db-4d34-a9fb-2e41ae66fb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.agents.callbacks import DefaultCallbacks\n",
    "\n",
    "class MyCallbacks(DefaultCallbacks):\n",
    "    def on_episode_end(self, *, worker, base_env, policies, episode, env_index, **kwargs):\n",
    "        info = episode.last_info_for()\n",
    "        episode.custom_metrics[\"goal_reached\"] = info[\"player\"] == info[\"goal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926e29c5-272f-400e-b75e-6aff33d61d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "\n",
    "trainer_RandomLakeObsRew = PPOTrainer({\"framework\" : \"torch\", \"create_env_on_driver\" : True, \"seed\" : 0, \n",
    "                                      \"callbacks\" : MyCallbacks},\n",
    "                                      env=RandomLakeObsRew)\n",
    "\n",
    "trainer_RandomLakeObs = PPOTrainer({\"framework\" : \"torch\", \"create_env_on_driver\" : True, \"seed\" : 0,\n",
    "                                   \"callbacks\" : MyCallbacks}, \n",
    "                                    env=RandomLakeObs)\n",
    "\n",
    "trainer_RandomLakeObsTest = PPOTrainer({\"framework\" : \"torch\", \"create_env_on_driver\" : True, \"seed\" : 0,\n",
    "                                   \"callbacks\" : MyCallbacks}, \n",
    "                                    env=RandomLakeObsTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cea945-95af-423b-88d8-b0baaf458227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(8):\n",
    "#     trainer_RandomLakeObsRew.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e702f47-9806-41c6-9ada-a53cd2fa4d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "outs_RandomLakeObs = []\n",
    "for i in range(8):\n",
    "    outs_RandomLakeObs.append(trainer_RandomLakeObs.train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbf002d-2d4d-4121-8178-7f85c63f4bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([x[\"custom_metrics\"][\"goal_reached_mean\"] for x in outs_RandomLakeObs]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056fe552-5e0e-4f4a-88e7-d7d1429ccb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_RandomLakeObs.evaluate()[\"evaluation\"][\"custom_metrics\"][\"goal_reached_mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee63059-4802-4166-9881-488e80020b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "outs_RandomLakeObsTest = []\n",
    "for i in range(8):\n",
    "    outs_RandomLakeObsTest.append(trainer_RandomLakeObsTest.train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ed94d2-5d9e-447f-b553-bc3e368d2d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([x[\"custom_metrics\"][\"goal_reached_mean\"] for x in outs_RandomLakeObsTest]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062d9f86-1d05-47d0-9868-db6309fb6760",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_RandomLakeObsTest.evaluate()[\"evaluation\"][\"custom_metrics\"][\"goal_reached_mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91625eea-c048-451c-813c-6e4ffdb883ce",
   "metadata": {},
   "source": [
    "OBSERVATION:\n",
    "\n",
    "- if you -0.1 from all rewards, it works OK, if you +0.1 to all rewards, it fails horribly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa21d40-8ebb-4c04-ba9c-d6a3f48cadb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ee9593-9c99-4392-9a1e-ac0cf392ee1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0eeb097-7420-47b4-a5a3-e459b787e4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_RandomLakeObs.config[\"model\"][\"vf_share_layers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7751f47-4279-4bfc-871b-38a4f695a860",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = RandomLakeObsTest()\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3df2833-3294-460a-a87f-1bb78888dadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = trainer_RandomLakeObsTest.compute_single_action(input_dict={\"obs\": obs})#, explore=False)\n",
    "res = env.step(action)\n",
    "obs = res[0]\n",
    "env.render()\n",
    "\n",
    "print(\"\\naction:\", action)\n",
    "print(\"reward:\", res[1])\n",
    "print(\"done:\", res[2])\n",
    "print(\"obs:\", obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1606db57-5542-41e3-831b-1f88722e7df4",
   "metadata": {},
   "source": [
    "# NEXT TODO\n",
    "\n",
    "figure out why the agent with the shaped reward isn't working at all\n",
    "\n",
    "- make sure it's actually bad (and not a metrics issue) by rendering - YES\n",
    "- does the absolute reward matter, e.g. if i subtract 6 from all the rewards what happens - SVEN SAYS NO BUT CHECK\n",
    "- is the problem that it's better to just go back and forth safely and not even try for the goal?\n",
    "- note: this reward shaping may help more for a bigger maze when it takes more actions to get feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446383ba-e096-4459-acaa-8a0c78b10dfd",
   "metadata": {},
   "source": [
    "#### callback thing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ecfc84-3829-44ba-9209-30164bb44d54",
   "metadata": {},
   "source": [
    "#### training curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0942516-b276-4ef9-9442-fffa0398924f",
   "metadata": {},
   "source": [
    "#### disadvantages - loss of generality\n",
    "\n",
    "- now only works if goal is at bottom-right\n",
    "give a few real-world examples here -> important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f11101-776d-4352-b44f-949b38c7bc9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rl-course]",
   "language": "python",
   "name": "conda-env-rl-course-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
