{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e23df55-837b-4ec9-86ad-3b60c6a7f00d",
   "metadata": {},
   "source": [
    "## Encoding Rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c8c796-3785-44d1-8aea-78872152b0cb",
   "metadata": {},
   "source": [
    "#### Encoding Rewards\n",
    "\n",
    "- We've now discussed the importance of encoding the observations.\n",
    "- We may also have some choice on the action space, though here (and often) it is relatively clear/fixed.\n",
    "- But what about the rewards? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e790dc-5146-4b03-a77a-24efd69ffabd",
   "metadata": {},
   "source": [
    "#### Current set-up\n",
    "\n",
    "- Currently, we get a reward of +1 for reaching the goal. \n",
    "- This is part of what makes RL so hard (and impressive):\n",
    "  - We want to learn about actions even though we don't know right away whether the action was \"good\". \n",
    "  - Contrast this with supervised learning, where every prediction we make on the training data can immediately be compared with the known target value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c10944be-9746-44d6-bb2b-66843ea7e4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: perhaps this next slide can be moved to Module 1, since it's very general?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff6ac9f-6815-4910-b3ce-aa7e0866101c",
   "metadata": {},
   "source": [
    "#### Agents can't just be greedy\n",
    "\n",
    "- Can agents simply learn to go for the best immediate reward?\n",
    "- No. For example, in a video recommendation system, showing the user another funny cat video might make them click (high immediate reward) but result in long-term loss of interest in the service (low long-term reward).\n",
    "- Our Frozen Lake is another example of the problem here: sometimes there is no immediate reward at all to learn from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c44cea8c-a28e-4d7c-a0c8-91c1c3f8fc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: perhaps this next section on \"Learned action probabilities\" could be moved much earlier, even as early as Module 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462e6340-cb1f-4eda-b642-d21505d18b52",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Learned action probabilities\n",
    "\n",
    "- RLlib lets us look inside the model at the probability of each action given an observation (i.e., the learned policy).\n",
    "- Let's load the trained model with our encoded observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16d9d371-0c26-46a0-90c5-6f26e25880f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from envs import RandomLakeObs\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "\n",
    "trainer_RandomLakeObs = PPOTrainer({\"framework\" : \"torch\", \"create_env_on_driver\" : True, \"seed\" : 0, \"horizon\" : 100}, \n",
    "                       env=RandomLakeObs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a71119d3-e3c9-427c-a697-fe733fc10ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-14 13:11:35,084\tINFO trainable.py:495 -- Restored on 127.0.0.1 from checkpoint: models/RandomLakeObs/checkpoint-8\n",
      "2022-06-14 13:11:35,085\tINFO trainable.py:503 -- Current state after restoring: {'_iteration': 8, '_timesteps_total': 32000, '_time_total': 26.671697854995728, '_episodes_total': 3536}\n"
     ]
    }
   ],
   "source": [
    "trainer_RandomLakeObs.restore(\"models/RandomLakeObs/checkpoint-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fec9d02-7e22-44ee-b171-2d9bf86b8681",
   "metadata": {},
   "source": [
    "#### Learned action probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66648b5d-abd8-45ef-b592-e56bdee1c9c9",
   "metadata": {},
   "source": [
    "The exact RLlib code is hidden inside `utils` for now, but we're mainly interested in the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84b56415-177b-414f-8985-1118cabec0b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04938994, 0.5083452 , 0.4160391 , 0.02622576], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import query_policy\n",
    "query_policy(trainer_RandomLakeObs, RandomLakeObs(), [0,0,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0256b6-3174-4719-931f-65342f5ae4a7",
   "metadata": {},
   "source": [
    "- Recall the (left, down, right, up) ordering.\n",
    "- When the observation is `[0 0 0 0]` (no holes or edges in sight), the agent prefers to go down and right."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99a889a-37bd-4a7c-bf6b-c9eb297a1520",
   "metadata": {},
   "source": [
    "What if there's a hole below you? We can feed in a different observation to the policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc66f33e-8b23-4bcc-9f12-04b47d18aea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03247996, 0.02870463, 0.9153594 , 0.02345585], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_policy(trainer_RandomLakeObs, RandomLakeObs(), [0,1,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6560c87a-c07e-4aec-a172-915cad2b376c",
   "metadata": {},
   "source": [
    "- Now the agent is very unlikely to go down, and very likely to go right!\n",
    "- Again, all this was learned from trial and error, with a reward earned only when the goal was reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffae4393-3058-40ce-821a-fe1e487cf4d4",
   "metadata": {},
   "source": [
    "#### Random Lake rewards\n",
    "\n",
    "- In the Random Lake example, can't be make life easier for the agent by giving immediate rewards?\n",
    "\n",
    "This is the current reward code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bb56001-259b-4b88-8bdd-29746e208042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward(self):\n",
    "    return int(self.player == self.goal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1ee605-1e87-4080-b8ee-274816dd67fd",
   "metadata": {},
   "source": [
    "- The agent has to learn, through trial and error over _entire episodes_, that moving down and right is generally a good thing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0cf72e-d732-4039-90cc-22a17360943f",
   "metadata": {},
   "source": [
    "#### Redefining rewards\n",
    "\n",
    "- Let's instead try giving a reward _at every step, that is higher as the agent gets closer to the goal_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9a0466b-44fd-43ed-a3c0-d93334748213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from envs import RandomLakeObs\n",
    "\n",
    "class RandomLakeObsRew(RandomLakeObs):\n",
    "    def reward(self):\n",
    "        return 6-(abs(self.player[0]-self.goal[0]) + abs(self.player[1]-self.goal[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce59aa5-1f90-457f-a3f9-c970cd8a7b7b",
   "metadata": {},
   "source": [
    "- The above method uses the [Manhattan Distance](https://en.wikipedia.org/wiki/Taxicab_geometry) between the player and the goal as the reward. \n",
    "- When the agent reaches the goal, the maximum reward of 6 is achieved.\n",
    "- When the agent is furthest from the goal, the minimum reward of 0 is given."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75f7b77-8962-43fe-a82b-647e4f2aee69",
   "metadata": {},
   "source": [
    "#### Redefining rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcba334f-f2a3-4cb0-8b8a-f5dd69a3346f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P...\n",
      "....\n",
      "O.O.\n",
      "...G\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = RandomLakeObsRew()\n",
    "env.reset()\n",
    "env.render()\n",
    "env.reward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5208f2-c7e9-4f1e-9ed7-e91daec89d8e",
   "metadata": {},
   "source": [
    "‚¨ÜÔ∏è the reward is 0\n",
    "\n",
    "‚¨áÔ∏è the reward is 1 because we moved closer to the goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "088715f0-3f06-4231-a149-401b6518dcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....\n",
      "P...\n",
      "O.O.\n",
      "...G\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, rew, done, _ = env.step(1)\n",
    "env.render()\n",
    "rew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e607ab59-e697-44b6-99a6-91a7a3a3d2b2",
   "metadata": {},
   "source": [
    "#### Redefining rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95ab62b7-e2b5-4f6e-8f81-9381b605a9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....\n",
      "....\n",
      "O.OP\n",
      "...G\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, rew, done, _ = env.step(2)\n",
    "obs, rew, done, _ = env.step(2)\n",
    "obs, rew, done, _ = env.step(2)\n",
    "obs, rew, done, _ = env.step(1)\n",
    "env.render()\n",
    "rew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1a183d-af4c-4c9d-a6d3-4bc1a7bfecca",
   "metadata": {},
   "source": [
    "Now, the reward is 5. Next, it will be 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "904e40cb-55b6-4b45-b063-5b335123e071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....\n",
      "....\n",
      "O.O.\n",
      "...P\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, rew, done, _ = env.step(1)\n",
    "env.render()\n",
    "rew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ac0aec-ddc4-4d44-833e-c2b2e253120f",
   "metadata": {},
   "source": [
    "#### Comparing rewards\n",
    "\n",
    "- So, we have two possible reward functions. Which one works better? \n",
    "- Recall that last time, after training for 8 iterations, we were able to reach the goal around 70% of the time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5dbcf34-37e8-4025-bcd2-7a308d77046f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7023809523809523"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_RandomLakeObs.evaluate()['evaluation']['episode_reward_mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c75b369-6e96-4192-b7cd-4216f354d901",
   "metadata": {},
   "source": [
    "#### Comparing rewards\n",
    "\n",
    "Let's train with the new reward function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01c1afa7-4452-43e4-ad2d-e3dc33934fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_RandomLakeObsRew = PPOTrainer({\"framework\" : \"torch\", \"create_env_on_driver\" : True, \"seed\" : 0, \"horizon\" : 100}, \n",
    "                                      env=RandomLakeObsRew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7478ee6-28d8-4e44-932f-1b4f47e6b911",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    trainer_RandomLakeObsRew.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "257cef04-752e-4fe1-9eb6-3e225ea1ba62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133.5897435897436"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_RandomLakeObsRew.evaluate()['evaluation']['episode_reward_mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bccc6f4-1e9a-4abd-af7a-08b4c6103e4b",
   "metadata": {},
   "source": [
    "Wait a minute, what's going on here??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874a6979-1882-4bb4-a5cf-a83282ed15f3",
   "metadata": {},
   "source": [
    "#### Comparing rewards?\n",
    "\n",
    "- We tried to improve our RL system by shaping the reward function.\n",
    "- This (presumably) affected training, but it also affected our evaluation.\n",
    "- In supervised learning, this is like changed the scoring metric from squared error to absolute error.\n",
    "- If the old system got a mean squared error of 20,000 and the new system got a mean absolute error of 40, which is better?\n",
    "- We're comparing apples and oranges here!\n",
    "- We want to compare both models on the same metric, for example the original metric. \n",
    "- Here, we want to see how frequently the agent reaches the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf05802-842d-4764-b408-72559e7bb8cf",
   "metadata": {},
   "source": [
    "#### Comparing rewards?\n",
    "\n",
    "- The code here is a bit more advanced.\n",
    "- It is included for completeness, but we won't go into detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "261074ea-c6db-4e1b-a35f-a647e8cf2776",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.agents.callbacks import DefaultCallbacks\n",
    "\n",
    "class MyCallbacks(DefaultCallbacks):\n",
    "    def on_episode_end(self, *, worker, base_env, policies, episode, env_index, **kwargs):\n",
    "        info = episode.last_info_for()\n",
    "        episode.custom_metrics[\"goal_reached\"] = info[\"player\"] == info[\"goal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3ab7223-77b0-46f0-af5f-669b18760a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_RandomLakeObsRew = PPOTrainer({\"framework\" : \"torch\", \"create_env_on_driver\" : True, \"seed\" : 0, \n",
    "                                      \"callbacks\" : MyCallbacks, \"horizon\" : 100,\n",
    "                                      \"evaluation_config\" : {\"callbacks\" : MyCallbacks}},\n",
    "                                      env=RandomLakeObsRew)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90980e8e-2ef2-479d-9c55-cbde6fcdebe1",
   "metadata": {},
   "source": [
    "The trainer above uses our new reward scheme but also reports/measures the rate of reaching the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f898830-4600-4ddf-8f05-073f7cb8e842",
   "metadata": {},
   "source": [
    "#### Comparing rewards?\n",
    "\n",
    "Let's try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f118473-544e-43a6-9ead-f742d0ee8fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    trainer_RandomLakeObsRew.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f52dd086-12bb-4482-88a2-30df5240573b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133.5897435897436"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HIDDEN\n",
    "trainer_RandomLakeObsRew.evaluate()[\"evaluation\"][\"episode_reward_mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0021f054-0f77-46c8-ac1c-f6e1f40ff509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.023809523809523808"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_RandomLakeObsRew.evaluate()[\"evaluation\"][\"custom_metrics\"][\"goal_reached_mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36e3784-b860-49a2-b3d1-b48a760e756c",
   "metadata": {},
   "source": [
    "- Hmm, these results are terrible!\n",
    "- We used to get a 70%+ win rate, and now we're close to zero.\n",
    "- What happened? ü§î"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398ee5c9-7e5c-4370-912c-b90d8864cac4",
   "metadata": {},
   "source": [
    "#### What is the agent really optimizing?\n",
    "\n",
    "- The agent is really optimizing _discounted total reward_.\n",
    "- _Total_: it values all the rewards it collects, not just the final reward.\n",
    "- _Discounted_: it values earlier rewards more than later ones.\n",
    "- Our agent is successfully maximizing discounted total reward, but this isn't corresponding to reaching the goal.\n",
    "- But why? The goal gives a higher reward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cee4522-4906-4a36-b5e8-714761a11b04",
   "metadata": {},
   "source": [
    "#### Exploration vs. exploitation\n",
    "\n",
    "- A fundamental concept in RL is _exploration vs. exploitation_\n",
    "- When the agent is learning the policy, it can choose to either:\n",
    "\n",
    "1. Do things that it knows are pretty good (\"exploit\")\n",
    "2. Try something totally new and crazy, just in case (\"explore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f02fef1-3506-4c10-ac66-cd4150b32d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO \n",
    "# diagram for this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954a891d-f16c-43bb-b538-ce246829d6fb",
   "metadata": {},
   "source": [
    "#### Exploration vs. exploitation\n",
    "\n",
    "- With the old reward structure, the agent gets a reward of 0 unless it reaches the goal.\n",
    "  - So, it keeps trying to find something better.\n",
    "- With the new reward structure, the agent is getting lots of reward just for walking around.\n",
    "  - It isn't very motivated to explore the environment.\n",
    "- In fact, because it is maximizing discounted **total** reward, finding the goal is a bad thing!\n",
    "  - This causes the episode to end, limiting the total reward of the agent.\n",
    "  - The agent actually learns to _avoid_ the goal, especially early on in the episode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83b297e-b4a6-4989-b623-5f53b52562c5",
   "metadata": {},
   "source": [
    "#### Designing a better reward structure\n",
    "\n",
    "Some nice properties we might want:\n",
    "\n",
    "- The agent is motivated to explore the environment.\n",
    "  - Avoid positive rewards for regular steps.\n",
    "- The agent is motivated to reach the goal.\n",
    "  - Large positive reward for reaching the goal.\n",
    "- We hint that the goal is down and right.\n",
    "  - Higher rewards for successful down/right moves than up/left moves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ed1a3f-148d-4585-a32e-6ea33cfe5640",
   "metadata": {},
   "source": [
    "#### Designing a better reward structure\n",
    "\n",
    "Putting this all together, we could try something like this:\n",
    "\n",
    "- Reward of -0.1 for down/right moves\n",
    "- Reward of -0.2 for up/left moves\n",
    "- Reward for +1 for reaching the goal\n",
    "\n",
    "Notes:\n",
    "\n",
    "In terms of the code, the reward now depends on the action and not just the player location. So we will need to modify the `reward` method to take in the action, and the `step` method to pass the action into `reward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78ecd38d-2a7c-4dc2-933d-86ae2456fe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomLakeObsRew2(RandomLakeObs):\n",
    "    def reward(self, action):\n",
    "        reward = 0\n",
    "        if action in (0,3): # up, left\n",
    "            reward -= 0.02\n",
    "        elif action in (1,2): #down, right\n",
    "            reward -= 0.01\n",
    "        \n",
    "        if self.player == self.goal:\n",
    "            reward += 1\n",
    "            \n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d80fdc45-5967-4cbd-8c75-155094275b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "from envs import RandomLakeObsRew2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804b515a-f635-4f48-8e0e-7774b003533a",
   "metadata": {},
   "source": [
    "#### Testing it out, again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7459cba1-5771-4d26-b215-4f51df7a7ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_RandomLakeObsRew2 = PPOTrainer({\"framework\" : \"torch\", \"create_env_on_driver\" : True, \"seed\" : 0, \n",
    "                                      \"callbacks\" : MyCallbacks, \"horizon\" : 100,\n",
    "                                      \"evaluation_config\" : {\"callbacks\" : MyCallbacks}},\n",
    "                                      env=RandomLakeObsRew2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "db135815-2e3a-4242-918e-dc682700c877",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    trainer_RandomLakeObsRew2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "03b065a2-237f-41e0-b586-72bc23bc5ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09602977667493798"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HIDDEN\n",
    "trainer_RandomLakeObsRew2.evaluate()[\"evaluation\"][\"episode_reward_mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6cc14e3c-c1cf-42a8-b6dd-52babb8f1324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5224586288416075"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_RandomLakeObsRew2.evaluate()[\"evaluation\"][\"custom_metrics\"][\"goal_reached_mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba293320-2b98-4188-bd6d-74805e20364d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb133e4-fbbc-4377-82e7-0720d794ba71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce307975-fa67-4ddd-861d-4bf2e0408802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43fb1f31-8354-48f0-a7ef-227e6cf7f750",
   "metadata": {},
   "outputs": [],
   "source": [
    "from envs import RandomLakeObsRew2, RandomLakeObs\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2bc391dd-331d-4172-ac72-4115401bd210",
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomLakeObs_results = []\n",
    "RandomLakeObsRew2_results = []\n",
    "\n",
    "for i in range(8):\n",
    "    RandomLakeObs_results.append(trainer_RandomLakeObs.train())\n",
    "for i in range(8):\n",
    "    RandomLakeObsRew_results.append(trainer_RandomLakeObsRew.train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "85d56b02-2a36-4b8c-8d58-195bd77e37d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2cdb7f48-f7f5-431c-867f-49ec5d4e6110",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'goal_reached_mean'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot([x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_metrics\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoal_reached_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m RandomLakeObs_results], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot([x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_metrics\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoal_reached_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m RandomLakeObsRew_results], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrew\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining iteration\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot([\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcustom_metrics\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoal_reached_mean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m RandomLakeObs_results], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot([x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_metrics\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoal_reached_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m RandomLakeObsRew_results], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrew\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining iteration\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'goal_reached_mean'"
     ]
    }
   ],
   "source": [
    "plt.plot([x[\"custom_metrics\"][\"goal_reached_mean\"] for x in RandomLakeObs_results], label=\"obs\")\n",
    "plt.plot([x[\"custom_metrics\"][\"goal_reached_mean\"] for x in RandomLakeObsRew_results], label=\"rew\")\n",
    "plt.xlabel(\"training iteration\")\n",
    "plt.ylabel(\"success rate\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c23d956-9fb7-4597-8d35-d329641030e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da40aedd-097e-4548-b00b-b0b308460361",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6017a4c8-f28d-4533-9859-e6c8d8572ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from envs import RandomLakeObsRew2, RandomLakeObs\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "926e29c5-272f-400e-b75e-6aff33d61d1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MyCallbacks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mppo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PPOTrainer\n\u001b[1;32m      4\u001b[0m trainer_RandomLakeObs \u001b[38;5;241m=\u001b[39m PPOTrainer({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframework\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreate_env_on_driver\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m----> 5\u001b[0m                                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[43mMyCallbacks\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhorizon\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m      6\u001b[0m                                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation_config\u001b[39m\u001b[38;5;124m\"\u001b[39m : {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m : MyCallbacks}},\n\u001b[1;32m      7\u001b[0m                                     env\u001b[38;5;241m=\u001b[39mRandomLakeObs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MyCallbacks' is not defined"
     ]
    }
   ],
   "source": [
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "\n",
    "\n",
    "trainer_RandomLakeObs = PPOTrainer({\"framework\" : \"torch\", \"create_env_on_driver\" : True, \"seed\" : 0,\n",
    "                                      \"callbacks\" : MyCallbacks, \"horizon\" : 100,\n",
    "                                      \"evaluation_config\" : {\"callbacks\" : MyCallbacks}},\n",
    "                                    env=RandomLakeObs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa52d99-518c-4deb-a9b6-c782238d33c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_RandomLakeObsRew2 = PPOTrainer({\"framework\" : \"torch\", \"create_env_on_driver\" : True, \"seed\" : 0, \n",
    "                                      \"callbacks\" : MyCallbacks, \"horizon\" : 100,\n",
    "                                      \"evaluation_config\" : {\"callbacks\" : MyCallbacks}},\n",
    "                                      env=RandomLakeObsRew2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "22cea945-95af-423b-88d8-b0baaf458227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(8):\n",
    "#     trainer_RandomLakeObsRew.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6e702f47-9806-41c6-9ada-a53cd2fa4d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "outs_RandomLakeObs = []\n",
    "for i in range(8):\n",
    "    outs_RandomLakeObs.append(trainer_RandomLakeObs.train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7dbf002d-2d4d-4121-8178-7f85c63f4bae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([x[\"custom_metrics\"][\"goal_reached_mean\"] for x in outs_RandomLakeObs]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "056fe552-5e0e-4f4a-88e7-d7d1429ccb10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7512953367875648"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_RandomLakeObs.evaluate()[\"evaluation\"][\"custom_metrics\"][\"goal_reached_mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eee63059-4802-4166-9881-488e80020b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "outs_RandomLakeObsRew2 = []\n",
    "for i in range(8):\n",
    "    outs_RandomLakeObsRew2.append(trainer_RandomLakeObsRew2.train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "50ed94d2-5d9e-447f-b553-bc3e368d2d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([x[\"custom_metrics\"][\"goal_reached_mean\"] for x in outs_RandomLakeObsRew2]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "062d9f86-1d05-47d0-9868-db6309fb6760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5011600928074246"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_RandomLakeObsRew2.evaluate()[\"evaluation\"][\"custom_metrics\"][\"goal_reached_mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa21d40-8ebb-4c04-ba9c-d6a3f48cadb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ee9593-9c99-4392-9a1e-ac0cf392ee1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0eeb097-7420-47b4-a5a3-e459b787e4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_RandomLakeObs.config[\"model\"][\"vf_share_layers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7751f47-4279-4bfc-871b-38a4f695a860",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = RandomLakeObsTest()\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3df2833-3294-460a-a87f-1bb78888dadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = trainer_RandomLakeObsTest.compute_single_action(input_dict={\"obs\": obs})#, explore=False)\n",
    "res = env.step(action)\n",
    "obs = res[0]\n",
    "env.render()\n",
    "\n",
    "print(\"\\naction:\", action)\n",
    "print(\"reward:\", res[1])\n",
    "print(\"done:\", res[2])\n",
    "print(\"obs:\", obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446383ba-e096-4459-acaa-8a0c78b10dfd",
   "metadata": {},
   "source": [
    "#### callback thing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ecfc84-3829-44ba-9209-30164bb44d54",
   "metadata": {},
   "source": [
    "#### training curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0942516-b276-4ef9-9442-fffa0398924f",
   "metadata": {},
   "source": [
    "#### disadvantages - loss of generality\n",
    "\n",
    "- now only works if goal is at bottom-right\n",
    "give a few real-world examples here -> important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f11101-776d-4352-b44f-949b38c7bc9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128e41d6-c1d2-44de-8711-9286aa828d70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf32143e-7c32-49d2-9aed-f1081c5b8c35",
   "metadata": {},
   "source": [
    "## Supervised learning analogy: reward shaping\n",
    "\n",
    "Earlier, we made an analogy between encoding observations in RL and feature preprocessing in supervised learning. What aspect of supervised learning is the best analogy to reward shaping in RL?\n",
    "\n",
    "- [ ] Feature engineering \n",
    "- [ ] Model selection | Not quite. But, as we'll see, there is a place for model selection in RL as well!\n",
    "- [ ] Hyperparameter tuning | Not quite. But, as we'll see, there is a place for hyperparameter tuning in RL as well!\n",
    "- [x] Selecting a loss function | Changing the loss function changes the \"best\" model, just like changing the rewards changes the \"best\" policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c458b5c7-bc0a-4084-8572-d35bf27ddc60",
   "metadata": {},
   "source": [
    "## Sign of rewards NAME TODO\n",
    "\n",
    "In our Random Lake environment, we tried giving a small negative reward for every step. How does this change affect the amount of time the agent might spend until it reaches the goal?\n",
    "\n",
    "- [x] The agent will try to reach the goal in as few steps as possible.\n",
    "- [ ] The agent will try to reach the goal in as many steps as possible.\n",
    "- [ ] This change to the reward will not affect the agent's \"motivation\" to reach t \n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93ced8a-1fc9-427a-80ae-faeacf2d72f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rl-course]",
   "language": "python",
   "name": "conda-env-rl-course-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
