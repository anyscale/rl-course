{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e23df55-837b-4ec9-86ad-3b60c6a7f00d",
   "metadata": {},
   "source": [
    "## Encoding Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58bc4b75-8b99-4450-9b25-6af748253913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import ray\n",
    "import logging\n",
    "ray.init(log_to_driver=False, ignore_reinit_error=True, logging_level=logging.ERROR); # logging.FATAL\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95a9faf-9b2c-45ec-a347-3aadca399516",
   "metadata": {},
   "source": [
    "#### Review: what is a policy?\n",
    "\n",
    "- In RL we're trying to learn a policy, what is this again, exactly?\n",
    "- A policy maps **observations** to **actions**.\n",
    "- In other words, the observations are all the policy \"sees\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacebed5-729c-4ea8-8570-3ac70481723b",
   "metadata": {},
   "source": [
    "#### Random lake policies\n",
    "\n",
    "- What are the observations in the random lake?\n",
    "- They are the player's location, represented as an integer from 0 to 15.  \n",
    "- As a refresher from module 1, a deterministic policy might look like this:\n",
    "\n",
    "| Observation | Action |\n",
    "|------|-------|\n",
    "| 0   |   0  |\n",
    "| 1   |   3  |\n",
    "| 2   |   1  |\n",
    "| 3   |   1  |\n",
    "| ... | ... |\n",
    "| 14 |  2  |\n",
    "| 15 | 2   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0d4278-ce03-4e94-9f2a-667b88bf1e85",
   "metadata": {},
   "source": [
    "#### Random lake policies\n",
    "\n",
    "And a non-deterministic policy might look like this:\n",
    "\n",
    "| Observation | P(left) | P(down) | P(right) | P(up) | \n",
    "|------------|-------|-----------|---------|-------|\n",
    "| 0   |   0  |  0.9 | 0.01      | 0.04      | 0.05\n",
    "| 1   |   3  |  0.05 | 0.05      | 0.05      | 0.85\n",
    "| ... | ... |  ... | ...      | ...      | ...\n",
    "| 15 | 2   |  0.0 | 0.0      | 0.99      | 0.01\n",
    "\n",
    "This does not mean RLlib learns such a table, by the way, but we can think of this table conceptually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b044dbcb-f7a2-4792-aaf2-2c72bda18af2",
   "metadata": {},
   "source": [
    "#### Random lake policies\n",
    "\n",
    "- In the random lake, our entire decision must be based on the player's position.\n",
    "- Sometimes this is sufficient: from position 11, you should go down.\n",
    "\n",
    "```\n",
    " 0   1   2   3\n",
    " 4   5   6   7\n",
    " 8   9  10  11\n",
    "12  13  14  15\n",
    "```\n",
    "\n",
    "- But what about position 5, what should you do from there?\n",
    "- Answer: _it depends_. If there's a hole at position 9, you don't want to go down. Likewise with 6. \n",
    "- How can I decide _without knowing where the holes are_?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fac00c2-6d9b-442f-ad3c-f9212a0dffac",
   "metadata": {},
   "source": [
    "#### State vs. observation, a recap\n",
    "\n",
    "- In Module 1, we defined the state informally as everything about the environment.\n",
    "- Here, that would include the location of the player and the holes.\n",
    "- The observation, on the other hand, only encodes part of the state: in this case, the location of the player."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbe12b1-eded-4618-a5dc-77dfd9fb75a0",
   "metadata": {},
   "source": [
    "#### Observation = state? Problem 1.\n",
    "\n",
    "- OK then, why not just set the observation to the state? \n",
    "- There are two issues here.\n",
    "- Problem 1: When the RL system is deployed, you may not know the whole state.\n",
    "  - Example: in a recommender system, the agent (recommender) does not have access to the user's mood (part of the state that affects outcomes)\n",
    "  - In supervised learning, we don't want to train on features that we won't have access to in deployment\n",
    "    - Likewise here, the observation needs to be something we can access in deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617197ea-fcfe-457e-a567-a1f876b0bb86",
   "metadata": {},
   "source": [
    "#### Observation = state? Problem 2.\n",
    "\n",
    "- Problem 2: It can be hard to generalize from a really complex observation.\n",
    "  - There are hundreds of thousands of possible states in just this small 4x4 random lake game.\n",
    "  - Too much information could be confusing to the agent or it could require unreasonable amounts of data (simulations) to make sense of."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6e363c-9255-442c-a598-1d33199b317b",
   "metadata": {},
   "source": [
    "#### Encoding observations\n",
    "\n",
    "- Part of our job as the RL practitioner is to pick a representation (or encoding) for the observation.\n",
    "- From the information the player allowed to know, find a useful representation of what the player needs to know.\n",
    "- In our case, we'll try one approach: the player gets to \"see\" whether the 4 spaces adjacent are holes or not.\n",
    "- We'll encode this as 4 binary numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb9fb21-9ad8-4800-ab56-fc70eec7b5ec",
   "metadata": {},
   "source": [
    "#### Encoding observations\n",
    "\n",
    "```\n",
    ".OO.\n",
    "....\n",
    "O.P.\n",
    "...G\n",
    "```\n",
    "\n",
    "- In this situation, there are no holes around the player, so the player \"sees\" `[0 0 0 0]`. \n",
    "- In other words, the observation here is `[0 0 0 0]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a2fb46-4e64-47d7-9a3c-2af76319760c",
   "metadata": {},
   "source": [
    "#### Encoding observations\n",
    "\n",
    "```\n",
    ".OO.\n",
    "..P.\n",
    "O.O.\n",
    "...G\n",
    "```\n",
    "\n",
    "- Here, the player \"sees\" holes up and down, so the observation is `[0 1 0 1]` (left, down, right, up)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114c2ad7-dc3d-45c8-9f59-46e55fd87e07",
   "metadata": {},
   "source": [
    "#### Encoding observations\n",
    "\n",
    "What about edges?\n",
    "\n",
    "```\n",
    "....\n",
    "..OP\n",
    "O.OO\n",
    "...G\n",
    "```\n",
    "\n",
    "- This is our choice as we design the observation space.\n",
    "- I'll choose to represent \"off the grid\" as holes, meaning we pretend the lake looks like this:\n",
    " \n",
    "```\n",
    "OOOOOO\n",
    "O....O\n",
    "O..OPO\n",
    "OO.OOO\n",
    "O...GO\n",
    "OOOOOO\n",
    "```\n",
    "\n",
    "- Here, the player sees holes left, down and right, so the observation is `[1 1 1 0]` (left, down, right, up)\n",
    "- There might be better approaches though, because falling into a hole is worse (episode ends) than walking off the edge (nothing happens)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415a2223-64cb-43ca-befb-2269d33f9639",
   "metadata": {},
   "source": [
    "#### Coding up our observations\n",
    "\n",
    "- Now that we have a plan, how do we modify the code?\n",
    "- Since we structured our class to have an `observation` method, that's all we need to modify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6046cff0-b83f-45b8-96ca-ab092b3c8599",
   "metadata": {},
   "outputs": [],
   "source": [
    "from envs_03 import RandomLake\n",
    "\n",
    "class RandomLakeObs(RandomLake):\n",
    "    def observation(self):\n",
    "        i, j = self.player\n",
    "\n",
    "        obs = []\n",
    "        obs.append(1 if j==0 else self.holes[i,j-1]) # left\n",
    "        obs.append(1 if i==3 else self.holes[i+1,j]) # down\n",
    "        obs.append(1 if j==3 else self.holes[i,j+1]) # right\n",
    "        obs.append(1 if i==0 else self.holes[i-1,j]) # up\n",
    "        \n",
    "        obs = np.array(obs, dtype=int) # cast to numpy array (optional)\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ee30d8-f9f9-4655-817d-84aebd8ca444",
   "metadata": {},
   "source": [
    "- The code creates an `obs` variable where each entry is 1 if that direction leads off the edge **or** a hole is present there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88f95238-ed09-4582-ab28-b9a85b48d992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53303bf5-5453-4798-92f3-618bb2774151",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Coding up our observations\n",
    "\n",
    "- One more code change is needed, which is the constructor where the observation space is defined.\n",
    "- Our observations were previously an integer from 0 to 15, so we used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0b2ebd3-1775-4bdf-9c93-c5af274dbc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space = gym.spaces.Discrete(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459308c9-a85a-4fcb-a8da-02a91401113f",
   "metadata": {},
   "source": [
    "And likewise for actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3cc9d30-715e-40a6-baa4-03496e91f9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = gym.spaces.Discrete(4)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53434076-3bb8-4fd1-9e8e-e23eb2be2808",
   "metadata": {},
   "source": [
    "- However, our observations are now arrays of 4 numbers rather than a single number.\n",
    "- To indicate this, we use `gym.spaces.MultiDiscrete` instead of `gym.spaces.Discrete`.\n",
    "- Multi, because we have multiple numbers, but still discrete, because each of the 4 numbers can only take on 2 possible values (0 or 1).\n",
    "- Here's the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b119b15e-4d45-4087-8895-0ef3c694f62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomLakeObs(RandomLake):\n",
    "    def __init__(self, env_config=None):\n",
    "        self.observation_space = gym.spaces.MultiDiscrete([2,2,2,2])\n",
    "        self.action_space = gym.spaces.Discrete(4)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47022e95-090c-494b-955e-7e4ccadf088e",
   "metadata": {},
   "source": [
    "(Note that `gym` also has a `MultiBinary` space type, but this is not currently supported by RLlib.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f5ebb5-6a8a-448e-a692-335ae56f4377",
   "metadata": {},
   "source": [
    "#### Testing out our new env\n",
    "\n",
    "Let's test it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "466d94ac-5b1b-4ce5-96ec-2b99b6849c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afccc03d-e0f8-4b8b-9be7-d13aad969084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from envs_03 import RandomLakeObs\n",
    "\n",
    "env = RandomLakeObs()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de7de150-479c-4a5c-98cf-4adf0abd378b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧑🧊🧊🧊\n",
      "🕳🕳🕳🧊\n",
      "🧊🧊🕳🧊\n",
      "🧊🧊🕳⛳️\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b6f296-1805-4467-8b34-fd61ebcdd732",
   "metadata": {},
   "source": [
    "Here, we see the expected observation indicating \"holes\" to the left, down, and up.\n",
    "\n",
    "Notes: \n",
    "\n",
    "The left and up are the map edges, and the down is an actual hole."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef2d1ac-4700-4dee-8aeb-23d09f18414f",
   "metadata": {},
   "source": [
    "#### Testing out our new env\n",
    "\n",
    "Let's try stepping right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db75ac7f-c4db-4e2e-8dbb-734fb0ff4ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 0, 1]), 0, False, {'player': (0, 1), 'goal': (3, 3)})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24287850-3c51-469c-86cd-17f2faa8cf3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧊🧑🧊🧊\n",
      "🕳🕳🕳🧊\n",
      "🧊🧊🕳🧊\n",
      "🧊🧊🕳⛳️\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bac1dc-f61f-41c5-85d5-095f880079e8",
   "metadata": {},
   "source": [
    "Now we see holes in the down and up directions, again as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3483468e-5524-4e8b-912e-ed2ab4e5a1ba",
   "metadata": {},
   "source": [
    "#### Training with our new observations\n",
    "\n",
    "- Our new observations seem to be working, but do they help the agent learn?\n",
    "- Recall that with our `Discrete(16)` observation space we were not able to get much more than a 30% success rate.\n",
    "- Let's try again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83267540-3d6c-43a1-a834-9fb258d0dd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "from utils_03 import lake_default_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "232e7a87-3c60-4ef3-91eb-e3b26db1e257",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo = lake_default_config.build(env=RandomLakeObs)\n",
    "\n",
    "for i in range(8):\n",
    "    ppo.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea82a7de-3556-4f7a-9e78-78ef708e3b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6420454545454546"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo.evaluate()[\"evaluation\"][\"episode_reward_mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c7214d-f48c-4f13-8bb1-a6de0fb92411",
   "metadata": {},
   "source": [
    "- This is way better than the ~30% we were getting before!\n",
    "- Which makes sense... our agent can \"see\" the holes now, instead of walking blindly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86425828-0bef-4b47-ad0e-29d8afaf465d",
   "metadata": {},
   "source": [
    "#### Let's apply what we learned!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4842178d-aa77-46b2-9da3-7deaea3d40f9",
   "metadata": {},
   "source": [
    "## Supervised learning analogy: observation space\n",
    "<!-- multiple choice -->\n",
    "\n",
    "In the slides we changed the observation space for our agent and, as a result, achieved higher rewards. What aspect of the supervised learning process is this most analogous to?\n",
    "\n",
    "- [x] Feature engineering | You got it! Our observation space acts as the feature space for our policy to act on.\n",
    "- [ ] Model selection | Not quite. But, as we'll see, there is a place for model selection in RL as well!\n",
    "- [ ] Hyperparameter tuning | Not quite. But, as we'll see, there is a place for hyperparameter tuning in RL as well!\n",
    "- [ ] Selecting a loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6438bf5-8970-4b51-a1b4-011f067d8903",
   "metadata": {},
   "source": [
    "## Including the player's location\n",
    "<!-- multiple choice -->\n",
    "\n",
    "In our new observation representation we actually _removed_ the player's location from the observation and _only_ include the presence of nearby holes. If we wanted an observation space that included both the nearby walls _and_ the player's location, which of the following gym spaces could we use?\n",
    "\n",
    "- [ ] `gym.spaces.Discrete(5)` | Try again!\n",
    "- [x] `gym.spaces.MultiDiscrete([2,2,2,2,16])` | Yes! The first 4 numbers represent the holes, and the last number represents the player's location.\n",
    "- [ ] `gym.spaces.MultiDiscrete([2,2,2,2]) + gym.spaces.Discrete(16)` | Try again; unfortunately we can't add gym spaces.\n",
    "- [ ] `gym.spaces.MultiDiscrete([32,32,32,32])` | This could be made to work, but is a confusing/redundant representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03e0f1b-e397-467a-8f62-8537173201c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Handling the edges\n",
    "<!-- multiple choice -->\n",
    "\n",
    "In the slides we decided to treat edges as holes. Recall this picture:\n",
    "\n",
    "```\n",
    "OOOOOO\n",
    "O....O\n",
    "O..OPO\n",
    "OO.OOO\n",
    "O...GO\n",
    "OOOOOO\n",
    "```\n",
    "\n",
    "However, edges and holes are actually different from each other: walking into an edge does nothing, whereas walking into a hole causes the episode to end. This might be an important distinction, especially in a \"slippery\" version of the environment where the results of actions are non-deterministic. \n",
    "\n",
    "To address this issue, we decide to change the observation space. The agent still only \"sees\" the four squares around it, but now it sees whether each square is an empty space, hole, or edge. For this representation, which of the following gym observation spaces could we use?\n",
    "\n",
    "- [ ] `gym.spaces.MultiDiscrete([2,2,2,2,2,2,2,2])` | Try again. Remember, the agent still only \"sees\" 4 squares.\n",
    "- [ ] `gym.spaces.MultiDiscrete([3,3,3,3,3,3,3,3])` | Try again!\n",
    "- [ ] `gym.spaces.MultiDiscrete([2,2,2,2])` | This is the same as the previous space, but we've made a change.\n",
    "- [x] `gym.spaces.MultiDiscrete([3,3,3,3])` | You got it! There are now 3 possible options for what the agent can \"see\" at each square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "485875e4-6694-4330-838c-1a54ed0499de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO / note to self\n",
    "# query_policy(trainer, RandomLakeObs(), [1,1,1,1])\n",
    "# shows that it wants to go up. this is because the above \"hole\" is probably an edge based on its learning. fascinating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c89bbb1-d5b4-41ef-bdd9-1f9ef08aa1eb",
   "metadata": {},
   "source": [
    "## Implementing the edges\n",
    "<!-- coding exercise -->\n",
    "\n",
    "The code below shows the `observation` function for the current observation space. Modify the code so that it uses the new observation space, where 0 represents an empty space, 1 represents a hole, and 2 represents and edge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cf9e9c-550c-43cb-8725-aa60d0cb91ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "import numpy as np\n",
    "from envs_03 import RandomLake, RandomLakeObs\n",
    "\n",
    "class RandomLakeObs2(RandomLakeObs):\n",
    "    def observation(self):\n",
    "        i, j = self.player\n",
    "\n",
    "        obs = []\n",
    "        obs.append(1 if j==0 else self.holes[i,j-1]) # left\n",
    "        obs.append(1 if i==3 else self.holes[i+1,j]) # down\n",
    "        obs.append(1 if j==3 else self.holes[i,j+1]) # right\n",
    "        obs.append(1 if i==0 else self.holes[i-1,j]) # up\n",
    "        \n",
    "        obs = np.array(obs, dtype=int) # cast to numpy array\n",
    "        return obs\n",
    "\n",
    "np.random.seed(42)\n",
    "env = RandomLakeObs2()\n",
    "obs = env.reset()\n",
    "env.render()\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06328a38-f19c-4c71-82b6-31e056f28f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧑🧊🧊🧊\n",
      "🕳🕳🕳🧊\n",
      "🧊🧊🕳🧊\n",
      "🧊🧊🕳⛳️\n",
      "[2 1 0 2]\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "import numpy as np\n",
    "from envs_03 import RandomLake, RandomLakeObs\n",
    "\n",
    "class RandomLakeObs2(RandomLakeObs):\n",
    "    def observation(self):\n",
    "        i, j = self.player\n",
    "\n",
    "        obs = []\n",
    "        obs.append(2 if j==0 else self.holes[i,j-1]) # left\n",
    "        obs.append(2 if i==3 else self.holes[i+1,j]) # down\n",
    "        obs.append(2 if j==3 else self.holes[i,j+1]) # right\n",
    "        obs.append(2 if i==0 else self.holes[i-1,j]) # up\n",
    "        \n",
    "        obs = np.array(obs, dtype=int) # cast to numpy array\n",
    "        return obs\n",
    "\n",
    "np.random.seed(42)\n",
    "env = RandomLakeObs2()\n",
    "obs = env.reset()\n",
    "env.render()\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0f55b9-a660-4a04-8caf-52d13ced50e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## What the agent sees\n",
    "<!-- coding exercise -->\n",
    "\n",
    "With our new observation space encoding, the agent only \"sees\" the 4 spaces around it and only has this information available to make its decisions. The code cell below creates a rendering of what the agent \"sees\" while navigating the random lake. You can enter actions with the keyboard by typing in the words \"left\", \"down\", \"right\" or \"up\" (or \"l\", \"d\", \"r\", \"u\" for short) and the simulation will show you the result. (Type \"quit\" to exit.) Play the game until you reach the goal. As you go, try to map out the lake (perhaps by drawing on a piece of paper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d5b24c4-0df7-47e3-8155-895ccbb9e5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO / NOTE:\n",
    "# THIS EXERCISE DOES NOT HAVE A \"solution\"\n",
    "# the code is here ONLY to help them answer the multiple choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8341e39f-8684-4106-94cc-a4a422f476b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation:\n",
      ".O.\n",
      "OP.\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE\n",
    "\n",
    "import numpy as np\n",
    "from envs_03 import RandomLakeObs\n",
    "\n",
    "actions = {\"left\" : 0, \"down\" : 1, \"right\" : 2, \"up\" : 3, \n",
    "           \"l\" : 0, \"d\" : 1, \"r\" : 2, \"u\" : 3}\n",
    "\n",
    "np.random.seed(45)\n",
    "env = RandomLakeObs()\n",
    "obs = env.reset()\n",
    "\n",
    "act = \"start\"\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "   \n",
    "    obs_print = [['.']*3 for i in range(3)]\n",
    "    obs_print[1][1] = \"P\"\n",
    "    if obs[0]:\n",
    "        obs_print[1][0] = \"O\"\n",
    "    if obs[1]:\n",
    "        obs_print[2][1] = \"O\"\n",
    "    if obs[2]:\n",
    "        obs_print[1][2] = \"O\"\n",
    "    if obs[3]:\n",
    "        obs_print[0][1] = \"O\"\n",
    "    print(\"Observation:\")\n",
    "    print(\"\\n\".join(list(map(lambda c: \"\".join(c), obs_print))))\n",
    "    print()\n",
    "    \n",
    "    while act != \"quit\" and act not in actions: \n",
    "        act = input() # gather keyboard input \n",
    "    \n",
    "    if act == \"quit\":\n",
    "        break\n",
    "        \n",
    "    obs, rew, done, _ = env.step(act)\n",
    "    \n",
    "if done:\n",
    "    if rew > 0:\n",
    "        print(\"You win! +1 reward 🎉\")\n",
    "    else:\n",
    "        print(\"You fell into the lake 😢\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c8469b-8040-44db-82cd-174ca76b82c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "import numpy as np\n",
    "from envs_03 import RandomLakeObs\n",
    "\n",
    "actions = {\"left\" : 0, \"down\" : 1, \"right\" : 2, \"up\" : 3, \n",
    "           \"l\" : 0, \"d\" : 1, \"r\" : 2, \"u\" : 3}\n",
    "\n",
    "np.random.seed(45)\n",
    "env = RandomLakeObs()\n",
    "obs = env.reset()\n",
    "\n",
    "act = \"start\"\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "   \n",
    "    obs_print = [['.']*3 for i in range(3)]\n",
    "    obs_print[1][1] = \"P\"\n",
    "    if obs[0]:\n",
    "        obs_print[1][0] = \"O\"\n",
    "    if obs[1]:\n",
    "        obs_print[2][1] = \"O\"\n",
    "    if obs[2]:\n",
    "        obs_print[1][2] = \"O\"\n",
    "    if obs[3]:\n",
    "        obs_print[0][1] = \"O\"\n",
    "    print(\"Observation:\")\n",
    "    print(\"\\n\".join(list(map(lambda c: \"\".join(c), obs_print))))\n",
    "    print()\n",
    "    \n",
    "    while act != \"quit\" and act not in actions: \n",
    "        act = input() # gather keyboard input \n",
    "    \n",
    "    if act == \"quit\":\n",
    "        break\n",
    "        \n",
    "    obs, rew, done, _ = env.step(act)\n",
    "    \n",
    "if done:\n",
    "    if rew > 0:\n",
    "        print(\"You win! +1 reward 🎉\")\n",
    "    else:\n",
    "        print(\"You fell into the lake 😢\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7024e752-4e71-4162-886e-c5be693c9ee5",
   "metadata": {},
   "source": [
    "#### What does the lake look like?\n",
    "\n",
    "Based on your explorations, which is the correct map of the lake in the above question?\n",
    "\n",
    "```\n",
    " (A)      (B)      (C)      (D)\n",
    "P..O     P.OO     P..O     P.OO\n",
    "..OO     .OOO     ..OO     .OOO\n",
    "O...     O...     O...     O..O\n",
    "...G     ...G     ..OG     ...G\n",
    "```\n",
    "\n",
    "- [x] (A)\n",
    "- [ ] (B)\n",
    "- [ ] (C)\n",
    "- [ ] (D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbad4252-94ef-480a-938d-6e4858ed3675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# could also considering showing a BAD environment encoding to contrast with this reasonable one, as in the next slide deck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ray200]",
   "language": "python",
   "name": "conda-env-ray200-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
