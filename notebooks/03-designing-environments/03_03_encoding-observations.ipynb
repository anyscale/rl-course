{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e23df55-837b-4ec9-86ad-3b60c6a7f00d",
   "metadata": {},
   "source": [
    "## Encoding Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95a9faf-9b2c-45ec-a347-3aadca399516",
   "metadata": {},
   "source": [
    "#### Review: what is a policy?\n",
    "\n",
    "- In RL we're trying to learn a policy, what is this again, exactly?\n",
    "- A policy maps **observations** to **actions**.\n",
    "- In other words, the observations are all the policy \"sees\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacebed5-729c-4ea8-8570-3ac70481723b",
   "metadata": {},
   "source": [
    "#### Random lake policies\n",
    "\n",
    "- What are the observations in the random lake?\n",
    "- They are the player's location, represented as an integer from 0 to 15.  \n",
    "- As a refresher from module 1, a deterministic policy might look like this:\n",
    "\n",
    "| Observation | Action |\n",
    "|------|-------|\n",
    "| 0   |   0  |\n",
    "| 1   |   3  |\n",
    "| 2   |   1  |\n",
    "| 3   |   1  |\n",
    "| ... | ... |\n",
    "| 14 |  2  |\n",
    "| 15 | 2   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0d4278-ce03-4e94-9f2a-667b88bf1e85",
   "metadata": {},
   "source": [
    "#### Random lake policies\n",
    "\n",
    "And a non-deterministic policy might look like this:\n",
    "\n",
    "| Observation | P(left) | P(down) | P(right) | P(up) | \n",
    "|------------|-------|-----------|---------|-------|\n",
    "| 0   |   0  |  0.9 | 0.01      | 0.04      | 0.05\n",
    "| 1   |   3  |  0.05 | 0.05      | 0.05      | 0.85\n",
    "| ... | ... |  ... | ...      | ...      | ...\n",
    "| 15 | 2   |  0.0 | 0.0      | 0.99      | 0.01\n",
    "\n",
    "This does not mean RLlib learns such a table, by the way, but we can think this way conceptually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b044dbcb-f7a2-4792-aaf2-2c72bda18af2",
   "metadata": {},
   "source": [
    "#### Random lake policies\n",
    "\n",
    "- In the random lake, our entire decision must be based on the player's position.\n",
    "- Sometimes this is sufficient: from position 11, you should go down.\n",
    "\n",
    "```\n",
    " 0   1   2   3\n",
    " 4   5   6   7\n",
    " 8   9  10  11\n",
    "12  13  14  15\n",
    "```\n",
    "\n",
    "- But what about position 5, what should you do from there?\n",
    "- Answer: _it depends_. How can I decide _without knowing where the holes are_?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fac00c2-6d9b-442f-ad3c-f9212a0dffac",
   "metadata": {},
   "source": [
    "#### State vs. observation, a recap\n",
    "\n",
    "- In Module 1, we defined the state informally as everything about the environment.\n",
    "- Here, that would include the location of the player and the holes.\n",
    "- The observation, on the other hand, only encodes part of the state.\n",
    "- In this case, the location of the player."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbe12b1-eded-4618-a5dc-77dfd9fb75a0",
   "metadata": {},
   "source": [
    "#### Observation = state? Problem 1.\n",
    "\n",
    "- OK then, why not just set the observation to the state? \n",
    "- There are two separate issues here.\n",
    "- Problem 1: You may want an agent that _learns where the holes are_.\n",
    "  - In reality, you may not have access to this information.\n",
    "  - Example: in a recommender system, the agent (recommender) does not have access to the user's mood (part of the state)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617197ea-fcfe-457e-a567-a1f876b0bb86",
   "metadata": {},
   "source": [
    "#### Observation = state? Problem 2.\n",
    "\n",
    "- Problem 2: It can be hard to generalize from a really complex observation.\n",
    "  - There are hundreds of thousands of possible states in just this small 4x4 random lake game.\n",
    "  - Too much information could be confusing to the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6e363c-9255-442c-a598-1d33199b317b",
   "metadata": {},
   "source": [
    "#### Encoding observations\n",
    "\n",
    "- Part of our job as the RL practitioner is to pick a representation (or encoding) for the observation.\n",
    "- What is the player allowed to know, and what does the player need to know?\n",
    "- In our case, we'll try one approach: the player gets to \"see\" whether the 4 spaces adjacent are holes or not.\n",
    "- We'll encode this as 4 binary numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb9fb21-9ad8-4800-ab56-fc70eec7b5ec",
   "metadata": {},
   "source": [
    "#### Encoding observations\n",
    "\n",
    "```\n",
    ".OO.\n",
    "....\n",
    "O.P.\n",
    "...G\n",
    "```\n",
    "\n",
    "- In this situation, there are no holes around the player, so the player \"sees\" `[0 0 0 0]`. \n",
    "- In other words, the observation here is `[0 0 0 0]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a2fb46-4e64-47d7-9a3c-2af76319760c",
   "metadata": {},
   "source": [
    "#### Encoding observations\n",
    "\n",
    "```\n",
    ".OO.\n",
    "..P.\n",
    "O.O.\n",
    "...G\n",
    "```\n",
    "\n",
    "- Here, the player \"sees\" holes up and down, so the observation is `[0 1 0 1]` (left, down, right, up)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114c2ad7-dc3d-45c8-9f59-46e55fd87e07",
   "metadata": {},
   "source": [
    "#### Encoding observations\n",
    "\n",
    "What about edges?\n",
    "\n",
    "```\n",
    "....\n",
    "..OP\n",
    "O.OO\n",
    "...G\n",
    "```\n",
    "\n",
    "- This is our choice as we design the observation space.\n",
    "- I'll choose to represent \"off the grid\" as holes, meaning we pretend the lake looks like this:\n",
    " \n",
    "```\n",
    "OOOOOO\n",
    "O....O\n",
    "O..OPO\n",
    "OO.OOO\n",
    "O...GO\n",
    "OOOOOO\n",
    "```\n",
    "\n",
    "- There might be better approaches though, because falling into a hole is worse (episode ends) than walking off the edge (nothing happens).\n",
    "- Here, the player sees holes left, down and right, so the observation is `[1 1 1 0]` (left, down, right, up)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415a2223-64cb-43ca-befb-2269d33f9639",
   "metadata": {},
   "source": [
    "#### Coding up our observations\n",
    "\n",
    "- Now that we have a plan, how do we modify the code?\n",
    "- Since we structured our class to have an `observation` method, that's all we need to modify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6046cff0-b83f-45b8-96ca-ab092b3c8599",
   "metadata": {},
   "outputs": [],
   "source": [
    "from envs import RandomLake\n",
    "\n",
    "class RandomLakeObs(RandomLake):\n",
    "    def observation(self):\n",
    "        i, j = self.player\n",
    "\n",
    "        obs = []\n",
    "        obs.append(1 if j==0 else self.holes[i,j-1]) # left\n",
    "        obs.append(1 if i==3 else self.holes[i+1,j]) # down\n",
    "        obs.append(1 if j==3 else self.holes[i,j+1]) # right\n",
    "        obs.append(1 if i==0 else self.holes[i-1,j]) # up\n",
    "        \n",
    "        obs = np.array(obs, dtype=int) # cast to numpy array (optional)\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ee30d8-f9f9-4655-817d-84aebd8ca444",
   "metadata": {},
   "source": [
    "- The code creates an `obs` variable where each entry is 1 if that direction leads off the edge **or** a hole is present there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88f95238-ed09-4582-ab28-b9a85b48d992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53303bf5-5453-4798-92f3-618bb2774151",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Coding up our observations\n",
    "\n",
    "- One more code change is needed, which is the constructor where the observation space is defined.\n",
    "- Our observations were previously an integer from 0 to 15, so we used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0b2ebd3-1775-4bdf-9c93-c5af274dbc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space = gym.spaces.Discrete(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459308c9-a85a-4fcb-a8da-02a91401113f",
   "metadata": {},
   "source": [
    "And likewise for actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3cc9d30-715e-40a6-baa4-03496e91f9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = gym.spaces.Discrete(4)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53434076-3bb8-4fd1-9e8e-e23eb2be2808",
   "metadata": {},
   "source": [
    "- However, our observations are now arrays of 4 numbers rather than a single number.\n",
    "- To indicate this, we use `gym.spaces.MultiDiscrete` instead of `gym.spaces.Discrete`.\n",
    "- Multi, because we have multiple numbers, but still discrete, because each of the 4 numbers can only take on 2 possible values (0 or 1).\n",
    "- Here's the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b119b15e-4d45-4087-8895-0ef3c694f62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomLakeObs(RandomLake):\n",
    "    def __init__(self, env_config=None):\n",
    "        self.observation_space = gym.spaces.MultiDiscrete([2,2,2,2])\n",
    "        self.action_space = gym.spaces.Discrete(4)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47022e95-090c-494b-955e-7e4ccadf088e",
   "metadata": {},
   "source": [
    "(Note that `gym` also has a `MultiBinary` space type, but this is not currently supported by RLlib.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f5ebb5-6a8a-448e-a692-335ae56f4377",
   "metadata": {},
   "source": [
    "#### Testing out our new env\n",
    "\n",
    "Let's test it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "466d94ac-5b1b-4ce5-96ec-2b99b6849c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "afccc03d-e0f8-4b8b-9be7-d13aad969084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from envs import RandomLakeObs\n",
    "\n",
    "env = RandomLakeObs()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de7de150-479c-4a5c-98cf-4adf0abd378b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P...\n",
      "OOO.\n",
      "..O.\n",
      "..OG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b6f296-1805-4467-8b34-fd61ebcdd732",
   "metadata": {},
   "source": [
    "Here, we see the expected observation indicating \"holes\" to the left, down, and right."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef2d1ac-4700-4dee-8aeb-23d09f18414f",
   "metadata": {},
   "source": [
    "#### Testing out our new env\n",
    "\n",
    "Let's try stepping right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "db75ac7f-c4db-4e2e-8dbb-734fb0ff4ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 0, 1]), 0, False, {})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "24287850-3c51-469c-86cd-17f2faa8cf3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".P..\n",
      "OOO.\n",
      "..O.\n",
      "..OG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bac1dc-f61f-41c5-85d5-095f880079e8",
   "metadata": {},
   "source": [
    "Now we see holes in the down and up directions, again as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3483468e-5524-4e8b-912e-ed2ab4e5a1ba",
   "metadata": {},
   "source": [
    "#### Training with our new observations\n",
    "\n",
    "- Our new observations seem to be working, but do they help the agent learn?\n",
    "- Recall that with our `Discrete(16)` observation space we were not able to get much more than a 30% success rate.\n",
    "- Let's try again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "83267540-3d6c-43a1-a834-9fb258d0dd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "\n",
    "from ray.rllib.agents.ppo import PPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "232e7a87-3c60-4ef3-91eb-e3b26db1e257",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = PPOTrainer({\"framework\" : \"torch\", \"create_env_on_driver\" : True, \"seed\" : 0}, \n",
    "                       env=RandomLakeObs)\n",
    "\n",
    "for i in range(8):\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea82a7de-3556-4f7a-9e78-78ef708e3b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7383720930232558\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(trainer.evaluate()['evaluation']['hist_stats']['episode_reward']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c7214d-f48c-4f13-8bb1-a6de0fb92411",
   "metadata": {},
   "source": [
    "- Wow, this is way better!\n",
    "- Which makes sense... our agent can \"see\" the holes now, instead of walking blindly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4842178d-aa77-46b2-9da3-7deaea3d40f9",
   "metadata": {},
   "source": [
    "## Supervised learning analogy\n",
    "\n",
    "In the slides we changed the observation space for our agent and, as a result, achieved higher rewards. What aspect of the supervised learning process is this most analogous to?\n",
    "\n",
    "- [x] Feature engineering | You got it! Our observation space acts as the feature space for our policy to act on.\n",
    "- [ ] Model selection | Not quite. But, as we'll see, there is a place for model selection in RL as well!\n",
    "- [ ] Hyperparameter tuning | Not quite. But, as we'll see, there is a place for hyperparameter tuning in RL as well!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7952f975-7bf8-4231-962f-6e343c43fed0",
   "metadata": {},
   "source": [
    "## Why does this work so well?\n",
    "\n",
    "An important property of a successful lake-navigator is that it tries to avoid holes. We have tested out two observation encodings:\n",
    "\n",
    "1. The player's location only, `Discrete(16)`. Let's call this PlayerLoc.\n",
    "2. The nearby holes, `MultiDiscrete([2,2,2,2])`. Let's call this HolesLoc.\n",
    "\n",
    "With the PlayerLoc observations, the agent will always perform the same action (or sample from the same probability distribution over actions, in the case of a stochastic policy) when it's at the same location in the lake. With the HolesLoc observations, on the other hand, the agent may behave differently at a given location, depending on where they nearby holes are. \n",
    "\n",
    "#### Frozen pond\n",
    "\n",
    "Let's imagine we're back in the Frozen Pond; that is, the holes are not randomized, but rather always in the same places. In this scenario, \n",
    "\n",
    "#### Random lake\n",
    "\n",
    "\n",
    "```\n",
    "P...\n",
    ".O.O\n",
    "...O\n",
    "O..G\n",
    "```\n",
    "\n",
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e205968-c3cd-4978-a25e-eb23873e7fd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6438bf5-8970-4b51-a1b4-011f067d8903",
   "metadata": {},
   "source": [
    "## Including the player's location\n",
    "\n",
    "In our new observation representation we actually _removed_ the player's location from the observation and _only_ include the presence of nearby holes. If we wanted an observation space that included both the nearby walls _and_ the player's location, which of the following gym spaces could we use?\n",
    "\n",
    "- [ ] `gym.spaces.Discrete(5)` | Try again!\n",
    "- [x] `gym.spaces.MultiDiscrete([2,2,2,2,16])` | Yes! The first 4 numbers represent the holes, and the last number represents the player's location.\n",
    "- [ ] `gym.spaces.MultiDiscrete([2,2,2,2]) + gym.spaces.Discrete(16)` | Try again; unfortunately we can't add gym spaces.\n",
    "- [ ] `gym.spaces.MultiDiscrete([32,32,32,32])` | This could be made to work, but is a confusing/redundant representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dc1f29-8ac3-4d48-b600-7401016ea1be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a923d3b-74a5-4a8d-a368-54b6b9ce8027",
   "metadata": {},
   "source": [
    "coding exercise - have them look at the agents and see how they behave in different scenarios, the animations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4493ba-7992-4bfd-a8fb-8a0443f1e6e0",
   "metadata": {},
   "source": [
    "exercise - show how the old agent did the same thing only depending on location, whereas the new agent does different things at the same location depending on walls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24051db-2f89-4ca0-98c2-af141aa718eb",
   "metadata": {},
   "source": [
    "LOTS of good exercises here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0f55b9-a660-4a04-8caf-52d13ced50e3",
   "metadata": {},
   "source": [
    "## What the agent sees\n",
    "\n",
    "With our new observation space encoding, the agent only \"sees\" the 4 spaces around it and only has this information available to make its decisions. \n",
    "\n",
    "could do an exercise where we show what the agent sees, like a rendering like this\n",
    "\n",
    "```\n",
    " O\n",
    ".P.\n",
    " O\n",
    "```\n",
    "\n",
    "and the player uses keyboard inputs to play the game a bit??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8341e39f-8684-4106-94cc-a4a422f476b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation:\n",
      ".O.\n",
      "OP.\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " left\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation:\n",
      ".O.\n",
      "OP.\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " down\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation:\n",
      "...\n",
      "OP.\n",
      ".O.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " down\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You fell into the lake ðŸ˜¢\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from envs import RandomLakeObs\n",
    "\n",
    "actions = {\"left\" : 0, \"down\" : 1, \"right\" : 2, \"up\" : 3, \n",
    "           \"l\" : 0, \"d\" : 1, \"r\" : 2, \"u\" : 3}\n",
    "\n",
    "np.random.seed(45)\n",
    "env = RandomLakeObs()\n",
    "obs = env.reset()\n",
    "\n",
    "act = \"start\"\n",
    "done = False\n",
    "\n",
    "while act != \"quit\" and not done:\n",
    "   \n",
    "    obs_print = [['.']*3 for i in range(3)]\n",
    "    obs_print[1][1] = \"P\"\n",
    "    if obs[0]:\n",
    "        obs_print[1][0] = \"O\"\n",
    "    if obs[1]:\n",
    "        obs_print[2][1] = \"O\"\n",
    "    if obs[2]:\n",
    "        obs_print[1][2] = \"O\"\n",
    "    if obs[3]:\n",
    "        obs_print[0][1] = \"O\"\n",
    "    print(\"Observation:\")\n",
    "    print(\"\\n\".join(list(map(lambda c: \"\".join(c), obs_print))))\n",
    "    print()\n",
    "    \n",
    "    while True: # gather keyboard input \n",
    "        act = input()\n",
    "        if act in actions:\n",
    "            act = actions[act]\n",
    "            break\n",
    "    obs, rew, done, _ = env.step(act)\n",
    "    \n",
    "if done:\n",
    "    if rew > 0:\n",
    "        print(\"You win! +1 reward ðŸŽ‰\")\n",
    "    else:\n",
    "        print(\"You fell into the lake ðŸ˜¢\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rl-course]",
   "language": "python",
   "name": "conda-env-rl-course-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
