{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90c01ab9-1cd0-47d8-a198-e07e36e2c1fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## RL Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddd909a-c86f-4e5c-9ba0-8b0ef1152730",
   "metadata": {},
   "source": [
    "#### What is a policy?\n",
    "\n",
    "Let's return to the \"API\" of RL:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50604c8f-7c62-495e-b9c6-f3b408a440d9",
   "metadata": {},
   "source": [
    "![](img/RL-API.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ac6598-7965-4496-ac1e-6b71cc7e487b",
   "metadata": {},
   "source": [
    "- The policy is the output of RL\n",
    "- It maps observations to actions\n",
    "- The policy is like the agent's brain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a207992-104b-4ecd-b470-341860f09ad6",
   "metadata": {},
   "source": [
    "#### What is a policy? Details.\n",
    "\n",
    "- A policy is a function mapping observations to actions.\n",
    "- Let's consider the Frozen Lake environment again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2a869f8-2fb6-4ef5-bd1c-d8c74493e93c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
    "obs = env.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbfda97-e12c-4284-b0b2-78a8f21c7ede",
   "metadata": {},
   "source": [
    "- We received an observation 0. What do we do next? \n",
    "- The policy will tell us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a98f1c-a7b6-40bc-9d18-dce492473ea9",
   "metadata": {},
   "source": [
    "#### Example policy\n",
    "\n",
    "A policy might look like this\n",
    "\n",
    "| Observation | Action |\n",
    "|------|-------|\n",
    "| 0   |   0  |\n",
    "| 1   |   3  |\n",
    "| 2   |   1  |\n",
    "| 3   |   1  |\n",
    "| ... | ... |\n",
    "| 14 |  2  |\n",
    "| 15 | 2   |\n",
    "\n",
    "- On the left, we have all possible observations (15 for Frozen Lake).\n",
    "- On the right, we have the corresponding action we will take _if we see that observation_.\n",
    "- \"If I see 0, I will do 0; if I see 1, I will do 3,\" etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c510ca5c-a9d5-44e3-9afb-c4a15979f874",
   "metadata": {},
   "source": [
    "#### Goal of RL\n",
    "\n",
    "**The goal of RL is to learn a good policy given an environment.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dac4894-b44a-42bc-8269-54f8a7d93c8e",
   "metadata": {},
   "source": [
    "#### Non-deterministic policies\n",
    "\n",
    "- Previously we learned about deterministic and non-deterministic environments. \n",
    "- Analogously, we have deterministic and non-deterministic _policies_.\n",
    "- Before we saw a deterministic policy: a given observation elicits a fixed action.\n",
    "- Here is an example non-deterministic policy:\n",
    "\n",
    "| Observation | P(left) | P(down) | P(right) | P(up) | \n",
    "|------------|-------|-----------|---------|-------|\n",
    "| 0   |   0  |  0.9 | 0.01      | 0.04      | 0.05\n",
    "| 1   |   3  |  0.05 | 0.05      | 0.05      | 0.85\n",
    "| ... | ... |  ... | ...      | ...      | ...\n",
    "| 15 | 2   |  0.0 | 0.0      | 0.99      | 0.01\n",
    "\n",
    "\"If I see 0, I will move left 99% of the time, down 1% of the time, right 4% of the time, and up 5% of the time.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34003758-53d7-4832-8e82-f805d723db9c",
   "metadata": {},
   "source": [
    "#### Continuous action spaces\n",
    "\n",
    "What if our action space is continuous? We can still have a policy. For example:\n",
    "\n",
    "| Observation | Action |\n",
    "|------|-------|\n",
    "| 0   |   0.42  |\n",
    "| 1   |   -3.99  |\n",
    "| ... | ... |\n",
    "| 15 | 2.24   |\n",
    "\n",
    "A non-deterministic policy would need to draw from a probability distribution, though."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8212fc5-3270-49b9-8e82-44e04ebc892c",
   "metadata": {},
   "source": [
    "#### Continuous observation spaces\n",
    "\n",
    "- What if our _observation_ space is continuous? \n",
    "- Well, now we can't draw the policy as a table anymore...\n",
    "- In this case, our policy is a _function_ of the observation value. \n",
    "- E.g. \"gas pedal angle with floor (action) = 1.5 x distance to closest obstacle (observation)\" \n",
    "- This toy example says that if the nearest obstacle is far away, you can speed up the car."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed8dfeec-6879-4d40-9c9e-b28e4bb260b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# too much text around here, need more code and/or images\n",
    "# I think we can remove some of this stuff (continuous, beyond scalars) and add it back it when it's paired with a concrete example\n",
    "# it's not very helpful/interesting as just ideas in isolation..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0043038d-d6f4-4364-b348-e968fdb37d3a",
   "metadata": {},
   "source": [
    "#### Beyond scalars\n",
    "\n",
    "- So far we've assumed the observation is a single number and the action is a single number.\n",
    "- However, both of these can be more complex data types: images, vectors, etc. \n",
    "- The actual observations of a self-driving car may be dozens of measurements, images, etc.\n",
    "- The actual actions of a self-driving car may be setting multiple values at each time step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dc8172-ae79-405f-801f-a2b750a62e31",
   "metadata": {},
   "source": [
    "#### Thinking about policies as functions\n",
    "\n",
    "- In general, this is a useful way of thinking: the policy is a function that maps observations to actions.\n",
    "- In **deep reinforcement learning**, this function is a neural network.\n",
    "- A bit more on this later!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a75dce-81a4-4ff6-923c-d5e0b736384c",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "\n",
    "- The \"agent\" or \"player\" are personifications of the policy\n",
    "- There is no additional \"intelligence\" or decision-making beyond the policy\n",
    "- Therefore, we don't technically need the notion of an agent/player\n",
    "- The policy is the output of RL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7210fa7d-3ebb-4906-83cb-853e8f4d91d3",
   "metadata": {},
   "source": [
    "#### Let's apply what we learned!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5780af6f-fa34-4b85-af30-32e20755f1c0",
   "metadata": {},
   "source": [
    "## Frozen Lake policy\n",
    "<!-- multiple choice -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a299f97-00fa-4e31-ad66-5e843b63e1d1",
   "metadata": {},
   "source": [
    "Recall the frozen lake environment:\n",
    "\n",
    "```\n",
    "üßëüßäüßäüßä\n",
    "üßäüï≥üßäüï≥\n",
    "üßäüßäüßäüï≥\n",
    "üï≥üßäüßä‚õ≥Ô∏è\n",
    "```\n",
    "\n",
    "with its observation space represented as:\n",
    "\n",
    "```\n",
    " 0   1   2   3\n",
    " 4   5   6   7\n",
    " 8   9  10  11\n",
    "12  13  14  15\n",
    "```\n",
    "\n",
    "and actions represented as\n",
    "\n",
    "| Action     |  Meaning    |\n",
    "|------|------|\n",
    "| 0 | left |\n",
    "| 1 | down |\n",
    "| 2 | right |\n",
    "| 3 | up |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8eca95c-cc54-42d3-a442-f41fb0c27b5e",
   "metadata": {},
   "source": [
    "#### Question 1\n",
    "\n",
    "The policy below contains a missing entry represented by a `?` symbol.\n",
    "\n",
    "| Observation | Action |\n",
    "|------|-------|\n",
    "| 0   |   0  |\n",
    "| 1   |   2  |\n",
    "| ... | ... |\n",
    "| 13 |  ? |\n",
    "| 14 | 2 |\n",
    "| 15 | 0   |\n",
    "\n",
    "Select the best choice to fill the `?` entry.\n",
    "\n",
    "- [ ] 0  | Try again!\n",
    "- [ ] 1 | Try again!\n",
    "- [x] 2 | Yes! Moving to the right takes you toward the goal.\n",
    "- [ ] 3 | Try again!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c41f87-143b-409f-a16c-b8b1ac4b0cc7",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Question 2\n",
    "\n",
    "_In the slippery version of the Frozen Lake, the agent has a 1/3 probability of moving in the intended direction and 1/3 each in the two perpendicular directions._\n",
    "\n",
    "Is the above a statement about the environment or the policy?\n",
    "\n",
    "- [x] Environment | You got it! \n",
    "- [ ] Policy | Remember, the policy describes how the agent responds to observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefe9bb5-2162-44df-9559-e98566491dff",
   "metadata": {},
   "source": [
    "#### Question 3\n",
    "\n",
    "_In the slippery version of the Frozen Lake, it is sometimes better not to walk in the direction you really want to go, because it's more important to avoid the chance of slipping into a hole._\n",
    "\n",
    "Is the above a statement about the environment or the policy?\n",
    "\n",
    "- [ ] Environment | The statement above is about the best action to take in a situation; this is determined by the policy.\n",
    "- [x] Policy | You got it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948f613a-9179-4965-9138-53ae1f7040a4",
   "metadata": {},
   "source": [
    "## Expected reward\n",
    "<!-- coding exercise -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2b5b57-e0e8-40c6-858c-34bc20ce5112",
   "metadata": {},
   "source": [
    "The code below loads the (non-deterministic) slippery Frozen Lake environment. A (deterministic) policy is defined as a Python dictionary that maps from observations to actions. The code loops over 1000 episodes. Within each episode, it iterates through time steps (observations and actions) until the episode is done and a reward is achieved. It then prints the average reward over the 1000 episodes. It usually gets an average reward around 0.05, meaning the goal is reached around 5% of the time. \n",
    "\n",
    "**Your task:** modify the policy so that an average reward of at least 0.02 is achieved (i.e., the agent reaches the goal 20% of the time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca010dee-36a5-452c-84db-9bd7059f28b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 0.057\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "\n",
    "policy = {\n",
    "    0 : 1,\n",
    "    1 : 1,\n",
    "    2 : 1,\n",
    "    3 : 1,\n",
    "    4 : 1,\n",
    "    5 : 1,\n",
    "    6 : 1,\n",
    "    7 : 1,\n",
    "    8 : 1,\n",
    "    9 : 1,\n",
    "    10: 1,\n",
    "    11: 1,\n",
    "    12: 1,\n",
    "    13: 1,\n",
    "    14: 1,\n",
    "    15: 1\n",
    "}\n",
    "\n",
    "rewards = []\n",
    "N = 1000\n",
    "for i in range(N): # loop over episodes\n",
    "\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = policy[obs]\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "    \n",
    "    rewards.append(reward)\n",
    "    \n",
    "print(\"Average reward:\", sum(rewards)/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "99564a1c-076f-4167-858f-ce8ce47107f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 0.044\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "\n",
    "policy = {\n",
    "    0 : 2,\n",
    "    1 : 2,\n",
    "    2 : 2,\n",
    "    3 : 2,\n",
    "    4 : 1,\n",
    "    5 : 1,\n",
    "    6 : 1,\n",
    "    7 : 1,\n",
    "    8 : 2,\n",
    "    9 : 2,\n",
    "    10: 2,\n",
    "    11: 0,\n",
    "    12: 2,\n",
    "    13: 2,\n",
    "    14: 2,\n",
    "    15: 2\n",
    "}\n",
    "\n",
    "rewards = []\n",
    "N = 1000\n",
    "for i in range(N): # loop over episodes\n",
    "\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = policy[obs]\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "    \n",
    "    rewards.append(reward)\n",
    "    \n",
    "print(\"Average reward:\", sum(rewards)/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "044bb229-2908-4e94-bdf6-6fa802ee110b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: I think we need one more step of scaffolding leading up to this exercise.\n",
    "# I think just the inner loop first, and then add the outer loop later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd18b1df-a23c-40f9-83e5-980033a6401e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rl-course]",
   "language": "python",
   "name": "conda-env-rl-course-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
