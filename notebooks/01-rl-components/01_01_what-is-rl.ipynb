{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5aa21ad-6bb9-4990-ab4d-fb9435b0bc00",
   "metadata": {},
   "source": [
    "## Supervised learning vs. reinforcement learning\n",
    "<!-- video shot=\"/0Ml-ccrTVS4\" start=\"00:31\" end=\"03:48\" -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281ff2e0-a58c-46a3-8250-aa0002f2ad9f",
   "metadata": {},
   "source": [
    "#### Supervised learning refresher\n",
    "\n",
    "- Supervised learning learns to predict y (most commonly a number or category) from x (most commonly a vector of numbers or an image) given a dataset.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "➡️ Press the right arrow key to advance to the next slide (and the escape key to see all slides)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb4df9c-fc80-4571-9f51-8efebc175d24",
   "metadata": {},
   "source": [
    "#### Supervised learning examples\n",
    "\n",
    "- Predicting house prices given house features\n",
    "- Classifying an email as spam or not spam\n",
    "- Identifying whether an image contains a dog\n",
    "\n",
    "General idea: predict the output given the input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0f26a4-2313-4474-b173-26b32526eecf",
   "metadata": {},
   "source": [
    "#### The \"API\" of supervised learning\n",
    "\n",
    "![](img/SL-API.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c2d25f-02fe-495f-9529-7117d2b91f5c",
   "metadata": {},
   "source": [
    "#### The \"API\" of reinforcement learning\n",
    "\n",
    "![](img/SL-vs-RL.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f2188e-0756-4fc7-a038-83b1d54fac8f",
   "metadata": {},
   "source": [
    "#### Reinforcement learning\n",
    "\n",
    "- RL input: an **environment** \n",
    "- RL output: a **policy** that makes decisions\n",
    "\n",
    "We will define environments and policies later in this module!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b566b4-2ef9-4545-834f-903a520c855c",
   "metadata": {},
   "source": [
    "#### Reinforcement learning examples\n",
    "\n",
    "- A self-driving car learning to drive\n",
    "  - Env: road conditions, how the car drives. Policy: the self-driving algorithm\n",
    "- Learning to play a video game\n",
    "  - Env: the game. Policy: the game AI\n",
    "- Learning the \"best\" sequence of movies to recommend to a user\n",
    "  - Env: user movie preferences/behaviors. Policy: the recommender system\n",
    "\n",
    "General idea: you need to make a sequence of decisions and want to act optimally at each step.\n",
    "\n",
    "These decisions affect future inputs/outputs (unlike SL).\n",
    "\n",
    "Notes:\n",
    "\n",
    "- The input to supervised learning is a dataset\n",
    "- The output is the trained model that can make predictions\n",
    "- The model itself has inputs and outputs ($x$ and $y$)\n",
    "- Note that, again, the output is a function which can be used to compute/predict something.\n",
    "- In the next section we'll delve into environments and policies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d070eb2b-121b-4925-b30e-80e8d2b1283b",
   "metadata": {},
   "source": [
    "#### Let's apply what we learned!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6714de6-59d5-428a-90d5-caa45d222203",
   "metadata": {},
   "source": [
    "## Supervised learning or reinforcement learning\n",
    "<!-- multiple choice -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c20387-a7d2-4697-895a-57e97d4c1013",
   "metadata": {},
   "source": [
    "#### Would you solve this problem with SL or RL?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956a69c9-859d-4a9f-8f49-ca69bdf64876",
   "metadata": {},
   "source": [
    "_Classifying the species based on an image of a tree_\n",
    "\n",
    "- [x] SL \n",
    "- [ ] RL | Try again -- this is a single prediction rather than a sequence of actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aec3e2f-b3b2-48fb-b466-fc81a2447655",
   "metadata": {},
   "source": [
    "#### Would you solve this problem with SL or RL?\n",
    "\n",
    "_Creating an AI that plays chess_\n",
    "\n",
    "- [ ] SL | Try again -- the chess AI needs to act optimally at each step in a sequence.\n",
    "- [x] RL "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21b1192-8d52-4178-9921-aef09cbd0286",
   "metadata": {},
   "source": [
    "#### Would you solve this problem with SL or RL?\n",
    "\n",
    "_Predicting whether or not a user will like a movie_\n",
    "\n",
    "- [x] SL \n",
    "- [ ] RL | Try again -- this is a single prediction rather than a sequence of actions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de48e14-752c-4415-82bf-79099a8e29b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Frozen Lake preview\n",
    "<!-- coding exercise -->\n",
    "\n",
    "In the next section you'll be exploring an RL environment called [Frozen Lake](https://www.gymlibrary.dev/environments/toy_text/frozen_lake/). It is a small game that looks like this:\n",
    "\n",
    "```\n",
    "P...\n",
    ".O.O\n",
    "...O\n",
    "O..G\n",
    "```\n",
    "\n",
    "The goal is for the player `P` to reach the goal `G` while walking on the ice `.` and avoiding falling into the holes `O`. The ice is slippery though, and sometimes when the player tries to walk in a certain direction, they slip and go a different way. This makes the game challenging as you may slip into a hole by accident.\n",
    "\n",
    "This is your first coding exercise. You'll access the coding exercises for the course by clicking the \"Open in Colab\" button below. **When running the code in the Colab notebook, make sure to run the cells at the top first, which will install the necessary dependencies for any of the exercises to run.** \n",
    "\n",
    "You won't understand the code yet (that is coming!) but you can start exploring already. First, run the code and you will observe a player behaving randomly in the environment. Then, change `USE_RL = False` to `USE_RL = True` and observe a player that learned how to play using RL. When you are done, answer the multiple choice questions below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59fc2d37-c09b-4ec7-b7eb-92ca4582ffdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Up)\n",
      "....\n",
      ".P.O\n",
      "...O\n",
      "O..G\n",
      "Agent failed to reach the goal :(\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE\n",
    "from ray.rllib.algorithms.ppo import PPO\n",
    "from utils import slippery_algo_config\n",
    "import gym\n",
    "from IPython import display\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "ppo = PPO(env=\"FrozenLake-v1\", config=slippery_algo_config)\n",
    "ppo.restore(\"models/FrozenLakeSlippery50/checkpoint_000050/\")\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "from utils import fix_frozen_lake_render\n",
    "fix_frozen_lake_render(env)\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "# The player behaves randomly by default.\n",
    "# To use a player that learned how to play using RL, change this to True!\n",
    "USE_RL = False\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    if USE_RL:\n",
    "        action = ppo.compute_single_action(obs, explore=False)\n",
    "    else:\n",
    "        action = env.action_space.sample()\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    env.render()\n",
    "    time.sleep(0.25)\n",
    "    \n",
    "if reward == 1:\n",
    "    print(\"Success!\")\n",
    "else:\n",
    "    print(\"Agent failed to reach the goal :(\")\n",
    "    \n",
    "ppo.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cfe158-9796-48b9-a08d-ec0ec6ca59c3",
   "metadata": {},
   "source": [
    "#### Random player\n",
    "\n",
    "How does the random player do at this Frozen Lake game?\n",
    "\n",
    "- [x] It usually fails to reach the goal.\n",
    "- [ ] It usually reaches the goal!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db29ba97-4008-45a0-bcf0-d3b909b71aa1",
   "metadata": {},
   "source": [
    "#### RL player\n",
    "\n",
    "How does the player trained with RL do at this Frozen Lake game?\n",
    "\n",
    "- [ ] It usually fails to reach the goal.\n",
    "- [x] It usually reaches the goal!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67cacbff-c6d3-4b5d-9ea7-4d4cd1e3e3a8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# start the course by showing some  cool results.\n",
    "# not sure yet whether that should be here, or at the end of module 1 (maybe a section 1.4)\n",
    "# let's see...\n",
    "\n",
    "# Also TODO:\n",
    "# Talk about simulations since we can't usually do RL training in a \"real\" environment\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ray200]",
   "language": "python",
   "name": "conda-env-ray200-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
