{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c331561e-c62d-4486-9252-b9ff362f8fa4",
   "metadata": {},
   "source": [
    "## RL Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de064a2a-0a07-4c94-a9ac-b6f2d978f412",
   "metadata": {},
   "source": [
    "#### What is an environment?\n",
    "\n",
    "- An environment could be:\n",
    "  - a game, like a video game.\n",
    "  - a simulation of a real world scenario, like a robot, user behavior, or the stock market\n",
    "  - any other setup with an _agent_ who takes _actions_, views _observations_, and receives _rewards_\n",
    "  \n",
    "TERMINOLOGY NOTICE: we will use _agent_ and _player_ interchangeably. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5748d00-5c07-49db-aa9f-3cc1613bd0a7",
   "metadata": {},
   "source": [
    "#### Running example: frozen lake\n",
    "\n",
    "As a running example of an environment, we will use the [Frozen Lake](https://gym.openai.com/envs/FrozenLake-v0/) environment from [OpenAI Gym](https://gym.openai.com/). We can visualize the environment like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f87dba7-b5de-4ba9-b89d-b5ea2d069f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693db547-8f4d-49ae-b759-80308d6803e0",
   "metadata": {},
   "source": [
    "The goal is for the player (red highlight) to reach the goal (`G`) by walking on the frozen lake segments (`F`) without falling in the holes (`H`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a34b31-5bbb-45f4-9e7f-e925fc96d0ee",
   "metadata": {},
   "source": [
    "#### Movement\n",
    "\n",
    "The player can move around the frozen lake. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d5d954a-7890-427c-aea1-7daec590c91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.step(1); # 1 -> Down\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb8ff83-f2b0-464f-934c-9e6144977c2c",
   "metadata": {},
   "source": [
    "Don't worry about `step(1)` for now; we'll get to that. \n",
    "\n",
    "What you can see is that the player (red highlight) moved downward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c7ab81-0402-4302-a46f-1b4bc115ca63",
   "metadata": {},
   "source": [
    "#### Goal\n",
    "\n",
    "Fast-forward a lot of steps, and you've completed the puzzle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a799321-538a-4f9c-ae1d-31e15a9e0e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env.step(1)\n",
    "env.step(2)\n",
    "env.step(1)\n",
    "env.step(2)\n",
    "env.step(1)\n",
    "env.step(2)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98858094-a152-4e3e-bd03-ed5c81af095b",
   "metadata": {},
   "source": [
    "You've achieved the goal by reaching `G`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1635d0e4-7d38-434c-a956-d47646e57b4a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### What makes an environment?\n",
    "\n",
    "An environment involves several key components, that we'll go through in the following slides."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73df9894-c37a-4d34-93d9-c4e43ecf0564",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### States\n",
    "\n",
    "- We'll use the term _state_ informally to refer to everything about the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a57b3f1-c76e-4b2c-9d84-742326bb827b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16f9748-129e-45c7-9bc1-106e0493fdad",
   "metadata": {},
   "source": [
    "- For example, this is the starting state of the environment.\n",
    "- The player is at the top-left, there's some frozen ice nearby, etc.\n",
    "- We'll use the concept of a state to talk about our environment, but it won't appear in the \"API\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0e72e4-8f86-4c05-9f33-7e888ef44045",
   "metadata": {},
   "source": [
    "#### Actions\n",
    "\n",
    "- Here, the player can choose between 4 possible actions: up, down, left, right\n",
    "- The space of all possible actions is called the **action space**.\n",
    "- In SL we distinguish between regression (continuous $y$) and classification (categorical $y$)\n",
    "- Likewise in RL the action space can be continuous or discrete\n",
    "- In this case, it is discrete (4 possibilities)\n",
    "- The code agrees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "757cfd69-d93d-4e8b-90d0-f3caf079f5a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1907e0-63fe-4c72-af65-f9869b590365",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "- The observations are the _parts of the state that the agent can see_.\n",
    "- Sometimes, the agent can see everything; we call this _fully observable_.\n",
    "- Oftentimes, we have _partially observable_ environments. \n",
    "- In the Frozen Lake example, the agent can only see its own location out of the 16 squares.\n",
    "- The agent is not \"told\" where the holes are via direct observations, so it will need to _learn_ this via trial and error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659ebb9b-4c6a-43f6-96c1-eb5117c50a54",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "- The space of all possible observations is called the **observation space**.\n",
    "- You can think of the action space as analogous to the target in supervised learning.\n",
    "- You can think of the observation space as analogous to the features in supervised learning.\n",
    "\n",
    "\n",
    "Here, we have a discrete observation space consisting of the 16 possible player positions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81aa1ad6-e1bb-492d-a5d0-a72c94a33e29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ee9cb4-8650-4d39-87b7-97202ffca199",
   "metadata": {},
   "source": [
    "#### Rewards\n",
    "\n",
    "- In supervised learning, the goal is usually to make good predictions.\n",
    "- You may still try different loss functions depending on your specific goal, but the general concept is the same.\n",
    "- In RL, the goal could be anything.\n",
    "- But, like in SL, you will need to be _optimizing_ something.\n",
    "- In RL, we aim to maximize the **reward**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf68c60b-f6fe-47e0-8482-bc51c4410846",
   "metadata": {},
   "source": [
    "#### Rewards \n",
    "\n",
    "In the Frozen Lake example, the agent receives a reward when it reaches the goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4807bbe7-7525-4772-9ce9-a0e06f4b21fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "reward = 0.0\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "obs, reward, done, _ = env.step(0)\n",
    "env.render()\n",
    "print(\"reward =\", reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aec3a5cd-5607-41fb-a932-7e5e10bf6569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward = 0.0\n"
     ]
    }
   ],
   "source": [
    "obs, reward, done, _ = env.step(1)\n",
    "print(\"reward =\", reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceae768d-d87b-4611-afce-e02137b9fd72",
   "metadata": {},
   "source": [
    "Still no reward, let's keep going..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4c7004-91a1-4dcc-ad52-943cfd179a97",
   "metadata": {},
   "source": [
    "#### Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89518a96-7c75-437d-9bb1-da7dbc7038b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "reward = 1.0\n"
     ]
    }
   ],
   "source": [
    "env.step(1)\n",
    "env.step(2)\n",
    "env.step(1)\n",
    "env.step(2)\n",
    "env.step(1)\n",
    "obs, reward, done, _ = env.step(2)\n",
    "env.render()\n",
    "print(\"reward =\", reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f9cfd3-9e69-4752-9927-ba108685aa83",
   "metadata": {},
   "source": [
    "We got a reward of 1.0 for reaching the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e888ae70-a3f2-400b-8fd9-54dd27369beb",
   "metadata": {},
   "source": [
    "#### Representing actions\n",
    "\n",
    "- To use RL software, we will need a numerical representation of our action space and our observation space.\n",
    "- In this case, we have 4 possible discrete actions, so we can encode them as {0,1,2,3} for (left, down, right, up).\n",
    "- This is why, earlier, we did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "420363c1-9385-47e0-abaf-935a5c0fb4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5a3c2f-ec47-47c4-85e8-95945dfa7019",
   "metadata": {},
   "source": [
    "to walk downward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6419f73c-8431-4e46-87af-0a99a95a58ba",
   "metadata": {},
   "source": [
    "#### Representing observations\n",
    "\n",
    "- Likewise, we will need a numerical representation of our observations.\n",
    "- Here, there are 16 possible positions of the player. These are encoded from 0-15 as follows:\n",
    "\n",
    "```\n",
    " 0   1   2   3\n",
    " 4   5   6   7\n",
    " 8   9  10  11\n",
    "12  13  14  15\n",
    "```\n",
    "\n",
    "These details of the Frozen Lake environment are also available in the [documentation](https://www.gymlibrary.ml/pages/environments/toy_text/frozen_lake)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cecba2-2529-4f24-a5ef-d730457ac994",
   "metadata": {},
   "source": [
    "#### Representing observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b616b8-5ccf-45bf-a4ac-65ae31f108e4",
   "metadata": {},
   "source": [
    "Initially, we observe \"0\" because we start at the upper-left:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "df316735-5a38-488f-80ae-dd97d715cc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b5dad3-f8bc-4298-8dc4-4a3f87285600",
   "metadata": {},
   "source": [
    "After moving to the down (action 1), we move to position 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7d540232-affa-45de-a10f-6f99f755d5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, reward, done, _ = env.step(1)\n",
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b84f58-bf29-4df0-9b85-08190d1815f0",
   "metadata": {},
   "source": [
    "The observation is returned by the `step()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f179e5d-68cb-4d31-a5e0-cd01f5b14124",
   "metadata": {},
   "source": [
    "#### Non-deterministic environments\n",
    "\n",
    "- So far, taking a particular action from a particular state always resulted in the same new state.\n",
    "- In other words, our Frozen Lake environment was _deterministic_.\n",
    "- Some environments are _non-deterministic_, meaning the outcome of an action can be random.\n",
    "- We can initialize a non-deterministic Frozen Lake like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9bdde4c3-e7f9-419a-a25b-4f2d6027354e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_slippery = gym.make(\"FrozenLake-v1\", is_slippery=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6a7d44ef-2828-4c7f-aae9-17eab6b7d7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "env_slippery.seed(4); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3943d88f-a44b-4fba-96f8-20177b6317b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env_slippery.reset()\n",
    "env_slippery.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b97800f-5c7a-43a8-9018-f8505bd9eafd",
   "metadata": {},
   "source": [
    "#### Non-deterministic environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "35e5cb48-3e0f-4b5f-8f0a-663ff31e99e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env_slippery.step(1) # move down\n",
    "env_slippery.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0755c6-0d78-42d0-a9b4-cd4aec36db82",
   "metadata": {},
   "source": [
    "Moving down did not work as planned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cba5c8e8-a7ab-4854-9c35-ca79584f7b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env_slippery.step(1) # move down\n",
    "env_slippery.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031a6a4e-f9a1-4965-86b8-d439a82491ad",
   "metadata": {},
   "source": [
    "Moving down worked this time.\n",
    "\n",
    "In this \"slippery\" Frozen Lake environment, movement only works as intended 1/3 of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0388fe5a-b6e4-484c-ac51-d376c353ffaa",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Episodes\n",
    "\n",
    "- Playing the Frozen Lake has an end - either you fall into a hole or you reach the goal.\n",
    "- However, one play-through is not enough for an RL algorithm to learn from.\n",
    "- It will need multiple play-throughs, called **episode**.\n",
    "- After an episode, the environment is reset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68508dc1-2009-48a7-85e1-3f4bba4b0220",
   "metadata": {},
   "source": [
    "#### Episodes\n",
    "\n",
    "The `step()` method returns a flag telling us whether the episode is over:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "534b8516-26ea-4df4-9062-e6fb1f57d3a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, reward, done, _ = env_slippery.step(1)\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a0684e7c-b767-4907-965d-6bcc86d64c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env_slippery.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdbecc7-7aac-432a-9e6e-301af52e7d2a",
   "metadata": {},
   "source": [
    "Here the episode is done because we fell into a hole."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854521ed-7fb7-453b-9eb6-eaf61074f533",
   "metadata": {},
   "source": [
    "#### Episodes\n",
    "\n",
    "- In some environments (like Frozen Lake), rewards are only received at the end of an episode.\n",
    "- In other environments, rewards can be received at any **time step** (i.e., after an action)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea0a22c-c13f-407e-beb1-5fd0a985dc4a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Putting it all together\n",
    "\n",
    "- We've now talked about the main components of an RL environment:\n",
    "  - States\n",
    "  - Actions\n",
    "  - Observations\n",
    "  - Rewards\n",
    "  - Episodes\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8de8cd1-8733-4c90-89d6-9586be21309a",
   "metadata": {},
   "source": [
    "#### SL datasets vs. RL environments\n",
    "\n",
    "- In supervised learning, you are typically given a dataset.\n",
    "- In RL, the environment acts as a _data generator_.\n",
    "  - The more you play through the environment, the more \"data\" you generate and the more you can learn.\n",
    "- One can also do RL on a pre-collected dataset (called _offline RL_), but that is out of scope for us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c511b50-fae2-4764-8eaa-c47ca7dab1d7",
   "metadata": {},
   "source": [
    "## Self-driving car environment\n",
    "<!-- multiple choice -->\n",
    "\n",
    "You're using RL to train a self-driving car. The car AI uses various cameras and sensors as its inputs and has to decide the angle of the steering wheel as well as the angle of gas/brake pedals on the floor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a443612-50ff-4564-bdf4-a4e1ba79be93",
   "metadata": {},
   "source": [
    "#### Is the observation space continuous or discrete?\n",
    "\n",
    "- [x] Continuous | Correct! \n",
    "- [ ] Discrete | In this case, the observations are the sensor inputs, e.g. depth estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cace1880-0440-42f1-a087-576821998a4a",
   "metadata": {},
   "source": [
    "#### Is the action space continuous or discrete?\n",
    "\n",
    "- [x] Continuous | Correct!\n",
    "- [ ] Discrete | The actions are angles; they don't come from a discrete set of options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124e3763-a526-4cd7-84c1-721bb7b9f102",
   "metadata": {},
   "source": [
    "#### What would be the most reasonable reward structure for this environment?\n",
    "\n",
    "- [ ] Reward equals the amount of time the car was able to drive without crashing | What would the reward be if the car never moves?\n",
    "- [x] Reward equals the distance the car was able to drive without crashing | Yes, that sounds good!\n",
    "- [ ] -1 reward for every time step that the car doesn't crash | Keep in mind we want to maximize reward, not minimize it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcfbbb6-1765-473c-8b7e-c24e914949d3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Episodes vs. time steps\n",
    "<!-- multiple choice -->\n",
    "\n",
    "Fill in the blanks in the following sentence: \n",
    "\n",
    "_In a reinforcement learning environment, one takes actions repeatedly until the \\_\\_\\_\\_\\_ ends. This may involve only one \\_\\_\\_\\_\\_, or very many._\n",
    "\n",
    "- [ ] time step / reward | Check the first blank carefully!\n",
    "- [ ] reward / time step | Try again!\n",
    "- [ ] time step / episode | Try again!\n",
    "- [x] episode / time step | You got it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547ac9b5-b354-44c0-8dbf-f0eaeda58ffd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Ex 3\n",
    "<!-- coding -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5286ef-84b7-4b04-a74b-d90b0216abae",
   "metadata": {},
   "source": [
    "## Gym's taxi environment\n",
    "<!-- coding -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22599988-4849-4a9b-aa83-2b792dedbc19",
   "metadata": {},
   "source": [
    "In this exercise we'll look at one of the text-based environments bundled with OpenAI gym, called the taxi environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8165060-f1cc-4b42-956a-de9486777812",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Playing Frozen Lake\n",
    "<!-- coding -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48735d62-91fd-49d3-ab4c-6a90894517bc",
   "metadata": {},
   "source": [
    "The code below creates a slippery Frozen Lake environment and walks half way to the goal. \n",
    "Add more steps until you reach the goal! As a reminder, here is how the actions are represented:\n",
    "\n",
    "| Direction | Action # |\n",
    "|-----|--------|\n",
    "| Left | 0 |\n",
    "| Down | 1 | \n",
    "| Right | 2 |\n",
    "| Up | 3 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93f3c34f-d821-4ae4-ae3e-d219d57e5e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "env.reset()\n",
    "env.seed(6);\n",
    "env.step(1);\n",
    "env.step(3);\n",
    "env.step(1);\n",
    "env.step(3);\n",
    "env.step(2);\n",
    "env.step(2);\n",
    "env.step(2);\n",
    "env.step(2);\n",
    "env.step(0);\n",
    "env.step(0);\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd1799d5-f03c-4284-8f9a-60c34acc4b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n"
     ]
    }
   ],
   "source": [
    "env.step()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c7e8a8-4186-49a6-9f86-f87b06511a59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e13a5e-2fd5-45b0-9053-7268783e2ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_to_end(env):\n",
    "    env.reset();\n",
    "    env.seed(6);\n",
    "    env.step(1);\n",
    "    env.step(3);\n",
    "    env.step(1);\n",
    "    env.step(3);\n",
    "    env.step(2);\n",
    "    env.step(2);\n",
    "    env.step(2);\n",
    "    env.step(2);\n",
    "    env.step(0);\n",
    "    env.step(0);\n",
    "    env.step(2);\n",
    "    env.step(0);\n",
    "    env.step(1);\n",
    "    return env\n",
    "env = step_to_end(env)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cfd80c-8791-4a95-a6d5-f9b2ac5bd2be",
   "metadata": {},
   "source": [
    "episode vs time step"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rl-course]",
   "language": "python",
   "name": "conda-env-rl-course-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
