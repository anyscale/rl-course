{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90c01ab9-1cd0-47d8-a198-e07e36e2c1fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## RL Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddd909a-c86f-4e5c-9ba0-8b0ef1152730",
   "metadata": {},
   "source": [
    "#### What is a policy?\n",
    "\n",
    "Let's return to the \"API\" of RL:\n",
    "\n",
    "![](img/RL-API.png)\n",
    "\n",
    "- The policy is the output of RL\n",
    "- It maps observations to actions\n",
    "- The policy is like the agent's brain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a207992-104b-4ecd-b470-341860f09ad6",
   "metadata": {},
   "source": [
    "#### What is a policy? Details.\n",
    "\n",
    "- A policy is a function mapping observations to actions.\n",
    "- Let's consider the Frozen Lake environment again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2a869f8-2fb6-4ef5-bd1c-d8c74493e93c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
    "obs = env.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbfda97-e12c-4284-b0b2-78a8f21c7ede",
   "metadata": {},
   "source": [
    "- We received an observation 0. What do we do next? \n",
    "- The policy will tell us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a98f1c-a7b6-40bc-9d18-dce492473ea9",
   "metadata": {},
   "source": [
    "#### Example policy\n",
    "\n",
    "A policy might look like this\n",
    "\n",
    "| Observation | Action |\n",
    "|------|-------|\n",
    "| 0   |   0  |\n",
    "| 1   |   3  |\n",
    "| 2   |   1  |\n",
    "| 3   |   1  |\n",
    "| ... | ... |\n",
    "| 14 |  2  |\n",
    "| 15 | 2   |\n",
    "\n",
    "- On the left, we have all possible observations (15 for Frozen Lake).\n",
    "- On the right, we have the corresponding action we will take _if we see that observation_.\n",
    "- \"If I see 0, I will do 0; if I see 1, I will do 3,\" etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c510ca5c-a9d5-44e3-9afb-c4a15979f874",
   "metadata": {},
   "source": [
    "#### Goal of RL\n",
    "\n",
    "**The goal of RL is to learn a good policy given an environment.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dac4894-b44a-42bc-8269-54f8a7d93c8e",
   "metadata": {},
   "source": [
    "#### Non-deterministic policies\n",
    "\n",
    "- Previously we learned about deterministic and non-deterministic environments. \n",
    "- Analogously, we have deterministic and non-deterministic _policies_.\n",
    "- Before we saw a deterministic policy: a given observation elicits a fixed action.\n",
    "- Here is an example non-deterministic policy:\n",
    "\n",
    "| Observation | P(left) | P(down) | P(right) | P(up) | \n",
    "|------------|-------|-----------|---------|-------|\n",
    "| 0   |   0  |  0.9 | 0.01      | 0.04      | 0.05\n",
    "| 1   |   3  |  0.05 | 0.05      | 0.05      | 0.85\n",
    "| ... | ... |  ... | ...      | ...      | ...\n",
    "| 15 | 2   |  0.0 | 0.0      | 0.99      | 0.01\n",
    "\n",
    "\"If I see 0, I will move left 99% of the time, down 1% of the time, right 4% of the time, and up 5% of the time.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34003758-53d7-4832-8e82-f805d723db9c",
   "metadata": {},
   "source": [
    "#### Continuous action spaces\n",
    "\n",
    "What if our action space is continuous? We can still have a policy. For example:\n",
    "\n",
    "| Observation | Action |\n",
    "|------|-------|\n",
    "| 0   |   0.42  |\n",
    "| 1   |   -3.99  |\n",
    "| ... | ... |\n",
    "| 15 | 2.24   |\n",
    "\n",
    "A non-deterministic policy would need to draw from a probability distribution, though."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8212fc5-3270-49b9-8e82-44e04ebc892c",
   "metadata": {},
   "source": [
    "#### Continuous observation spaces\n",
    "\n",
    "- What if our _observation_ space is continuous? \n",
    "- Well, now we can't draw the policy as a table anymore...\n",
    "- In this case, our policy is a _function_ of the observation value. \n",
    "- E.g. \"gas pedal angle with floor (action) = 1.5 x distance to closest obstacle (observation)\" \n",
    "- This toy example says that if the nearest obstacle is far away, you can speed up the car."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed8dfeec-6879-4d40-9c9e-b28e4bb260b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# too much text around here, need more code and/or images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dc8172-ae79-405f-801f-a2b750a62e31",
   "metadata": {},
   "source": [
    "#### Thinking about policies as functions\n",
    "\n",
    "- In general, this is a useful way of thinking: the policy is a function that maps observations to actions.\n",
    "- In **deep reinforcement learning**, this function is a neural network.\n",
    "- A bit more on this later!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0043038d-d6f4-4364-b348-e968fdb37d3a",
   "metadata": {},
   "source": [
    "#### Beyond scalars\n",
    "\n",
    "- So far we've assumed the observation is a single number and the action is a single number.\n",
    "- However, both of these can be more complex data types: images, vectors, etc. \n",
    "- The actual observations of a self-driving car may be dozens of measurements, images, etc.\n",
    "- The actual actions of a self-driving car may be setting multiple values at each time step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a75dce-81a4-4ff6-923c-d5e0b736384c",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "- The \"agent\" or \"player\" are personifications of the policy\n",
    "- There is no additional \"intelligence\" or decision-making beyond the policy\n",
    "- Therefore, we don't technically need the notion of an agent/player\n",
    "- The policy is the output of RL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5780af6f-fa34-4b85-af30-32e20755f1c0",
   "metadata": {},
   "source": [
    "## Ex 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "120536ac-81b1-4b85-85ed-0657134202a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "env.seed(1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948f613a-9179-4965-9138-53ae1f7040a4",
   "metadata": {},
   "source": [
    "## Ex 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7065f31-0cbb-4d00-af18-ef1261c43aa3",
   "metadata": {},
   "source": [
    "some policy \"by hand\" questions where they fill in what makes sense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f29872-b1b5-46f3-95e9-7665872474dd",
   "metadata": {},
   "source": [
    "## Ex 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078a1b39-8190-4fba-a67d-d8808d5db88b",
   "metadata": {},
   "source": [
    "provide a trained policy and have them explore it, or see how it could be improved\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7723a3e-9c1b-483f-b862-12ab1d4acf9d",
   "metadata": {},
   "source": [
    "## Ex 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rl-course]",
   "language": "python",
   "name": "conda-env-rl-course-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
