{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90c01ab9-1cd0-47d8-a198-e07e36e2c1fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## RL Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddd909a-c86f-4e5c-9ba0-8b0ef1152730",
   "metadata": {},
   "source": [
    "#### What is a policy?\n",
    "\n",
    "Let's return to the \"API\" of RL:\n",
    "\n",
    "![](img/RL-API.png)\n",
    "\n",
    "- The policy is the output of RL\n",
    "- It maps observations to actions\n",
    "- The policy is like the agent's brain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a207992-104b-4ecd-b470-341860f09ad6",
   "metadata": {},
   "source": [
    "#### What is a policy? Details.\n",
    "\n",
    "- A policy is a function mapping observations to actions.\n",
    "- Let's consider the Frozen Lake environment again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2a869f8-2fb6-4ef5-bd1c-d8c74493e93c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
    "obs = env.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbfda97-e12c-4284-b0b2-78a8f21c7ede",
   "metadata": {},
   "source": [
    "- We received an observation 0. What do we do next? \n",
    "- The policy will tell us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a98f1c-a7b6-40bc-9d18-dce492473ea9",
   "metadata": {},
   "source": [
    "#### Example policy\n",
    "\n",
    "A policy might look like this\n",
    "\n",
    "| Observation | Action |\n",
    "|------|-------|\n",
    "| 0   |   0  |\n",
    "| 1   |   3  |\n",
    "| 2   |   1  |\n",
    "| 3   |   1  |\n",
    "| ... | ... |\n",
    "| 14 |  2  |\n",
    "| 15 | 2   |\n",
    "\n",
    "- On the left, we have all possible observations (15 for Frozen Lake).\n",
    "- On the right, we have the corresponding action we will take _if we see that observation_.\n",
    "- \"If I see 0, I will do 0; if I see 1, I will do 3,\" etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c510ca5c-a9d5-44e3-9afb-c4a15979f874",
   "metadata": {},
   "source": [
    "#### Goal of RL\n",
    "\n",
    "**The goal of RL is to learn a good policy given an environment.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dac4894-b44a-42bc-8269-54f8a7d93c8e",
   "metadata": {},
   "source": [
    "#### Non-deterministic policies\n",
    "\n",
    "- Previously we learned about deterministic and non-deterministic environments. \n",
    "- Analogously, we have deterministic and non-deterministic _policies_.\n",
    "- Before we saw a deterministic policy: a given observation elicits a fixed action.\n",
    "- Here is an example non-deterministic policy:\n",
    "\n",
    "| Observation | P(left) | P(down) | P(right) | P(up) | \n",
    "|------------|-------|-----------|---------|-------|\n",
    "| 0   |   0  |  0.9 | 0.01      | 0.04      | 0.05\n",
    "| 1   |   3  |  0.05 | 0.05      | 0.05      | 0.85\n",
    "| ... | ... |  ... | ...      | ...      | ...\n",
    "| 15 | 2   |  0.0 | 0.0      | 0.99      | 0.01\n",
    "\n",
    "\"If I see 0, I will move left 99% of the time, down 1% of the time, right 4% of the time, and up 5% of the time.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34003758-53d7-4832-8e82-f805d723db9c",
   "metadata": {},
   "source": [
    "#### Continuous action spaces\n",
    "\n",
    "What if our action space is continuous? We can still have a policy. For example:\n",
    "\n",
    "| Observation | Action |\n",
    "|------|-------|\n",
    "| 0   |   0.42  |\n",
    "| 1   |   -3.99  |\n",
    "| ... | ... |\n",
    "| 15 | 2.24   |\n",
    "\n",
    "A non-deterministic policy would need to draw from a probability distribution, though."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8212fc5-3270-49b9-8e82-44e04ebc892c",
   "metadata": {},
   "source": [
    "#### Continuous observation spaces\n",
    "\n",
    "- What if our _observation_ space is continuous? \n",
    "- Well, now we can't draw the policy as a table anymore...\n",
    "- In this case, our policy is a _function_ of the observation value. \n",
    "- E.g. \"gas pedal angle with floor (action) = 1.5 x distance to closest obstacle (observation)\" \n",
    "- This toy example says that if the nearest obstacle is far away, you can speed up the car."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed8dfeec-6879-4d40-9c9e-b28e4bb260b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# too much text around here, need more code and/or images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dc8172-ae79-405f-801f-a2b750a62e31",
   "metadata": {},
   "source": [
    "#### Thinking about policies as functions\n",
    "\n",
    "- In general, this is a useful way of thinking: the policy is a function that maps observations to actions.\n",
    "- In **deep reinforcement learning**, this function is a neural network.\n",
    "- A bit more on this later!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0043038d-d6f4-4364-b348-e968fdb37d3a",
   "metadata": {},
   "source": [
    "#### Beyond scalars\n",
    "\n",
    "- So far we've assumed the observation is a single number and the action is a single number.\n",
    "- However, both of these can be more complex data types: images, vectors, etc. \n",
    "- The actual observations of a self-driving car may be dozens of measurements, images, etc.\n",
    "- The actual actions of a self-driving car may be setting multiple values at each time step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a75dce-81a4-4ff6-923c-d5e0b736384c",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "- The \"agent\" or \"player\" are personifications of the policy\n",
    "- There is no additional \"intelligence\" or decision-making beyond the policy\n",
    "- Therefore, we don't technically need the notion of an agent/player\n",
    "- The policy is the output of RL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5780af6f-fa34-4b85-af30-32e20755f1c0",
   "metadata": {},
   "source": [
    "## Frozen Lake policy\n",
    "<!-- multiple choice -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a299f97-00fa-4e31-ad66-5e843b63e1d1",
   "metadata": {},
   "source": [
    "Recall the frozen lake environment:\n",
    "\n",
    "```\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "```\n",
    "\n",
    "with its observation space represented as:\n",
    "\n",
    "```\n",
    " 0   1   2   3\n",
    " 4   5   6   7\n",
    " 8   9  10  11\n",
    "12  13  14  15\n",
    "```\n",
    "\n",
    "and actions represented as\n",
    "\n",
    "| Action     |  Meaning    |\n",
    "|------|------|\n",
    "| 0 | left |\n",
    "| 1 | down |\n",
    "| 2 | right |\n",
    "| 3 | up |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8eca95c-cc54-42d3-a442-f41fb0c27b5e",
   "metadata": {},
   "source": [
    "#### Question 1\n",
    "\n",
    "The policy below contains a missing entry represented by a `?` symbol.\n",
    "\n",
    "| Observation | Action |\n",
    "|------|-------|\n",
    "| 0   |   0  |\n",
    "| 1   |   2  |\n",
    "| ... | ... |\n",
    "| 13 |  ? |\n",
    "| 14 | 2 |\n",
    "| 15 | 0   |\n",
    "\n",
    "Select the best choice to fill the `?` entry.\n",
    "\n",
    "- [ ] 0  | Try again!\n",
    "- [ ] 1 | Try again!\n",
    "- [x] 2 | Yes! Moving to the right takes you toward the goal.\n",
    "- [ ] 3 | Try again!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c41f87-143b-409f-a16c-b8b1ac4b0cc7",
   "metadata": {},
   "source": [
    "#### Question 2\n",
    "\n",
    "_In the slippery version of the Frozen Lake, the agent has a 1/3 probability of moving in the intended direction and 1/3 each in the two perpendicular directions._\n",
    "\n",
    "Is the above a statement about the environment or the policy?\n",
    "\n",
    "- [x] Environment | You got it! \n",
    "- [ ] Policy | Remember, the policy describes how the agent responds to observations.\n",
    "\n",
    "#### Question 3\n",
    "\n",
    "_In the slippery version of the Frozen Lake, it is sometimes better not to walk in the direction you really want to go, because it's more important to avoid the chance of slipping into a hole._\n",
    "\n",
    "Is the above a statement about the environment or the policy?\n",
    "\n",
    "- [ ] Environment | The statement above is about the best action to take in a situation; this is determined by the policy.\n",
    "- [x] Policy | You got it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948f613a-9179-4965-9138-53ae1f7040a4",
   "metadata": {},
   "source": [
    "## Ex 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f29872-b1b5-46f3-95e9-7665872474dd",
   "metadata": {},
   "source": [
    "## Ex 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078a1b39-8190-4fba-a67d-d8808d5db88b",
   "metadata": {},
   "source": [
    "provide a trained policy and have them explore it, or see how it could be improved\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7723a3e-9c1b-483f-b862-12ab1d4acf9d",
   "metadata": {},
   "source": [
    "## Ex 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rl-course]",
   "language": "python",
   "name": "conda-env-rl-course-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
