{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90c01ab9-1cd0-47d8-a198-e07e36e2c1fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## RLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e74417a3-63bc-4000-951a-72ab882b689e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddd909a-c86f-4e5c-9ba0-8b0ef1152730",
   "metadata": {},
   "source": [
    "#### What about the learning?\n",
    "\n",
    "Let's return to the \"API\" of RL:\n",
    "\n",
    "![](img/RL-API.png)\n",
    "\n",
    "- We've talked about the input (environment) and output (policy)\n",
    "- Let's talk about the reinforcement learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7503593-2928-46b8-891a-913ff3fbc3c1",
   "metadata": {},
   "source": [
    "#### What we'll cover\n",
    "\n",
    "- Many, many supervised learning algorithms exist... random forests, logistic regression, neural networks, etc.\n",
    "- Likewise, there are many RL algorithms.\n",
    "- This is not a course on RL algorithms, though many good ones exist!\n",
    "- This course is about _applying_ RL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14840b72-5c0f-42a3-b29f-d9b14995d2c1",
   "metadata": {},
   "source": [
    "#### Introducing Ray RLlib\n",
    "\n",
    "![](img/rllib-logo.png)\n",
    "\n",
    "- In this course we'll use Ray RLlib as our \"scikit-learn of reinforcement learning\"\n",
    "- We will look under the hood only as needed, and focus on the inputs and outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251a0d49-fad7-4620-bc2f-71ab58d228f9",
   "metadata": {},
   "source": [
    "#### Our first RLlib code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497d2332-bdf4-43c2-ba76-c5e3c4d37afc",
   "metadata": {},
   "source": [
    "First, we import RLlib, which is part of the Ray project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6aacb650-3d39-4117-b9a9-2eccdbd9a350",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import rllib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b300e4-266f-45ef-94c5-f72d1928f94b",
   "metadata": {},
   "source": [
    "Next, we create a trainer object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cd844399-d193-4961-9bb3-8be1e2f95ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "trainer_config = {\n",
    "    \"framework\"  : \"torch\",\n",
    "    \"seed\"       : 0,\n",
    "    \"env_config\" : {\"is_slippery\" : False}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0fc412c4-e739-417a-b973-a0819bc5e8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = rllib.agents.ppo.PPOTrainer(env=\"FrozenLake-v1\", config=trainer_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c04a477-934c-4317-a69b-9eaef6d7563f",
   "metadata": {},
   "source": [
    "For clarity we've hidden the config for now, but we'll get back to it soon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4400014-5524-4360-9c2f-988fa2fd114d",
   "metadata": {},
   "source": [
    "#### Using the policy\n",
    "\n",
    "- We haven't trained the agent yet, but we can still see what it does.\n",
    "- This is like calling `predict` before running `fit` with supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "52c46765-0db9-4f0f-be98-6bd15fed1292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
    "obs = env.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "72c9f289-1c06-46a7-89bd-a8553104f1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "env.seed(3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1ce06add-1fc8-4211-9850-f419ddfca832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = trainer.compute_single_action(obs, explore=False)\n",
    "action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59ecbe8-4c8b-45d3-bfc7-d1f4e5d93684",
   "metadata": {},
   "source": [
    "- We gave the trainer our initial observation, 0, and it recommended action 0 (left).\n",
    "- This action came from the initialized **policy**.\n",
    "- Remember, the policy maps observations to actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48087f0-f3bd-47fb-b368-51c1f28601ac",
   "metadata": {},
   "source": [
    "#### Using the policy\n",
    "\n",
    "We can see what happened after taking that action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9b6a30b0-d591-4c30-b85e-fe839f518744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "obs, reward, done, _ = env.step(action)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5073dcea-e9d9-420b-b22c-8735e5b6bf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "# Apparently, we attempted to move left but actually moved down, because we're in the slippery environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7068f1-2f74-4d01-b6a6-455cdb964e9a",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "- So far our policy was just a random/arbitrary initialization.\n",
    "- What we want is to train it _based on experience interacting with the environment_.\n",
    "- In order to do this, RLlib will _play through many episodes_ and learn as it goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "26694604-767d-437d-b29b-377d38c67999",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_info = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0024b5a6-b7de-4e0b-a2ac-1b17af442c99",
   "metadata": {},
   "source": [
    "- Note that, unlike sklearn's `fit()`, here we don't provide the dataset to `train()`.\n",
    "- We gave it the environment during initialization, and it uses the environment to generate data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c34155-1045-4582-8995-30ca063dd941",
   "metadata": {},
   "source": [
    "#### Training iterations\n",
    "\n",
    "- In fact, what we just did was one _iteration_ of training.\n",
    "- RLlib will play through a bunch of episodes per iteration, depending on its hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d5e474bd-6aa7-4801-8268-f8b643d6d091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "516"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_info[\"hist_stats\"][\"episode_lengths\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e586701-6cc9-4706-83fc-66f8082c0ddd",
   "metadata": {},
   "source": [
    "Looks like it ran ~500 episodes in that one iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9af3fd-5931-471b-ba85-85de280e1501",
   "metadata": {},
   "source": [
    "#### RL mindset: data generation\n",
    "\n",
    "- This is a key departure from the supervised learning mindset\n",
    "- In SL, we take a fixed amount of data and train for some number of iterations\n",
    "- In RL, more iterations means more training _on more data_ because we learn from the environment as we interact with it\n",
    "- If you only play one episode, you might never see observation 10, so how can you learn what to do given observation 10?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a4fb60-74bb-4e18-aac0-7f13ad5fa4a0",
   "metadata": {},
   "source": [
    "#### Training info: episode lengths\n",
    "\n",
    "Let's look at the lengths of the last 100 episodes we played:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "69d6edba-b9d7-4e41-8fe5-c9181870faef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 6, 3, 4, 9, 13, 8, 8, 4, 7, 5, 5, 7, 16, 16, 7, 2, 5, 4, 14, 3, 7, 4, 14, 3, 5, 7, 10, 2, 5, 17, 9, 27, 6, 7, 7, 19, 5, 18, 8, 4, 5, 3, 2, 11, 11, 18, 4, 15, 9, 6, 2, 8, 13, 7, 6, 5, 25, 2, 2, 6, 2, 2, 10, 8, 18, 9, 4, 6, 21, 3, 4, 14, 12, 4, 12, 7, 16, 11, 18, 7, 3, 8, 5, 8, 5, 6, 6, 5, 7, 5, 9, 3, 4, 9, 9, 8, 17, 14, 2]\n"
     ]
    }
   ],
   "source": [
    "print(train_info[\"hist_stats\"][\"episode_lengths\"][-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb727bf9-bc04-4127-8290-4b4a76a961f2",
   "metadata": {},
   "source": [
    "- Remember that an episode ends when `.step()` returns `True` for the `done` flag.\n",
    "- We see some very short episodes, where the agent fell into a hole right away."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cdf5ae-db28-4da8-a55d-eb1c0222b277",
   "metadata": {},
   "source": [
    "#### Training info: episode rewards\n",
    "\n",
    "- For those longer episodes, did the agent reach the goal?\n",
    "- To assess this, we can print out the first 100 _episode rewards_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cd6ffa8d-206d-47e6-8d5e-c2ce350c92a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print(train_info[\"hist_stats\"][\"episode_reward\"][-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ab826887-1e08-4821-9ea4-79baba755237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(sum(train_info[\"hist_stats\"][\"episode_reward\"][-100:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c846b7-297a-4878-a6de-a91663176660",
   "metadata": {},
   "source": [
    "- This is not very impressive. Let's keep training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb75a84-3cab-487b-8476-2740a686af92",
   "metadata": {},
   "source": [
    "#### More training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dcc1cbb9-0c16-447f-88a6-027bd5b05b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    train_info = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8413bd9d-59d5-4620-b5f1-2f4f466f2029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.0\n"
     ]
    }
   ],
   "source": [
    "print(sum(train_info[\"hist_stats\"][\"episode_reward\"][-100:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9e1c65-6eb7-4b72-adf6-f63845487fd4",
   "metadata": {},
   "source": [
    "- Nice! Now we're reaching the goal almost every time!\n",
    "- This non-slippery Frozen Lake is a very easy environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80d7905-3fab-417f-b9b1-c9ffa4d4cecf",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Using the policy\n",
    "\n",
    "We can run the **observation-policy-action loop** for multiple time steps to watch the policy in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "06f47779-3725-4bb6-9c92-f1877fa148b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "\n",
    "for i in range(3):\n",
    "    action = trainer.compute_single_action(obs, explore=False)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f9fe3e-a69d-4a40-ba94-db9f3c39ee54",
   "metadata": {},
   "source": [
    "#### Using the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "af239179-38df-4a33-bbea-4c7b4ae9c684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    action = trainer.compute_single_action(obs, explore=False)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8753cf53-da7f-4fad-b67c-200a9e844a85",
   "metadata": {},
   "source": [
    "Using this policy we reliably reach the goal every time because the non-slippery Frozen Lake environment is deterministic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f64eb5c-946c-4454-8b7c-675b18a629d5",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f20647-6960-419c-9fec-fea914ae4af6",
   "metadata": {},
   "source": [
    "- Talk about train/test/etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72beef15-af71-4a24-8a5e-59ec40fe271a",
   "metadata": {},
   "source": [
    "#### Configuring the trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc8423f-f5bb-483b-9491-2d30818cdd55",
   "metadata": {},
   "source": [
    "- `PPOTrainer`: we're using the PPO algorithm\n",
    "- `env=\"FrozenLake-v1\"`: RLlib knows about OpenAI Gym environments\n",
    "  - In the next module we'll learn how to make our own environments!\n",
    "- `config={\"framework\" : \"torch\"}`: RLlib works with tensorflow and pytorch\n",
    "  - Here we can include additional hyperparameters like we would in sklearn\n",
    "- `\"env_config\" : {\"is_slippery\" : False}`: this selects the non-slippery Frozen Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd539d44-aa7c-4958-b448-7759f3adc20e",
   "metadata": {},
   "source": [
    "## Ex 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26562cd0-18dd-4904-91d7-0c09692c0157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "from ipywidgets import Output\n",
    "from IPython import display\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34d5c5b-a1be-447a-abe6-e034c9487cce",
   "metadata": {},
   "source": [
    "## Slippery Frozen Lake\n",
    "\n",
    "In the slides we trained an agent to reliably reach the goal in the non-slippery Frozen Lake environment. Here, try the same thing with the slippery Frozen Lake. Train your agent until it reaches the goal at least 25% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f2fdf9-ec1a-488d-8864-0a929e42190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import rl\n",
    "\n",
    "# BEGIN SOLUTION\n",
    "trainer_config = {\n",
    "    \"framework\"  : \"torch\",\n",
    "    \"seed\"       : 0,\n",
    "    \"env_config\" : {\"is_slippery\" : True}}\n",
    "\n",
    "trainer = rllib.agents.ppo.PPOTrainer(env=\"FrozenLake-v1\", config=trainer_config)\n",
    "\n",
    "for i in range(10):\n",
    "    train_info = trainer.train()\n",
    "\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6572684-0981-4aaf-8744-5baf38df5f1c",
   "metadata": {},
   "source": [
    "## Ex 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a10ff1-69be-4eb5-a35d-501f9fc39131",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rl-course]",
   "language": "python",
   "name": "conda-env-rl-course-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
