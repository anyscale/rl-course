{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90c01ab9-1cd0-47d8-a198-e07e36e2c1fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## RLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e74417a3-63bc-4000-951a-72ab882b689e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddd909a-c86f-4e5c-9ba0-8b0ef1152730",
   "metadata": {},
   "source": [
    "#### What about the learning?\n",
    "\n",
    "Let's return to the \"API\" of RL:\n",
    "\n",
    "![](img/RL-API.png)\n",
    "\n",
    "- We've talked about the input (environment) and output (policy)\n",
    "- Let's talk about the reinforcement learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7503593-2928-46b8-891a-913ff3fbc3c1",
   "metadata": {},
   "source": [
    "#### What we'll cover\n",
    "\n",
    "- Many, many supervised learning algorithms exist... random forests, logistic regression, neural networks, etc.\n",
    "- Likewise, there are many RL algorithms.\n",
    "- This is not a course on RL algorithms, though many good ones exist!\n",
    "- This course is about _applying_ RL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14840b72-5c0f-42a3-b29f-d9b14995d2c1",
   "metadata": {},
   "source": [
    "#### Introducing Ray RLlib\n",
    "\n",
    "![](img/rllib-logo.png)\n",
    "\n",
    "- In this course we'll use Ray RLlib as our \"scikit-learn of reinforcement learning\"\n",
    "- We will look under the hood only as needed, and focus on the inputs and outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251a0d49-fad7-4620-bc2f-71ab58d228f9",
   "metadata": {},
   "source": [
    "#### Our first RLlib code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497d2332-bdf4-43c2-ba76-c5e3c4d37afc",
   "metadata": {},
   "source": [
    "First, we import RLlib, which is part of the Ray project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6aacb650-3d39-4117-b9a9-2eccdbd9a350",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import rllib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b300e4-266f-45ef-94c5-f72d1928f94b",
   "metadata": {},
   "source": [
    "Next, we create a trainer object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd844399-d193-4961-9bb3-8be1e2f95ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "trainer_config = {\n",
    "    \"framework\"             : \"torch\",\n",
    "    \"create_env_on_driver\"  : True,\n",
    "    \"seed\"                  : 0,\n",
    "    \"env_config\"            : {\"is_slippery\" : False}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fc412c4-e739-417a-b973-a0819bc5e8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = rllib.agents.ppo.PPOTrainer(env=\"FrozenLake-v1\", \n",
    "                                      config=trainer_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c04a477-934c-4317-a69b-9eaef6d7563f",
   "metadata": {},
   "source": [
    "- `PPOTrainer`: we're using the PPO algorithm\n",
    "- `env=\"FrozenLake-v1\"`: RLlib knows about OpenAI Gym environments\n",
    "  - In the next module we'll learn how to make our own environments!\n",
    "- `config=trainer_config`: this contains all hyperparameters of the algorithm and the environment.\n",
    "  - For clarity we've hidden the config for now, but we'll get back to it soon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4400014-5524-4360-9c2f-988fa2fd114d",
   "metadata": {},
   "source": [
    "#### Using the policy\n",
    "\n",
    "- We haven't trained the agent yet, but we can still see what it does.\n",
    "- This is like calling `predict` before running `fit` with supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52c46765-0db9-4f0f-be98-6bd15fed1292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
    "obs = env.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72c9f289-1c06-46a7-89bd-a8553104f1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "env.seed(3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ce06add-1fc8-4211-9850-f419ddfca832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = trainer.compute_single_action(obs, explore=False)\n",
    "action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59ecbe8-4c8b-45d3-bfc7-d1f4e5d93684",
   "metadata": {},
   "source": [
    "- We gave the trainer our initial observation, 0, and it recommended action 0 (left).\n",
    "- This action came from the initialized **policy**.\n",
    "- Remember, the policy maps observations to actions.\n",
    "- For now we'll ignore the `explore=False`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48087f0-f3bd-47fb-b368-51c1f28601ac",
   "metadata": {},
   "source": [
    "#### Using the policy\n",
    "\n",
    "We can see what happened after taking that action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b6a30b0-d591-4c30-b85e-fe839f518744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "obs, reward, done, _ = env.step(action)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5073dcea-e9d9-420b-b22c-8735e5b6bf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "# Apparently, we attempted to move left but actually moved down, because we're in the slippery environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7068f1-2f74-4d01-b6a6-455cdb964e9a",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "- So far our policy was just a random/arbitrary initialization.\n",
    "- What we want is to train it _based on experience interacting with the environment_.\n",
    "- In order to do this, RLlib will _play through many episodes_ and learn as it goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26694604-767d-437d-b29b-377d38c67999",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_info = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0024b5a6-b7de-4e0b-a2ac-1b17af442c99",
   "metadata": {},
   "source": [
    "- Note that, unlike sklearn's `fit()`, here we don't provide the dataset to `train()`.\n",
    "- We gave it the environment during initialization, and it uses the environment to generate data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c34155-1045-4582-8995-30ca063dd941",
   "metadata": {},
   "source": [
    "#### Training iterations\n",
    "\n",
    "- In fact, what we just did was one _iteration_ of training.\n",
    "- RLlib will play through a bunch of episodes per iteration, depending on its hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5e474bd-6aa7-4801-8268-f8b643d6d091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "516"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_info[\"hist_stats\"][\"episode_lengths\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e586701-6cc9-4706-83fc-66f8082c0ddd",
   "metadata": {},
   "source": [
    "Looks like it ran ~500 episodes in that one iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9af3fd-5931-471b-ba85-85de280e1501",
   "metadata": {},
   "source": [
    "#### RL mindset: data generation\n",
    "\n",
    "- This is a key departure from the supervised learning mindset\n",
    "- In SL, we take a fixed amount of data and train for some number of iterations\n",
    "- In RL, more iterations means more training _on more data_ because we learn from the environment as we interact with it\n",
    "- If you only play one episode, you might never see observation 10, so how can you learn what to do given observation 10?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a4fb60-74bb-4e18-aac0-7f13ad5fa4a0",
   "metadata": {},
   "source": [
    "#### Training info: episode lengths\n",
    "\n",
    "Let's look at the lengths of the last 100 episodes we played:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69d6edba-b9d7-4e41-8fe5-c9181870faef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 6, 3, 4, 9, 13, 8, 8, 4, 7, 5, 5, 7, 16, 16, 7, 2, 5, 4, 14, 3, 7, 4, 14, 3, 5, 7, 10, 2, 5, 17, 9, 27, 6, 7, 7, 19, 5, 18, 8, 4, 5, 3, 2, 11, 11, 18, 4, 15, 9, 6, 2, 8, 13, 7, 6, 5, 25, 2, 2, 6, 2, 2, 10, 8, 18, 9, 4, 6, 21, 3, 4, 14, 12, 4, 12, 7, 16, 11, 18, 7, 3, 8, 5, 8, 5, 6, 6, 5, 7, 5, 9, 3, 4, 9, 9, 8, 17, 14, 2]\n"
     ]
    }
   ],
   "source": [
    "print(train_info[\"hist_stats\"][\"episode_lengths\"][-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb727bf9-bc04-4127-8290-4b4a76a961f2",
   "metadata": {},
   "source": [
    "- Remember that an episode ends when `.step()` returns `True` for the `done` flag.\n",
    "- We see some very short episodes, where the agent fell into a hole right away."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cdf5ae-db28-4da8-a55d-eb1c0222b277",
   "metadata": {},
   "source": [
    "#### Training info: episode rewards\n",
    "\n",
    "- For those longer episodes, did the agent reach the goal?\n",
    "- To assess this, we can print out the first 100 _episode rewards_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd6ffa8d-206d-47e6-8d5e-c2ce350c92a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print(train_info[\"hist_stats\"][\"episode_reward\"][-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab826887-1e08-4821-9ea4-79baba755237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(train_info[\"hist_stats\"][\"episode_reward\"][-100:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c846b7-297a-4878-a6de-a91663176660",
   "metadata": {},
   "source": [
    "- This is not very impressive. Let's keep training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb75a84-3cab-487b-8476-2740a686af92",
   "metadata": {},
   "source": [
    "#### More training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcc1cbb9-0c16-447f-88a6-027bd5b05b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    train_info = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8413bd9d-59d5-4620-b5f1-2f4f466f2029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(train_info[\"hist_stats\"][\"episode_reward\"][-100:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9e1c65-6eb7-4b72-adf6-f63845487fd4",
   "metadata": {},
   "source": [
    "- Nice! Now we're reaching the goal almost every time!\n",
    "- The _average reward_ (across 100 episodes) is 0.99.\n",
    "- Interpretation: we're reaching the goal 99% of the time.\n",
    "- Why? Because in this environment, we only receive a reward at the end of the episode, 0 for failure and 1 for success."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134f484d-3aa5-4cc6-b878-869f591102ec",
   "metadata": {},
   "source": [
    "#### Declare victory?\n",
    "\n",
    "- We did well.\n",
    "- But, this non-slippery Frozen Lake is a very easy environment. \n",
    "- Kind of like a supervised learning dataset where y=x would be \"easy\".\n",
    "- Later we'll ramp up the difficulty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80d7905-3fab-417f-b9b1-c9ffa4d4cecf",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Using the policy\n",
    "\n",
    "We can run the **observation-policy-action loop** for multiple time steps to watch the policy in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06f47779-3725-4bb6-9c92-f1877fa148b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "\n",
    "for i in range(3):\n",
    "    action = trainer.compute_single_action(obs, explore=False)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f9fe3e-a69d-4a40-ba94-db9f3c39ee54",
   "metadata": {},
   "source": [
    "#### Using the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af239179-38df-4a33-bbea-4c7b4ae9c684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    action = trainer.compute_single_action(obs, explore=False)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8753cf53-da7f-4fad-b67c-200a9e844a85",
   "metadata": {},
   "source": [
    "Using this policy we reliably reach the goal every time because the non-slippery Frozen Lake environment is deterministic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f64eb5c-946c-4454-8b7c-675b18a629d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### sklearn / RLlib analogies\n",
    "\n",
    "- We've seen some analogies (and departures) between SL and RL, both in concept and syntax. \n",
    "- Let's compare:\n",
    "\n",
    "| SL/sklearn | RL/RLlib | Description  |\n",
    "|-------------|---------|--------------|\n",
    "| `ModelName(**hypers)` | `AlgoName(hypers, env)` | Initialize a model/algorithm |\n",
    "| `.fit(X,y)`  | `.train()` | Training (fully for sklearn, one iteration for RLlib) |\n",
    "| `.predict(x)` | `.compute_single_action(obs)` | Use the trained model once |\n",
    "| `.score(X,y)` | `.evaluate` | Evaluate the model |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4562a4a-1e69-4fe7-9c50-0a6302ce24d4",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "In RLlib, we can evaluate with `.evaluate()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "827fb5eb-ac68-44bb-b4a1-b2004e4dd3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eadfb34-0130-421c-bd1d-82f797a3ca10",
   "metadata": {},
   "source": [
    "The evaluation output contains a lot of info. Let's look at a couple pieces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a29d4cf4-e5bd-44a4-b5c9-7430570a94fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9836601307189542"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results[\"evaluation\"][\"episode_reward_mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7653739-2225-4f69-a762-d6894a8df931",
   "metadata": {},
   "source": [
    "That is, we reached the goal in 98% of the evaluation episodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38ddb68-39ee-4af9-8dc8-22efbb2d1e38",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "Histogram of episode rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "599bc340-3adf-4d48-9609-35cec8ef2836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(eval_results[\"evaluation\"][\"hist_stats\"][\"episode_reward\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0cc4db-d89c-4089-b4bd-dbfb9998ebf1",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "Histogram of episode _lengths_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95387fc3-4f24-4e20-9b0b-42f959a28e28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAQOklEQVR4nO3df6xfdX3H8edrxbH5K8B6IRXoCqa6qNGy3bAfRMNEJxMDuARX4lxVskoim25LZtFkmCUkOEW3ZBNThdFlWGAgg0x0dMxITAbaAsNCQQpUuNC1V3DqpmFree+Peypfyr3ee78/+q2fPh/JN99z3uec73nfE3jd08/3nHtSVUiS2vIz425AkjR8hrskNchwl6QGGe6S1CDDXZIaZLhLUoPmDfckxyf5SpJtSe5N8oGuflSSTUke7N6P7NnmwiTbkzyQ5C2j/AEkSc+X+a5zT7IMWFZVdyZ5CbAFOBt4N/BUVV2SZB1wZFV9KMmrgI3AycDLgH8FXlFVe0f3Y0iSes175l5VO6vqzm76B8A24FjgLGBDt9oGZgKfrn51VT1dVY8A25kJeknSAXLYYlZOsgI4CbgDOKaqdsLML4AkR3erHQvc3rPZVFeb09KlS2vFihWLaUWSDnlbtmz5TlVNzLZsweGe5MXA9cAHq+r7SeZcdZba88Z+kqwF1gIsX76czZs3L7QVSRKQ5NtzLVvQ1TJJXsBMsF9VVV/oyru68fh94/K7u/oUcHzP5scBT+z/mVW1vqomq2pyYmLWXzySpD4t5GqZAJcD26rqkz2LbgLWdNNrgBt76quTHJ7kBGAl8PXhtSxJms9ChmVOAd4FfDPJ3V3tw8AlwLVJzgMeBc4BqKp7k1wL3AfsAd7vlTKSdGDNG+5V9TVmH0cHOG2ObS4GLh6gL0nSALxDVZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYv68wPSinVfHMt+d1xyxlj2K/208sxdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ1ayAOyr0iyO8nWnto1Se7uXjv2PVs1yYokP+pZ9pkR9i5JmsNC/irklcDfAH+/r1BVv7tvOsmlwPd61n+oqlYNqT9JUh8W8oDs25KsmG1ZkgDvAN445L4kSQMYdMz99cCuqnqwp3ZCkruSfDXJ6wf8fElSHwZ9WMe5wMae+Z3A8qp6MsmvAP+U5NVV9f39N0yyFlgLsHz58gHbkCT16vvMPclhwO8A1+yrVdXTVfVkN70FeAh4xWzbV9X6qpqsqsmJiYl+25AkzWKQYZk3AfdX1dS+QpKJJEu66ROBlcDDg7UoSVqshVwKuRH4d+CVSaaSnNctWs1zh2QA3gDck+Q/gOuA86vqqWE2LEma30Kuljl3jvq7Z6ldD1w/eFuSpEF4h6okNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYt5BmqVyTZnWRrT+2jSR5Pcnf3emvPsguTbE/yQJK3jKpxSdLcFnLmfiVw+iz1T1XVqu51M0CSVzHz4OxXd9t8OsmSYTUrSVqYecO9qm4Dnlrg550FXF1VT1fVI8B24OQB+pMk9WGQMfcLktzTDdsc2dWOBR7rWWeqq0mSDqB+w/0y4OXAKmAncGlXzyzr1mwfkGRtks1JNk9PT/fZhiRpNn2Fe1Xtqqq9VfUM8FmeHXqZAo7vWfU44Ik5PmN9VU1W1eTExEQ/bUiS5tBXuCdZ1jP7dmDflTQ3AauTHJ7kBGAl8PXBWpQkLdZh862QZCNwKrA0yRRwEXBqklXMDLnsAN4HUFX3JrkWuA/YA7y/qvaOpHNJ0pzmDfeqOneW8uU/Yf2LgYsHaUqSNBjvUJWkBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaNG+4J7kiye4kW3tqH09yf5J7ktyQ5IiuviLJj5Lc3b0+M8LeJUlzWMiZ+5XA6fvVNgGvqarXAt8CLuxZ9lBVrepe5w+nTUnSYswb7lV1G/DUfrVbqmpPN3s7cNwIepMk9WkYY+7vBb7UM39CkruSfDXJ6+faKMnaJJuTbJ6enh5CG5KkfQYK9yQfAfYAV3WlncDyqjoJ+BPg80leOtu2VbW+qiaranJiYmKQNiRJ++k73JOsAd4GvLOqCqCqnq6qJ7vpLcBDwCuG0agkaeH6CvckpwMfAs6sqh/21CeSLOmmTwRWAg8Po1FJ0sIdNt8KSTYCpwJLk0wBFzFzdczhwKYkALd3V8a8AfiLJHuAvcD5VfXUrB8sSRqZecO9qs6dpXz5HOteD1w/aFOSpMF4h6okNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAbNG+5JrkiyO8nWntpRSTYlebB7P7Jn2YVJtid5IMlbRtW4JGluCzlzvxI4fb/aOuDWqloJ3NrNk+RVwGrg1d02n06yZGjdSpIWZN5wr6rbgKf2K58FbOimNwBn99Svrqqnq+oRYDtw8nBalSQtVL9j7sdU1U6A7v3orn4s8FjPelNd7XmSrE2yOcnm6enpPtuQJM1m2F+oZpZazbZiVa2vqsmqmpyYmBhyG5J0aOs33HclWQbQve/u6lPA8T3rHQc80X97kqR+9BvuNwFruuk1wI099dVJDk9yArAS+PpgLUqSFuuw+VZIshE4FViaZAq4CLgEuDbJecCjwDkAVXVvkmuB+4A9wPurau+IepckzWHecK+qc+dYdNoc618MXDxIU5KkwXiHqiQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBs37mL25JHklcE1P6UTgz4EjgD8Aprv6h6vq5n73I0lavL7DvaoeAFYBJFkCPA7cALwH+FRVfWIYDUqSFm9YwzKnAQ9V1beH9HmSpAEMK9xXAxt75i9Ick+SK5IcOdsGSdYm2Zxk8/T09GyrSJL6NHC4J/lZ4EzgH7vSZcDLmRmy2QlcOtt2VbW+qiaranJiYmLQNiRJPYZx5v7bwJ1VtQugqnZV1d6qegb4LHDyEPYhSVqEYYT7ufQMySRZ1rPs7cDWIexDkrQIfV8tA5DkhcCbgff1lP8yySqggB37LZMkHQADhXtV/RD4hf1q7xqoI2kWK9Z9cWz73nHJGWPbt9Qv71CVpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktSgQZ+hugP4AbAX2FNVk0mOAq4BVjDzDNV3VNV3B2tTkrQYwzhz/82qWlVVk938OuDWqloJ3NrNS5IOoFEMy5wFbOimNwBnj2AfkqSfYNBwL+CWJFuSrO1qx1TVToDu/egB9yFJWqSBxtyBU6rqiSRHA5uS3L/QDbtfBmsBli9fPmAbkqReA525V9UT3ftu4AbgZGBXkmUA3fvuObZdX1WTVTU5MTExSBuSpP30He5JXpTkJfumgd8CtgI3AWu61dYANw7apCRpcQYZljkGuCHJvs/5fFV9Ock3gGuTnAc8CpwzeJuSpMXoO9yr6mHgdbPUnwROG6QpSdJgvENVkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDBnlA9vFJvpJkW5J7k3ygq380yeNJ7u5ebx1eu5KkhRjkAdl7gD+tqjuTvATYkmRTt+xTVfWJwduTJPVjkAdk7wR2dtM/SLINOHZYjUmS+jfImfuPJVkBnATcAZwCXJDk94HNzJzdf3cY+5HGYcW6L45lvzsuOWMs+1UbBv5CNcmLgeuBD1bV94HLgJcDq5g5s790ju3WJtmcZPP09PSgbUiSegwU7klewEywX1VVXwCoql1VtbeqngE+C5w827ZVtb6qJqtqcmJiYpA2JEn7GeRqmQCXA9uq6pM99WU9q70d2Np/e5Kkfgwy5n4K8C7gm0nu7mofBs5NsgooYAfwvgH2IUnqwyBXy3wNyCyLbu6/HUnSMHiHqiQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0aygOyJbXFh4L/9DPcpYPUuAJWbXBYRpIaNLJwT3J6kgeSbE+yblT7kSQ930iGZZIsAf4WeDMwBXwjyU1Vdd8o9jcujktKOliNasz9ZGB7VT0MkORq4CygqXCX1IZxfr8xqpO1UYX7scBjPfNTwK+OaF+H3BdPh9rPK2nxRhXumaVWz1khWQus7Wb/O8kDI+rlQFkKfGfcTRxEPB7P5fF41pzHIh87wJ0cBPKxgf7b+MW5Fowq3KeA43vmjwOe6F2hqtYD60e0/wMuyeaqmhx3HwcLj8dzeTye5bF4rlEdj1FdLfMNYGWSE5L8LLAauGlE+5Ik7WckZ+5VtSfJBcC/AEuAK6rq3lHsS5L0fCO7Q7WqbgZuHtXnH4SaGWIaEo/Hc3k8nuWxeK6RHI9U1fxrSZJ+qvjnBySpQYb7kCRZkuSuJP887l7GLckRSa5Lcn+SbUl+fdw9jUuSP05yb5KtSTYm+blx93QgJbkiye4kW3tqRyXZlOTB7v3IcfZ4IM1xPD7e/b9yT5IbkhwxjH0Z7sPzAWDbuJs4SPw18OWq+iXgdRyixyXJscAfAZNV9RpmLi5YPd6uDrgrgdP3q60Dbq2qlcCt3fyh4kqefzw2Aa+pqtcC3wIuHMaODPchSHIccAbwuXH3Mm5JXgq8AbgcoKr+t6r+a6xNjddhwM8nOQx4Ifvd79G6qroNeGq/8lnAhm56A3D2gexpnGY7HlV1S1Xt6WZvZ+a+oIEZ7sPxV8CfAc+MuY+DwYnANPB33TDV55K8aNxNjUNVPQ58AngU2Al8r6puGW9XB4VjqmonQPd+9Jj7OZi8F/jSMD7IcB9QkrcBu6tqy7h7OUgcBvwycFlVnQT8D4fWP7t/rBtLPgs4AXgZ8KIkvzfernSwSvIRYA9w1TA+z3Af3CnAmUl2AFcDb0zyD+NtaaymgKmquqObv46ZsD8UvQl4pKqmq+r/gC8AvzHmng4Gu5IsA+jed4+5n7FLsgZ4G/DOGtL16Yb7gKrqwqo6rqpWMPNl2b9V1SF7dlZV/wk8luSVXek0Dt0/9fwo8GtJXpgkzByLQ/LL5f3cBKzpptcAN46xl7FLcjrwIeDMqvrhsD7XZ6hqFP4QuKr7u0IPA+8Zcz9jUVV3JLkOuJOZf27fxSF2d2aSjcCpwNIkU8BFwCXAtUnOY+YX4Dnj6/DAmuN4XAgcDmyaOQfg9qo6f+B9eYeqJLXHYRlJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSg/4fFfM6eXnO7cIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(eval_results[\"evaluation\"][\"hist_stats\"][\"episode_lengths\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f663184c-6cf5-4d8c-b500-f47d51319094",
   "metadata": {},
   "source": [
    "- The short ones are the failures, since it's impossible to reach the goal in 4 steps.\n",
    "- Most of the time we reach the goal in the minimum number of steps (6)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72beef15-af71-4a24-8a5e-59ec40fe271a",
   "metadata": {},
   "source": [
    "#### Configuring the trainer\n",
    "\n",
    "```python\n",
    "trainer = rllib.agents.ppo.PPOTrainer(env=\"FrozenLake-v1\", \n",
    "                                      config=trainer_config)\n",
    "```\n",
    "\n",
    "Remember this line? Earlier, we hid the `trainer_config`. Here's the config we hid:\n",
    "\n",
    "```python\n",
    "trainer_config = {\n",
    "    \"framework\"             : \"torch\",\n",
    "    \"create_env_on_driver\"  : True,\n",
    "    \"seed\"                  : 0,\n",
    "    \"env_config\"            : {\"is_slippery\" : False}}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cfa9d1-fbfa-455f-a722-950b634d4d45",
   "metadata": {},
   "source": [
    "#### Configuring the trainer\n",
    "\n",
    "```python\n",
    "trainer_config = {\n",
    "    \"framework\"             : \"torch\",\n",
    "    \"create_env_on_driver\"  : True,\n",
    "    \"seed\"                  : 0,\n",
    "    \"env_config\"            : {\"is_slippery\" : False}}\n",
    "```\n",
    "\n",
    "- `config={\"framework\" : \"torch\"}`: RLlib works with tensorflow (`\"tf\"` or `\"tf2\"`) and pytorch (`\"torch\"`)\n",
    "- `\"create_env_on_driver\" : True`: This relates to Ray, and we will touch on it briefly, but not much in this course\n",
    "-  `\"seed\" : 0`: This is used for reproducibility of the teaching materials, and is not normally needed\n",
    "- `\"env_config\" : {\"is_slippery\" : False}`: this selects the non-slippery Frozen Lake\n",
    "  - All environment hyperparameters go in this sub-dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d205da70-1255-437c-8a99-6ea8a5e8d2a0",
   "metadata": {},
   "source": [
    "#### Let's apply what we learned!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd539d44-aa7c-4958-b448-7759f3adc20e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## RLlib trainer methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17580d1a-2245-4390-bfee-449b1bfff570",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "_Which of the following most accurately describes the role of `trainer.train()` in RLlib?_\n",
    "\n",
    "- [ ] It neither collects a data set of episodes nor learns a policy. | Are you sure?\n",
    "- [ ] It learns a policy from a fixed data set of episodes. | Remember, calling train() causes the agent to play through episodes.\n",
    "- [ ] It creates a data set of episodes but does not learn a policy. | Remember, calling train() learns a policy.\n",
    "- [x] It simultaneously collects a data set of episodes and also learns a policy. | You got it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48245b9-3007-4d9c-a657-97978c4fea1e",
   "metadata": {},
   "source": [
    "#### Passing in the dataset\n",
    "\n",
    "_When using scikit-learn for supervised learning we call `fit(X,y)`, but with RLlib we call `train()` without passing in the dataset. Why?_\n",
    "\n",
    "- [x] Because RLlib was given access to the environment when the trainer was initialized, and this is all it needs.\n",
    "- [ ] Because reinforcement learning does not involve data.\n",
    "- [ ] Because X and y are passed into a different RLlib method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc34288f-7416-4041-8b70-5182715bfd0e",
   "metadata": {},
   "source": [
    "#### Scoring\n",
    "\n",
    "_Which of the following RLlib trainer methods is most analogous to scikit-learn's `.predict()` function?_\n",
    "\n",
    "- [ ] `train` | This is more like `fit` in scikit-learn.\n",
    "- [x] `compute_single_action` | Correct!\n",
    "- [ ] `evaluate` | This is more like `score()` in scikit-learn. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34d5c5b-a1be-447a-abe6-e034c9487cce",
   "metadata": {},
   "source": [
    "## Slippery Frozen Lake\n",
    "\n",
    "In the slides, we trained an agent to reliably reach the goal in the **non-slippery** Frozen Lake environment. Here, try the same thing with the **slippery** Frozen Lake. Train your agent until it reaches the goal at least 20% of the time. Then, answer the multiple choice question below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2847c21-1709-4039-b77d-d634e43e6096",
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '____' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rllib\n\u001b[1;32m      3\u001b[0m slippery_trainer_config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframework\u001b[39m\u001b[38;5;124m\"\u001b[39m             : \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreate_env_on_driver\u001b[39m\u001b[38;5;124m\"\u001b[39m  : \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m                  : \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv_config\u001b[39m\u001b[38;5;124m\"\u001b[39m            : {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_slippery\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;28;01mTrue\u001b[39;00m}}\n\u001b[0;32m----> 9\u001b[0m slippery_trainer \u001b[38;5;241m=\u001b[39m rllib\u001b[38;5;241m.\u001b[39magents\u001b[38;5;241m.\u001b[39mppo\u001b[38;5;241m.\u001b[39mPPOTrainer(\u001b[43m____\u001b[49m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(____):\n\u001b[1;32m     12\u001b[0m     train_info \u001b[38;5;241m=\u001b[39m slippery_trainer\u001b[38;5;241m.\u001b[39m____()\n",
      "\u001b[0;31mNameError\u001b[0m: name '____' is not defined"
     ]
    }
   ],
   "source": [
    "from ray import rllib\n",
    "\n",
    "slippery_trainer_config = {\n",
    "    \"framework\"             : \"torch\",\n",
    "    \"create_env_on_driver\"  : True,\n",
    "    \"seed\"                  : 0,\n",
    "    \"env_config\"            : {\"is_slippery\" : True}}\n",
    "\n",
    "slippery_trainer = rllib.agents.ppo.PPOTrainer(____)\n",
    "\n",
    "for i in range(____):\n",
    "    train_info = slippery_trainer.____()\n",
    "    \n",
    "eval_results = ____.evaluate()\n",
    "\n",
    "print(eval_results[\"evaluation\"][\"episode_reward_mean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2f2fdf9-ec1a-488d-8864-0a929e42190c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of reaching goal: 32.9%\n",
      "Action performed from top-right: 3\n"
     ]
    }
   ],
   "source": [
    "# BEGIN SOLUTION\n",
    "from ray import rllib\n",
    "\n",
    "slippery_trainer_config = {\n",
    "    \"framework\"             : \"torch\",\n",
    "    \"create_env_on_driver\"  : True,\n",
    "    \"seed\"                  : 0,\n",
    "    \"env_config\"            : {\"is_slippery\" : True},\n",
    "    \"evaluation_config\"     : {\"explore\" : False}}\n",
    "\n",
    "slippery_trainer = rllib.agents.ppo.PPOTrainer(env=\"FrozenLake-v1\", config=slippery_trainer_config)\n",
    "\n",
    "for i in range(12): # There is randomness here, but 12 should be enough most of the time\n",
    "    train_info = slippery_trainer.train()\n",
    "    \n",
    "eval_results = slippery_trainer.evaluate()\n",
    "\n",
    "print(\"Frequency of reaching goal: %.1f%%\" % (eval_results[\"evaluation\"][\"episode_reward_mean\"]*100))\n",
    "\n",
    "print(\"Action performed from top-right:\", slippery_trainer.compute_single_action(3, explore=False))\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6485b269-1c62-4cd8-a8d2-0a6cea590aeb",
   "metadata": {},
   "source": [
    "#### Action performed from top-right\n",
    "\n",
    "What action is performed when the agent is at the top-right of the arena? This is printed out by the code.\n",
    "\n",
    "- [ ] left (0) | When we ran the code, we got something different here.\n",
    "- [ ] down (1) | When we ran the code, we got something different here.\n",
    "- [ ] right (2) | When we ran the code, we got something different here.\n",
    "- [x] up (3) | You got it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb1e37b-ca77-41ba-ba20-6a9558485b80",
   "metadata": {},
   "source": [
    "#### Interpreting the policy\n",
    "\n",
    "Recall that the arena looks like this:\n",
    "\n",
    "```\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "```\n",
    "\n",
    "In this slippery environment, you do your intended action 1/3 of the time, and each of the two perpendicular directions 1/3 of the time. You never go the opposite of the intended direction.\n",
    "\n",
    "In the previous question, we saw that, from the **top-right corner** (observation 3), the agent tries to move up (action 3). Why do you think this is?\n",
    "\n",
    "- [ ] \"Up\" is an arbitrary choice because we did not train the agent.\n",
    "- [ ] The agent receives a reward for the \"up\" action. \n",
    "- [x] The agent wants to avoid falling into the hole below it, so the \"up\" action is the safest choice.\n",
    "- [ ] Moving up brings the agent closer to the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6572684-0981-4aaf-8744-5baf38df5f1c",
   "metadata": {},
   "source": [
    "## Ex 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26562cd0-18dd-4904-91d7-0c09692c0157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "from ipywidgets import Output\n",
    "from IPython import display\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a10ff1-69be-4eb5-a35d-501f9fc39131",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bf59ce-cfb6-444f-95ef-7204882e0bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bccc0b3b-0034-4241-9781-c0c298cc9f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: should I switch from PPO to DQN so that I don't need explore=False when getting an action? and from evaluation config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee01e4a-aba9-4dc0-9e42-7a081493e90f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rl-course]",
   "language": "python",
   "name": "conda-env-rl-course-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
