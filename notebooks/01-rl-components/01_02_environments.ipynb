{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c331561e-c62d-4486-9252-b9ff362f8fa4",
   "metadata": {},
   "source": [
    "## RL Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de064a2a-0a07-4c94-a9ac-b6f2d978f412",
   "metadata": {},
   "source": [
    "#### What is an environment?\n",
    "\n",
    "- An environment could be:\n",
    "  - a game, like a video game.\n",
    "  - a simulation of a real world scenario, like a robot, user behavior, or the stock market\n",
    "  - any other setup with an _agent_ who takes _actions_, views _observations_, and receives _rewards_\n",
    "  \n",
    "Terminology note: we will use _agent_ and _player_ interchangeably. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5748d00-5c07-49db-aa9f-3cc1613bd0a7",
   "metadata": {},
   "source": [
    "#### Running example: frozen lake\n",
    "\n",
    "As a running example of an environment, we will use the [Frozen Lake](https://gym.openai.com/envs/FrozenLake-v0/) environment from [OpenAI Gym](https://gym.openai.com/), which provides the standard interface for RL problems. We can visualize the environment like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f87dba7-b5de-4ba9-b89d-b5ea2d069f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693db547-8f4d-49ae-b759-80308d6803e0",
   "metadata": {},
   "source": [
    "The goal is for the player (red highlight) to reach the goal (`G`) by walking on the frozen lake segments (`F`) without falling in the holes (`H`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a34b31-5bbb-45f4-9e7f-e925fc96d0ee",
   "metadata": {},
   "source": [
    "#### Movement\n",
    "\n",
    "The player can move around the frozen lake. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d5d954a-7890-427c-aea1-7daec590c91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.step(1); # 1 -> Down\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb8ff83-f2b0-464f-934c-9e6144977c2c",
   "metadata": {},
   "source": [
    "Don't worry about `step(1)` for now; we'll get to that. \n",
    "\n",
    "What you can see is that the player (red highlight) moved downward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c7ab81-0402-4302-a46f-1b4bc115ca63",
   "metadata": {},
   "source": [
    "#### Goal\n",
    "\n",
    "Fast-forward a lot of steps, and you've completed the puzzle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a799321-538a-4f9c-ae1d-31e15a9e0e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env.step(1)\n",
    "env.step(2)\n",
    "env.step(1)\n",
    "env.step(2)\n",
    "env.step(1)\n",
    "env.step(2)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98858094-a152-4e3e-bd03-ed5c81af095b",
   "metadata": {},
   "source": [
    "You've achieved the goal by reaching `G`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1635d0e4-7d38-434c-a956-d47646e57b4a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### What makes an environment?\n",
    "\n",
    "An environment involves several key components, that we'll go through in the following slides."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73df9894-c37a-4d34-93d9-c4e43ecf0564",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### States\n",
    "\n",
    "- We'll use the term _state_ informally to refer to everything about the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a57b3f1-c76e-4b2c-9d84-742326bb827b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16f9748-129e-45c7-9bc1-106e0493fdad",
   "metadata": {},
   "source": [
    "- For example, this is the starting state of the environment.\n",
    "- The player is at the top-left, there's some frozen ice nearby, etc.\n",
    "- We'll use the concept of a state to talk about our environment, but it won't appear in the \"API\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0e72e4-8f86-4c05-9f33-7e888ef44045",
   "metadata": {},
   "source": [
    "#### Actions\n",
    "\n",
    "- Here, the player can choose between 4 possible actions: up, down, left, right\n",
    "- The space of all possible actions is called the **action space**.\n",
    "- In SL we distinguish between regression (continuous $y$) and classification (categorical $y$)\n",
    "- Likewise in RL the action space can be continuous or discrete\n",
    "- In this case, it is discrete (4 possibilities)\n",
    "- The code agrees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "757cfd69-d93d-4e8b-90d0-f3caf079f5a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1907e0-63fe-4c72-af65-f9869b590365",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "- The observations are the _parts of the state that the agent can see_.\n",
    "- Sometimes, the agent can see everything; we call this _fully observable_.\n",
    "- Oftentimes, we have _partially observable_ environments. \n",
    "- In the Frozen Lake example, the agent can only see its own location out of the 16 squares.\n",
    "- The agent is not \"told\" where the holes are via direct observations, so it will need to _learn_ this via trial and error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659ebb9b-4c6a-43f6-96c1-eb5117c50a54",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "- The space of all possible observations is called the **observation space**.\n",
    "- You can think of the action space as analogous to the target in supervised learning.\n",
    "- You can think of the observation space as analogous to the features in supervised learning.\n",
    "\n",
    "\n",
    "Here, we have a discrete observation space consisting of the 16 possible player positions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81aa1ad6-e1bb-492d-a5d0-a72c94a33e29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ee9cb4-8650-4d39-87b7-97202ffca199",
   "metadata": {},
   "source": [
    "#### Rewards\n",
    "\n",
    "- In supervised learning, the goal is usually to make good predictions.\n",
    "- You may still try different loss functions depending on your specific goal, but the general concept is the same.\n",
    "- In RL, the goal could be anything.\n",
    "- But, like in SL, you will need to be _optimizing_ something.\n",
    "- In RL, we aim to maximize the **reward**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf68c60b-f6fe-47e0-8482-bc51c4410846",
   "metadata": {},
   "source": [
    "#### Rewards \n",
    "\n",
    "In the Frozen Lake example, the agent receives a reward when it reaches the goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4807bbe7-7525-4772-9ce9-a0e06f4b21fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "reward = 0.0\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "obs, reward, done, _ = env.step(0)\n",
    "env.render()\n",
    "print(\"reward =\", reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aec3a5cd-5607-41fb-a932-7e5e10bf6569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward = 0.0\n"
     ]
    }
   ],
   "source": [
    "obs, reward, done, _ = env.step(1)\n",
    "print(\"reward =\", reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceae768d-d87b-4611-afce-e02137b9fd72",
   "metadata": {},
   "source": [
    "Still no reward, let's keep going..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4c7004-91a1-4dcc-ad52-943cfd179a97",
   "metadata": {},
   "source": [
    "#### Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89518a96-7c75-437d-9bb1-da7dbc7038b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "reward = 1.0\n"
     ]
    }
   ],
   "source": [
    "env.step(1)\n",
    "env.step(2)\n",
    "env.step(1)\n",
    "env.step(2)\n",
    "env.step(1)\n",
    "obs, reward, done, _ = env.step(2)\n",
    "env.render()\n",
    "print(\"reward =\", reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f9cfd3-9e69-4752-9927-ba108685aa83",
   "metadata": {},
   "source": [
    "We got a reward of 1.0 for reaching the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e888ae70-a3f2-400b-8fd9-54dd27369beb",
   "metadata": {},
   "source": [
    "#### Representing actions\n",
    "\n",
    "- To use RL software, we will need a numerical representation of our action space and our observation space.\n",
    "- In this case, we have 4 possible discrete actions, so we can encode them as {0,1,2,3} for (left, down, right, up).\n",
    "- This is why, earlier, we did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "420363c1-9385-47e0-abaf-935a5c0fb4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5a3c2f-ec47-47c4-85e8-95945dfa7019",
   "metadata": {},
   "source": [
    "to walk downward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6419f73c-8431-4e46-87af-0a99a95a58ba",
   "metadata": {},
   "source": [
    "#### Representing observations\n",
    "\n",
    "- Likewise, we will need a numerical representation of our observations.\n",
    "- Here, there are 16 possible positions of the player. These are encoded from 0-15 as follows:\n",
    "\n",
    "```\n",
    " 0   1   2   3\n",
    " 4   5   6   7\n",
    " 8   9  10  11\n",
    "12  13  14  15\n",
    "```\n",
    "\n",
    "These details of the Frozen Lake environment are also available in the [documentation](https://www.gymlibrary.ml/pages/environments/toy_text/frozen_lake)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cecba2-2529-4f24-a5ef-d730457ac994",
   "metadata": {},
   "source": [
    "#### Representing observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b616b8-5ccf-45bf-a4ac-65ae31f108e4",
   "metadata": {},
   "source": [
    "Initially, we observe \"0\" because we start at the upper-left:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "df316735-5a38-488f-80ae-dd97d715cc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b5dad3-f8bc-4298-8dc4-4a3f87285600",
   "metadata": {},
   "source": [
    "After moving to the down (action 1), we move to position 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7d540232-affa-45de-a10f-6f99f755d5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, reward, done, _ = env.step(1)\n",
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b84f58-bf29-4df0-9b85-08190d1815f0",
   "metadata": {},
   "source": [
    "The observation is returned by the `step()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f179e5d-68cb-4d31-a5e0-cd01f5b14124",
   "metadata": {},
   "source": [
    "#### Non-deterministic environments\n",
    "\n",
    "- So far, taking a particular action from a particular state always resulted in the same new state.\n",
    "- In other words, our Frozen Lake environment was _deterministic_.\n",
    "- Some environments are _non-deterministic_, meaning the outcome of an action can be random.\n",
    "- We can initialize a non-deterministic Frozen Lake like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9bdde4c3-e7f9-419a-a25b-4f2d6027354e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_slippery = gym.make(\"FrozenLake-v1\", is_slippery=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6a7d44ef-2828-4c7f-aae9-17eab6b7d7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "env_slippery.seed(4); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3943d88f-a44b-4fba-96f8-20177b6317b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env_slippery.reset()\n",
    "env_slippery.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b97800f-5c7a-43a8-9018-f8505bd9eafd",
   "metadata": {},
   "source": [
    "#### Non-deterministic environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "35e5cb48-3e0f-4b5f-8f0a-663ff31e99e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env_slippery.step(1) # move down\n",
    "env_slippery.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0755c6-0d78-42d0-a9b4-cd4aec36db82",
   "metadata": {},
   "source": [
    "Moving down did not work as planned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cba5c8e8-a7ab-4854-9c35-ca79584f7b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env_slippery.step(1) # move down\n",
    "env_slippery.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031a6a4e-f9a1-4965-86b8-d439a82491ad",
   "metadata": {},
   "source": [
    "Moving down worked this time.\n",
    "\n",
    "In this \"slippery\" Frozen Lake environment, movement only works as intended 1/3 of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0388fe5a-b6e4-484c-ac51-d376c353ffaa",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Episodes\n",
    "\n",
    "- Playing the Frozen Lake has an end - either you fall into a hole or you reach the goal.\n",
    "- However, one play-through is not enough for an RL algorithm to learn from.\n",
    "- It will need multiple play-throughs, called **episode**.\n",
    "- After an episode, the environment is reset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68508dc1-2009-48a7-85e1-3f4bba4b0220",
   "metadata": {},
   "source": [
    "#### Episodes\n",
    "\n",
    "The `step()` method returns a flag telling us whether the episode is over:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "534b8516-26ea-4df4-9062-e6fb1f57d3a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, reward, done, _ = env_slippery.step(1)\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a0684e7c-b767-4907-965d-6bcc86d64c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env_slippery.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdbecc7-7aac-432a-9e6e-301af52e7d2a",
   "metadata": {},
   "source": [
    "Here the episode is done because we fell into a hole."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854521ed-7fb7-453b-9eb6-eaf61074f533",
   "metadata": {},
   "source": [
    "#### Episodes\n",
    "\n",
    "- In some environments (like Frozen Lake), rewards are only received at the end of an episode.\n",
    "- In other environments, rewards can be received at any **time step** (i.e., after an action)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea0a22c-c13f-407e-beb1-5fd0a985dc4a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Putting it all together\n",
    "\n",
    "- We've now talked about the main components of an RL environment:\n",
    "  - States\n",
    "  - Actions\n",
    "  - Observations\n",
    "  - Rewards\n",
    "  - Episodes\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8de8cd1-8733-4c90-89d6-9586be21309a",
   "metadata": {},
   "source": [
    "#### SL datasets vs. RL environments\n",
    "\n",
    "- In supervised learning, you are typically given a dataset.\n",
    "- In RL, the environment acts as a _data generator_.\n",
    "  - The more you play through the environment, the more \"data\" you generate and the more you can learn.\n",
    "- One can also do RL on a pre-collected dataset (called _offline RL_), but that is out of scope for us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be83b356-8218-44dd-9ccd-f0bc44ade74e",
   "metadata": {},
   "source": [
    "#### Let's apply what we learned!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c511b50-fae2-4764-8eaa-c47ca7dab1d7",
   "metadata": {},
   "source": [
    "## Self-driving car environment\n",
    "<!-- multiple choice -->\n",
    "\n",
    "You're using RL to train a self-driving car. The car AI uses various cameras and sensors as its inputs and has to decide the angle of the steering wheel as well as the angle of gas/brake pedals on the floor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a443612-50ff-4564-bdf4-a4e1ba79be93",
   "metadata": {},
   "source": [
    "#### Is the observation space continuous or discrete?\n",
    "\n",
    "- [x] Continuous | Correct! \n",
    "- [ ] Discrete | In this case, the observations are the sensor inputs, e.g. depth estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cace1880-0440-42f1-a087-576821998a4a",
   "metadata": {},
   "source": [
    "#### Is the action space continuous or discrete?\n",
    "\n",
    "- [x] Continuous\n",
    "- [ ] Discrete | The actions are angles; they don't come from a discrete set of options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124e3763-a526-4cd7-84c1-721bb7b9f102",
   "metadata": {},
   "source": [
    "#### What would be the most reasonable reward structure for this environment?\n",
    "\n",
    "- [ ] Reward equals the amount of time the car was able to drive without crashing | What would the reward be if the car never moves?\n",
    "- [x] Reward equals the distance the car was able to drive without crashing | Yes, that sounds good!\n",
    "- [ ] +1 reward every time the car crashes | Keep in mind we want to maximize reward, not minimize it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcfbbb6-1765-473c-8b7e-c24e914949d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Episodes vs. time steps\n",
    "<!-- multiple choice -->\n",
    "\n",
    "Fill in the blanks in the following sentence: \n",
    "\n",
    "*In a reinforcement learning environment, one takes actions repeatedly until the \\_\\_\\_\\_\\_ ends. This may involve only one \\_\\_\\_\\_\\_, or very many.*\n",
    "\n",
    "- [ ] time step / reward | Check the first blank carefully!\n",
    "- [ ] reward / time step | Try again!\n",
    "- [ ] time step / episode | Try again!\n",
    "- [x] episode / time step | You got it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5286ef-84b7-4b04-a74b-d90b0216abae",
   "metadata": {},
   "source": [
    "## Gym's taxi environment\n",
    "<!-- coding exercise -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22599988-4849-4a9b-aa83-2b792dedbc19",
   "metadata": {},
   "source": [
    "In this exercise we'll look at one of the text-based environments bundled with OpenAI gym, called the taxi environment. Documentation [here](https://www.gymlibrary.ml/environments/toy_text/taxi/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cb32c250-3e05-4040-bad0-5f8f0d7e3123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE\n",
    "import gym\n",
    "\n",
    "taxi = gym.make(\"Taxi-v3\")\n",
    "taxi.seed(4)\n",
    "obs = taxi.reset()\n",
    "\n",
    "taxi.____\n",
    "\n",
    "taxi.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413240b7-6df6-48a5-bb24-4f8a518dc81e",
   "metadata": {},
   "source": [
    "Here, the taxi is represented by the yellow highlight, currently at position (0,1). The `:` can be crossed but the `|` cannot. The goal is to pick up passengers and drop them off. \n",
    "\n",
    "There as 6 possible actions: down (0), up (1), right (2), left (3), pick up (4), drop off (5). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "99f35e5c-5e97-4122-9ae0-44e849a51c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(6)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5201b9-8041-4044-b172-a736f43451df",
   "metadata": {},
   "source": [
    "The following code prints out the observation in a more human-readable format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f0083927-ad7d-4e78-b04f-316c394f0139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taxi row: 3\n",
      "Taxi col: 3\n",
      "Passenger loc: 3\n",
      "Destination loc: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Taxi row: %d\\nTaxi col: %d\\nPassenger loc: %d\\nDestination loc: %d\" % tuple(taxi.decode(obs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2553c73a-e6e8-4096-9b67-1806dae7f79f",
   "metadata": {},
   "source": [
    "The possible locations are `R` (0), `G` (1), `Y` (2), `B` (3) and in taxi (4). Thus, the passenger is currently at `G` and is heading to `Y`.\n",
    "\n",
    "To answer the following question, you will need to modify the code, run it (possibly multiple times), and print out any relevant output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a4208ce9-efb2-48e7-8e7a-3b6042288309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "import gym\n",
    "\n",
    "taxi = gym.make(\"Taxi-v3\")\n",
    "taxi.seed(4)\n",
    "obs = taxi.reset()\n",
    "taxi.render()\n",
    "taxi.step(0)\n",
    "taxi.step(4)\n",
    "taxi.step(2)\n",
    "taxi.step(1)\n",
    "taxi.step(1)\n",
    "taxi.step(1)\n",
    "taxi.step(1)\n",
    "obs, reward, done, _ = taxi.step(5)\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "598a018b-569e-4378-bb2a-0de521c18881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: in this case there is no testing of the code; the code is only used to explore\n",
    "# this may not work well with the framework - add an automated test to the code as well? \n",
    "# e.g. can check that the state of the env is correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9564fe-af4a-4ded-97d3-4b754df46265",
   "metadata": {},
   "source": [
    "#### How much reward does the agent get for successfully dropping off the passenger?\n",
    "\n",
    "- [ ] 0 | Try performing the necessary actions with `taxi.step()` and printing out the rewards.\n",
    "- [ ] 5 | Try performing the necessary actions with `taxi.step()` and printing out the rewards.\n",
    "- [ ] 10 | Try performing the necessary actions with `taxi.step()` and printing out the rewards.\n",
    "- [x] 20 | You got it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fffe5b6-36c3-4b2b-9513-a6125f0eaac3",
   "metadata": {},
   "source": [
    "## Observations vs. renderings\n",
    "<!-- coding exercise -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a7f02e-4367-48d5-b491-8ee5cce4f468",
   "metadata": {},
   "source": [
    "The frozen lake environment allows us to create a visual rendering of the environment, which we've seen earlier. This is for human/debugging purposes, and is _not_ seen by the agent/algorithm. Your task here is to play a given Frozen Lake environment **without looking at the visual rendering** (no cheating!). Populate the `actions` list to contain a set of actions that correctly gets the agent to the goal. What you're experiencing is what a RL algorithm \"sees\" when learning!\n",
    "\n",
    "Note that the given Frozen Lake environment is 3x3 instead of 4x4. Thus, the observation space runs from 0 to 8 instead of 0 to 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2ba815",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "env = gym.make(\"FrozenLake-v1\", \n",
    "               desc=gym.envs.toy_text.frozen_lake.generate_random_map(size=3, p=0.3), \n",
    "               is_slippery=False)\n",
    "env.render = None\n",
    "\n",
    "obs = env.reset()\n",
    "actions = ____\n",
    "for action in actions:\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    print(\"Obs:\", obs, \"Reward:\", reward, \"Done:\", done)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c7a5f827-8aad-4f48-b762-1feb4b51748a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs: 3 Reward: 0.0 Done: False\n",
      "Obs: 4 Reward: 0.0 Done: False\n",
      "Obs: 7 Reward: 0.0 Done: False\n",
      "Obs: 8 Reward: 1.0 Done: True\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "env = gym.make(\"FrozenLake-v1\", desc=gym.envs.toy_text.frozen_lake.generate_random_map(size=3, p=0.3), is_slippery=False)\n",
    "env.render = None\n",
    "\n",
    "obs = env.reset()\n",
    "actions = []\n",
    "# BEGIN SOLUTION\n",
    "actions = [1,2,1,2]\n",
    "# END SOLUTION\n",
    "for action in actions:\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    print(\"Obs:\", obs, \"Reward:\", reward, \"Done:\", done)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e8a4b0-b102-4047-8e67-942ef81b0ed9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### What does the environment look like?\n",
    "\n",
    "Recalling that the actions 0, 1, 2, 3 represent left, down, right, up (respectively), which of the following statements correctly describes the best path to the goal?\n",
    "\n",
    "- [ ] Start by moving down, then down again\n",
    "- [x] Start by moving down, then right\n",
    "- [ ] Start by moving right, then right again\n",
    "- [ ] Start by moving right, then down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8690d35a-5d51-4452-bd9c-8b44161feb81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rl-course]",
   "language": "python",
   "name": "conda-env-rl-course-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
