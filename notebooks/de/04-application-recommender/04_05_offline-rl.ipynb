{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5aa21ad-6bb9-4990-ab4d-fb9435b0bc00",
   "metadata": {},
   "source": [
    "## Offline RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "204bbdb7-e15f-4e5c-be77-b58b39dd3dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f337b2ca-8719-4938-8983-3a02bc76626f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import ray\n",
    "import logging\n",
    "ray.init(log_to_driver=False, ignore_reinit_error=True, logging_level=logging.ERROR); # logging.FATAL\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fd0b8c-0fe5-4295-b752-061d9e199421",
   "metadata": {},
   "source": [
    "#### Ist das realistisch?\n",
    "\n",
    "- Bis jetzt haben wir eine Simulation des Nutzerverhaltens erstellt\n",
    "- Bei einigen Anwendungen k√∂nnen wir vielleicht genaue Simulationen erstellen:\n",
    "  - physiksimulationen (z. B. Roboter)\n",
    "  - spiele\n",
    "  - wirtschaftliche/finanzielle Simulationen?\n",
    "- F√ºr das Nutzerverhalten ist dies jedoch schwierig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb08a59-42f4-4fce-b709-1907ea5ef3c9",
   "metadata": {},
   "source": [
    "#### Ist das realistisch? \n",
    "\n",
    "- Am besten w√§re es, RL live einzusetzen, aber das ist nicht praktikabel\n",
    "- Eine andere M√∂glichkeit: aus Nutzerdaten lernen?\n",
    "- Wir k√∂nnen das mit **offline reinforcement learning** machen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba4b7ef-32c4-4caa-bb22-eace97236370",
   "metadata": {},
   "source": [
    "#### Offline RL\n",
    "\n",
    "- Was ist Offline-RL?\n",
    "- Erinnere dich an unsere RL-Schleife:\n",
    "\n",
    "![](img/RL-loop-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d89bcc9-17c8-4dbc-afb0-9f9e4810dad0",
   "metadata": {},
   "source": [
    "#### Offline RL\n",
    "\n",
    "- Im Offline-RL haben wir keine Environment, mit der wir in einer Feedbackschleife interagieren k√∂nnen:\n",
    "\n",
    "![](img/offline-RL-loop.png)\n",
    "\n",
    "Diese historischen Daten wurden durch eine andere, unbekannte Policy/andere Policyen erzeugt.\n",
    "\n",
    "Anmerkungen:\n",
    "\n",
    "K√∂nnten von echten Nutzern oder von einer anderen Quelle (zuf√§llig oder von einem RL-Agenten!) erzeugt worden sein"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d6cd2e-9bff-4b43-a49f-38e3211c4a09",
   "metadata": {},
   "source": [
    "#### Herausforderung des Offline-RL\n",
    "\n",
    "- Wir k√∂nnen keine \"Was w√§re wenn\"-Fragen beantworten\n",
    "- Wir k√∂nnen nur die Ergebnisse der versuchten Aktionen im Datensatz sehen\n",
    "\n",
    "Anmerkungen:\n",
    "\n",
    "Vielleicht lernen wir dadurch zu sch√§tzen, wie wertvoll und toll es ist, eine Simulation zur Verf√ºgung zu haben, die wir den ganzen Kurs √ºber hatten. Sie erm√∂glicht es uns, alles auszuprobieren, ohne dass es uns etwas kostet, au√üer den Rechenaufwand (vorausgesetzt, es handelt sich um einen Simulator und nicht um eine reale Environment) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f9ff23b-8073-44b0-bb32-320ab7fa0a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # HIDDEN\n",
    "\n",
    "# # generate the offline dataset\n",
    "# env_config = {\n",
    "#     \"num_candidates\" : 2,\n",
    "#     \"alpha\"          : 0.5,\n",
    "#     \"seed\"           : 42\n",
    "# }\n",
    "\n",
    "# from ray.rllib.algorithms.ppo import PPO, PPOConfig\n",
    "\n",
    "# ppo_config = (\n",
    "#     PPOConfig()\\\n",
    "#     .framework(\"torch\")\\\n",
    "#     # need to set num_rollout_workers=1 for now per https://github.com/ray-project/ray/issues/25696\n",
    "#     .rollouts(create_env_on_local_worker=True, num_rollout_workers=1)\\\n",
    "#     .debugging(seed=0, log_level=\"ERROR\")\\\n",
    "#     .training(model={\"fcnet_hiddens\" : [64, 64]}, lr=0.001)\\\n",
    "#     # .environment(env_config=env_config)\\\n",
    "#     .offline_data(output=\"data/recommender2\")\n",
    "# )\n",
    "\n",
    "# from envs_04 import BasicRecommenderWithHistory\n",
    "\n",
    "# ppo_history = ppo_config.build(env=\"CartPole-v1\")\n",
    "\n",
    "# rewards_history = []\n",
    "# for i in range(25):\n",
    "#     result = ppo_history.train()\n",
    "#     rewards_history.append(result[\"episode_reward_mean\"])\n",
    "    \n",
    "# ppo_history.evaluate(duration_fn=1000)[\"evaluation\"][\"episode_reward_mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23be8a47-e338-408d-abac-63bbefe41dac",
   "metadata": {},
   "source": [
    "#### Empfehlungsdatensatz\n",
    "\n",
    "- Schauen wir uns einen Offline-Datensatz an, aus dem wir lernen k√∂nnen.\n",
    "- Wir brauchen ein bisschen Code, um alle JSON-Objekte in der Datei zu lesen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8610f1a2-b704-4b22-aeba-3eb3826693e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json_dataset_file = \"data/recommender_offline.json\"\n",
    "\n",
    "rollouts = []\n",
    "with open(json_dataset_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        rollouts.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9540859-74d2-4c4f-a06c-1cd6fd848994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rollouts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6f10f9-acb9-4aff-9694-622faf8792e1",
   "metadata": {},
   "source": [
    "Wir haben 50 \"Rollouts\" von Daten.\n",
    "\n",
    "Anmerkungen:\n",
    "\n",
    "Die Datei ist in dem Format, aus dem RLlib lernt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433484e4-8250-473f-9579-f712e9fa9e17",
   "metadata": {},
   "source": [
    "#### Empfehlungsdatensatz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b685e2f5-4381-40ca-bff8-cc1398d76ddb",
   "metadata": {},
   "source": [
    "Jedes Rollout ist ein Diktat mit Informationen √ºber den Zeitschritt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c1e82cd-f6f4-4076-aa36-2a16dc761447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.rllib.utils.compression import unpack, pack\n",
    "\n",
    "obs = unpack(rollouts[0][\"obs\"])\n",
    "obs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ea21ad-940d-4aa0-8a63-43c527d233bd",
   "metadata": {},
   "source": [
    "- Wir haben 200 Zeitschritte an Daten in jedem Rollout\n",
    "- Schauen wir uns zuerst die Beobachtungen an\n",
    "\n",
    "Anmerkungen:\n",
    "\n",
    "Die Zahl 200 wird durch den Konfigurationsparameter \"rollout_fragment_length\" des Algorithmus festgelegt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fce3e9-db24-4050-bcd4-fc08f760bf5d",
   "metadata": {},
   "source": [
    "#### Empfehlungsdatensatz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a579d766-069c-446c-aa3d-7798335ece38",
   "metadata": {},
   "source": [
    "Hier sind die ersten 3 Beobachtungen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7946e442-7f6a-4653-8e84-75bc37fe952a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6545137 , 0.29728338],\n",
       "       [0.5238871 , 0.5144319 ],\n",
       "       [0.6741674 , 0.10163702]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93775945-d5ff-4f9f-96c1-3a04e4ac2a85",
   "metadata": {},
   "source": [
    "\n",
    "Wir k√∂nnen sehen, dass \"Anzahl_Kandidaten\" auf 2 gesetzt wurde"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a1b0f3-6394-4afe-b444-37914d84ac89",
   "metadata": {},
   "source": [
    "#### Empfehlungsdatensatz\n",
    "\n",
    "Wir k√∂nnen uns auch die ersten 3 Aktionen, Belohnungen und Spenden ansehen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ace3caf4-ef70-4393-9846-e68648b41bfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rollouts[0][\"actions\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29ff0ca7-6fe4-4786-a19f-c9bee9bf0b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6545137166976929, 0.3524414300918579, 0.05838315561413765]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rollouts[0][\"rewards\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37cafb64-4837-40c3-aa09-7b9e9871906d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, False, False]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rollouts[0][\"dones\"][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff27284-e91c-4ef1-9b01-7e950eda2cd4",
   "metadata": {},
   "source": [
    "Anmerkungen:\n",
    "\n",
    "Zuerst sah der Agent also die Beobachtung [0,65, 0,297] aus der vorherigen Folie, dann f√ºhrte er die Aktion 0 aus, erhielt eine Belohnung von 0,65 und die Episode wurde nicht ausgef√ºhrt.\n",
    "\n",
    "Der Datensatz enth√§lt noch mehr Informationen als die oben genannten, aber das sind die wichtigsten Punkte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a206ba60-13ae-47b3-a406-eb06e701af5a",
   "metadata": {},
   "source": [
    "#### √úberwachtes Lernen\n",
    "\n",
    "- Moment, das ist ein Datensatz... k√∂nnen wir nicht einfach √ºberwachtes Lernen anwenden, um Zust√§nde auf Aktionen abzubilden? ü§î\n",
    "- Ja, das k√∂nnen wir, und das w√ºrde darauf abzielen, die Policy, die den Datensatz erzeugt hat, wiederherzustellen (_Nachahmungslernen_).\n",
    "- Aber es ist tats√§chlich m√∂glich, etwas _Besseres_ zu tun ... und das ist unser Ziel mit Offline-RL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59b604b7-484a-4ad8-976d-6f49c941f3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# maybe going overboard here, but for a synthetic example we could actually show it does better than the policy that generated the data,\n",
    "# since we generated the data ourselves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e35709e-45fd-4d70-a111-172580296fea",
   "metadata": {},
   "source": [
    "#### Offline-RL-Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8049ef58-4f5e-4116-abd7-4aad06f65ee0",
   "metadata": {},
   "source": [
    "- Viele Infos √ºber Offline-RL mit RLlib [hier](https://docs.ray.io/en/latest/rllib/rllib-offline.html).\n",
    "- Zuerst brauchen wir einen Algorithmus.\n",
    "- F√ºr Offline-RL k√∂nnen wir `PPO` nicht verwenden.\n",
    "- Wir werden den Algorithmus \"MARWIL\" verwenden, der in der RLlib enthalten ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34d0e449-de74-444c-894e-4d58c6e33c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.marwil import MARWIL, MARWILConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0385a4b2-594b-46e0-bd2f-2854a6935111",
   "metadata": {},
   "source": [
    "#### Offline-RL-Training\n",
    "\n",
    "Als N√§chstes erstellen wir die Konfiguration, wobei wir mit `MARWILConfig` statt mit `PPOConfig` beginnen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56e22e98-6ca9-4c34-850e-488c4bb38db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the same as before\n",
    "offline_config = ( \n",
    "    MARWILConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(create_env_on_local_worker=True)\\\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\\\n",
    "    .training(model={\"fcnet_hiddens\" : [64, 64]})\\\n",
    ")\n",
    "\n",
    "# This is new for offline RL\n",
    "num_candidates = 2\n",
    "offline_config = offline_config.environment(\n",
    "    observation_space = gym.spaces.Box(low=0, high=1, shape=(num_candidates,)), \n",
    "    action_space = gym.spaces.Discrete(num_candidates),\n",
    ").offline_data(\n",
    "    input_ = [json_dataset_file],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108dd79c-dddb-4153-a89d-258ee623da8a",
   "metadata": {},
   "source": [
    "Anmerkungen:\n",
    "\n",
    "- Die Konfigurationselemente auf der Oberseite sollten dir bekannt vorkommen. In der zweiten H√§lfte sind die Dinge ein bisschen anders:\n",
    "  - Wir m√ºssen den Pfad zur Dataset-Datei angeben\n",
    "  - Da es keine env gibt, m√ºssen wir die Beobachtungs- und Aktionsr√§ume manuell angeben\n",
    "- Wir haben keine Environmentskonfiguration, weil es keine Environment gibt!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c811603-4264-479d-bd0a-b5ec9ff38de2",
   "metadata": {},
   "source": [
    "#### Ausbildung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7280c730-eeee-49a2-a84b-7c42388df308",
   "metadata": {},
   "outputs": [],
   "source": [
    "marwil = offline_config.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8172e9d-bc26-4e00-b431-c9438863028d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    marwil.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d7651b-e0cb-4e37-bec3-976ac37af9cb",
   "metadata": {},
   "source": [
    "#### Bewertung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fe0e89-2c77-4a99-98e7-307e7502693f",
   "metadata": {},
   "source": [
    "wie k√∂nnen wir ohne einen Simulator evaluieren?\n",
    "\n",
    "- Das nennt man Off-Policy-Sch√§tzung.\n",
    "- Das ist eine kompliziertere Technik und liegt au√üerhalb des Rahmens dieses Kurses.\n",
    "- Siehe die RLlib-Dokumente [hier] (https://docs.ray.io/en/latest/rllib/rllib-offline.html).\n",
    "- Wir werden die Auswertung mit unserem Simulator durchf√ºhren, da wir ihn ja haben.\n",
    "\n",
    "Anmerkungen:\n",
    "\n",
    "In einem realistischen Offline-RL-Szenario hast du keinen Simulator zur Verf√ºgung. Dann musst du komplexere Evaluierungstechniken anwenden, die Off-Policy-Sch√§tzung genannt werden. Du h√§ttest Trainings- und Testdatens√§tze wie beim √ºberwachten Lernen \n",
    "\n",
    "Da wir aber einen Simulator haben, werden wir ihn f√ºr die Auswertung verwenden. RLlib hat eine Option, die das erlaubt, denn es ist n√ºtzlich f√ºr die Fehlersuche und so weiter. Wir sehen uns das auf der n√§chsten Folie an."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202d38ea-9ad6-4d17-9137-720251d1f6a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Bewertung mit unserem Simulator\n",
    "\n",
    "Wir k√∂nnen den Algorithmus mit unserem env-Simulator bewerten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5da21ac0-fcba-4d96-8b23-721f98dc1638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "from envs_04 import BasicRecommender\n",
    "\n",
    "env_config = {\n",
    "    \"num_candidates\" : 2,\n",
    "    \"alpha\"          : 0.5,\n",
    "    \"seed\"           : 42\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b582c0ac-88f4-40e0-8078-31ee041bfe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BasicRecommender(env_config)\n",
    "\n",
    "def get_episode_reward(env, algo):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = algo.compute_single_action(obs)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a8671a-26c8-45f5-a468-944dfa4c01b6",
   "metadata": {},
   "source": [
    "- Oben: Richte eine Funktion ein, die eine Episode mit dem Simulator abspielt.\n",
    "- Unten: lasse dies f√ºr 100 Episoden laufen, um den Mittelwert zu erhalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "632fbd40-c2bb-44fe-af47-39b9507530dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.537665635536726"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([get_episode_reward(env, marwil) for i in range(100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8818fa-2e46-4d04-85c7-d55797fc169c",
   "metadata": {},
   "source": [
    "Das scheint wieder in etwa so zu sein wie beim Zufallsprinzip (25,5)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e507580-77c8-47c1-a9a5-8259ee4bdcd9",
   "metadata": {},
   "source": [
    "#### RLlib f√ºr simulatorbasierte Bewertung\n",
    "\n",
    "RLlib bietet dies ebenfalls als Funktion an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7468c451-e68b-446b-a0f2-5dc86bb1af11",
   "metadata": {},
   "outputs": [],
   "source": [
    "offline_config = offline_config.evaluation(\n",
    "    off_policy_estimation_methods={},\n",
    "    evaluation_config={\n",
    "        \"input\": \"sampler\",\n",
    "        \"env\": BasicRecommender,\n",
    "        \"env_config\" : env_config\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0db4ec-8023-4ada-83ac-e4f0f42acb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "#marwil.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dfe966-9734-42c7-bf5f-bf8655944f36",
   "metadata": {},
   "source": [
    "Wenn wir die Konfiguration so eingerichtet und trainiert h√§tten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5bccaf63-54f6-41da-a034-3da05a1ae5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "marwil = offline_config.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "56152208-587f-4306-8e53-9c0eb9f99f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    marwil.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d980136-8c96-42e9-90fb-9d4a6a564922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.3457144503546"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marwil.evaluate()[\"evaluation\"][\"episode_reward_mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d21639-5531-46b6-8cce-68b3cfb013f0",
   "metadata": {},
   "source": [
    "Auch hier sehen wir ein √§hnliches Ergebnis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e979de95-c039-4493-b958-e2e5d85098ec",
   "metadata": {},
   "source": [
    "#### Lass uns das Gelernte anwenden!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643f5645-5a4d-4fcd-aa28-32894a99ffc2",
   "metadata": {},
   "source": [
    "## Beispiel f√ºr offline RL\n",
    "<!-- multiple choice -->\n",
    "\n",
    "- [Einer KI das Schachspielen beibringen, indem man sie wiederholt gegen andere KIs spielen l√§sst.\n",
    "- [x] Einer KI das Schachspielen beibringen, indem man sie vergangene Partien von professionellen Schachspielern spielen l√§sst."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcb97ca-5ccd-4eed-b3ac-b58cee110c78",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Helfen Offline-Daten?\n",
    "<!-- multiple choice -->\n",
    "\n",
    "Stell dir vor, du hast eine Environment, die deine Environment perfekt abbildet; zum Beispiel trainierst du eine KI f√ºr ein Einzelspielerspiel wie [Atari Breakout](https://en.wikipedia.org/wiki/Breakout_(video_game)). Zus√§tzlich zum Simulator hast du auch einige Offline-Daten zur Verf√ºgung. Auch wenn es in den obigen Folien nicht erw√§hnt wird, ist es m√∂glich, einen Simulator mit Offline-Daten zu kombinieren, um gemeinsam einen Agenten mit RL zu trainieren (siehe [hier](https://docs.ray.io/en/latest/rllib/rllib-offline.html#mixing-simulation-and-offline-data)). Wenn du bereits √ºber einen perfekten Simulator verf√ºgst, k√∂nnten Offline-Daten einen zus√§tzlichen Nutzen bringen, wenn sie beim Training mit dem Simulator kombiniert werden?\n",
    "\n",
    "#### Beste Policy nach dem Training\n",
    "\n",
    "W√§hle die richtige Aussage zum Finden der besten Policy nach einem unbegrenzten Training. Du kannst davon ausgehen, dass du einen \"perfekten\" RL-Algorithmus hast, d.h. er kann jede Policy darstellen und jede Funktion optimieren.\n",
    "\n",
    "- [x] Mit einem \"perfekten\" RL-Algorithmus und gen√ºgend Rechenzeit bringt es keinen Vorteil, die historischen Daten zu verwenden, weil der Simulator alles enth√§lt, was man √ºber die Environment wissen muss.\n",
    "- [Bei einem \"perfekten\" RL-Algorithmus und unbegrenzter Rechenzeit k√∂nnen dir die historischen Daten helfen, eine bessere Policy zu finden als nur mit dem Simulator. | Mit einem \"perfekten\" RL-Algorithmus solltest du irgendwann die optimale Policy finden. Das ist so, als h√§tte man unendlich viele Daten, unendlich viel Rechenzeit, ein beliebig komplexes Modell und eine perfekte Optimierungsmethode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bf6575-837f-458d-be6f-924ee4659254",
   "metadata": {},
   "source": [
    "#### Trainingsgeschwindigkeit\n",
    "\n",
    "W√§hle die richtige Aussage √ºber das Finden der besten Policy nach ein wenig Training. Du kannst davon ausgehen, dass du einen \"perfekten\" RL-Algorithmus hast, d.h. er kann jede Policy darstellen und jede Funktion optimieren.\n",
    "\n",
    "- [Wenn du einen \"perfekten\" RL-Algorithmus hast, aber nur ein bisschen Rechenzeit, bringt es nichts, die historischen Daten zu verwenden, weil der Simulator alles enth√§lt, was man √ºber die Environment wissen muss. | Was w√§re, wenn die historischen Daten mit der *optimalen* Policy erzeugt worden w√§ren?\n",
    "- [Mit einem \"perfekten\" RL-Algorithmus, aber nur wenig Rechenzeit, k√∂nnen dir die historischen Daten helfen, eine bessere Policy zu finden als nur mit dem Simulator. | Wenn die historischen Daten mit einer sehr guten Policy erzeugt wurden, k√∂nntest du schnell daraus lernen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab3ebb6-c70d-44d8-93e1-ae4d0f7b54a7",
   "metadata": {},
   "source": [
    "## Historische Datenpolicy\n",
    "<!-- multiple choice -->\n",
    "\n",
    "Offline-RL basiert auf Daten, die von einer Policy erzeugt werden, die mit der Environment interagiert. Welche der folgenden Eigenschaften ist **NICHT** eine w√ºnschenswerte Eigenschaft dieses Datensatzes / dieser historischen Policy?\n",
    " \n",
    "- [x] Die Environment und die historische Policy sind beide deterministisch. | In diesem Fall w√ºrden wir nur eine Trajektorie durch die Environment erkunden \n",
    "- [ ] Der Datensatz enth√§lt eine gro√üe Anzahl von Episoden.\n",
    "- [Die historische Policy erkundet eine Vielzahl von Zust√§nden in der Environment.\n",
    "- [Die historische Policy erzielt in einigen Episoden eine hohe Belohnung.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06aebc9-1031-4635-bb8a-43417dffcb2c",
   "metadata": {},
   "source": [
    "## Offline RL f√ºr Cartpole\n",
    "<!-- coding exercise -->\n",
    "\n",
    "In dieser √úbung werden wir das ber√ºhmte [Cartpole-Benchmark-Problem] (https://www.gymlibrary.ml/environments/classic_control/cart_pole/) angehen, das mit der `gym`-Bibliothek ausgeliefert wird. Das Ziel ist es, das umgedrehte Pendel vor dem Umfallen zu bewahren, indem wir bei jedem Zeitschritt eine Kraft nach links oder rechts aus√ºben \n",
    "\n",
    "Wir trainieren den Agenten mit Offline-Daten, die in der Datei `cartpolev1_offline.json` enthalten sind, die vom Code eingelesen wird.\n",
    "\n",
    "Wir werden den Agenten mit dem echten Cartpole-Simulator _auswerten_. (Auch hier gilt: Wenn wir Offline-RL verwenden w√ºrden, h√§tten wir wahrscheinlich keinen Zugriff auf den echten Simulator, aber wir f√ºgen ihn hier ein, damit wir unseren Agenten auf der Basis von Fakten bewerten k√∂nnen)\n",
    "\n",
    "F√ºlle den fehlenden Code aus. F√ºhre dann den Code aus und beantworte die Multiple-Choice-Frage unten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd176dc-9a29-4813-b10e-224a61c37aaa",
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ray.rllib.algorithms.marwil import MARWIL, MARWILConfig\n",
    "\n",
    "offline_trainer_config = {\n",
    "    # These should look familiar:\n",
    "    \"framework\"             : \"torch\",\n",
    "    \"create_env_on_driver\"  : True,\n",
    "    \"seed\"                  : 0,\n",
    "    \"model\"                 : {\n",
    "        \"fcnet_hiddens\"     : [64, 64]\n",
    "    },\n",
    "    \n",
    "    # These are new for offline RL:\n",
    "    ____: [\"data/cartpolev1_offline.json\"],\n",
    "    \"observation_space\": gym.spaces.Box(low=____, \n",
    "                                        high=np.array([4.8,  np.inf,  0.42,  np.inf])),\n",
    "    \"action_space\": gym.spaces.Discrete(2),\n",
    "    \"input_evaluation\" : [\"simulation\", \"is\", \"wis\"],\n",
    "    \"env\" : \"CartPole-v1\" # for evaluation only\n",
    "}\n",
    "\n",
    "algo = ...\n",
    "\n",
    "# Training (and storing results)\n",
    "results_off = []\n",
    "results_sim = []\n",
    "for i in range(200):\n",
    "    r = algo.____()\n",
    "    results_off.append(r[\"off_policy_estimator\"][\"wis\"]['V_gain_est'])\n",
    "    results_sim.append(r[\"episode_reward_mean\"])\n",
    "\n",
    "plt.plot(results_sim);\n",
    "plt.xlabel('iterations') \n",
    "plt.ylabel('simulator reward') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e100ab56-3d05-494e-a151-2e82701842ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# This is new for offline RL\u001b[39;00m\n\u001b[1;32m     17\u001b[0m offline_config \u001b[38;5;241m=\u001b[39m offline_config\u001b[38;5;241m.\u001b[39menvironment(\n\u001b[1;32m     18\u001b[0m     observation_space \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mspaces\u001b[38;5;241m.\u001b[39mBox(low\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4.8\u001b[39m, \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.42\u001b[39m, \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf]), \n\u001b[1;32m     19\u001b[0m                                         high\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m4.8\u001b[39m,  np\u001b[38;5;241m.\u001b[39minf,  \u001b[38;5;241m0.42\u001b[39m,  np\u001b[38;5;241m.\u001b[39minf])),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     off_policy_estimation_methods\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msimulation\u001b[39m\u001b[38;5;124m'\u001b[39m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msimulation\u001b[39m\u001b[38;5;124m'\u001b[39m}, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwis\u001b[39m\u001b[38;5;124m'\u001b[39m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwis\u001b[39m\u001b[38;5;124m'\u001b[39m}}\n\u001b[1;32m     26\u001b[0m )\n\u001b[0;32m---> 28\u001b[0m algo \u001b[38;5;241m=\u001b[39m \u001b[43moffline_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Training (and storing results)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m results_off \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/rllib/algorithms/algorithm_config.py:307\u001b[0m, in \u001b[0;36mAlgorithmConfig.build\u001b[0;34m(self, env, logger_creator)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logger_creator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger_creator \u001b[38;5;241m=\u001b[39m logger_creator\n\u001b[0;32m--> 307\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malgo_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/rllib/algorithms/algorithm.py:308\u001b[0m, in \u001b[0;36mAlgorithm.__init__\u001b[0;34m(self, config, env, logger_creator, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;66;03m# Initialize common evaluation_metrics to nan, before they become\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;66;03m# available. We want to make sure the metrics are always present\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;66;03m# (although their values may be nan), so that Tune does not complain\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;66;03m# when we use these as stopping criteria.\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_metrics \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode_reward_max\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mnan,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m     }\n\u001b[1;32m    306\u001b[0m }\n\u001b[0;32m--> 308\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m# Check, whether `training_iteration` is still a tune.Trainable property\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# and has not been overridden by the user in the attempt to implement the\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;66;03m# algos logic (this should be done now inside `training_step`).\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/tune/trainable/trainable.py:157\u001b[0m, in \u001b[0;36mTrainable.__init__\u001b[0;34m(self, config, logger_creator, remote_checkpoint_dir, custom_syncer)\u001b[0m\n\u001b[1;32m    155\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_local_ip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_current_ip()\n\u001b[0;32m--> 157\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m setup_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m setup_time \u001b[38;5;241m>\u001b[39m SETUP_TIME_THRESHOLD:\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/rllib/algorithms/crr/crr.py:163\u001b[0m, in \u001b[0;36mCRR.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetup\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: PartialAlgorithmConfigDict):\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_network_update_freq\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_network_update_freq\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    166\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_batch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m    167\u001b[0m         )\n",
      "File \u001b[0;32m~/git/anyscale/ray/python/ray/rllib/algorithms/algorithm.py:572\u001b[0m, in \u001b[0;36mAlgorithm.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(method_type, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    571\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to import from string: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m method_type)\n\u001b[0;32m--> 572\u001b[0m     mod, obj \u001b[38;5;241m=\u001b[39m method_type\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    573\u001b[0m     mod \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(mod)\n\u001b[1;32m    574\u001b[0m     method_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(mod, obj)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ray.rllib.algorithms.marwil import MARWIL, MARWILConfig\n",
    "from ray.rllib.algorithms.crr import CRR, CRRConfig\n",
    "\n",
    "# This is the same as before\n",
    "offline_config = ( \n",
    "    CRRConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(create_env_on_local_worker=True)\\\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\\\n",
    "    .training(model={\"fcnet_hiddens\" : [32, 32]})\\\n",
    ")\n",
    "# This is new for offline RL\n",
    "offline_config = offline_config.environment(\n",
    "    observation_space = gym.spaces.Box(low=np.array([-4.8, -np.inf, -0.42, -np.inf]), \n",
    "                                        high=np.array([4.8,  np.inf,  0.42,  np.inf])),\n",
    "    action_space = gym.spaces.Discrete(2),\n",
    "    env = \"CartPole-v1\" # for evaluation only\n",
    ").offline_data(\n",
    "    input_ = [\"data/cartpolev1_offline.json\"]\n",
    ").evaluation(\n",
    "    off_policy_estimation_methods={'simulation': {'type': 'simulation'}}\n",
    ")\n",
    "\n",
    "algo = offline_config.build()\n",
    "\n",
    "# Training (and storing results)\n",
    "results_off = []\n",
    "results_sim = []\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "    r = algo.train()\n",
    "    results_off.append(r[\"\"][\"wis\"]['v_new_mean'])\n",
    "    results_sim.append(r[\"episode_reward_mean\"])\n",
    "\n",
    "plt.plot(results_sim);\n",
    "plt.xlabel('iterations') \n",
    "plt.ylabel('simulator reward') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bcc68e84-014c-46ba-ba70-37bf63f6234a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# see the tuned examples here: https://docs.ray.io/en/master/rllib/rllib-algorithms.html#offline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad6fe1d-dc59-41ff-99cb-c688dc12fd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# - ALSO: this might be a great example to try supervised learning and show why it doesn't work...???\n",
    "#   - in some ways this is a way better example than frozen lake... because there IS a short-term reward, it's just not what you should look at.\n",
    "#   - with frozen lake there is no short-term reward, so RL seems \"obvious\"\n",
    "# Or, that could go in the offline RL section, since we already have a data file there and could do SL directly on it\n",
    "# Yes, that seems cool.\n",
    "# could use scipy/numpy to just do the normal equations if we want to avoid adding a dependency on sklearn \n",
    "\n",
    "# Supervised learning only gives you imitation learning, which can only get as good as the policy that generated the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766cc60b-fb10-411c-a798-18ae6f447369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34ae237-8b75-48b9-a998-2914625e71e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ray200]",
   "language": "python",
   "name": "conda-env-ray200-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
