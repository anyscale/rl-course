{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e23df55-837b-4ec9-86ad-3b60c6a7f00d",
   "metadata": {},
   "source": [
    "## Kodierung Belohnungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28d9b458-b46e-48de-ab18-bf884c1eab85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import ray\n",
    "import logging\n",
    "ray.init(log_to_driver=False, ignore_reinit_error=True, logging_level=logging.ERROR); # logging.FATAL\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c8c796-3785-44d1-8aea-78872152b0cb",
   "metadata": {},
   "source": [
    "#### Kodierung Belohnungen\n",
    "\n",
    "- Wir haben jetzt besprochen, wie wichtig es ist, die Beobachtungen zu kodieren.\n",
    "- M√∂glicherweise haben wir auch eine gewisse Auswahl im Handlungsraum, obwohl dieser hier (und oft) relativ klar/fest ist.\n",
    "- Aber was ist mit den Belohnungen? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e790dc-5146-4b03-a77a-24efd69ffabd",
   "metadata": {},
   "source": [
    "#### Aktuelle Einrichtung\n",
    "\n",
    "- Derzeit erhalten wir eine Belohnung von +1 f√ºr das Erreichen des Ziels \n",
    "- Das ist ein Teil dessen, was RL so schwer (und beeindruckend) macht:\n",
    "  - Wir wollen √ºber Aktionen lernen, obwohl wir nicht sofort wissen, ob die Aktion \"gut\" war \n",
    "  - Im Gegensatz dazu steht das √ºberwachte Lernen, bei dem jede Vorhersage, die wir anhand der Trainingsdaten treffen, sofort mit dem bekannten Zielwert verglichen werden kann.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c10944be-9746-44d6-bb2b-66843ea7e4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: perhaps this next slide can be moved to Module 1, since it's very general?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff6ac9f-6815-4910-b3ce-aa7e0866101c",
   "metadata": {},
   "source": [
    "#### Agenten k√∂nnen nicht nur gierig sein\n",
    "\n",
    "- K√∂nnen Agenten und Agentinnen einfach lernen, die beste sofortige Belohnung anzustreben?\n",
    "- Nein. Wenn man dem Nutzer eines Videoempfehlungssystems zum Beispiel ein weiteres lustiges Katzenvideo zeigt, k√∂nnte das dazu f√ºhren, dass er klickt (hohe unmittelbare Belohnung), aber langfristig das Interesse an dem Dienst verliert (geringe langfristige Belohnung).\n",
    "- Unser Frozen Lake ist ein weiteres Beispiel f√ºr das Problem: Manchmal gibt es √ºberhaupt keine unmittelbare Belohnung, aus der man lernen k√∂nnte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c44cea8c-a28e-4d7c-a0c8-91c1c3f8fc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: perhaps this next section on \"Learned action probabilities\" could be moved much earlier, even as early as Module 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462e6340-cb1f-4eda-b642-d21505d18b52",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Gelernte Handlungswahrscheinlichkeiten\n",
    "\n",
    "- Mit RLlib k√∂nnen wir innerhalb des Modells die Wahrscheinlichkeit jeder Aktion in Abh√§ngigkeit von einer Beobachtung (d.h. der gelernten Policy) betrachten.\n",
    "- Laden wir das trainierte Modell mit unseren kodierten Beobachtungen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16d9d371-0c26-46a0-90c5-6f26e25880f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from envs_03 import RandomLakeObs\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "ppo_config = (\n",
    "    PPOConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(create_env_on_local_worker=True, horizon=100)\\\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\n",
    ")\n",
    "ppo_RandomLakeObs = ppo_config.build(env=RandomLakeObs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1420e1fd-22e9-474c-aad6-928428976b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # HIDDEN\n",
    "\n",
    "# for i in range(16):\n",
    "#     ppo_RandomLakeObs.train()\n",
    "    \n",
    "# print(ppo_RandomLakeObs.evaluate()[\"evaluation\"][\"episode_reward_mean\"])\n",
    "\n",
    "# ppo_RandomLakeObs.save(\"models/RandomLakeObs-Ray2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a71119d3-e3c9-427c-a697-fe733fc10ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_RandomLakeObs.restore(\"models/RandomLakeObs-Ray2/checkpoint_000016\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fec9d02-7e22-44ee-b171-2d9bf86b8681",
   "metadata": {},
   "source": [
    "#### Gelernte Handlungswahrscheinlichkeiten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66648b5d-abd8-45ef-b592-e56bdee1c9c9",
   "metadata": {},
   "source": [
    "Wir verwenden die Funktion `query_Policy` aus Modul 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84b56415-177b-414f-8985-1118cabec0b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00902206, 0.5078786 , 0.47434822, 0.00875122], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils_03 import query_policy\n",
    "query_policy(ppo_RandomLakeObs, RandomLakeObs(), [0,0,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0256b6-3174-4719-931f-65342f5ae4a7",
   "metadata": {},
   "source": [
    "- Erinnere dich an die Reihenfolge (links, unten, rechts, oben).\n",
    "- Wenn die Beobachtung `[0 0 0 0]` ist (keine L√∂cher oder Kanten in Sicht), geht der Agent lieber nach unten und rechts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99a889a-37bd-4a7c-bf6b-c9eb297a1520",
   "metadata": {},
   "source": [
    "Was ist, wenn es unter dir ein Loch gibt? Wir k√∂nnen eine andere Beobachtung in die Policy einbringen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc66f33e-8b23-4bcc-9f12-04b47d18aea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01965651, 0.00299437, 0.9645823 , 0.01276694], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_policy(ppo_RandomLakeObs, RandomLakeObs(), [0,1,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6560c87a-c07e-4aec-a172-915cad2b376c",
   "metadata": {},
   "source": [
    "- Jetzt ist es sehr unwahrscheinlich, dass der Agent nach unten geht, und sehr wahrscheinlich, dass er nach rechts geht!\n",
    "- Auch dies wurde durch Versuch und Irrtum gelernt, und eine Belohnung gab es nur, wenn das Ziel erreicht wurde."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffae4393-3058-40ce-821a-fe1e487cf4d4",
   "metadata": {},
   "source": [
    "#### Random Lake Belohnungen\n",
    "\n",
    "- Kann man dem Agenten im Beispiel von Random Lake das Leben nicht leichter machen, indem man ihm sofortige Belohnungen gibt?\n",
    "\n",
    "Dies ist der aktuelle Belohnungscode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bb56001-259b-4b88-8bdd-29746e208042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward(self):\n",
    "    return int(self.player == self.goal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1ee605-1e87-4080-b8ee-274816dd67fd",
   "metadata": {},
   "source": [
    "- Der Agent muss durch Versuch und Irrtum √ºber _ganze Episoden_ lernen, dass es im Allgemeinen gut ist, sich nach unten und rechts zu bewegen "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0cf72e-d732-4039-90cc-22a17360943f",
   "metadata": {},
   "source": [
    "#### Belohnungen neu definieren\n",
    "\n",
    "- Lass uns stattdessen versuchen, _bei jedem Schritt eine Belohnung zu geben, die umso h√∂her ist, je n√§her der Agent dem Ziel kommt_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9a0466b-44fd-43ed-a3c0-d93334748213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from envs_03 import RandomLakeObs\n",
    "\n",
    "class RandomLakeObsRew(RandomLakeObs):\n",
    "    def reward(self):\n",
    "        return 6-(abs(self.player[0]-self.goal[0]) + abs(self.player[1]-self.goal[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce59aa5-1f90-457f-a3f9-c970cd8a7b7b",
   "metadata": {},
   "source": [
    "- Bei der obigen Methode wird die [Manhattan-Distanz] (https://en.wikipedia.org/wiki/Taxicab_geometry) zwischen dem Spieler und dem Ziel als Belohnung verwendet \n",
    "- Wenn der Agent das Ziel erreicht, erh√§lt er die maximale Belohnung von 6.\n",
    "- Wenn der Agent am weitesten vom Ziel entfernt ist, erh√§lt er die Mindestbelohnung von 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75f7b77-8962-43fe-a82b-647e4f2aee69",
   "metadata": {},
   "source": [
    "#### Belohnungen neu definieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcba334f-f2a3-4cb0-8b8a-f5dd69a3346f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßëüßäüßäüßä\n",
      "üßäüßäüßäüßä\n",
      "üï≥üßäüï≥üßä\n",
      "üßäüßäüßä‚õ≥Ô∏è\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = RandomLakeObsRew()\n",
    "env.reset()\n",
    "env.render()\n",
    "env.reward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5208f2-c7e9-4f1e-9ed7-e91daec89d8e",
   "metadata": {},
   "source": [
    "‚¨ÜÔ∏è die Belohnung ist 0\n",
    "\n",
    "‚¨áÔ∏è die Belohnung ist 1, weil wir dem Ziel n√§her gekommen sind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "088715f0-3f06-4231-a149-401b6518dcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßäüßäüßäüßä\n",
      "üßëüßäüßäüßä\n",
      "üï≥üßäüï≥üßä\n",
      "üßäüßäüßä‚õ≥Ô∏è\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, rew, done, _ = env.step(1)\n",
    "env.render()\n",
    "rew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e607ab59-e697-44b6-99a6-91a7a3a3d2b2",
   "metadata": {},
   "source": [
    "#### Belohnungen neu definieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95ab62b7-e2b5-4f6e-8f81-9381b605a9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßäüßäüßäüßä\n",
      "üßäüßäüßäüßä\n",
      "üï≥üßäüï≥üßë\n",
      "üßäüßäüßä‚õ≥Ô∏è\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, rew, done, _ = env.step(2)\n",
    "obs, rew, done, _ = env.step(2)\n",
    "obs, rew, done, _ = env.step(2)\n",
    "obs, rew, done, _ = env.step(1)\n",
    "env.render()\n",
    "rew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1a183d-af4c-4c9d-a6d3-4bc1a7bfecca",
   "metadata": {},
   "source": [
    "Jetzt ist die Belohnung 5. Als N√§chstes wird es 6 sein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "904e40cb-55b6-4b45-b063-5b335123e071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßäüßäüßäüßä\n",
      "üßäüßäüßäüßä\n",
      "üï≥üßäüï≥üßä\n",
      "üßäüßäüßäüßë\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, rew, done, _ = env.step(1)\n",
    "env.render()\n",
    "rew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ac0aec-ddc4-4d44-833e-c2b2e253120f",
   "metadata": {},
   "source": [
    "#### Belohnungen im Vergleich\n",
    "\n",
    "- Wir haben also zwei m√∂gliche Belohnungsfunktionen. Welche davon funktioniert besser? \n",
    "- Erinnere dich daran, dass wir beim letzten Mal, nachdem wir 8 Iterationen trainiert hatten, das Ziel in etwa 70 % der Zeit erreichen konnten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5dbcf34-37e8-4025-bcd2-7a308d77046f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8166666666666667"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_RandomLakeObs.evaluate()['evaluation']['episode_reward_mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c75b369-6e96-4192-b7cd-4216f354d901",
   "metadata": {},
   "source": [
    "#### Belohnungen im Vergleich\n",
    "\n",
    "Lass uns mit der neuen Belohnungsfunktion trainieren!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01c1afa7-4452-43e4-ad2d-e3dc33934fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_RandomLakeObsRew = ppo_config.build(env=RandomLakeObsRew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7478ee6-28d8-4e44-932f-1b4f47e6b911",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    ppo_RandomLakeObsRew.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "257cef04-752e-4fe1-9eb6-3e225ea1ba62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101.90566037735849"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_RandomLakeObsRew.evaluate()['evaluation']['episode_reward_mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bccc6f4-1e9a-4abd-af7a-08b4c6103e4b",
   "metadata": {},
   "source": [
    "Moment mal, was ist hier los??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874a6979-1882-4bb4-a5cf-a83282ed15f3",
   "metadata": {},
   "source": [
    "#### Belohnungen im Vergleich?\n",
    "\n",
    "- Wir haben versucht, unser RL-System zu verbessern, indem wir die Belohnungsfunktion geformt haben.\n",
    "- Das wirkte sich (vermutlich) auf das Training aus, aber auch auf unsere Bewertung.\n",
    "- Beim √ºberwachten Lernen ist das so, als w√ºrde man die Bewertungsmetrik vom quadratischen Fehler zum absoluten Fehler √§ndern.\n",
    "- Wenn das alte System einen mittleren quadratischen Fehler von 20.000 und das neue System einen mittleren absoluten Fehler von 40 hat, welches System ist dann besser?\n",
    "- Wir vergleichen hier √Ñpfel mit Birnen!\n",
    "- Wir wollen beide Modelle anhand derselben Metrik vergleichen, zum Beispiel der urspr√ºnglichen Metrik \n",
    "- Hier wollen wir sehen, wie oft der Agent das Ziel erreicht."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf05802-842d-4764-b408-72559e7bb8cf",
   "metadata": {},
   "source": [
    "#### Belohnungen im Vergleich?\n",
    "\n",
    "- Der Code hier ist ein bisschen fortgeschrittener.\n",
    "- Er ist nur der Vollst√§ndigkeit halber enthalten, aber wir werden nicht ins Detail gehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "261074ea-c6db-4e1b-a35f-a647e8cf2776",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.agents.callbacks import DefaultCallbacks\n",
    "\n",
    "class MyCallbacks(DefaultCallbacks):\n",
    "    def on_episode_end(self, *, worker, base_env, policies, episode, env_index, **kwargs):\n",
    "        info = episode.last_info_for()\n",
    "        episode.custom_metrics[\"goal_reached\"] = info[\"player\"] == info[\"goal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3ab7223-77b0-46f0-af5f-669b18760a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_config_callback = (\n",
    "    PPOConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(create_env_on_local_worker=True, horizon=100)\\\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\\\n",
    "    .callbacks(callbacks_class=MyCallbacks)\\\n",
    "    .evaluation(evaluation_config={\"callbacks\" : MyCallbacks})\n",
    ")\n",
    "\n",
    "ppo_RandomLakeObsRew = ppo_config_callback.build(env=RandomLakeObsRew)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90980e8e-2ef2-479d-9c55-cbde6fcdebe1",
   "metadata": {},
   "source": [
    "Der Trainer oben verwendet unser neues Belohnungsschema, meldet/ misst aber auch die Rate der Zielerreichung."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f898830-4600-4ddf-8f05-073f7cb8e842",
   "metadata": {},
   "source": [
    "#### Belohnungen im Vergleich?\n",
    "\n",
    "Lass es uns ausprobieren!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f118473-544e-43a6-9ead-f742d0ee8fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    ppo_RandomLakeObsRew.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f52dd086-12bb-4482-88a2-30df5240573b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101.90566037735849"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HIDDEN\n",
    "ppo_RandomLakeObsRew.evaluate()[\"evaluation\"][\"episode_reward_mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0021f054-0f77-46c8-ac1c-f6e1f40ff509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04081632653061224"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_RandomLakeObsRew.evaluate()[\"evaluation\"][\"custom_metrics\"][\"goal_reached_mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36e3784-b860-49a2-b3d1-b48a760e756c",
   "metadata": {},
   "source": [
    "- Hmm, diese Ergebnisse sind schrecklich!\n",
    "- Fr√ºher hatten wir eine Gewinnquote von 70 % und mehr, und jetzt sind wir fast bei Null.\n",
    "- Was ist passiert? ü§î"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398ee5c9-7e5c-4370-912c-b90d8864cac4",
   "metadata": {},
   "source": [
    "#### Was will der Agent wirklich optimieren?\n",
    "\n",
    "- Der Agent optimiert in Wirklichkeit die _diskontierte Gesamtbelohnung_.\n",
    "- _Gesamt_: Er bewertet alle Belohnungen, die er sammelt, nicht nur die letzte Belohnung.\n",
    "- _Abgezinst_: Er bewertet fr√ºhere Belohnungen mehr als sp√§tere.\n",
    "- Unser Agent maximiert erfolgreich die abgezinste Gesamtbelohnung, aber das entspricht nicht dem Erreichen des Ziels.\n",
    "- Aber warum? Das Ziel gibt eine h√∂here Belohnung."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cee4522-4906-4a36-b5e8-714761a11b04",
   "metadata": {},
   "source": [
    "#### Erforschung vs. Ausbeutung\n",
    "\n",
    "- Ein grundlegendes Konzept im RL ist _Exploration vs. Exploitation_\n",
    "- Wenn der Agent eine Policy lernt, kann er sich entscheiden, ob er:\n",
    "\n",
    "1. Dinge tun, von denen er wei√ü, dass sie ziemlich gut sind (\"exploit\")\n",
    "2. Etwas v√∂llig Neues und Verr√ºcktes ausprobieren, nur f√ºr den Fall (\"explore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f02fef1-3506-4c10-ac66-cd4150b32d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO \n",
    "# diagram for this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954a891d-f16c-43bb-b538-ce246829d6fb",
   "metadata": {},
   "source": [
    "#### Erforschung vs. Ausbeutung\n",
    "\n",
    "- Bei der alten Belohnungsstruktur erh√§lt der Agent eine Belohnung von 0, wenn er das Ziel nicht erreicht.\n",
    "  - Also versucht er weiter, etwas Besseres zu finden.\n",
    "- Mit der neuen Belohnungsstruktur erh√§lt der Agent eine Menge Belohnungen, nur weil er heruml√§uft.\n",
    "  - Er ist nicht sehr motiviert, die Environment zu erkunden.\n",
    "- Weil er die abgezinste **Gesamtbelohnung** maximiert, ist es sogar schlecht, das Ziel zu finden!\n",
    "  - Das f√ºhrt dazu, dass die Episode endet und die Gesamtbelohnung des Agenten begrenzt wird.\n",
    "  - Der Agent lernt, das Ziel zu _vermeiden_, vor allem am Anfang der Episode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83b297e-b4a6-4989-b623-5f53b52562c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Eine bessere Belohnungsstruktur entwerfen\n",
    "\n",
    "- Stattdessen sollten wir versuchen, den Agenten zu bestrafen, wenn er in ein Loch oder √ºber die Kante l√§uft.\n",
    "- Es wird einfacher sein, dies direkt in `step` zu implementieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c41fd832-6144-41cc-9643-e7f3eb8737d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomLakeObsRew2(RandomLakeObs):\n",
    "    def step(self, action):\n",
    "        # (not shown) existing code gets new_loc, where the player is trying to go\n",
    "        \n",
    "        reward = 0\n",
    "        \n",
    "        if self.is_valid_loc(new_loc):\n",
    "            self.player = new_loc\n",
    "        else:\n",
    "            reward -= 0.1 # small penalty\n",
    "            \n",
    "        if self.holes[self.player]:\n",
    "            reward -= 0.1 # small penalty\n",
    "            \n",
    "        if self.player == self.goal:\n",
    "            reward += 1\n",
    "        \n",
    "        # Return observation/reward/done\n",
    "        return self.observation(), reward, self.done(), {\"player\" : self.player, \"goal\" : self.goal}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5622237f-517f-4a1c-99f0-0e7a0ede3e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "from envs_03 import RandomLakeObsRew2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804b515a-f635-4f48-8e0e-7774b003533a",
   "metadata": {},
   "source": [
    "#### Nochmal zum Ausprobieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c20f637-e0f8-4ed5-8133-d49d59d504f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "# redefine ppo_RandomLakeObs to include the new callbacks\n",
    "# so that you can measure the custom metric instead of the reward\n",
    "# they will give the same value but this is better for consistency\n",
    "ppo_RandomLakeObs = ppo_config_callback.build(env=RandomLakeObs)\n",
    "\n",
    "for i in range(8):\n",
    "    ppo_RandomLakeObs.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7459cba1-5771-4d26-b215-4f51df7a7ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_RandomLakeObsRew2 = ppo_config_callback.build(env=RandomLakeObsRew2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dbd1ac25-6d5c-4ed9-a49c-c9564b92f08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    ppo_RandomLakeObsRew2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cdbb4747-9b58-487b-9353-1a1e8a2022c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6853448275862069"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_RandomLakeObs.evaluate()[\"evaluation\"][\"custom_metrics\"][\"goal_reached_mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8e32e1de-b153-4fef-a208-a0d574d547c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.734982332155477"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_RandomLakeObsRew2.evaluate()[\"evaluation\"][\"custom_metrics\"][\"goal_reached_mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8ae49b-73af-43b4-92fa-cd3f608b504d",
   "metadata": {},
   "source": [
    "Es sieht so aus, als ob die beiden Methoden dieses Mal sehr viel √§hnlicher abschneiden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3481f6-4505-48f6-a787-33146ff663af",
   "metadata": {},
   "source": [
    "#### L√§nge der Episode\n",
    "\n",
    "- Neben der Erfolgsquote k√∂nnen wir auch andere Statistiken √ºber das Verhalten des Agenten berechnen.\n",
    "- Ein interessantes Ma√ü ist die Episodenl√§nge.\n",
    "- RLlib zeichnet diese standardm√§√üig auf, so dass wir sie leicht abrufen k√∂nnen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e8819cf4-bc66-4ce7-a723-ea6127d4c1cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.585470085470085"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_RandomLakeObs.evaluate()[\"evaluation\"][\"episode_len_mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "879d945f-57e6-409e-8f8a-e0d95e6cfe4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.20216606498195"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_RandomLakeObsRew2.evaluate()[\"evaluation\"][\"episode_len_mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de01c11-01b5-4f86-9f9e-9e2e43a3d62d",
   "metadata": {},
   "source": [
    "Obwohl die beiden Mittel die gleiche Erfolgsquote haben, neigt das neue Mittel zu k√ºrzeren Episoden.\n",
    "\n",
    "Anmerkungen \n",
    "\n",
    "- Das ist sehr interessant, weil der Agent den Unterschied zwischen L√∂chern und Kanten nicht \"sehen\" kann.\n",
    "- Wir k√∂nnten dies weiter erforschen, indem wir weitere benutzerdefinierte Metriken hinzuf√ºgen, z. B. die Anzahl der Beulen in der Kante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd02d93-0255-4ae2-aa46-5a9eb5111fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "#### disadvantages - loss of generality\n",
    "\n",
    "#- now only works if goal is at bottom-right\n",
    "#give a few real-world examples here -> important"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf32143e-7c32-49d2-9aed-f1081c5b8c35",
   "metadata": {},
   "source": [
    "## Analogie zum √ºberwachten Lernen: Reward Shaping\n",
    "<!-- multiple choice -->\n",
    "\n",
    "Vorhin haben wir eine Analogie zwischen der Kodierung von Beobachtungen im RL und der Vorverarbeitung von Merkmalen beim √ºberwachten Lernen gezogen. Welcher Aspekt des √ºberwachten Lernens ist die beste Analogie zum Reward Shaping im RL?\n",
    "\n",
    "- [Merkmalstechnik \n",
    "- [Modellauswahl | Nicht ganz. Aber wie wir sehen werden, gibt es auch im RL einen Platz f√ºr die Modellauswahl!\n",
    "- [ ] Abstimmung der Hyperparameter | Nicht ganz. Aber wie wir noch sehen werden, gibt es auch im RL einen Platz f√ºr Hyperparameter-Tuning!\n",
    "- [x] Auswahl einer Verlustfunktion | Eine √Ñnderung der Verlustfunktion √§ndert das \"beste\" Modell, genau wie eine √Ñnderung der Belohnungen die \"beste\" Policy √§ndert."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c458b5c7-bc0a-4084-8572-d35bf27ddc60",
   "metadata": {},
   "source": [
    "## Jeden Schritt belohnen: kleine negative Belohnungen\n",
    "<!-- multiple choice -->\n",
    "\n",
    "In RL-Environmenten wie Random Lake, wo der Agent ein bestimmtes Ziel erreichen muss, stellen wir uns vor, wir w√ºrden f√ºr _jeden_ Schritt, den der Agent macht, eine kleine negative Belohnung vergeben. Wie w√ºrde sich das auf die Zeit auswirken, die der Agent braucht, bis er das Ziel erreicht?\n",
    "\n",
    "- [x] Der Agent wird versuchen, das Ziel in so wenigen Schritten wie m√∂glich zu erreichen.\n",
    "- [Der Agent wird versuchen, das Ziel in so vielen Schritten wie m√∂glich zu erreichen. | Wenn wir jeden Schritt bestrafen, werden mehr Schritte mit einer geringeren Belohnung belohnt.\n",
    "- [ ] Keine √Ñnderung. | Wenn wir jeden Schritt bestrafen, werden mehr Schritte mit einer geringeren Belohnung belohnt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96265e6-b392-4923-b694-088fb282efb0",
   "metadata": {},
   "source": [
    "## Erkundung vs. Ausbeutung\n",
    "<!-- multiple choice -->\n",
    "\n",
    "Welche der folgenden Aussagen √ºber den Kompromiss zwischen Erkundung und Ausbeutung im RL ist richtig?\n",
    "\n",
    "- [Wenn ein Spieler nur erforscht, wird er nie eine gute Policy finden. | Er wird tats√§chlich gute Policyn finden, nur EXTREM langsam.\n",
    "- [x] Wenn ein Agent nur ausnutzt, wird er nie eine gute Policy finden. | Er kann einfach immer wieder das Gleiche versuchen.\n",
    "- [Agenten finden immer gute Policyn, auch ohne Exploration/Exploitation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93d7454-eaff-4c79-8398-19cb541b4b37",
   "metadata": {},
   "source": [
    "## Unbeabsichtigte Folgen\n",
    "<!-- coding exercise -->\n",
    "\n",
    "In dieser √úbung probierst du eine schlechte Idee aus: Jedes Mal, wenn der Agent einen Schritt macht, erh√§lt er eine gro√üe negative Belohnung. Wir werden -1 pro Schritt verwenden. F√ºr das Erreichen des Ziels erh√§lt der Agent immer noch eine Belohnung von +1. Implementiere diese Belohnung, trainiere den Agenten und schau dir die durchschnittliche Episodenl√§nge an, die der Code ausgibt. Vergleiche dies mit der durchschnittlichen Episodenl√§nge eines Agenten, der nur nach dem Zufallsprinzip handelt. Beantworte dann die Multiple-Choice-Frage zum Verhalten des Agenten. Was denkst du, was hier vor sich geht? \n",
    "\n",
    "(Zu deiner Information: Wie bereits erw√§hnt, kann diese Art der Ver√§nderung einer Environment auch mit Gym-Wrappern erreicht werden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f8434a-7575-4bfb-af7b-9858f580ae7b",
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "from utils_03 import lake_default_config\n",
    "from envs_03 import RandomLakeObs\n",
    "\n",
    "class RandomLakeBadIdea(RandomLakeObs):\n",
    "    def reward(self):\n",
    "        old_reward = int(self.player == self.goal) \n",
    "        return ____\n",
    "    \n",
    "ppo = lake_default_config.build(env=____)\n",
    "\n",
    "for i in range(8):\n",
    "    ppo.train()\n",
    "    \n",
    "print(\"Average episode length for trained agent: %.1f\" % \n",
    "      ppo.evaluate()[\"evaluation\"][____])\n",
    "\n",
    "random_agent_config = (\n",
    "    lake_default_config\\\n",
    "    .exploration(exploration_config={\"type\": \"Random\"})\\\n",
    "    .evaluation(evaluation_config={\"explore\" : True})\n",
    ")\n",
    "random_agent = random_agent_config.build(env=RandomLakeBadIdea)\n",
    "\n",
    "print(\"Average episode length for random agent: %.1f\" % \n",
    "      random_agent.evaluate()[\"evaluation\"][____])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "265575eb-94ef-4899-b7c0-c00c70c155e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "Average episode length for trained agent: 4.4\n",
      "Average episode length for random agent: 12.1\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "from utils_03 import lake_default_config\n",
    "from envs_03 import RandomLakeObs\n",
    "\n",
    "class RandomLakeBadIdea(RandomLakeObs):\n",
    "    def reward(self):\n",
    "        old_reward = int(self.player == self.goal) \n",
    "        return old_reward - 1\n",
    "\n",
    "ppo = lake_default_config.build(env=RandomLakeBadIdea)\n",
    "\n",
    "\n",
    "for i in range(8):\n",
    "    ppo.train()\n",
    "    \n",
    "print(\"Average episode length for trained agent: %.1f\" % \n",
    "      ppo.evaluate()[\"evaluation\"][\"episode_len_mean\"])\n",
    "\n",
    "random_agent_config = (\n",
    "    lake_default_config\\\n",
    "    .exploration(exploration_config={\"type\": \"Random\"})\\\n",
    "    .evaluation(evaluation_config={\"explore\" : True})\n",
    ")\n",
    "random_agent = random_agent_config.build(env=RandomLakeBadIdea)\n",
    "\n",
    "print(\"Average episode length for random agent: %.1f\" % \n",
    "      random_agent.evaluate()[\"evaluation\"][\"episode_len_mean\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dbab96-1590-409b-b684-d6a05c880c1c",
   "metadata": {},
   "source": [
    "#### Das Verhalten des Agenten\n",
    "\n",
    "Was glaubst du, was dieser Agent tut, wenn er in einer Environment trainiert wird, in der es bei jedem Schritt eine gro√üe negative Belohnung gibt?\n",
    "\n",
    "- [ ] Der Agent bleibt still, weil er davon abgehalten wird, sich zu bewegen. | Versuch es noch einmal!\n",
    "- [ ] Der Agent ist nicht daran interessiert, das Ziel zu erreichen, weil die Belohnung vergleichsweise gering ist. | Versuche es noch einmal!\n",
    "- [x] Der Agent lernt, so schnell wie m√∂glich in den See zu springen, um die negative Belohnung der Bewegung zu vermeiden. | Pfui! ü•∂\n",
    "- [ ] Der Agent erreicht das Ziel auf Anhieb. | Das w√§re aber w√ºnschenswert!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ray200]",
   "language": "python",
   "name": "conda-env-ray200-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
