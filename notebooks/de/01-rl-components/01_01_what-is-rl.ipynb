{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5aa21ad-6bb9-4990-ab4d-fb9435b0bc00",
   "metadata": {},
   "source": [
    "## Überwachtes Lernen vs. Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281ff2e0-a58c-46a3-8250-aa0002f2ad9f",
   "metadata": {},
   "source": [
    "#### Auffrischung des betreuten Lernens\n",
    "\n",
    "- Überwachtes Lernen lernt, y (meist eine Zahl oder Kategorie) aus x (meist ein Zahlenvektor oder ein Bild) vorherzusagen, wenn ein Datensatz vorliegt.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "➡️ Drücke die rechte Pfeiltaste, um zur nächsten Folie zu gelangen (und die Escape-Taste, um alle Folien zu sehen)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb4df9c-fc80-4571-9f51-8efebc175d24",
   "metadata": {},
   "source": [
    "#### Beispiele für überwachtes Lernen\n",
    "\n",
    "- Vorhersage von Hauspreisen anhand von Hausmerkmalen\n",
    "- Klassifizierung einer E-Mail als Spam oder nicht Spam\n",
    "- Erkennen, ob ein Bild einen Hund enthält\n",
    "\n",
    "Allgemeine Idee: Vorhersage der Ausgabe anhand der Eingabe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0f26a4-2313-4474-b173-26b32526eecf",
   "metadata": {},
   "source": [
    "#### Die \"API\" des überwachten Lernens\n",
    "\n",
    "![](img/SL-API.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c2d25f-02fe-495f-9529-7117d2b91f5c",
   "metadata": {},
   "source": [
    "#### Die \"API\" des verstärkenden Lernens\n",
    "\n",
    "![](img/SL-vs-RL.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f2188e-0756-4fc7-a038-83b1d54fac8f",
   "metadata": {},
   "source": [
    "#### Reinforcement learning\n",
    "\n",
    "- RL-Input: eine **Environment** \n",
    "- RL-Ausgang: eine **Policy**, die Entscheidungen trifft\n",
    "\n",
    "Wir werden Environmenten und Policyn später in diesem Modul definieren!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b566b4-2ef9-4545-834f-903a520c855c",
   "metadata": {},
   "source": [
    "\n",
    "#### Beispiele für Reinforcement Learning\n",
    "\n",
    "- Ein selbstfahrendes Auto lernt zu fahren\n",
    "  - Env: Straßenbedingungen, wie das Auto fährt. Policy: der selbstfahrende Algorithmus\n",
    "- Lernen, wie man ein Videospiel spielt\n",
    "  - Env: das Spiel. Policy: die Spiel-KI\n",
    "- Lernen der \"besten\" Filmsequenz, die einem Nutzer empfohlen wird\n",
    "  - Env: Vorlieben/Verhaltensweisen des Nutzers in Bezug auf Filme. Policy: das Empfehlungssystem\n",
    "\n",
    "Allgemeine Idee: Du musst eine Reihe von Entscheidungen treffen und willst bei jedem Schritt optimal handeln.\n",
    "\n",
    "Diese Entscheidungen wirken sich auf zukünftige Eingaben/Ausgaben aus (im Gegensatz zu SL).\n",
    "\n",
    "Anmerkungen:\n",
    "\n",
    "- Die Eingabe beim überwachten Lernen ist ein Datensatz\n",
    "- Die Ausgabe ist das trainierte Modell, das Vorhersagen machen kann\n",
    "- Das Modell selbst hat Eingaben und Ausgaben ($x$ und $y$)\n",
    "- Beachte, dass die Ausgabe wiederum eine Funktion ist, mit der etwas berechnet/vorgesagt werden kann.\n",
    "- Im nächsten Abschnitt werden wir uns mit Environmenten und Policyn befassen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d070eb2b-121b-4925-b30e-80e8d2b1283b",
   "metadata": {},
   "source": [
    "#### Lass uns das Gelernte anwenden!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6714de6-59d5-428a-90d5-caa45d222203",
   "metadata": {},
   "source": [
    "## Überwachtes Lernen oder Reinforcement Learning\n",
    "<!-- multiple choice -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c20387-a7d2-4697-895a-57e97d4c1013",
   "metadata": {},
   "source": [
    "#### Würdest du dieses Problem mit SL oder RL lösen?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956a69c9-859d-4a9f-8f49-ca69bdf64876",
   "metadata": {},
   "source": [
    "_Klassifizierung der Arten anhand eines Bildes von einem Baum_\n",
    "\n",
    "- [x] SL \n",
    "- [ ] RL | Versuch es noch einmal - das ist eine einzelne Vorhersage und keine Folge von Aktionen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aec3e2f-b3b2-48fb-b466-fc81a2447655",
   "metadata": {},
   "source": [
    "#### Würdest du dieses Problem mit SL oder RL lösen?\n",
    "\n",
    "erschaffung einer KI, die Schach spielt\n",
    "\n",
    "- [SL | Versuch es noch einmal - die Schach-KI muss bei jedem Schritt in einer Sequenz optimal handeln.\n",
    "- [x] RL "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21b1192-8d52-4178-9921-aef09cbd0286",
   "metadata": {},
   "source": [
    "#### Würdest du dieses Problem mit SL oder RL lösen?\n",
    "\n",
    "_Vorhersage, ob ein User einen Film mag oder nicht_\n",
    "\n",
    "- [x] SL \n",
    "- [ ] RL | Versuch es noch einmal - es handelt sich um eine einzelne Vorhersage und nicht um eine Folge von Aktionen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67cacbff-c6d3-4b5d-9ea7-4d4cd1e3e3a8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# start the course by showing some really cool results.\n",
    "# not sure yet whether that should be here, or at the end of module 1 (maybe a section 1.4)\n",
    "# let's see...\n",
    "\n",
    "# Also TODO:\n",
    "# Talk about simulations since we can't usually do RL training in a \"real\" environment\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ray200]",
   "language": "python",
   "name": "conda-env-ray200-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
