{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90c01ab9-1cd0-47d8-a198-e07e36e2c1fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## RL Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddd909a-c86f-4e5c-9ba0-8b0ef1152730",
   "metadata": {},
   "source": [
    "#### Was ist eine Policy?\n",
    "\n",
    "Kehren wir zurück zur \"API\" von RL:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50604c8f-7c62-495e-b9c6-f3b408a440d9",
   "metadata": {},
   "source": [
    "![](img/RL-API.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ac6598-7965-4496-ac1e-6b71cc7e487b",
   "metadata": {},
   "source": [
    "- Die Policy ist die Ausgabe von RL\n",
    "- Sie bildet Beobachtungen auf Aktionen ab\n",
    "- Die Policy ist wie das Gehirn des Agenten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a207992-104b-4ecd-b470-341860f09ad6",
   "metadata": {},
   "source": [
    "#### Was ist eine Policy? Details.\n",
    "\n",
    "- Eine Policy ist eine Funktion, die Beobachtungen auf Aktionen abbildet.\n",
    "- Betrachten wir noch einmal die Frozen Lake Environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2a869f8-2fb6-4ef5-bd1c-d8c74493e93c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
    "obs = env.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbfda97-e12c-4284-b0b2-78a8f21c7ede",
   "metadata": {},
   "source": [
    "- Wir haben eine Beobachtung 0 erhalten. Was machen wir als nächstes? \n",
    "- Die Policy wird es uns sagen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a98f1c-a7b6-40bc-9d18-dce492473ea9",
   "metadata": {},
   "source": [
    "#### Beispielpolicy\n",
    "\n",
    "Eine Policy könnte wie folgt aussehen\n",
    "\n",
    "| Beobachtung | Aktion |\n",
    "|------|-------|\n",
    "| 0 | 0 |\n",
    "| 1 | 3 |\n",
    "| 2 | 1 |\n",
    "| 3 | 1 |\n",
    "| ... | ... |\n",
    "| 14 | 2 |\n",
    "| 15 | 2 |\n",
    "\n",
    "- Auf der linken Seite stehen alle möglichen Beobachtungen (15 für Frozen Lake).\n",
    "- Auf der rechten Seite steht die entsprechende Aktion, die wir ausführen werden, _wenn wir diese Beobachtung sehen_.\n",
    "- \"Wenn ich 0 sehe, werde ich 0 tun; wenn ich 1 sehe, werde ich 3 tun\" usw."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c510ca5c-a9d5-44e3-9afb-c4a15979f874",
   "metadata": {},
   "source": [
    "#### Ziel von RL\n",
    "\n",
    "**Das Ziel von RL ist es, in einer gegebenen Environment eine gute Policy zu lernen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dac4894-b44a-42bc-8269-54f8a7d93c8e",
   "metadata": {},
   "source": [
    "#### Nicht-deterministische Policy\n",
    "\n",
    "- Zuvor haben wir etwas über deterministische und nicht-deterministische Environmenten gelernt \n",
    "- Analog dazu gibt es deterministische und nicht-deterministische _Policyen_.\n",
    "- Zuvor haben wir eine deterministische Policy gesehen: Eine bestimmte Beobachtung führt zu einer festgelegten Aktion.\n",
    "- Hier ist ein Beispiel für eine nicht-deterministische Policy:\n",
    "\n",
    "| Beobachtung | P(links) | P(unten) | P(rechts) | P(oben) | \n",
    "|------------|-------|-----------|---------|-------|\n",
    "| 0 | 0 | 0.9 | 0.01 | 0.04 | 0.05\n",
    "| 1 | 3 | 0.05 | 0.05 | 0.05 | 0.85\n",
    "| ... | ... | ... | ...      | ...      | ...\n",
    "| 15 | 2 | 0.0 | 0.0 | 0.99 | 0.01\n",
    "\n",
    "\"Wenn ich 0 sehe, werde ich 99% der Zeit nach links gehen, 1% der Zeit nach unten, 4% der Zeit nach rechts und 5% der Zeit nach oben.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34003758-53d7-4832-8e82-f805d723db9c",
   "metadata": {},
   "source": [
    "#### Kontinuierliche Aktionsräume\n",
    "\n",
    "Was ist, wenn unser Handlungsraum kontinuierlich ist? Wir können trotzdem eine Policy haben. Zum Beispiel:\n",
    "\n",
    "| Beobachtung | Aktion |\n",
    "|------|-------|\n",
    "| 0 | 0.42 |\n",
    "| 1 | -3.99 |\n",
    "| ... | ... |\n",
    "| 15 | 2.24 |\n",
    "\n",
    "Eine nicht-deterministische Policy müsste allerdings aus einer Wahrscheinlichkeitsverteilung gezogen werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8212fc5-3270-49b9-8e82-44e04ebc892c",
   "metadata": {},
   "source": [
    "#### Kontinuierliche Beobachtungsräume\n",
    "\n",
    "- Was ist, wenn unser _Beobachtungsraum_ kontinuierlich ist? \n",
    "- Nun, dann können wir die Policy nicht mehr als Tabelle zeichnen...\n",
    "- In diesem Fall ist unsere Policy eine _Funktion_ des Beobachtungswertes \n",
    "- Zum Beispiel: \"Gaspedalwinkel zum Boden (Aktion) = 1,5 x Entfernung zum nächsten Hindernis (Beobachtung)\" \n",
    "- Dieses Spielzeugbeispiel besagt, dass du das Auto beschleunigen kannst, wenn das nächste Hindernis weit entfernt ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed8dfeec-6879-4d40-9c9e-b28e4bb260b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# too much text around here, need more code and/or images\n",
    "# I think we can remove some of this stuff (continuous, beyond scalars) and add it back it when it's paired with a concrete example\n",
    "# it's not very helpful/interesting as just ideas in isolation..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0043038d-d6f4-4364-b348-e968fdb37d3a",
   "metadata": {},
   "source": [
    "#### Jenseits von Skalaren\n",
    "\n",
    "- Bis jetzt sind wir davon ausgegangen, dass die Beobachtung eine einzelne Zahl und die Aktion eine einzelne Zahl ist.\n",
    "- Beide können aber auch komplexere Datentypen sein: Bilder, Vektoren usw \n",
    "- Die tatsächlichen Beobachtungen eines selbstfahrenden Autos können aus Dutzenden von Messungen, Bildern usw. bestehen.\n",
    "- Die tatsächlichen Aktionen eines selbstfahrenden Autos können in jedem Zeitschritt mehrere Werte festlegen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dc8172-ae79-405f-801f-a2b750a62e31",
   "metadata": {},
   "source": [
    "#### Über Policyen als Funktionen nachdenken\n",
    "\n",
    "- Im Allgemeinen ist dies eine nützliche Denkweise: Die Policy ist eine Funktion, die Beobachtungen auf Aktionen abbildet.\n",
    "- Beim **tiefen Reinforcement Learning** ist diese Funktion ein neuronales Netz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a75dce-81a4-4ff6-923c-d5e0b736384c",
   "metadata": {},
   "source": [
    "#### Zusammenfassung\n",
    "\n",
    "- Der \"Agent\" oder \"Spieler\" ist die Verkörperung der Policy\n",
    "- Es gibt keine zusätzliche \"Intelligenz\" oder Entscheidungsfindung jenseits der Policy\n",
    "- Daher brauchen wir den Begriff des Agenten/Spielers technisch gesehen nicht\n",
    "- Die Policy ist das Ergebnis von RL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7210fa7d-3ebb-4906-83cb-853e8f4d91d3",
   "metadata": {},
   "source": [
    "#### Lass uns das Gelernte anwenden!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5780af6f-fa34-4b85-af30-32e20755f1c0",
   "metadata": {},
   "source": [
    "## Policy des gefrorenen Sees\n",
    "<!-- multiple choice -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a299f97-00fa-4e31-ad66-5e843b63e1d1",
   "metadata": {},
   "source": [
    "Erinnere dich an die Environment des gefrorenen Sees:\n",
    "\n",
    "```\n",
    "P...\n",
    ".O.O\n",
    "...O\n",
    "O..G\n",
    "```\n",
    "\n",
    "mit seinem Beobachtungsraum dargestellt als:\n",
    "\n",
    "```\n",
    " 0 1 2 3\n",
    " 4 5 6 7\n",
    " 8 9 10 11\n",
    "12 13 14 15\n",
    "```\n",
    "\n",
    "und Aktionen dargestellt als\n",
    "\n",
    "| Aktion | Bedeutung |\n",
    "|------|------|\n",
    "| 0 | links |\n",
    "| 1 | unten |\n",
    "| 2 | rechts |\n",
    "| 3 | oben |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8eca95c-cc54-42d3-a442-f41fb0c27b5e",
   "metadata": {},
   "source": [
    "#### Frage 1\n",
    "\n",
    "Die folgende Policy enthält einen fehlenden Eintrag, der durch ein `?`-Symbol dargestellt wird.\n",
    "\n",
    "| Beobachtung | Aktion |\n",
    "|------|-------|\n",
    "| 0 | 0 |\n",
    "| 1 | 2 |\n",
    "| ... | ... |\n",
    "| 13 | ? |\n",
    "| 14 | 2 |\n",
    "| 15 | 0 |\n",
    "\n",
    "Wähle die beste Wahl aus, um den Eintrag `?` zu füllen.\n",
    "\n",
    "- [ ] 0 | Versuche es noch einmal!\n",
    "- [ ] 1 | Versuch es noch einmal!\n",
    "- [x] 2 | Ja! Wenn du nach rechts gehst, kommst du dem Ziel näher.\n",
    "- [ ] 3 | Versuch es noch einmal!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c41f87-143b-409f-a16c-b8b1ac4b0cc7",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Frage 2\n",
    "\n",
    "_In der rutschigen Version des Gefrorenen Sees hat der Agent eine Wahrscheinlichkeit von 1/3, sich in die beabsichtigte Richtung zu bewegen und je 1/3 in die beiden senkrechten Richtungen._\n",
    "\n",
    "Ist die obige Aussage eine Aussage über die Environment oder die Policy?\n",
    "\n",
    "- [x] Environment | Du hast es erfasst! \n",
    "- [ ] Policy | Denk daran, die Policy beschreibt, wie der Agent auf Beobachtungen reagiert."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefe9bb5-2162-44df-9559-e98566491dff",
   "metadata": {},
   "source": [
    "#### Frage 3\n",
    "\n",
    "in der glitschigen Version des Gefrorenen Sees ist es manchmal besser, nicht in die Richtung zu gehen, in die du eigentlich gehen willst, weil es wichtiger ist, nicht in ein Loch zu rutschen._\n",
    "\n",
    "Ist die obige Aussage eine Aussage über die Environment oder über die Policy?\n",
    "\n",
    "- [ ] Environment | In der obigen Aussage geht es darum, wie man sich in einer bestimmten Situation am besten verhält; dies wird durch die Policy bestimmt.\n",
    "- [x] Policy | Du hast es erfasst!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2b5b57-e0e8-40c6-858c-34bc20ce5112",
   "metadata": {},
   "source": [
    "## Berechnung der erwarteten Belohnung\n",
    "<!-- coding exercise -->\n",
    "\n",
    "Vervollständige den folgenden Code so, dass er die erwartete Belohnung in der rutschigen (nicht-deterministischen)\n",
    "Frozen Lake Environment über 1000 Episoden berechnet. Die innere Schleife läuft über die Schritte innerhalb einer einzelnen Episode.\n",
    "Die äußere Schleife erstreckt sich über Episoden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca010dee-36a5-452c-84db-9bd7059f28b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 0.047\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "\n",
    "rewards = []\n",
    "N = 1000\n",
    "\n",
    "for ____:\n",
    "\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while ____:\n",
    "        action = np.random.randint(low=0, high=4)\n",
    "        obs, reward, done, _ = env.step(____)\n",
    "    \n",
    "    rewards.append(reward)\n",
    "    \n",
    "print(\"Average reward:\", sum(rewards)/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99564a1c-076f-4167-858f-ce8ce47107f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 0.014\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "\n",
    "rewards = []\n",
    "N = 1000\n",
    "\n",
    "for i in range(N): # loop over N episodes\n",
    "\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.random.randint(low=0, high=4)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "    \n",
    "    rewards.append(reward)\n",
    "    \n",
    "print(\"Average reward:\", sum(rewards)/N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8a29b5-f8b4-452c-9cd2-a045ba136d04",
   "metadata": {},
   "source": [
    "## Handgefertigte Policy\n",
    "<!-- coding exercise -->\n",
    "\n",
    "Der folgende Code lädt die (nicht-deterministische) rutschige Frozen Lake Environment.\n",
    "Eine (deterministische) Policy ist als Python-dictionary definiert, das Beobachtungen auf Aktionen abbildet.\n",
    "Der Code durchläuft eine Schleife über 1000 Episoden. Innerhalb jeder Episode durchläuft er die Zeitschritte (Beobachtungen und Aktionen), bis die Episode abgeschlossen ist und eine Belohnung erreicht wurde. Dann gibt er die durchschnittliche Belohnung über die 1000 Episoden aus. Normalerweise liegt die durchschnittliche Belohnung bei 0,05, was bedeutet, dass das Ziel in etwa 5 % der Fälle erreicht wird.\n",
    "\n",
    "**Deine Aufgabe:** Ändere die Policy so, dass eine durchschnittliche Belohnung von mindestens 0,02 erreicht wird (d.h. der Agent erreicht das Ziel in 20% der Fälle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f68e9b",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "\n",
    "import gym\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d9fcefc-ba10-45a3-9e03-adb283d6c08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 0.034\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "\n",
    "import gym\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "\n",
    "policy = {\n",
    "    0 : 2,\n",
    "    1 : 2,\n",
    "    2 : 2,\n",
    "    3 : 2,\n",
    "    4 : 1,\n",
    "    5 : 1,\n",
    "    6 : 1,\n",
    "    7 : 1,\n",
    "    8 : 2,\n",
    "    9 : 2,\n",
    "    10: 2,\n",
    "    11: 0,\n",
    "    12: 2,\n",
    "    13: 2,\n",
    "    14: 2,\n",
    "    15: 2\n",
    "}\n",
    "\n",
    "rewards = []\n",
    "N = 1000\n",
    "for i in range(N): # loop over N episodes\n",
    "\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = policy[obs]\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "    \n",
    "    rewards.append(reward)\n",
    "    \n",
    "print(\"Average reward:\", sum(rewards)/N)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ray200]",
   "language": "python",
   "name": "conda-env-ray200-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
