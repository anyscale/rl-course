{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5aa21ad-6bb9-4990-ab4d-fb9435b0bc00",
   "metadata": {},
   "source": [
    "## Recommender env: implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fdd0ce7d-4a5b-4d03-8d93-b176de9e4db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38ea949-1d66-4b7b-a25d-a1ef65ca63a7",
   "metadata": {},
   "source": [
    "#### Implementation\n",
    "\n",
    "Now we're ready to implement our environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0293f36-0ca4-40d4-8c4f-456c07627269",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Review: what does an env need?\n",
    "\n",
    "Requirements for an RL environment:\n",
    "\n",
    "- Observation space\n",
    "- Action space\n",
    "- Constructor\n",
    "- `reset()` method\n",
    "- `step()` method\n",
    "\n",
    "We'll now create all of these for our recommender environment. Let's go!\n",
    "\n",
    "Notes:\n",
    "\n",
    "In the previous module, we listed the following requirements for an RL environment:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c68cf7-4dc6-4ab4-86c5-93b10fced0ae",
   "metadata": {},
   "source": [
    "#### Observation space\n",
    "\n",
    "- The first step is to set the observation space. \n",
    "- Our observations are the _features of candidate items_.\n",
    "- For simplicity, we'll assume only 1 feature.\n",
    "- Here, our observations are continuous-valued, not discrete like Frozen Lake.\n",
    "- If you'd like, pause for a moment and think about what the observation space should be.\n",
    "  - You may wish to consult the gym library documentation [here](https://www.gymlibrary.ml/content/spaces/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64a506ca-cb76-4fba-a53d-65de3e467e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# we could have a multiple choice in here\n",
    "# I guess it's potentially fine to have tons of short notebooks in this module???\n",
    "# might be a bit of a hassle for video recording?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fdd370-3e7b-45f9-92e0-95bf26ca09ec",
   "metadata": {},
   "source": [
    "#### Observation space\n",
    "\n",
    "- We want the `Box` space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d66f7f10-c5e5-4e7b-ad4f-ad90934ba1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "num_candidates=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28bfa81a-a401-4bf1-a400-d97b305c7fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space = gym.spaces.Box(low=0, high=1, shape=(num_candidates,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ad3b01-c30a-4b2b-bdeb-aa9a923f7405",
   "metadata": {},
   "source": [
    "- Features are between 0 and 1, number of features is number of item candidates. \n",
    "- Here is an sample from the observation space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d360bb64-1776-452a-976a-5c2ee3718169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5112677 , 0.21406838, 0.2754315 , 0.6045677 , 0.1501461 ,\n",
       "       0.8971636 , 0.42283583, 0.25541636, 0.76292586, 0.62952304],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931c9abd-3444-4f33-a31e-4cba72b1751d",
   "metadata": {},
   "source": [
    "Looks good üëç"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb5cadd-bbdc-4a19-931c-522c8902407f",
   "metadata": {},
   "source": [
    "#### Action space\n",
    "\n",
    "- In this environment, the action is the chosen item to recommend.\n",
    "- If you'd like, pause for a moment and think about what the action space should be.\n",
    "  - You may wish to consult the gym library documentation [here](https://www.gymlibrary.ml/content/spaces/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b954af2-20ad-4b11-bdee-f19fca5abd22",
   "metadata": {},
   "source": [
    "#### Action space\n",
    "\n",
    "Unlike the observation space, the action space _is_ discrete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ee176e9-a81d-48a1-9338-bace6fe0dd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = gym.spaces.Discrete(num_candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba417790-94a0-4dab-b3a7-ba3fd98ee567",
   "metadata": {},
   "source": [
    "Here are 15 randomly sampled actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "537f7414-e182-45bb-877f-4812b44b9e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 6, 4, 2, 8, 2, 2, 2, 7, 5, 2, 2, 7, 5, 7]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[action_space.sample() for i in range(15)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539c8ffb-4ebd-4e99-8e0a-08a8c32c9e5f",
   "metadata": {},
   "source": [
    "Looks good üëç"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58280654-1e51-492c-96da-4764c0ceba37",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Constructor\n",
    "\n",
    "Starting with the constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f91a5816-bd80-4863-ba43-7af4cf1ca8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicRecommender(gym.Env):\n",
    "    def __init__(self, env_config=None):\n",
    "        # Set parameters    \n",
    "        self.num_candidates = env_config.get(\"num_candidates\", 10)\n",
    "        self.alpha = env_config.get(\"alpha\", 0.9)\n",
    "\n",
    "        # Set observation and action spaces\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(self.num_candidates,))\n",
    "        self.action_space = gym.spaces.Discrete(self.num_candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722e84ed-5e80-42d2-a452-9da102588ddc",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "The observation and action spaces follow what we saw in the previous slide deck. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1f4d98-9a6b-4669-98eb-3e59b6b12354",
   "metadata": {},
   "source": [
    "#### Reset\n",
    "\n",
    "Next we need our `reset()` method, which needs to:\n",
    "\n",
    "- Reset the sugar level\n",
    "- Sample/simulate random document features\n",
    "- Return the observation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25a91fe9-1554-41ca-9a34-ec8fffbab641",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicRecommender(gym.Env):\n",
    "\n",
    "    def reset(self):\n",
    "        self.sugar_level = 0.0                \n",
    "        self.documents = np.random.rand(self.num_candidates)\n",
    "        return self.documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa79d76-ad0d-49f6-9a1b-e53d5ae50064",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "We'll reset the sugar level to zero, though other choices could be reasonable as well. We'll also have the features be between 0 and 1 for simplicity, and will choose them uniformly at random. Again, all of these are choices that we are making to model the real situation. We are going with the simplest choices for instructional purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf79eb86-84d8-49c6-81e7-b3d4cb310121",
   "metadata": {},
   "source": [
    "#### Step\n",
    "\n",
    "- As usual, the `step()` method is the most complicated.\n",
    "- However, we can make use of our `update_sugar_level` and `reward` functions from earlier.\n",
    "- As a reminder, here they are again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9982ede4-b330-41c7-8c18-ab32e0d43c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_sugar_level(self, item_sweetness):\n",
    "    self.sugar_level = self.alpha * self.sugar_level + (1 - self.alpha) * item_sweetness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af8f291f-3510-4567-bb2c-3d18d5c5d52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward(self, item_sweetness):\n",
    "    return item_sweetness * (1 - self.sugar_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd64f496-0c84-4110-ac2f-bf84ba4051f3",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "We made slight modifications because alpha and sugar level are now stored in the class as self.alpha and self.sugar_level rather than being passed into the functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd598b75-5ad8-4616-91c4-e916c38036fd",
   "metadata": {},
   "source": [
    "#### Step\n",
    "\n",
    "And now for `step()`, which needs to:\n",
    "\n",
    "- Compute the reward\n",
    "- Update the sugar level\n",
    "- Simulate a set of new candidate items for the next iteration\n",
    "- Return the observation, reward, and done flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bc3b3c3-8ca9-4498-a8be-883cdaa2481d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def step(self, action):\n",
    "\n",
    "        # Get the sweetness of the recommended item\n",
    "        item_sweetness = self.documents[action]\n",
    "\n",
    "        # Compute reward\n",
    "        reward = self.reward(item_sweetness)\n",
    "\n",
    "        # Update sugar level\n",
    "        self.update_sugar_level(item_sweetness)\n",
    "\n",
    "        # Generate new candidates for the next recommendation\n",
    "        if self.resample_documents:\n",
    "            self.documents = np.random.rand(self.num_candidates)\n",
    "\n",
    "        # Set the done flag (not shown for brevity)\n",
    "        done = False\n",
    "            \n",
    "        return self.documents, reward, done, {\"sugar_level\" : self.sugar_level}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3f449e-cc13-4275-8d33-260a8614d7ef",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "Some points to note here. For now, we are always returning done=False. In reality, we will set a fixed maximum number of steps before the episode ends, as shown in the previous module. However, this is not shown here for brevity.\n",
    "Note also that the `step` method returns the sugar level. This is not accessed by the agent, but rather is just for our convenience as we debug/introspect the RL system. In the past, we had just ignored this optional info returned by `step`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e248fdac-86c4-49df-b55e-4502761f77a3",
   "metadata": {},
   "source": [
    "#### Let's test it out!\n",
    "\n",
    "Let's test out our new environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "beef24dd-4068-4872-a414-5d1c718c8e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "from envs import BasicRecommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e310dd9-823b-4bde-8592-54361416e048",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config = {\n",
    "    \"num_candidates\" : 2,\n",
    "    \"alpha\"          : 0.5,\n",
    "    \"seed\"           : 42\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70484c30-0ff8-4c21-aaf7-d6e7a364f542",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BasicRecommender(env_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e9fcf1-8da5-4568-94b2-649e66014b2a",
   "metadata": {},
   "source": [
    "Notes: \n",
    "\n",
    "We'll set the number of candidates to 2 for ease of illustration later on, and the default alpha to 0.5, meaning the new sugar level is updated to be the (unweighted) average of the old sugar level and the item sweetness. \n",
    "\n",
    "We set the random seed to 0 for reproducibility. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bc64af-9d2d-487b-a6f1-d97ed22d754b",
   "metadata": {},
   "source": [
    "#### Let's test it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5745a4f-7796-4ed5-9133-b1ec870087a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "# env.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ed63dba-2640-4eda-9bae-0a3647b7b989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.37454012, 0.95071431])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b639e0d-1db7-45aa-a19e-7fd80e4eb8b4",
   "metadata": {},
   "source": [
    "- This looks good.\n",
    "- Since we set `num_candidates` to 2 in `env_config`, we see 2 candidates.\n",
    "- Each of the numbers above represents the sweetness of a simulated item candidate.\n",
    "- Let's have the agent select (recommend) the first item, with sweetness 0.37."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d578c363-e5cb-4ce9-b4b8-031df870ff56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.73199394, 0.59865848]),\n",
       " 0.3745401188473625,\n",
       " False,\n",
       " {'sugar_level': 0.18727005942368125})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272f90f0-bee8-417b-a6a1-0a334351ed97",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "We can observe that the sugar level went up from 0 to 1-alpha, or 0.5, times the item sweetness of 0.37. As a reminder, the sugar level info is not available to the agent, it's just for us. Note also that, after our action, a new set of candidate documents are given as the observation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a44686e-5215-4e24-b564-0275df9a3501",
   "metadata": {},
   "source": [
    "#### Baseline\n",
    "\n",
    "Before training our agent, let's first establish two baselines:\n",
    "\n",
    "- Greedy baseline: always pick the sweetest item\n",
    "- Random baseline: pick randomly every time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4322dc8e-9605-40b5-8576-37374d85c79d",
   "metadata": {},
   "source": [
    "#### Greedy baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1925170f-4870-43c6-b031-3f40d8defa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BasicRecommender(env_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6b88f41-c1e0-43a9-abee-3c7044291398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "# env.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38bc595c-6e7b-4b8a-88d1-87a18bc8714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_agent_episode(env):\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        sweetest_item = np.argmax(obs)\n",
    "        obs, reward, done, info = env.step(sweetest_item)\n",
    "        total_reward += reward\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c137ca74-5045-4153-b8fc-235007d9ce31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.059502585678352"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedy_agent_episode(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3821619e-b9e5-4f2b-a175-38f98c53d28b",
   "metadata": {},
   "source": [
    "Because of the randomness of the item sweetness values, we should repeat this for many episodes and take the average:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca857aef-87dd-4f08-b9c7-82de1a502ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.078751951739157"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([greedy_agent_episode(env) for episode in range(1000)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fe5487-e939-48a4-b480-efe780abd70d",
   "metadata": {},
   "source": [
    "- We can get a reward of 23.1 by always picking the sweetest items.\n",
    "- To make this more exciting, assume this is \\\\$23.1 for our business, and we have 1M users, so \\\\$23.1M.\n",
    "\n",
    "Notes:\n",
    "\n",
    "This is the total for 100 time steps, or around $0.23 per time step. If we were to have longer episodes, this number would be larger."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c79af7-c0f2-4787-bbde-ba745e45c8b0",
   "metadata": {},
   "source": [
    "#### Random baseline\n",
    "\n",
    "Now let's try a random approach:\n",
    "\n",
    "Notes: \n",
    "\n",
    "Here, the agent recommends a random item from the 10 candidates each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39ed7bff-551d-4939-9fcf-8c43886b5b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BasicRecommender(env_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "85e1be55-f27f-48c9-8285-e649ed159036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_agent_episode(env):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        random_item = np.random.randint(len(obs))\n",
    "        obs, reward, done, info = env.step(random_item)\n",
    "        total_reward += reward\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9a555ff5-ade4-4aef-9d43-d5124673df56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.73955296374227"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_agent_episode(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7059d9-fe85-4aec-901a-b9b485f90d55",
   "metadata": {},
   "source": [
    "Because of the randomness, we should repeat this for many episodes and take the average:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5324e3a1-da59-4c2c-9059-4699457d032e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.525583133463225"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([random_agent_episode(env) for episode in range(1000)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709316fe-dc62-4ea1-ae32-0b84f123cf62",
   "metadata": {},
   "source": [
    "- With random actions, we do better: around $25.5M revenues.\n",
    "- Time to pay me a bonus! üí∞"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73eb2a5e-ccde-46d7-90f5-1254e5dad886",
   "metadata": {},
   "source": [
    "#### Let's apply what we learned!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b52e45a-c183-4483-a44b-b98a4ccf35bd",
   "metadata": {},
   "source": [
    "## Greedy vs random\n",
    "<!-- multiple choice -->\n",
    "\n",
    "Above, we saw our random baseline outperform our greedy baseline. Do you think this would always be the case?\n",
    "\n",
    "- [ ] The random strategy always outperforms the greedy strategy. | What if the episode only had 1 step?\n",
    "- [ ] The greedy strategy always outperforms the random strategy. | We saw the random strategy do better, though.\n",
    "- [x] The random strategy performs better vs. greedy as the episode duration increases. | The greedy strategy works in the short-term, but not the long term.\n",
    "- [ ] The greedy strategy performs better vs. random as the episode duration increases. | I think you have this backwards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4a1094-e99e-4134-8ec5-baa646f8c4ec",
   "metadata": {},
   "source": [
    "## Improving the user model\n",
    "<!-- multiple choice -->\n",
    "\n",
    "We created a very simple user model/simulation for this example. Which of the following would **NOT** be a reasonable extension to our user model for a recommender system?\n",
    "\n",
    "- [ ] Increase the number of features that we use to represent each item candidate.\n",
    "- [ ] Account for features of each user rather than treating all users as the same.\n",
    "- [ ] Capture user interaction more richly, e.g. which items they click on, or consume/purchase.\n",
    "- [x] Simulate the neurons in the user's brain and how they react to item recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be4d10ce-0abf-452d-b840-127da197f5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# add a coding exercise here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05901a19-20be-442d-b794-a96d87f0f04b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rl-course-dev-2]",
   "language": "python",
   "name": "conda-env-rl-course-dev-2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
