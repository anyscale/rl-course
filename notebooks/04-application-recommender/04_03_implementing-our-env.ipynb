{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5aa21ad-6bb9-4990-ab4d-fb9435b0bc00",
   "metadata": {},
   "source": [
    "## Recommender env: implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdd0ce7d-4a5b-4d03-8d93-b176de9e4db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38ea949-1d66-4b7b-a25d-a1ef65ca63a7",
   "metadata": {},
   "source": [
    "#### Implementation\n",
    "\n",
    "Now we're ready to implement our environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0293f36-0ca4-40d4-8c4f-456c07627269",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Review: what does an env need?\n",
    "\n",
    "Requirements for an RL environment:\n",
    "\n",
    "- Observation space\n",
    "- Action space\n",
    "- Constructor\n",
    "- `reset()` method\n",
    "- `step()` method\n",
    "\n",
    "We'll now create all of these for our recommender environment. Let's go!\n",
    "\n",
    "Notes:\n",
    "\n",
    "In the previous module, we listed the following requirements for an RL environment:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c68cf7-4dc6-4ab4-86c5-93b10fced0ae",
   "metadata": {},
   "source": [
    "#### Observation space\n",
    "\n",
    "- The first step is to set the observation space. \n",
    "- Our observations are the _features of candidate items_.\n",
    "- For simplicity, we'll assume only 1 feature.\n",
    "- Here, our observations are continuous-valued, not discrete like Frozen Lake.\n",
    "- If you'd like, pause for a moment and think about what the observation space should be.\n",
    "  - You may wish to consult the gym library documentation [here](https://www.gymlibrary.ml/content/spaces/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64a506ca-cb76-4fba-a53d-65de3e467e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# we could have a multiple choice in here\n",
    "# I guess it's potentially fine to have tons of short notebooks in this module???\n",
    "# might be a bit of a hassle for video recording?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fdd370-3e7b-45f9-92e0-95bf26ca09ec",
   "metadata": {},
   "source": [
    "#### Observation space\n",
    "\n",
    "- We want the `Box` space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d66f7f10-c5e5-4e7b-ad4f-ad90934ba1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "num_candidates=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28bfa81a-a401-4bf1-a400-d97b305c7fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space = gym.spaces.Box(low=0, high=1, shape=(num_candidates,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ad3b01-c30a-4b2b-bdeb-aa9a923f7405",
   "metadata": {},
   "source": [
    "- Features are between 0 and 1, number of features is number of item candidates. \n",
    "- Here is an sample from the observation space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d360bb64-1776-452a-976a-5c2ee3718169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.753929  , 0.19141148, 0.57253695, 0.93540746, 0.31168196,\n",
       "       0.60167515, 0.46667033, 0.2824403 , 0.09364203, 0.14965677],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931c9abd-3444-4f33-a31e-4cba72b1751d",
   "metadata": {},
   "source": [
    "Looks good üëç"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb5cadd-bbdc-4a19-931c-522c8902407f",
   "metadata": {},
   "source": [
    "#### Action space\n",
    "\n",
    "- In this environment, the action is the chosen item to recommend.\n",
    "- If you'd like, pause for a moment and think about what the action space should be.\n",
    "  - You may wish to consult the gym library documentation [here](https://www.gymlibrary.ml/content/spaces/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b954af2-20ad-4b11-bdee-f19fca5abd22",
   "metadata": {},
   "source": [
    "#### Action space\n",
    "\n",
    "Unlike the observation space, the action space _is_ discrete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ee176e9-a81d-48a1-9338-bace6fe0dd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = gym.spaces.Discrete(num_candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba417790-94a0-4dab-b3a7-ba3fd98ee567",
   "metadata": {},
   "source": [
    "Here are 15 randomly sampled actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "537f7414-e182-45bb-877f-4812b44b9e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 3, 4, 5, 3, 2, 1, 1, 6, 8, 6, 3, 7, 8, 8]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[action_space.sample() for i in range(15)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539c8ffb-4ebd-4e99-8e0a-08a8c32c9e5f",
   "metadata": {},
   "source": [
    "Looks good üëç"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58280654-1e51-492c-96da-4764c0ceba37",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Constructor\n",
    "\n",
    "Starting with the constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f91a5816-bd80-4863-ba43-7af4cf1ca8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicRecommender(gym.Env):\n",
    "    def __init__(self, env_config=None):\n",
    "        # Set parameters    \n",
    "        self.num_candidates = env_config.get(\"num_candidates\", 10)\n",
    "        self.resample_documents = env_config.get(\"resample_documents\", True)\n",
    "        self.sugar_momentum = env_config.get(\"sugar_momentum\", 0.8) \n",
    "\n",
    "        # Set observation and action spaces\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(self.num_candidates,))\n",
    "        self.action_space = gym.spaces.Discrete(self.num_candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722e84ed-5e80-42d2-a452-9da102588ddc",
   "metadata": {},
   "source": [
    "- The constructor will take in various parameters, and store them.\n",
    "- We won't show all these lines, but they look something like this:\n",
    "\n",
    "```python\n",
    "self.num_candidates = env_config.get(\"num_candidates\", 10)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1f4d98-9a6b-4669-98eb-3e59b6b12354",
   "metadata": {},
   "source": [
    "#### Reset\n",
    "\n",
    "- Next we need our `reset()` method, which needs to:\n",
    "  - Reset the sugar level\n",
    "  - Sample/simulate random document features\n",
    "  - Return the observation\n",
    "  \n",
    "Notes:\n",
    "\n",
    "We'll reset the sugar level to zero, though other choices could be reasonable as well. We'll also have the features be between 0 and 1 for simplicity, and will choose them uniformly at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25a91fe9-1554-41ca-9a34-ec8fffbab641",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicRecommender(gym.Env):\n",
    "\n",
    "    def reset(self):\n",
    "        self.sugar_level = 0.0                \n",
    "        self.documents = np.random.rand(self.num_candidates)\n",
    "        return self.documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf79eb86-84d8-49c6-81e7-b3d4cb310121",
   "metadata": {},
   "source": [
    "#### Step\n",
    "\n",
    "- As usual, the `step()` method is the most complicated.\n",
    "- However, we can make use of our `update_sugar_level` and `reward` functions.\n",
    "- As a reminder, here they are again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9982ede4-b330-41c7-8c18-ab32e0d43c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_sugar_level(self, item_sweetness):\n",
    "    self.sugar_level = self.alpha * self.sugar_level + (1 - self.alpha) * item_sweetness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af8f291f-3510-4567-bb2c-3d18d5c5d52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward(self, item_sweetness):\n",
    "    return item_sweetness * (1 - self.sugar_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd64f496-0c84-4110-ac2f-bf84ba4051f3",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "We have modified update_sugar_level so that alpha is stored in the class. That way you can pass different values of alpha in when you create the env. The sugar_level is also stored in the class, which is accessed in both of these methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd598b75-5ad8-4616-91c4-e916c38036fd",
   "metadata": {},
   "source": [
    "#### Step\n",
    "\n",
    "And now for `step()`, which needs to:\n",
    "\n",
    "- Compute the reward\n",
    "- Update the sugar level\n",
    "- Simulate a set of new candidate items for the next iteration\n",
    "- Return the observation, reward, and done flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bc3b3c3-8ca9-4498-a8be-883cdaa2481d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def step(self, action):\n",
    "\n",
    "        # Get the sweetness of the recommended item\n",
    "        item_sweetness = self.documents[action]\n",
    "\n",
    "        # Compute reward\n",
    "        reward = self.reward(item_sweetness)\n",
    "\n",
    "        # Update sugar level\n",
    "        self.update_sugar_level(item_sweetness)\n",
    "\n",
    "        # Generate new candidates for the next recommendation\n",
    "        if self.resample_documents:\n",
    "            self.documents = np.random.rand(self.num_candidates)\n",
    "\n",
    "        return self.documents, reward, False, {\"sugar_level\" : self.sugar_level}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3f449e-cc13-4275-8d33-260a8614d7ef",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "Some points to note here. For now, we are always returning done=False. In reality, we will set a fixed maximum number of steps before the episode ends, as shown in the previous module. However, this is not shown here for brevity.\n",
    "Note also that the `step` method returns the sugar level. This is not accessed by the agent, but rather is just for our convenience as we debug/introspect the RL system. In the past, we had just ignored this optional info returned by `step`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e248fdac-86c4-49df-b55e-4502761f77a3",
   "metadata": {},
   "source": [
    "#### Let's test it out!\n",
    "\n",
    "Let's test out our new environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "beef24dd-4068-4872-a414-5d1c718c8e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "from envs import BasicRecommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70484c30-0ff8-4c21-aaf7-d6e7a364f542",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BasicRecommender()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5745a4f-7796-4ed5-9133-b1ec870087a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "env.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ed63dba-2640-4eda-9bae-0a3647b7b989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.37454012, 0.95071431, 0.73199394, 0.59865848, 0.15601864,\n",
       "       0.15599452, 0.05808361, 0.86617615, 0.60111501, 0.70807258])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b639e0d-1db7-45aa-a19e-7fd80e4eb8b4",
   "metadata": {},
   "source": [
    "- This looks good, since the default `num_candidates` is 10.\n",
    "- Each of the numbers above represents the sweetness of a simulated item candidate.\n",
    "- Let's recommend the first item, with sweetness 0.37."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d578c363-e5cb-4ce9-b4b8-031df870ff56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.02058449, 0.96990985, 0.83244264, 0.21233911, 0.18182497,\n",
       "        0.18340451, 0.30424224, 0.52475643, 0.43194502, 0.29122914]),\n",
       " 0.3745401188473625,\n",
       " False,\n",
       " {'sugar_level': 0.07490802376947248})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272f90f0-bee8-417b-a6a1-0a334351ed97",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "We can observe that the sugar level went up from 0 to 1-alpha, or 0.2, times the item sweetness of 0.37. As a reminder, the sugar level info is not available to the agent, it's just for us. Note also that, after our action, a new set of candidate documents are given as the observation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a44686e-5215-4e24-b564-0275df9a3501",
   "metadata": {},
   "source": [
    "#### Baseline\n",
    "\n",
    "Before training our agent, let's first establish two baselines:\n",
    "\n",
    "- Greedy baseline: always pick the sweetest item\n",
    "- Random baseline: pick randomly every time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4322dc8e-9605-40b5-8576-37374d85c79d",
   "metadata": {},
   "source": [
    "#### Greedy baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1925170f-4870-43c6-b031-3f40d8defa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BasicRecommender()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6b88f41-c1e0-43a9-abee-3c7044291398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "env.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ded0541-ef95-4f9f-9965-aaaf20268d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38bc595c-6e7b-4b8a-88d1-87a18bc8714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reward = 0\n",
    "for i in range(100):\n",
    "    sweetest_item = np.argmax(obs)\n",
    "    obs, reward, done, info = env.step(sweetest_item)\n",
    "    total_reward += reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c137ca74-5045-4153-b8fc-235007d9ce31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.523302493047304"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fe5487-e939-48a4-b480-efe780abd70d",
   "metadata": {},
   "source": [
    "- We can get a reward of 7.7 by always picking the sweetest items.\n",
    "- To make this more exciting, let's assume this is $7.7M for our business."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c79af7-c0f2-4787-bbde-ba745e45c8b0",
   "metadata": {},
   "source": [
    "#### Random baseline\n",
    "\n",
    "- Now let's try a random approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39ed7bff-551d-4939-9fcf-8c43886b5b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BasicRecommender()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5486826e-4cec-4b09-84d7-c4ccf6eccd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "85e1be55-f27f-48c9-8285-e649ed159036",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reward = 0\n",
    "for i in range(100):\n",
    "    random_item = np.random.randint(len(obs))\n",
    "    obs, reward, done, info = env.step(random_item)\n",
    "    total_reward += reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5324e3a1-da59-4c2c-9059-4699457d032e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.505863187409155"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709316fe-dc62-4ea1-ae32-0b84f123cf62",
   "metadata": {},
   "source": [
    "- With random actions, we do better: $26M revenues.\n",
    "- Time to pay me a bonus! üí∞\n",
    "\n",
    "Notes:\n",
    "\n",
    "If you rerun the code above you will get a slightly different results each time, but the results do not vary wildly. The total reward after 100 steps is usually around 26."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af6cdbd-0c1a-4ff8-ac50-4acbf0321136",
   "metadata": {},
   "source": [
    "#### Solving with RLlib\n",
    "\n",
    "Let's see if we can do better than 26 with RLlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01c744b-8afa-453c-ab37-b72daa9ec31e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b52e45a-c183-4483-a44b-b98a4ccf35bd",
   "metadata": {},
   "source": [
    "## Greedy vs random\n",
    "<!-- multiple choice -->\n",
    "\n",
    "Above, we saw our random baseline outperform our greedy baseline. Do you think this would always be the case?\n",
    "\n",
    "- [ ] The random strategy always outperforms the greedy strategy. | What if the episode only had 1 step?\n",
    "- [ ] The greedy strategy always outperforms the random strategy. | We saw the random strategy do better, though.\n",
    "- [x] The random strategy performs better vs. greedy as the episode duration increases. | The greedy strategy works in the short-term, but not the long term.\n",
    "- [ ] The greedy strategy performs better vs. random as the episode duration increases. | I think you have this backwards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74764335-63a9-4f86-9671-4d12a994362a",
   "metadata": {},
   "source": [
    "#### Outline\n",
    "\n",
    "- observation space, action space\n",
    "- variable candidates\n",
    "- turn on resampling (or just have this always on by defualt)\n",
    "- increase slate size\n",
    "- long term effect\n",
    "- etc\n",
    "- TODO IF LOW ON TIME: not great, but if needed,could even do the recommender as module 2, where we provide it. and then module 4 we start building it, and also experimenting with observation and reward shaping\n",
    "- REWARD SHAPING 2 types:\n",
    "  - help it learn\n",
    "  - actually change what you care about\n",
    "- examples here: ethical recommendations\n",
    "- some sort of total reward for the whole episode, not interim rewards\n",
    "- other stuff\n",
    "- or right and the offline RL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aaae24-ab58-4fc9-a4ad-1f23ff4db7a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402dab96-4af1-415c-a844-5102de9f5246",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e923ff5c-a819-4e35-8e60-ba9938344369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd05524e-0af9-4c36-b335-636df53f5945",
   "metadata": {},
   "source": [
    "#### Let's apply what we learned!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rl-course-dev-2]",
   "language": "python",
   "name": "conda-env-rl-course-dev-2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
