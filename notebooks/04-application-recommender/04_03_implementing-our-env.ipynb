{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5aa21ad-6bb9-4990-ab4d-fb9435b0bc00",
   "metadata": {},
   "source": [
    "## Recommender env: implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdd0ce7d-4a5b-4d03-8d93-b176de9e4db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38ea949-1d66-4b7b-a25d-a1ef65ca63a7",
   "metadata": {},
   "source": [
    "#### Implementation\n",
    "\n",
    "Now we're ready to implement our environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0293f36-0ca4-40d4-8c4f-456c07627269",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Review: what does an env need?\n",
    "\n",
    "Requirements for an RL environment:\n",
    "\n",
    "- Observation space\n",
    "- Action space\n",
    "- Constructor\n",
    "- `reset()` method\n",
    "- `step()` method\n",
    "\n",
    "We'll now create all of these for our recommender environment. Let's go!\n",
    "\n",
    "Notes:\n",
    "\n",
    "In the previous module, we listed the following requirements for an RL environment:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c68cf7-4dc6-4ab4-86c5-93b10fced0ae",
   "metadata": {},
   "source": [
    "#### Observation space\n",
    "\n",
    "- The first step is to set the observation space. \n",
    "- Our observations are the _features of candidate items_.\n",
    "- For simplicity, we'll assume only 1 feature.\n",
    "- Here, our observations are continuous-valued, not discrete like Frozen Lake.\n",
    "- If you'd like, pause for a moment and think about what the observation space should be.\n",
    "  - You may wish to consult the gym library documentation [here](https://www.gymlibrary.ml/content/spaces/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64a506ca-cb76-4fba-a53d-65de3e467e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# we could have a multiple choice in here\n",
    "# I guess it's potentially fine to have tons of short notebooks in this module???\n",
    "# might be a bit of a hassle for video recording?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fdd370-3e7b-45f9-92e0-95bf26ca09ec",
   "metadata": {},
   "source": [
    "#### Observation space\n",
    "\n",
    "- We want the `Box` space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d66f7f10-c5e5-4e7b-ad4f-ad90934ba1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "num_candidates=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28bfa81a-a401-4bf1-a400-d97b305c7fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space = gym.spaces.Box(low=0, high=1, shape=(num_candidates,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ad3b01-c30a-4b2b-bdeb-aa9a923f7405",
   "metadata": {},
   "source": [
    "- Features are between 0 and 1, number of features is number of item candidates. \n",
    "- Here is an sample from the observation space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d360bb64-1776-452a-976a-5c2ee3718169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.753929  , 0.19141148, 0.57253695, 0.93540746, 0.31168196,\n",
       "       0.60167515, 0.46667033, 0.2824403 , 0.09364203, 0.14965677],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931c9abd-3444-4f33-a31e-4cba72b1751d",
   "metadata": {},
   "source": [
    "Looks good üëç"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb5cadd-bbdc-4a19-931c-522c8902407f",
   "metadata": {},
   "source": [
    "#### Action space\n",
    "\n",
    "- In this environment, the action is the chosen item to recommend.\n",
    "- If you'd like, pause for a moment and think about what the action space should be.\n",
    "  - You may wish to consult the gym library documentation [here](https://www.gymlibrary.ml/content/spaces/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b954af2-20ad-4b11-bdee-f19fca5abd22",
   "metadata": {},
   "source": [
    "#### Action space\n",
    "\n",
    "Unlike the observation space, the action space _is_ discrete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ee176e9-a81d-48a1-9338-bace6fe0dd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = gym.spaces.Discrete(num_candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba417790-94a0-4dab-b3a7-ba3fd98ee567",
   "metadata": {},
   "source": [
    "Here are 15 randomly sampled actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "537f7414-e182-45bb-877f-4812b44b9e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 3, 4, 5, 3, 2, 1, 1, 6, 8, 6, 3, 7, 8, 8]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[action_space.sample() for i in range(15)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539c8ffb-4ebd-4e99-8e0a-08a8c32c9e5f",
   "metadata": {},
   "source": [
    "Looks good üëç"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58280654-1e51-492c-96da-4764c0ceba37",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Constructor\n",
    "\n",
    "Starting with the constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8745e885-fe88-436e-8238-50da5a8a2486",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f91a5816-bd80-4863-ba43-7af4cf1ca8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicRecommender(gym.Env):\n",
    "    def __init__(self, env_config=None):\n",
    "        # Set parameters    \n",
    "        self.num_candidates = env_config.get(\"num_candidates\", 10)\n",
    "        self.resample_documents = env_config.get(\"resample_documents\", True)\n",
    "        self.alpha = env_config.get(\"alpha\", 0.5) \n",
    "\n",
    "        # Set observation and action spaces\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(self.num_candidates,))\n",
    "        self.action_space = gym.spaces.Discrete(self.num_candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722e84ed-5e80-42d2-a452-9da102588ddc",
   "metadata": {},
   "source": [
    "- The constructor will take in various parameters, and store them.\n",
    "- We won't show all these lines, but they look something like this:\n",
    "\n",
    "```python\n",
    "self.num_candidates = env_config.get(\"num_candidates\", 10)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1f4d98-9a6b-4669-98eb-3e59b6b12354",
   "metadata": {},
   "source": [
    "#### Reset\n",
    "\n",
    "- Next we need our `reset()` method, which needs to:\n",
    "  - Reset the sugar level\n",
    "  - Sample/simulate random document features\n",
    "  - Return the observation\n",
    "  \n",
    "Notes:\n",
    "\n",
    "We'll reset the sugar level to zero, though other choices could be reasonable as well. We'll also have the features be between 0 and 1 for simplicity, and will choose them uniformly at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25a91fe9-1554-41ca-9a34-ec8fffbab641",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicRecommender(gym.Env):\n",
    "\n",
    "    def reset(self):\n",
    "        self.sugar_level = 0.0                \n",
    "        self.documents = np.random.rand(self.num_candidates)\n",
    "        return self.documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf79eb86-84d8-49c6-81e7-b3d4cb310121",
   "metadata": {},
   "source": [
    "#### Step\n",
    "\n",
    "- As usual, the `step()` method is the most complicated.\n",
    "- However, we can make use of our `update_sugar_level` and `reward` functions.\n",
    "- As a reminder, here they are again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9982ede4-b330-41c7-8c18-ab32e0d43c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_sugar_level(self, item_sweetness):\n",
    "    self.sugar_level = self.alpha * self.sugar_level + (1 - self.alpha) * item_sweetness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af8f291f-3510-4567-bb2c-3d18d5c5d52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward(self, item_sweetness):\n",
    "    return item_sweetness * (1 - self.sugar_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd64f496-0c84-4110-ac2f-bf84ba4051f3",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "We have modified update_sugar_level so that alpha is stored in the class. That way you can pass different values of alpha in when you create the env. The sugar_level is also stored in the class, which is accessed in both of these methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd598b75-5ad8-4616-91c4-e916c38036fd",
   "metadata": {},
   "source": [
    "#### Step\n",
    "\n",
    "And now for `step()`, which needs to:\n",
    "\n",
    "- Compute the reward\n",
    "- Update the sugar level\n",
    "- Simulate a set of new candidate items for the next iteration\n",
    "- Return the observation, reward, and done flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bc3b3c3-8ca9-4498-a8be-883cdaa2481d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def step(self, action):\n",
    "\n",
    "        # Get the sweetness of the recommended item\n",
    "        item_sweetness = self.documents[action]\n",
    "\n",
    "        # Compute reward\n",
    "        reward = self.reward(item_sweetness)\n",
    "\n",
    "        # Update sugar level\n",
    "        self.update_sugar_level(item_sweetness)\n",
    "\n",
    "        # Generate new candidates for the next recommendation\n",
    "        if self.resample_documents:\n",
    "            self.documents = np.random.rand(self.num_candidates)\n",
    "\n",
    "        return self.documents, reward, False, {\"sugar_level\" : self.sugar_level}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3f449e-cc13-4275-8d33-260a8614d7ef",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "Some points to note here. For now, we are always returning done=False. In reality, we will set a fixed maximum number of steps before the episode ends, as shown in the previous module. However, this is not shown here for brevity.\n",
    "Note also that the `step` method returns the sugar level. This is not accessed by the agent, but rather is just for our convenience as we debug/introspect the RL system. In the past, we had just ignored this optional info returned by `step`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e248fdac-86c4-49df-b55e-4502761f77a3",
   "metadata": {},
   "source": [
    "#### Let's test it out!\n",
    "\n",
    "Let's test out our new environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "beef24dd-4068-4872-a414-5d1c718c8e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "from envs import BasicRecommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70484c30-0ff8-4c21-aaf7-d6e7a364f542",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BasicRecommender()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5745a4f-7796-4ed5-9133-b1ec870087a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "env.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ed63dba-2640-4eda-9bae-0a3647b7b989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.37454012, 0.95071431, 0.73199394, 0.59865848, 0.15601864,\n",
       "       0.15599452, 0.05808361, 0.86617615, 0.60111501, 0.70807258])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b639e0d-1db7-45aa-a19e-7fd80e4eb8b4",
   "metadata": {},
   "source": [
    "- This looks good, since the default `num_candidates` is 10.\n",
    "- Each of the numbers above represents the sweetness of a simulated item candidate.\n",
    "- Let's recommend the first item, with sweetness 0.37."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d578c363-e5cb-4ce9-b4b8-031df870ff56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.02058449, 0.96990985, 0.83244264, 0.21233911, 0.18182497,\n",
       "        0.18340451, 0.30424224, 0.52475643, 0.43194502, 0.29122914]),\n",
       " 0.3745401188473625,\n",
       " False,\n",
       " {'sugar_level': 0.18727005942368125})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272f90f0-bee8-417b-a6a1-0a334351ed97",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "We can observe that the sugar level went up from 0 to 1-alpha, or 0.5, times the item sweetness of 0.37. As a reminder, the sugar level info is not available to the agent, it's just for us. Note also that, after our action, a new set of candidate documents are given as the observation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a44686e-5215-4e24-b564-0275df9a3501",
   "metadata": {},
   "source": [
    "#### Baseline\n",
    "\n",
    "Before training our agent, let's first establish two baselines:\n",
    "\n",
    "- Greedy baseline: always pick the sweetest item\n",
    "- Random baseline: pick randomly every time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4322dc8e-9605-40b5-8576-37374d85c79d",
   "metadata": {},
   "source": [
    "#### Greedy baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1925170f-4870-43c6-b031-3f40d8defa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BasicRecommender()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f6b88f41-c1e0-43a9-abee-3c7044291398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "env.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9ded0541-ef95-4f9f-9965-aaaf20268d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "38bc595c-6e7b-4b8a-88d1-87a18bc8714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reward = 0\n",
    "done = False\n",
    "while not done:\n",
    "    sweetest_item = np.argmax(obs)\n",
    "    obs, reward, done, info = env.step(sweetest_item)\n",
    "    total_reward += reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c137ca74-5045-4153-b8fc-235007d9ce31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.130508837384937"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fe5487-e939-48a4-b480-efe780abd70d",
   "metadata": {},
   "source": [
    "- We can get a reward of 10.1 by always picking the sweetest items.\n",
    "- To make this more exciting, assume this is \\\\$10.1 for our business, and we have 1M users, so \\\\$10.1M.\n",
    "\n",
    "Notes:\n",
    "\n",
    "This is the total for 100 time steps, or around $0.1 per time step. If we were to have longer episodes, this number would be larger."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c79af7-c0f2-4787-bbde-ba745e45c8b0",
   "metadata": {},
   "source": [
    "#### Random baseline\n",
    "\n",
    "Now let's try a random approach:\n",
    "\n",
    "Notes: \n",
    "\n",
    "Here, the agent recommends a random item from the 10 candidates each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "39ed7bff-551d-4939-9fcf-8c43886b5b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BasicRecommender()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "85e1be55-f27f-48c9-8285-e649ed159036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_agent_episode(env):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        random_item = np.random.randint(len(obs))\n",
    "        obs, reward, done, info = env.step(random_item)\n",
    "        total_reward += reward\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9a555ff5-ade4-4aef-9d43-d5124673df56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.811391451616075"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_agent_episode(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7059d9-fe85-4aec-901a-b9b485f90d55",
   "metadata": {},
   "source": [
    "Because of the randomness, we should repeat this for many episodes and take the average:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5324e3a1-da59-4c2c-9059-4699457d032e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.476488843988097"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([random_agent_episode(env) for episode in range(1000)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709316fe-dc62-4ea1-ae32-0b84f123cf62",
   "metadata": {},
   "source": [
    "- With random actions, we do better: around $25.5M revenues.\n",
    "- Time to pay me a bonus! üí∞"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af6cdbd-0c1a-4ff8-ac50-4acbf0321136",
   "metadata": {},
   "source": [
    "#### Solving with RLlib\n",
    "\n",
    "- Let's see if we can do better than 25.5 with RLlib.\n",
    "- We'll set up a config file for our RLlib trainer as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f01c744b-8afa-453c-ab37-b72daa9ec31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config = {\n",
    "    \"framework\"             : \"torch\",\n",
    "    \"create_env_on_driver\"  : True,\n",
    "    \"seed\"                  : 0,\n",
    "    \"lr\"                    : 0.001\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51756b13-6dca-406d-acf9-cf51692ac25b",
   "metadata": {},
   "source": [
    "- The `\"lr\"` parameter sets the learning rate. \n",
    "  - I found 0.001 to work a bit better than the default of 0.0001."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d2d584-c377-478e-b994-cef2f4adf5df",
   "metadata": {},
   "source": [
    "#### Solving with RLlib\n",
    "\n",
    "Next, we set up our trainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96ba76e4-6c70-408d-b0e3-2cfb033c83b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.agents.ppo import PPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "631e4fb8-8f1c-4546-a01f-5a35bacb56f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = PPOTrainer(trainer_config, env=BasicRecommender)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc69b79c-57dc-4eaf-980c-ddff35b73eab",
   "metadata": {},
   "source": [
    "And then we train for 30 iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a184ca95-a583-47e1-a14a-4aac8dcbf3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "for i in range(30):\n",
    "    result = trainer.train()\n",
    "    rewards.append(result[\"episode_reward_mean\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697efa46-2b27-410d-9f1e-2109848ff8a0",
   "metadata": {},
   "source": [
    "Finally, we print out the average episode reward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa483620-fe82-4317-ae52-fcedd89326fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.601294361531853"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()[\"evaluation\"][\"episode_reward_mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e484fd49-46d6-416c-9948-751a29586cb1",
   "metadata": {},
   "source": [
    "It looks like we maybe increased our reward very slightly?\n",
    "\n",
    "Notes:\n",
    "\n",
    "The results look underwhelming here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192d84b0-e354-4445-ad8d-91a695087c80",
   "metadata": {},
   "source": [
    "#### Is the agent learning?\n",
    "\n",
    "We can try plotting the reward across training iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9eb22af0-b123-484b-b773-ecb959478e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "84e0c396-76d5-49ca-a392-de47fe1c019b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [49]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl-course-dev-2/lib/python3.8/site-packages/matplotlib/pyplot.py:2769\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2767\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[1;32m   2768\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\u001b[38;5;241m*\u001b[39margs, scalex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, scaley\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 2769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2770\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscalex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaley\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2771\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl-course-dev-2/lib/python3.8/site-packages/matplotlib/axes/_axes.py:1632\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1391\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1392\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1629\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1630\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1631\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[0;32m-> 1632\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[1;32m   1633\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m   1634\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m~/miniconda3/envs/rl-course-dev-2/lib/python3.8/site-packages/matplotlib/axes/_base.py:312\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    311\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 312\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl-course-dev-2/lib/python3.8/site-packages/matplotlib/axes/_base.py:495\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39mxaxis\u001b[38;5;241m.\u001b[39mupdate_units(x)\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39myaxis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 495\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43myaxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_units\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m    498\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    499\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/rl-course-dev-2/lib/python3.8/site-packages/matplotlib/axis.py:1449\u001b[0m, in \u001b[0;36mAxis.update_units\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1447\u001b[0m neednew \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconverter \u001b[38;5;241m!=\u001b[39m converter\n\u001b[1;32m   1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconverter \u001b[38;5;241m=\u001b[39m converter\n\u001b[0;32m-> 1449\u001b[0m default \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_units\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m default \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munits \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1451\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_units(default)\n",
      "File \u001b[0;32m~/miniconda3/envs/rl-course-dev-2/lib/python3.8/site-packages/matplotlib/category.py:116\u001b[0m, in \u001b[0;36mStrCategoryConverter.default_units\u001b[0;34m(data, axis)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# the conversion call stack is default_units -> axis_info -> convert\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis\u001b[38;5;241m.\u001b[39munits \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 116\u001b[0m     axis\u001b[38;5;241m.\u001b[39mset_units(\u001b[43mUnitData\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     axis\u001b[38;5;241m.\u001b[39munits\u001b[38;5;241m.\u001b[39mupdate(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/rl-course-dev-2/lib/python3.8/site-packages/matplotlib/category.py:192\u001b[0m, in \u001b[0;36mUnitData.__init__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_counter \u001b[38;5;241m=\u001b[39m itertools\u001b[38;5;241m.\u001b[39mcount()\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 192\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl-course-dev-2/lib/python3.8/site-packages/matplotlib/category.py:225\u001b[0m, in \u001b[0;36mUnitData.update\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;66;03m# check if convertible to number:\u001b[39;00m\n\u001b[1;32m    224\u001b[0m convertible \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 225\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m \u001b[43mOrderedDict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;66;03m# OrderedDict just iterates over unique values in data.\u001b[39;00m\n\u001b[1;32m    227\u001b[0m     _api\u001b[38;5;241m.\u001b[39mcheck_isinstance((\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m), value\u001b[38;5;241m=\u001b[39mval)\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convertible:\n\u001b[1;32m    229\u001b[0m         \u001b[38;5;66;03m# this will only be called so long as convertible is True.\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'dict'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff4a6fb-0806-4f0b-8c4f-d1fe6a216d87",
   "metadata": {},
   "source": [
    "It looks like something is going wrong here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6fcbf8c4-b85e-4c3f-ad80-2a3a32b3506d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import query_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4283fcb3-b858-4461-960d-ef353d4727fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.7268853e-01, 1.7551240e-03, 8.4948372e-03, 1.5276692e-02,\n",
       "       8.7164127e-04, 3.0396803e-04, 2.8898768e-02, 6.9322920e-04,\n",
       "       7.0149995e-02, 8.6706434e-04], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_policy(trainer, env, np.array([0,0,0,0,0,0,0,0,0,1]), actions=[0,1,2,3,4,5,6,7,8,9])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfa27a0-6679-4d3b-bb6c-9e760951399c",
   "metadata": {},
   "source": [
    "#### What is this agent doing?\n",
    "\n",
    "- show the action probabilities?\n",
    "- show that a deterministic agent will do terribly (e.g. DQN)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7748b30-2e97-4d81-84ce-57c3b58e9a06",
   "metadata": {},
   "source": [
    "#### New observations\n",
    "\n",
    "or make this a coding exercise???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b52e45a-c183-4483-a44b-b98a4ccf35bd",
   "metadata": {},
   "source": [
    "## Greedy vs random\n",
    "<!-- multiple choice -->\n",
    "\n",
    "Above, we saw our random baseline outperform our greedy baseline. Do you think this would always be the case?\n",
    "\n",
    "- [ ] The random strategy always outperforms the greedy strategy. | What if the episode only had 1 step?\n",
    "- [ ] The greedy strategy always outperforms the random strategy. | We saw the random strategy do better, though.\n",
    "- [x] The random strategy performs better vs. greedy as the episode duration increases. | The greedy strategy works in the short-term, but not the long term.\n",
    "- [ ] The greedy strategy performs better vs. random as the episode duration increases. | I think you have this backwards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74764335-63a9-4f86-9671-4d12a994362a",
   "metadata": {},
   "source": [
    "#### Outline\n",
    "\n",
    "- observation space, action space\n",
    "- variable candidates\n",
    "- turn on resampling (or just have this always on by defualt)\n",
    "- increase slate size\n",
    "- long term effect\n",
    "- etc\n",
    "- TODO IF LOW ON TIME: not great, but if needed,could even do the recommender as module 2, where we provide it. and then module 4 we start building it, and also experimenting with observation and reward shaping\n",
    "- REWARD SHAPING 2 types:\n",
    "  - help it learn\n",
    "  - actually change what you care about\n",
    "- examples here: ethical recommendations\n",
    "- some sort of total reward for the whole episode, not interim rewards\n",
    "- other stuff\n",
    "- or right and the offline RL\n",
    "- possible modification: did they click? if not, sugar level is not updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aaae24-ab58-4fc9-a4ad-1f23ff4db7a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402dab96-4af1-415c-a844-5102de9f5246",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e923ff5c-a819-4e35-8e60-ba9938344369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd05524e-0af9-4c36-b335-636df53f5945",
   "metadata": {},
   "source": [
    "#### Let's apply what we learned!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rl-course-dev-2]",
   "language": "python",
   "name": "conda-env-rl-course-dev-2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
