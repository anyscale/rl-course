{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5aa21ad-6bb9-4990-ab4d-fb9435b0bc00",
   "metadata": {},
   "source": [
    "## Recommender env: design\n",
    "<!-- video shot=\"/eOFzSon7dK0\" start=\"03:42\" end=\"14:03\" -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdd0ce7d-4a5b-4d03-8d93-b176de9e4db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd89af26-eb12-45d8-95b1-14fdd7027deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: adapt images from Sven for this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0293f36-0ca4-40d4-8c4f-456c07627269",
   "metadata": {},
   "source": [
    "#### Simulating user behavior\n",
    "\n",
    "- Simulate user behavior when **repeatedly** responding to item recommendations.\n",
    "- The key behavior to simulate:\n",
    "\n",
    "> Recommending \"junk food\" items is good in the short term, but bad in the long term.\n",
    "\n",
    "- This is completely our choice as the designer of the environment. \n",
    "- You may want to simulate/capture a different type of user behavior.\n",
    "  - Or, learn from user behavior data -- more on this later!\n",
    "- But this will be our running example for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbb31ad-3891-4d9a-91be-09854ff73c31",
   "metadata": {},
   "source": [
    "#### Candy\n",
    "\n",
    "- We will model every item as having a \"sweetness\" level. \n",
    "- We'll refer to high-sweetness items as \"candy\".\n",
    "\n",
    "![](img/candy.jpg)\n",
    "\n",
    "- This could be short, silly videos, or cheap trinkets on sale, etc.\n",
    "- Users love candy in the short term, but too much candy leads to dissatisfaciton in the long term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7aee0e-e60a-419b-bdd0-b0680b5d2f0c",
   "metadata": {},
   "source": [
    "#### Veggies\n",
    "\n",
    "- On the other hand, we'll refer to low-sweetness items as \"veggies\".\n",
    "\n",
    "![](img/veggies.jpg)\n",
    "\n",
    "- These could be educational documentaries, or boring-but-useful items, etc.\n",
    "- Users don't enjoy veggies much in the short term, but they boost satisfaction in the long term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58280654-1e51-492c-96da-4764c0ceba37",
   "metadata": {},
   "source": [
    "#### Sugar level\n",
    "\n",
    "- Each **item** has a sweetness level, that is a fixed property of the item.\n",
    "- We'll model our **users** as having a variable **sugar level** that measures how much candy they've eaten recently.\n",
    "- The user's sugar level (or the notion of a sugar level) _will not be known to the agent_!\n",
    "- But for now, we're designing the simulator, so we are all-knowing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ddc3a6-98b4-4a61-a229-4b0ae27c62fd",
   "metadata": {},
   "source": [
    "#### Sugar level dynamics\n",
    "\n",
    "- We need to decide how the sugar level changes with item consumption.\n",
    "- A simple approach is:\n",
    "\n",
    "> When an item is consumed, the sugar level moves towards the sweetness of that item.\n",
    "\n",
    "Examples:\n",
    "\n",
    "- If your sugar level is 0.2 and you consume an item with sweetness 0.5, your sugar level goes up ⬆️\n",
    "- If your sugar level is 0.2 and you consume an item with sweetness 0.1, your sugar level goes down ⬇️"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5fb6ed-8479-40ae-a489-b279f03a61ac",
   "metadata": {},
   "source": [
    "#### Sugar level dynamics\n",
    "\n",
    "- How do we represent this mathematically?\n",
    "- We can try this:\n",
    "\n",
    "> new sugar level = ⍺ (old sugar level) + (1 - ⍺) (item sweetness)\n",
    "\n",
    "- Here, ⍺ is a number between 0 and 1 that controls how \"stubborn\" the sugar level is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce8adf56-8b56-4dc7-bb3d-3a2cebfe6f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN \n",
    "# note the slide above and below are partially the same - just want to hide the bottom half at first"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b8ebcf-d522-42e6-8777-a32b516259e4",
   "metadata": {},
   "source": [
    "#### Sugar level dynamics\n",
    "\n",
    "- How do we represent this mathematically?\n",
    "- We can try this:\n",
    "\n",
    "> new sugar level = ⍺ (old sugar level) + (1 - ⍺) (item sweetness)\n",
    "\n",
    "- Here, ⍺ is a number between 0 and 1 that controls how \"stubborn\" the sugar level is.\n",
    "- For example, if ⍺=1 then the above equation becomes\n",
    "\n",
    "> new sugar level = old sugar level\n",
    "\n",
    "and the sugar level never changes. If ⍺=0 then we have\n",
    "\n",
    "> new sugar level = item sweetness\n",
    "\n",
    "meaning a single item can complete change the user's sugar level.\n",
    "\n",
    "- For ⍺ between 0 and 1, we have a combination of the old sugar level and the item sweetness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fca4d06-9168-41b9-8ab2-fc1fd34ce0d1",
   "metadata": {},
   "source": [
    "#### Sugar level dynamics\n",
    "\n",
    "We can implement the above using this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7821a2f-b42f-4c64-a3d7-a02c7658120f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_sugar_level(sugar_level, item_sweetness, alpha=0.9):\n",
    "    return alpha * sugar_level + (1 - alpha) * item_sweetness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796a261f-2cf0-48f4-ab95-c8ff4c7800be",
   "metadata": {},
   "source": [
    "Let's test it out to make sure the behavior makes sense (using the default value of ⍺=0.9):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b934fd76-bda0-4e6d-ad2e-17de12d3b4e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sugar_level = 0.2\n",
    "sugar_level = update_sugar_level(sugar_level, 0.8)\n",
    "sugar_level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eda542e-c236-4906-bf8f-fa5f7ab5d238",
   "metadata": {},
   "source": [
    "The item was sweet (0.8), so the sugar level went up quite a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e9e6f2a-ee8a-4458-82eb-afb57e858676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.264"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sugar_level = update_sugar_level(sugar_level, 0.3)\n",
    "sugar_level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d9bda3-2c8f-44f8-8cf0-4e3a31cfb404",
   "metadata": {},
   "source": [
    "The item sweetness was slightly above the sugar level, so the sugar level went up slightly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27253dfb-a220-4d0f-9816-1fb542f1045b",
   "metadata": {},
   "source": [
    "#### Sugar level dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8c863a1-c37e-4c29-8136-0529501118b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2386"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sugar_level = update_sugar_level(sugar_level, 0.01)\n",
    "sugar_level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eeb5116-a9ab-49d9-8748-d751662a994b",
   "metadata": {},
   "source": [
    "The item was un-sweet, so the sugar level went down."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c20efd-9ca1-43be-8529-aacb7244ad4c",
   "metadata": {},
   "source": [
    "#### Effect of alpha\n",
    "\n",
    "We can see that, with a smaller alpha, the sugar level changes much faster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b684f74b-3519-4300-b4f2-5ff68c0199e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1193"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sugar_level = update_sugar_level(sugar_level, 0.0, alpha=0.5)\n",
    "sugar_level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5d8350-5daa-4377-9ec4-79819528d0bb",
   "metadata": {},
   "source": [
    "#### Reward\n",
    "\n",
    "- Ok great, we have the sugar level dynamics all sorted out!\n",
    "- The second major piece of the puzzle is the reward.\n",
    "- What we want:\n",
    "\n",
    "1. Higher item sweetness leads to higher reward (yum, candy!)\n",
    "2. Higher sugar level leads to lower reward (ahh, too much candy!)\n",
    "\n",
    "A simple way to combine these effects is to multiply them together:\n",
    "\n",
    "> reward = item sweetness * (1 - sugar level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f6d77a-2458-4a51-91c4-070d074c1978",
   "metadata": {},
   "source": [
    "#### Reward implementation\n",
    "\n",
    "> reward = item sweetness * (1 - sugar level)\n",
    "\n",
    "We can code this as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "258777c5-b896-4ec4-b999-38c4aadafdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward(sugar_level, item_sweetness):\n",
    "    return item_sweetness * (1 - sugar_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a1eeea-f027-419c-8f80-756eec6be9ea",
   "metadata": {},
   "source": [
    "We will be using this pieces in the next section when we implement our environment!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c68cf7-4dc6-4ab4-86c5-93b10fced0ae",
   "metadata": {},
   "source": [
    "#### Observation space\n",
    "\n",
    "- Next, we will need to set up the observations. \n",
    "- Our observations will be the _features of candidate items_.\n",
    "- For simplicity, we'll assume only 1 feature, the item sweetness.\n",
    "- So, the agent will see a bunch of sweetness levels, and choose one of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb5cadd-bbdc-4a19-931c-522c8902407f",
   "metadata": {},
   "source": [
    "#### Action space\n",
    "\n",
    "- In this environment, the action is the chosen item to recommend, given the canididates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7ccbd3-57d6-4914-94a0-7fb03fa0b96e",
   "metadata": {},
   "source": [
    "#### Let's apply what we learned!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bebfec4-9043-498b-b3da-bc91700a74bf",
   "metadata": {},
   "source": [
    "## Big-picture\n",
    "<!-- multiple choice -->\n",
    "\n",
    "Which of the following is **NOT** true about the simulated recommender RL environment we are creating?\n",
    "\n",
    "- [ ] The environment contains a vastly oversimplified model of user behavior, but a trained agent still may be useful in making recommendations.\n",
    "- [x] The environment accurately represents how real users behave.\n",
    "- [ ] The environment is a good starting point, and we may wish to add complexity as our work progresses.\n",
    "- [ ] The environment captures the notion that users will respond differently to different items, and this response may depend on their history."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe02b60a-f753-4299-9953-2a6109b77f8a",
   "metadata": {},
   "source": [
    "## Recommender rewards\n",
    "<!-- multiple choice -->\n",
    "\n",
    "Recall that our reward function is\n",
    "\n",
    "> reward = item sweetness * (1 - sugar level)\n",
    "\n",
    "#### Short-term satisfaction\n",
    "\n",
    "True or False: at any given moment, the _immediate_ reward is _always_ larger for candy than for veggies.\n",
    "\n",
    "- [x] True | The immediate reward is directly proportional to item sweetness.\n",
    "- [ ] False | Take a closer look at the formula above!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e512009-9deb-4c39-a055-3e82a7acdf14",
   "metadata": {},
   "source": [
    "#### Long-term satisfaction\n",
    "\n",
    "True or False: at any given moment, the _long-term total_ reward is _always_ larger for recommending veggies than for candy.\n",
    "\n",
    "- [ ] True | It's complicated to determine what will be best in the long term - this is what our agent has to learn!\n",
    "- [x] False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2622b06b-90fc-4fc7-9448-a782be431d63",
   "metadata": {},
   "source": [
    "## Sugar crash\n",
    "<!-- coding exercise -->\n",
    "\n",
    "Let's assume your sugar level starts at 0.5, and at each step you only have two items to choose from, mega-veggie (sweetness = 0) and mega-candy (sweetness = 1). You will be making 3 recommendations in a row, using alpha = 0.7. Use the coding window below to play around with different options, and find the best sequence of recommendations in terms of _total_ reward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8695dc5-2538-4158-95fa-50a9135b42f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Received reward 0.00000, new sugar level 0.35000\n",
      "  Received reward 0.00000, new sugar level 0.24500\n",
      "  Received reward 0.00000, new sugar level 0.17150\n",
      "Total reward after 5 recommendations: 0.0\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE\n",
    "\n",
    "def update_sugar_level(sugar_level, item_sweetness, alpha=0.9):\n",
    "    return alpha * sugar_level + (1 - alpha) * item_sweetness\n",
    "\n",
    "def reward(sugar_level, item_sweetness):\n",
    "    return item_sweetness * (1 - sugar_level)\n",
    "\n",
    "# MODIFY THIS LIST\n",
    "# But make sure it always contains 3 items, each 0 or 1\n",
    "recommendations = [0, 0, 0]\n",
    "\n",
    "# starting sugar level\n",
    "sugar_level = 0.5\n",
    "\n",
    "total_reward = 0\n",
    "\n",
    "for item_sweetness in recommendations:\n",
    "    \n",
    "    # add reward\n",
    "    immediate_reward = reward(sugar_level, item_sweetness)\n",
    "    total_reward += immediate_reward\n",
    "    \n",
    "    # update sugar level\n",
    "    sugar_level = update_sugar_level(sugar_level, item_sweetness, alpha=0.7)\n",
    "    \n",
    "    print(f\"  Received reward {immediate_reward:.5f}, new sugar level {sugar_level:.5f}\")\n",
    "    \n",
    "print(\"Total reward after 5 recommendations:\", total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2d62233-1f33-47fe-b155-15e12dd9a8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Received reward 0.00000, new sugar level 0.35000\n",
      "  Received reward 0.65000, new sugar level 0.54500\n",
      "  Received reward 0.45500, new sugar level 0.68150\n",
      "Total reward after 5 recommendations 1.105\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "\n",
    "def update_sugar_level(sugar_level, item_sweetness, alpha=0.9):\n",
    "    return alpha * sugar_level + (1 - alpha) * item_sweetness\n",
    "\n",
    "def reward(sugar_level, item_sweetness):\n",
    "    return item_sweetness * (1 - sugar_level)\n",
    "\n",
    "# MODIFY THIS LIST\n",
    "# But make sure it always contains 3 items, each 0 or 1\n",
    "recommendations = [0,1,1]\n",
    "\n",
    "# starting sugar level\n",
    "sugar_level = 0.5\n",
    "\n",
    "total_reward = 0\n",
    "\n",
    "for item_sweetness in recommendations:\n",
    "    \n",
    "    # add reward\n",
    "    immediate_reward = reward(sugar_level, item_sweetness)\n",
    "    total_reward += immediate_reward\n",
    "    \n",
    "    # update sugar level\n",
    "    sugar_level = update_sugar_level(sugar_level, item_sweetness, alpha=0.7)\n",
    "    \n",
    "    print(f\"  Received reward {immediate_reward:.5f}, new sugar level {sugar_level:.5f}\")\n",
    "    \n",
    "print(\"Total reward after 5 recommendations\", total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a377412-aefd-4b85-8b94-de3869d0858a",
   "metadata": {},
   "source": [
    "#### Was was the best strategy in this example?\n",
    "\n",
    "- [x] 1 veggie to lower sugar levels, then 2 candies for that sweet, sweet reward.\n",
    "- [ ] Candy, then veggies for good health, then more candy.\n",
    "- [ ] Veggies all the way!\n",
    "- [ ] Candy all the way!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080aeba6-d6fd-461c-8ab7-c6420f692f93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ray200]",
   "language": "python",
   "name": "conda-env-ray200-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
