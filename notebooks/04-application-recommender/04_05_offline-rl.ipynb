{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5aa21ad-6bb9-4990-ab4d-fb9435b0bc00",
   "metadata": {},
   "source": [
    "## Offline RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "204bbdb7-e15f-4e5c-be77-b58b39dd3dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fd0b8c-0fe5-4295-b752-061d9e199421",
   "metadata": {},
   "source": [
    "#### Is this realistic?\n",
    "\n",
    "- So far we've built a simulation of user behavior\n",
    "- In some applications, we may be able to build accurate simulations:\n",
    "  - physics simulations (e.g. robots)\n",
    "  - games\n",
    "  - economic/financial simulations?\n",
    "- However, for user behavior, this is hard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb08a59-42f4-4fce-b709-1907ea5ef3c9",
   "metadata": {},
   "source": [
    "#### Is this realistic? \n",
    "\n",
    "- Best would be to deploy RL live, but not practical\n",
    "- Another possibility: learn from user data?\n",
    "- We can do this with **offline reinforcement learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba4b7ef-32c4-4caa-bb22-eace97236370",
   "metadata": {},
   "source": [
    "#### Offline RL\n",
    "\n",
    "- What is offline RL?\n",
    "- Recall our RL loop:\n",
    "\n",
    "![](img/RL-loop-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d89bcc9-17c8-4dbc-afb0-9f9e4810dad0",
   "metadata": {},
   "source": [
    "#### Offline RL\n",
    "\n",
    "- In offline RL we don't have an environment to interact with in a feedback loop:\n",
    "\n",
    "![](img/offline-RL-loop.png)\n",
    "\n",
    "This historic data was generated by some other, unknown policy/policies.\n",
    "\n",
    "Notes:\n",
    "\n",
    "Could be generate by real users, or by a different source (random, or RL agent!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d6cd2e-9bff-4b43-a49f-38e3211c4a09",
   "metadata": {},
   "source": [
    "#### Challenge of offline RL\n",
    "\n",
    "- Can't answer \"what if\" questions\n",
    "- We can only see the results of actions attempted in the dataset\n",
    "\n",
    "Notes:\n",
    "\n",
    "Perhaps this makes us appreciate how valuable/awesome it is to actually have an env available, which we have had for all the rest of the course. It allows us to try anything with no cost except computational cost (assuming it's a simulator, not a real world environment). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60c2e5cd-76c7-48d3-ae96-56a83a8e7065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "# #### Types of offline RL\n",
    "\n",
    "# Two main categories:\n",
    "\n",
    "# 1. No rewards available: try to _imitate_ historic policy\n",
    "\n",
    "# This boils down to supervised learning of observations -> actions\n",
    " \n",
    "# 2. Rewards available: try to _improve upon_ historic policy\n",
    "\n",
    "# Use rewards to improve policy. This is what we want, ideally!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23be8a47-e338-408d-abac-63bbefe41dac",
   "metadata": {},
   "source": [
    "#### Recommender dataset\n",
    "\n",
    "- Let's explore an offline dataset that we can learn from.\n",
    "- We'll need a bit of code to read all the JSON objects in the file:\n",
    "\n",
    "Notes:\n",
    "\n",
    "The file is in the format that RLlib learns from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8610f1a2-b704-4b22-aeba-3eb3826693e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json_dataset_file = \"data/offline/recommender_offline.json\"\n",
    "\n",
    "rollouts = []\n",
    "with open(json_dataset_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        rollouts.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9540859-74d2-4c4f-a06c-1cd6fd848994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rollouts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6f10f9-acb9-4aff-9694-622faf8792e1",
   "metadata": {},
   "source": [
    "We have 50 \"rollouts\" of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433484e4-8250-473f-9579-f712e9fa9e17",
   "metadata": {},
   "source": [
    "#### Recommender dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b685e2f5-4381-40ca-bff8-cc1398d76ddb",
   "metadata": {},
   "source": [
    "Each rollout is a dict containing info about the time step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c1e82cd-f6f4-4076-aa36-2a16dc761447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.rllib.utils.compression import unpack, pack\n",
    "\n",
    "obs = unpack(rollouts[0][\"obs\"])\n",
    "obs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ea21ad-940d-4aa0-8a63-43c527d233bd",
   "metadata": {},
   "source": [
    "- We have 200 time steps worth of data in each rollout\n",
    "- Each is an observation:\n",
    "\n",
    "Notes:\n",
    "\n",
    "This number 200 is set by the \"rollout_fragment_length\" algorithm config parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7946e442-7f6a-4653-8e84-75bc37fe952a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6545137 , 0.29728338],\n",
       "       [0.5238871 , 0.5144319 ],\n",
       "       [0.6741674 , 0.10163702]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93775945-d5ff-4f9f-96c1-3a04e4ac2a85",
   "metadata": {},
   "source": [
    "- Here are the first 3 observations\n",
    "- We can see `num_candidates` was set as 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a1b0f3-6394-4afe-b444-37914d84ac89",
   "metadata": {},
   "source": [
    "#### Recommender dataset\n",
    "\n",
    "We can also look at the first 3 actions, rewards, dones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ace3caf4-ef70-4393-9846-e68648b41bfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rollouts[0][\"actions\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29ff0ca7-6fe4-4786-a19f-c9bee9bf0b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6545137166976929, 0.3524414300918579, 0.05838315561413765]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rollouts[0][\"rewards\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37cafb64-4837-40c3-aa09-7b9e9871906d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, False, False]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rollouts[0][\"dones\"][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff27284-e91c-4ef1-9b01-7e950eda2cd4",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "So, first the agent saw the observation [0.65, 0.297] from the previous slide, then it took action 0, got a reward of 0.65, and the episode was not done.\n",
    "\n",
    "There is more information stored in the dataset than just the above, but these are the key points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e35709e-45fd-4d70-a111-172580296fea",
   "metadata": {},
   "source": [
    "#### Offline RL training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8049ef58-4f5e-4116-abd7-4aad06f65ee0",
   "metadata": {},
   "source": [
    "- Lots of info on offline RL with RLlib [here](https://docs.ray.io/en/latest/rllib/rllib-offline.html)\n",
    "- First we need our trainer config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56e22e98-6ca9-4c34-850e-488c4bb38db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_candidates = 2\n",
    "\n",
    "offline_trainer_config = {\n",
    "    # These should look familiar:\n",
    "    \"framework\"             : \"torch\",\n",
    "    \"create_env_on_driver\"  : True,\n",
    "    \"seed\"                  : 0,\n",
    "    \"model\"                 : {\n",
    "        \"fcnet_hiddens\"     : [64, 64]\n",
    "    },\n",
    "    \n",
    "    # These are new for offline RL:\n",
    "    \"input\": [json_dataset_file],\n",
    "    \"observation_space\": gym.spaces.Box(low=0, high=1, shape=(num_candidates,)),\n",
    "    \"action_space\": gym.spaces.Discrete(num_candidates),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108dd79c-dddb-4153-a89d-258ee623da8a",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "- The config items on the top should look familiar. On the second half, things are a bit different:\n",
    "  - We need to give it the path to the dataset file\n",
    "  - Because there is no env, we need to manually specify the observation and action spaces\n",
    "- We don't have an environment config because there is no environment!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c811603-4264-479d-bd0a-b5ec9ff38de2",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89a9af6-8f77-4115-9668-3c807031aa2f",
   "metadata": {},
   "source": [
    "- For offline RL we can't use `PPO`.\n",
    "- We'll use the `MARWIL` algorithm that is included with RLlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9465ecd-1562-4ae6-a2be-8950c56a988b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.agents.marwil import MARWILTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7280c730-eeee-49a2-a84b-7c42388df308",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = MARWILTrainer(config=offline_trainer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8172e9d-bc26-4e00-b431-c9438863028d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fe0e89-2c77-4a99-98e7-307e7502693f",
   "metadata": {},
   "source": [
    "Ok, we apparently did some training. _How do we evaluate without a simulator?_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cb0c38-43eb-42f3-b1bb-759ceeebb133",
   "metadata": {},
   "source": [
    "#### Offline evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92254765-f2eb-4bd3-b358-a015cdd98564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'V_prev': 16.325394203305805,\n",
       " 'V_step_WIS': 22.619529016166922,\n",
       " 'V_gain_est': 1.3817053996773574}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()[\"evaluation\"][\"off_policy_estimator\"][\"wis\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd563218-7b95-4e3f-89a1-5358a15ea098",
   "metadata": {},
   "source": [
    "- Recall that we achieved rewards of >25 last time\n",
    "- But this may be the best we can do with the given data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc17ce2-f34e-44f6-930a-96d1f7f02e1d",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "- V_prev shows the estimated episode reward of the historical policy\n",
    "- V_step_WIS shows the estimate episode reward of the new policy\n",
    "- V_gain_est is the ratio of these two quantities; here we see an estimated improvement of 38%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202d38ea-9ad6-4d17-9137-720251d1f6a1",
   "metadata": {},
   "source": [
    "#### Ground-truth evaluation\n",
    "\n",
    "- We can also evaluate the algorithm using our env simulator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5da21ac0-fcba-4d96-8b23-721f98dc1638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "from envs import BasicRecommender\n",
    "\n",
    "env_config = {\n",
    "    \"num_candidates\" : 2,\n",
    "    \"alpha\"          : 0.5,\n",
    "    \"seed\"           : 42\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b582c0ac-88f4-40e0-8078-31ee041bfe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BasicRecommender(env_config)\n",
    "\n",
    "def get_episode_reward(env, trainer):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = trainer.compute_single_action(obs)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "632fbd40-c2bb-44fe-af47-39b9507530dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.490920840544344"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([get_episode_reward(env, trainer) for i in range(100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea31f5b-fd9e-4129-ad52-030b2f6bcc81",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "In a real offline RL situation where all we had is the user data, we wouldn't have an accurate simulator available. We're just using it here as a sanity check to see if our MARWIL algorithm learned effectively. \n",
    "\n",
    "It is also possible to have RLlib do this for you by setting `\"input_evaluation\": [\"simulation\"]` and providing the env. However, then we have to trust it not to use the env for learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e979de95-c039-4493-b958-e2e5d85098ec",
   "metadata": {},
   "source": [
    "#### Let's apply what we learned!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643f5645-5a4d-4fcd-aa28-32894a99ffc2",
   "metadata": {},
   "source": [
    "## When to use offline RL?\n",
    "<!-- multiple choice -->\n",
    "\n",
    "#### In which of the following situations would you use offline RL?\n",
    "\n",
    "- [ ] Teaching an AI to play chess by having it play against other AIs repeatedly.\n",
    "- [x] Teaching an AI to play chess based on past games by professional chess players."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcb97ca-5ccd-4eed-b3ac-b58cee110c78",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### If you have a simulator available, does it make sense to use offline RL techniques?\n",
    "\n",
    "- [ ] Yes, offline RL methods are more effective than traditional RL methods.\n",
    "- [ ] No, RL methods should never be used if a simulator is available.\n",
    "- [x] Maybe, if high-quality historical data is available and the simulator is highly flawed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab3ebb6-c70d-44d8-93e1-ae4d0f7b54a7",
   "metadata": {},
   "source": [
    "## Historical data policy\n",
    "<!-- multiple choice -->\n",
    "\n",
    "Offline RL relies on data generated by some policy interacting with the environment.\n",
    " \n",
    "random?\n",
    "\n",
    "- [ ] \n",
    "- [ ] \n",
    "- [ ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06aebc9-1031-4635-bb8a-43417dffcb2c",
   "metadata": {},
   "source": [
    "## Coding\n",
    "<!-- coding exercise -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd176dc-9a29-4813-b10e-224a61c37aaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rl-course-dev-2]",
   "language": "python",
   "name": "conda-env-rl-course-dev-2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
