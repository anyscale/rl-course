{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5aa21ad-6bb9-4990-ab4d-fb9435b0bc00",
   "metadata": {},
   "source": [
    "## Offline RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "204bbdb7-e15f-4e5c-be77-b58b39dd3dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f337b2ca-8719-4938-8983-3a02bc76626f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import ray\n",
    "import logging\n",
    "ray.init(log_to_driver=False, ignore_reinit_error=True, logging_level=logging.ERROR); # logging.FATAL\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fd0b8c-0fe5-4295-b752-061d9e199421",
   "metadata": {},
   "source": [
    "#### Is this realistic?\n",
    "\n",
    "- So far we've built a simulation of user behavior\n",
    "- In some applications, we may be able to build accurate simulations:\n",
    "  - physics simulations (e.g. robots)\n",
    "  - games\n",
    "  - economic/financial simulations?\n",
    "- However, for user behavior, this is hard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb08a59-42f4-4fce-b709-1907ea5ef3c9",
   "metadata": {},
   "source": [
    "#### Is this realistic? \n",
    "\n",
    "- Best would be to deploy RL live, but not practical\n",
    "- Another possibility: learn from user data?\n",
    "- We can do this with **offline reinforcement learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba4b7ef-32c4-4caa-bb22-eace97236370",
   "metadata": {},
   "source": [
    "#### Offline RL\n",
    "\n",
    "- What is offline RL?\n",
    "- Recall our RL loop:\n",
    "\n",
    "![](img/RL-loop-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d89bcc9-17c8-4dbc-afb0-9f9e4810dad0",
   "metadata": {},
   "source": [
    "#### Offline RL\n",
    "\n",
    "- In offline RL we don't have an environment to interact with in a feedback loop:\n",
    "\n",
    "![](img/offline-RL-loop.png)\n",
    "\n",
    "This historic data was generated by some other, unknown policy/policies.\n",
    "\n",
    "Notes:\n",
    "\n",
    "Could be generate by real users, or by a different source (random, or RL agent!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d6cd2e-9bff-4b43-a49f-38e3211c4a09",
   "metadata": {},
   "source": [
    "#### Challenge of offline RL\n",
    "\n",
    "- Can't answer \"what if\" questions\n",
    "- We can only see the results of actions attempted in the dataset\n",
    "\n",
    "Notes:\n",
    "\n",
    "Perhaps this makes us appreciate how valuable/awesome it is to actually have an env available, which we have had for all the rest of the course. It allows us to try anything with no cost except computational cost (assuming it's a simulator, not a real world environment). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f9ff23b-8073-44b0-bb32-320ab7fa0a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # HIDDEN\n",
    "\n",
    "# # generate the offline dataset\n",
    "# env_config = {\n",
    "#     \"num_candidates\" : 2,\n",
    "#     \"alpha\"          : 0.5,\n",
    "#     \"seed\"           : 42\n",
    "# }\n",
    "\n",
    "# from ray.rllib.algorithms.ppo import PPO, PPOConfig\n",
    "\n",
    "# ppo_config = (\n",
    "#     PPOConfig()\\\n",
    "#     .framework(\"torch\")\\\n",
    "#     # need to set num_rollout_workers=1 for now per https://github.com/ray-project/ray/issues/25696\n",
    "#     .rollouts(create_env_on_local_worker=True, num_rollout_workers=1)\\\n",
    "#     .debugging(seed=0, log_level=\"ERROR\")\\\n",
    "#     .training(model={\"fcnet_hiddens\" : [64, 64]}, lr=0.001)\\\n",
    "#     # .environment(env_config=env_config)\\\n",
    "#     .offline_data(output=\"data/recommender2\")\n",
    "# )\n",
    "\n",
    "# from envs import BasicRecommenderWithHistory\n",
    "\n",
    "# ppo_history = ppo_config.build(env=\"CartPole-v1\")\n",
    "\n",
    "# rewards_history = []\n",
    "# for i in range(25):\n",
    "#     result = ppo_history.train()\n",
    "#     rewards_history.append(result[\"episode_reward_mean\"])\n",
    "    \n",
    "# ppo_history.evaluate(duration_fn=1000)[\"evaluation\"][\"episode_reward_mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23be8a47-e338-408d-abac-63bbefe41dac",
   "metadata": {},
   "source": [
    "#### Recommender dataset\n",
    "\n",
    "- Let's explore an offline dataset that we can learn from.\n",
    "- We'll need a bit of code to read all the JSON objects in the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8610f1a2-b704-4b22-aeba-3eb3826693e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json_dataset_file = \"data/recommender_offline.json\"\n",
    "\n",
    "rollouts = []\n",
    "with open(json_dataset_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        rollouts.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9540859-74d2-4c4f-a06c-1cd6fd848994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rollouts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6f10f9-acb9-4aff-9694-622faf8792e1",
   "metadata": {},
   "source": [
    "We have 50 \"rollouts\" of data.\n",
    "\n",
    "Notes:\n",
    "\n",
    "The file is in the format that RLlib learns from."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433484e4-8250-473f-9579-f712e9fa9e17",
   "metadata": {},
   "source": [
    "#### Recommender dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b685e2f5-4381-40ca-bff8-cc1398d76ddb",
   "metadata": {},
   "source": [
    "Each rollout is a dict containing info about the time step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c1e82cd-f6f4-4076-aa36-2a16dc761447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.rllib.utils.compression import unpack, pack\n",
    "\n",
    "obs = unpack(rollouts[0][\"obs\"])\n",
    "obs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ea21ad-940d-4aa0-8a63-43c527d233bd",
   "metadata": {},
   "source": [
    "- We have 200 time steps worth of data in each rollout\n",
    "- Let's look at the observations first\n",
    "\n",
    "Notes:\n",
    "\n",
    "This number 200 is set by the \"rollout_fragment_length\" algorithm config parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fce3e9-db24-4050-bcd4-fc08f760bf5d",
   "metadata": {},
   "source": [
    "#### Recommender dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a579d766-069c-446c-aa3d-7798335ece38",
   "metadata": {},
   "source": [
    "Here are the first 3 observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7946e442-7f6a-4653-8e84-75bc37fe952a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6545137 , 0.29728338],\n",
       "       [0.5238871 , 0.5144319 ],\n",
       "       [0.6741674 , 0.10163702]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93775945-d5ff-4f9f-96c1-3a04e4ac2a85",
   "metadata": {},
   "source": [
    "\n",
    "We can see `num_candidates` was set as 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a1b0f3-6394-4afe-b444-37914d84ac89",
   "metadata": {},
   "source": [
    "#### Recommender dataset\n",
    "\n",
    "We can also look at the first 3 actions, rewards, dones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ace3caf4-ef70-4393-9846-e68648b41bfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rollouts[0][\"actions\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29ff0ca7-6fe4-4786-a19f-c9bee9bf0b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6545137166976929, 0.3524414300918579, 0.05838315561413765]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rollouts[0][\"rewards\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37cafb64-4837-40c3-aa09-7b9e9871906d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, False, False]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rollouts[0][\"dones\"][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff27284-e91c-4ef1-9b01-7e950eda2cd4",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "So, first the agent saw the observation [0.65, 0.297] from the previous slide, then it took action 0, got a reward of 0.65, and the episode was not done.\n",
    "\n",
    "There is more information stored in the dataset than just the above, but these are the key points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a206ba60-13ae-47b3-a406-eb06e701af5a",
   "metadata": {},
   "source": [
    "#### Supervised learning\n",
    "\n",
    "- Wait, this is a dataset... can't we just do supervised learning to map states to actions? ðŸ¤”\n",
    "- Yes, we can, and this would aim to recover the policy that generated the dataset (_imitation learning_).\n",
    "- But it is actually possible to do _better_... and that is our goal with offline RL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59b604b7-484a-4ad8-976d-6f49c941f3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# maybe going overboard here, but for a synthetic example we could actually show it does better than the policy that generated the data,\n",
    "# since we generated the data ourselves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e35709e-45fd-4d70-a111-172580296fea",
   "metadata": {},
   "source": [
    "#### Offline RL training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8049ef58-4f5e-4116-abd7-4aad06f65ee0",
   "metadata": {},
   "source": [
    "- Lots of info on offline RL with RLlib [here](https://docs.ray.io/en/latest/rllib/rllib-offline.html).\n",
    "- First we need an algorithm.\n",
    "- For offline RL we can't use `PPO`.\n",
    "- We'll use the `MARWIL` algorithm that is included with RLlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34d0e449-de74-444c-894e-4d58c6e33c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.marwil import MARWIL, MARWILConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0385a4b2-594b-46e0-bd2f-2854a6935111",
   "metadata": {},
   "source": [
    "#### Offline RL training\n",
    "\n",
    "- Next, we create the config, starting with `MARWILConfig` instead of `PPOConfig`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56e22e98-6ca9-4c34-850e-488c4bb38db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the same as before\n",
    "offline_config = ( \n",
    "    MARWILConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(create_env_on_local_worker=True)\\\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\\\n",
    "    .training(model={\"fcnet_hiddens\" : [64, 64]})\\\n",
    ")\n",
    "\n",
    "# This is new for offline RL\n",
    "num_candidates = 2\n",
    "offline_config = offline_config.environment(\n",
    "    observation_space = gym.spaces.Box(low=0, high=1, shape=(num_candidates,)), \n",
    "    action_space = gym.spaces.Discrete(num_candidates),\n",
    ").offline_data(\n",
    "    input_ = [json_dataset_file],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108dd79c-dddb-4153-a89d-258ee623da8a",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "- The config items on the top should look familiar. On the second half, things are a bit different:\n",
    "  - We need to give it the path to the dataset file\n",
    "  - Because there is no env, we need to manually specify the observation and action spaces\n",
    "- We don't have an environment config because there is no environment!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c811603-4264-479d-bd0a-b5ec9ff38de2",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7280c730-eeee-49a2-a84b-7c42388df308",
   "metadata": {},
   "outputs": [],
   "source": [
    "marwil = offline_config.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8172e9d-bc26-4e00-b431-c9438863028d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    marwil.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d7651b-e0cb-4e37-bec3-976ac37af9cb",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fe0e89-2c77-4a99-98e7-307e7502693f",
   "metadata": {},
   "source": [
    "_How do we evaluate without a simulator?_\n",
    "\n",
    "- This is called off-policy estimation.\n",
    "- It's a more sophisticated technique and outside the scope of this course.\n",
    "- See the RLlib docs [here](https://docs.ray.io/en/latest/rllib/rllib-offline.html).\n",
    "- What we'll do is evaluate with our simulator, since we do have it.\n",
    "\n",
    "Notes:\n",
    "\n",
    "In a realistic offline RL scenario, you don't have a simulator available. Then you need to use some more complex evaluation techniques called off-policy estimation. You would have train/test data sets like in supervised learning. \n",
    "\n",
    "For our purposes though, since we do have a simulator, we'll make use of them for evaluation. RLlib actually has an option to allow this, because it is useful for debugging and such. We'll see that on the next slide."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202d38ea-9ad6-4d17-9137-720251d1f6a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Evaluation with our simulator\n",
    "\n",
    "We can evaluate the algorithm using our env simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5da21ac0-fcba-4d96-8b23-721f98dc1638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "from envs import BasicRecommender\n",
    "\n",
    "env_config = {\n",
    "    \"num_candidates\" : 2,\n",
    "    \"alpha\"          : 0.5,\n",
    "    \"seed\"           : 42\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b582c0ac-88f4-40e0-8078-31ee041bfe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BasicRecommender(env_config)\n",
    "\n",
    "def get_episode_reward(env, algo):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = algo.compute_single_action(obs)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a8671a-26c8-45f5-a468-944dfa4c01b6",
   "metadata": {},
   "source": [
    "- Above: set up a function that runs one episode with the simulator.\n",
    "- Below: run this for 100 episodes to get the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "632fbd40-c2bb-44fe-af47-39b9507530dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.523076776072696"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([get_episode_reward(env, marwil) for i in range(100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8818fa-2e46-4d04-85c7-d55797fc169c",
   "metadata": {},
   "source": [
    "This seems to be doing about the same as random again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e507580-77c8-47c1-a9a5-8259ee4bdcd9",
   "metadata": {},
   "source": [
    "#### RLlib for simulator-based evaluation\n",
    "\n",
    "RLlib also offers this as a feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3b2aa5b-980b-4e68-85e1-ab55417ccaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# it seems this will be moved from offline_data() to evaluation() \n",
    "# in a future version of Ray, which makes more sense indeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7468c451-e68b-446b-a0f2-5dc86bb1af11",
   "metadata": {},
   "outputs": [],
   "source": [
    "offline_config = offline_config.offline_data(\n",
    "    off_policy_estimation_methods={'simulation': {'type': 'simulation'}}\n",
    ").environment(env=BasicRecommender, env_config=env_config) # only used for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef0db4ec-8023-4ada-83ac-e4f0f42acb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "marwil.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dfe966-9734-42c7-bf5f-bf8655944f36",
   "metadata": {},
   "source": [
    "If we had set up the config this way and trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5bccaf63-54f6-41da-a034-3da05a1ae5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "marwil = offline_config.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56152208-587f-4306-8e53-9c0eb9f99f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    marwil.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d980136-8c96-42e9-90fb-9d4a6a564922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.440636122178013"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marwil.evaluate()[\"evaluation\"][\"episode_reward_mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e979de95-c039-4493-b958-e2e5d85098ec",
   "metadata": {},
   "source": [
    "#### Let's apply what we learned!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643f5645-5a4d-4fcd-aa28-32894a99ffc2",
   "metadata": {},
   "source": [
    "## Example of offline RL\n",
    "<!-- multiple choice -->\n",
    "\n",
    "- [ ] Teaching an AI to play chess by having it play against other AIs repeatedly.\n",
    "- [x] Teaching an AI to play chess based on past games by professional chess players."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcb97ca-5ccd-4eed-b3ac-b58cee110c78",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Does offline data help?\n",
    "<!-- multiple choice -->\n",
    "\n",
    "Imagine you have an env that perfectly represents your environment; for example, maybe you are training an AI to play a single-player game like [Atari Breakout](https://en.wikipedia.org/wiki/Breakout_(video_game)). In addition to the simulator, you also have some offline data available. Though not discussed in the above slides, it is possible to mix a simulator with offline data to jointly train an agent using RL (see [here](https://docs.ray.io/en/latest/rllib/rllib-offline.html#mixing-simulation-and-offline-data)). If you already have a perfect simulator, could offline data provide any additional utility, if combined with the simulator during training?\n",
    "\n",
    "#### Best policy after training\n",
    "\n",
    "Choose the correct statement regarding finding the best policy after unlimited training. You can assume you have a 'perfect' RL algorithm, meaning it can represent any policy and is able to optimize any function.\n",
    "\n",
    "- [x] With a 'perfect' RL algorithm and enough compute time, there is no benefit to using the historical data because the simulator contains everything there is to know about the environment.\n",
    "- [ ] With a 'perfect' RL algorithm and unlimited compute time, the historical data may help you find a better policy than only using the simulator. | With a 'perfect' RL algorithm, you should eventually find the optimal policy. This is like having infinite data, infinite compute time, an arbitrarily complex model, and a perfect optimization method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bf6575-837f-458d-be6f-924ee4659254",
   "metadata": {},
   "source": [
    "#### Training speed\n",
    "\n",
    "Choose the correct statement regarding finding the best policy after a bit of training. You can assume you have a 'perfect' RL algorithm, meaning it can represent any policy and is able to optimize any function.\n",
    "\n",
    "- [ ] With a 'perfect' RL algorithm but only a bit of compute time, there is no benefit to using the historical data because the simulator contains everything there is to know about the environment. | What if the historical data was generated using the *optimal* policy?\n",
    "- [x] With a 'perfect' RL algorithm but only a bit of compute time, the historical data may help you find a better policy than only using the simulator. | If the historical data was generated using a very good policy, you could learn from it quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab3ebb6-c70d-44d8-93e1-ae4d0f7b54a7",
   "metadata": {},
   "source": [
    "## Historical data policy\n",
    "<!-- multiple choice -->\n",
    "\n",
    "Offline RL relies on data generated by some policy interacting with the environment. Which of the following is **NOT** a desirable property of this dataset / historical policy?\n",
    " \n",
    "- [x] The environment and the historical policy are both deterministic. | In this case, we would only explore one trajectory through the environment. \n",
    "- [ ] The dataset contains a large number of episodes.\n",
    "- [ ] The historical policy explores a variety of states in the environment.\n",
    "- [ ] The historical policy achieves high reward in some episodes.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06aebc9-1031-4635-bb8a-43417dffcb2c",
   "metadata": {},
   "source": [
    "## Offline RL for Cartpole\n",
    "<!-- coding exercise -->\n",
    "\n",
    "In this exercise we'll tackle the famous [Cartpole benchmark problem](https://www.gymlibrary.ml/environments/classic_control/cart_pole/) that is bundled with the `gym` library. The goal is to keep the inverted pendulum from falling over by applying force, either left or right, at each time step. \n",
    "\n",
    "We will train the agent using offline data contained in a file `cartpolev1_offline.json` that is read in by the code.\n",
    "\n",
    "We will _evaluate_ the agent using the off-policy evaluation discussed in the slides, _but also_ using the real Cartpole simulator. In reality, if we were using offline RL we probably wouldn't have access to the real simulator, but we're including it here so that we can do ground-truth evaluation of our agent.\n",
    "\n",
    "Fill in the missing code. Then, run the code and answer the multiple-choice question below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "edd176dc-9a29-4813-b10e-224a61c37aaa",
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '____' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgorithms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmarwil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MARWIL, MARWILConfig\n\u001b[1;32m      8\u001b[0m offline_trainer_config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# These should look familiar:\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframework\u001b[39m\u001b[38;5;124m\"\u001b[39m             : \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreate_env_on_driver\u001b[39m\u001b[38;5;124m\"\u001b[39m  : \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m                  : \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m                 : {\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfcnet_hiddens\u001b[39m\u001b[38;5;124m\"\u001b[39m     : [\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m]\n\u001b[1;32m     15\u001b[0m     },\n\u001b[1;32m     16\u001b[0m     \n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# These are new for offline RL:\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     \u001b[43m____\u001b[49m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/cartpolev1_offline.json\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobservation_space\u001b[39m\u001b[38;5;124m\"\u001b[39m: gym\u001b[38;5;241m.\u001b[39mspaces\u001b[38;5;241m.\u001b[39mBox(low\u001b[38;5;241m=\u001b[39m____, \n\u001b[1;32m     20\u001b[0m                                         high\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m4.8\u001b[39m,  np\u001b[38;5;241m.\u001b[39minf,  \u001b[38;5;241m0.42\u001b[39m,  np\u001b[38;5;241m.\u001b[39minf])),\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction_space\u001b[39m\u001b[38;5;124m\"\u001b[39m: gym\u001b[38;5;241m.\u001b[39mspaces\u001b[38;5;241m.\u001b[39mDiscrete(\u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_evaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m : [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimulation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwis\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCartPole-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# for evaluation only\u001b[39;00m\n\u001b[1;32m     24\u001b[0m }\n\u001b[1;32m     26\u001b[0m trainer \u001b[38;5;241m=\u001b[39m MARWILTrainer(config\u001b[38;5;241m=\u001b[39m____)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Training (and storing results)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name '____' is not defined"
     ]
    }
   ],
   "source": [
    "# EXERCISE\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ray.rllib.algorithms.marwil import MARWIL, MARWILConfig\n",
    "\n",
    "offline_trainer_config = {\n",
    "    # These should look familiar:\n",
    "    \"framework\"             : \"torch\",\n",
    "    \"create_env_on_driver\"  : True,\n",
    "    \"seed\"                  : 0,\n",
    "    \"model\"                 : {\n",
    "        \"fcnet_hiddens\"     : [64, 64]\n",
    "    },\n",
    "    \n",
    "    # These are new for offline RL:\n",
    "    ____: [\"data/cartpolev1_offline.json\"],\n",
    "    \"observation_space\": gym.spaces.Box(low=____, \n",
    "                                        high=np.array([4.8,  np.inf,  0.42,  np.inf])),\n",
    "    \"action_space\": gym.spaces.Discrete(2),\n",
    "    \"input_evaluation\" : [\"simulation\", \"is\", \"wis\"],\n",
    "    \"env\" : \"CartPole-v1\" # for evaluation only\n",
    "}\n",
    "\n",
    "trainer = MARWILTrainer(config=____)\n",
    "\n",
    "# Training (and storing results)\n",
    "results_off = []\n",
    "results_sim = []\n",
    "for i in range(200):\n",
    "    r = trainer.____()\n",
    "    results_off.append(r[\"off_policy_estimator\"][\"wis\"]['V_gain_est'])\n",
    "    results_sim.append(r[\"episode_reward_mean\"])\n",
    "\n",
    "# Plotting code (you can ignore)\n",
    "fig, ax1 = plt.subplots() \n",
    "ax1.set_xlabel('iterations') \n",
    "ax1.set_ylabel('V_gain_est', color='r') \n",
    "ax1.plot(results_off, color='r') \n",
    "ax1.tick_params(axis ='y', labelcolor='red') \n",
    "ax2 = ax1.twinx() \n",
    "ax2.set_ylabel('simulator reward', color='b') \n",
    "ax2.plot(results_sim, color='b') \n",
    "ax2.tick_params(axis ='y', labelcolor='blue') \n",
    "\n",
    "trainer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e100ab56-3d05-494e-a151-2e82701842ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 71 batches (26802 ts) into the replay buffer, which has capacity 50000.\n",
      "0\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[1;32m     34\u001b[0m     r \u001b[38;5;241m=\u001b[39m algo\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 35\u001b[0m     results_off\u001b[38;5;241m.\u001b[39mappend(\u001b[43mr\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwis\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv_new_mean\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     36\u001b[0m     results_sim\u001b[38;5;241m.\u001b[39mappend(r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode_reward_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Plotting code (you can ignore)\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: ''"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ray.rllib.algorithms.marwil import MARWIL, MARWILConfig\n",
    "from ray.rllib.algorithms.crr import CRR, CRRConfig\n",
    "\n",
    "# This is the same as before\n",
    "offline_config = ( \n",
    "    CRRConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(create_env_on_local_worker=True)\\\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\\\n",
    "    .training(model={\"fcnet_hiddens\" : [32, 32]})\\\n",
    ")\n",
    "# This is new for offline RL\n",
    "offline_config = offline_config.environment(\n",
    "    observation_space = gym.spaces.Box(low=np.array([-4.8, -np.inf, -0.42, -np.inf]), \n",
    "                                        high=np.array([4.8,  np.inf,  0.42,  np.inf])),\n",
    "    action_space = gym.spaces.Discrete(2),\n",
    "    env = \"CartPole-v1\" # for evaluation only\n",
    ").offline_data(\n",
    "    input_ = [\"data/cartpolev1_offline.json\"],\n",
    "    off_policy_estimation_methods={'simulation': {'type': 'simulation'}, 'wis': {'type': 'wis'}}\n",
    ")\n",
    "\n",
    "algo = offline_config.build()\n",
    "\n",
    "# Training (and storing results)\n",
    "results_off = []\n",
    "results_sim = []\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "    r = algo.train()\n",
    "    results_off.append(r[\"\"][\"wis\"]['v_new_mean'])\n",
    "    results_sim.append(r[\"episode_reward_mean\"])\n",
    "\n",
    "# Plotting code (you can ignore)\n",
    "fig, ax1 = plt.subplots() \n",
    "ax1.set_xlabel('iterations') \n",
    "ax1.set_ylabel('off policy estimated gain', color='r') \n",
    "ax1.plot(results_off, color='r') \n",
    "ax1.tick_params(axis ='y', labelcolor='red') \n",
    "ax2 = ax1.twinx() \n",
    "ax2.set_ylabel('simulator reward', color='b') \n",
    "ax2.plot(results_sim, color='b') \n",
    "ax2.tick_params(axis ='y', labelcolor='blue') \n",
    "\n",
    "marwil.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f3a77a-e2c2-4ae1-9cb2-59c3fbece21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "r[\"info\"][\"learner\"][\"default_policy\"][\"learner_stats\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a6d80f-da73-4e80-8ee5-4dfd1b5af2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "r[\"off_policy_estimator\"][\"wis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894b9d37-3d9b-4f6d-8ae6-84e4ff2c1a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "r[\"off_policy_estimator\"][\"wis\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f62c9bd-9123-4e74-b59b-06e6236f4d7e",
   "metadata": {},
   "source": [
    "# ANOTHER THOUGHT\n",
    "\n",
    "maybe off-policy estimation is just too advanced for this course\n",
    "\n",
    "it seems hard to understand and also changing quickly - good?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6162e16-9b70-41c0-8c2f-eaca5e0dd6bc",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "see https://docs.ray.io/en/master/rllib/rllib-offline.html\n",
    "You should use separate datasets for algorithm training and OPE, as shown here.\n",
    "Need to deal with that splitting!!\n",
    "and also, why do our curves look so bad??\n",
    "\n",
    "also see the tuned examples here: https://docs.ray.io/en/master/rllib/rllib-algorithms.html#offline\n",
    "\n",
    "TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a53dae-daac-4f60-98f5-dc4bff875a69",
   "metadata": {},
   "source": [
    "#### What happens during the training with offline RL?\n",
    "\n",
    "- [ ] The agent 'thinks' it is improving, and in fact it really is improving, according to the simulator.\n",
    "- [ ] The agent 'thinks' it is improving, but it actually isn't learning, according to the simulator. | Take a look at the blue line on the graph; is the reward improving?\n",
    "- [x] The agent does not necessarily 'think' it is improving, but in fact it really is improving, according to the simulator. | Take a look at the red line on the graph; is the reward metric improving?\n",
    "- [ ] The agent does not necessarily 'think' it is improving, and indeed it isn't learning, according to the simulator. | Take a look at the red line on the graph; is the reward metric improving?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad6fe1d-dc59-41ff-99cb-c688dc12fd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# - ALSO: this might be a great example to try supervised learning and show why it doesn't work...???\n",
    "#   - in some ways this is a way better example than frozen lake... because there IS a short-term reward, it's just not what you should look at.\n",
    "#   - with frozen lake there is no short-term reward, so RL seems \"obvious\"\n",
    "# Or, that could go in the offline RL section, since we already have a data file there and could do SL directly on it\n",
    "# Yes, that seems cool.\n",
    "# could use scipy/numpy to just do the normal equations if we want to avoid adding a dependency on sklearn \n",
    "\n",
    "# Supervised learning only gives you imitation learning, which can only get as good as the policy that generated the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766cc60b-fb10-411c-a798-18ae6f447369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34ae237-8b75-48b9-a998-2914625e71e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3cf43a-3c4c-4ced-b263-f8ad2f204efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "offline_trainer_config = {\n",
    "    # These should look familiar:\n",
    "    \"framework\"             : \"torch\",\n",
    "    \"create_env_on_driver\"  : True,\n",
    "    \"seed\"                  : 0,\n",
    "    \"model\"                 : {\n",
    "        \"fcnet_hiddens\"     : [64, 64]\n",
    "    },\n",
    "    \n",
    "    # These are new for offline RL:\n",
    "    \"input\": [\"data/cartpolev1_offline.json\"],\n",
    "    \"observation_space\": gym.spaces.Box(low=np.array([-4.8, -np.inf, -0.42, -np.inf]), \n",
    "                                        high=np.array([4.8,  np.inf,  0.42,  np.inf])),\n",
    "    \"action_space\": gym.spaces.Discrete(2),\n",
    "    \"input_evaluation\" : [\"simulation\", \"is\", \"wis\"],\n",
    "    \"env\" : \"CartPole-v1\" # for evaluation only\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ray2beta]",
   "language": "python",
   "name": "conda-env-ray2beta-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
