{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5aa21ad-6bb9-4990-ab4d-fb9435b0bc00",
   "metadata": {},
   "source": [
    "## Offline RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "204bbdb7-e15f-4e5c-be77-b58b39dd3dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fd0b8c-0fe5-4295-b752-061d9e199421",
   "metadata": {},
   "source": [
    "#### Is this realistic?\n",
    "\n",
    "- So far we've built a simulation of user behavior\n",
    "- In some applications, we may be able to build accurate simulations:\n",
    "  - physics simulations (e.g. robots)\n",
    "  - games\n",
    "  - economic/financial simulations?\n",
    "- However, for user behavior, this is hard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb08a59-42f4-4fce-b709-1907ea5ef3c9",
   "metadata": {},
   "source": [
    "#### Is this realistic? \n",
    "\n",
    "- Best would be to deploy RL live, but not practical\n",
    "- Another possibility: learn from user data?\n",
    "- We can do this with **offline reinforcement learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba4b7ef-32c4-4caa-bb22-eace97236370",
   "metadata": {},
   "source": [
    "#### Offline RL\n",
    "\n",
    "- What is offline RL?\n",
    "- Recall our RL loop:\n",
    "\n",
    "![](img/RL-loop-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d89bcc9-17c8-4dbc-afb0-9f9e4810dad0",
   "metadata": {},
   "source": [
    "#### Offline RL\n",
    "\n",
    "- In offline RL we don't have an environment to interact with in a feedback loop:\n",
    "\n",
    "![](img/offline-RL-loop.png)\n",
    "\n",
    "This historic data was generated by some other, unknown policy/policies.\n",
    "\n",
    "Notes:\n",
    "\n",
    "Could be generate by real users, or by a different source (random, or RL agent!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d6cd2e-9bff-4b43-a49f-38e3211c4a09",
   "metadata": {},
   "source": [
    "#### Challenge of offline RL\n",
    "\n",
    "- Can't answer \"what if\" questions\n",
    "- We can only see the results of actions attempted in the dataset\n",
    "\n",
    "Notes:\n",
    "\n",
    "Perhaps this makes us appreciate how valuable/awesome it is to actually have an env available, which we have had for all the rest of the course. It allows us to try anything with no cost except computational cost (assuming it's a simulator, not a real world environment). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c291ac-77de-44e3-9d7e-b7e9993e53ef",
   "metadata": {},
   "source": [
    "#### Types of offline RL\n",
    "\n",
    "Two main categories:\n",
    "\n",
    "1. No rewards available: try to _imitate_ historic policy\n",
    "\n",
    "This boils down to supervised learning of observations -> actions\n",
    " \n",
    "2. Rewards available: try to _improve upon_ historic policy\n",
    "\n",
    "Use rewards to improve policy. This is what we want, ideally!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23be8a47-e338-408d-abac-63bbefe41dac",
   "metadata": {},
   "source": [
    "#### Recommender dataset\n",
    "\n",
    "- Let's explore an offline dataset that we can learn from.\n",
    "- We'll need a bit of code to read all the JSON objects in the file:\n",
    "\n",
    "Notes:\n",
    "\n",
    "The file is in the format that RLlib learns from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "8610f1a2-b704-4b22-aeba-3eb3826693e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json_dataset_file = \"data/offline/recommender_offline.json\"\n",
    "\n",
    "rollouts = []\n",
    "with open(json_dataset_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        rollouts.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "c9540859-74d2-4c4f-a06c-1cd6fd848994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rollouts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6f10f9-acb9-4aff-9694-622faf8792e1",
   "metadata": {},
   "source": [
    "We have 50 \"rollouts\" of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433484e4-8250-473f-9579-f712e9fa9e17",
   "metadata": {},
   "source": [
    "#### Recommender dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b685e2f5-4381-40ca-bff8-cc1398d76ddb",
   "metadata": {},
   "source": [
    "Each rollout is a dict containing info about the time step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "8c1e82cd-f6f4-4076-aa36-2a16dc761447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 2)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.rllib.utils.compression import unpack, pack\n",
    "\n",
    "obs = unpack(rollouts[0][\"obs\"])\n",
    "obs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ea21ad-940d-4aa0-8a63-43c527d233bd",
   "metadata": {},
   "source": [
    "- We have 200 time steps worth of data in each rollout\n",
    "- Each is an observation:\n",
    "\n",
    "Notes:\n",
    "\n",
    "This number 200 is set by the \"rollout_fragment_length\" algorithm config parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "7946e442-7f6a-4653-8e84-75bc37fe952a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6545137 , 0.29728338],\n",
       "       [0.5238871 , 0.5144319 ],\n",
       "       [0.6741674 , 0.10163702]], dtype=float32)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93775945-d5ff-4f9f-96c1-3a04e4ac2a85",
   "metadata": {},
   "source": [
    "- Here are the first 3 observations\n",
    "- We can see `num_candidates` was set as 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a1b0f3-6394-4afe-b444-37914d84ac89",
   "metadata": {},
   "source": [
    "#### Recommender dataset\n",
    "\n",
    "We can also look at the first 3 actions, rewards, dones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "ace3caf4-ef70-4393-9846-e68648b41bfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rollouts[0][\"actions\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "29ff0ca7-6fe4-4786-a19f-c9bee9bf0b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6545137166976929, 0.3524414300918579, 0.05838315561413765]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rollouts[0][\"rewards\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "37cafb64-4837-40c3-aa09-7b9e9871906d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, False, False]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rollouts[0][\"dones\"][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff27284-e91c-4ef1-9b01-7e950eda2cd4",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "So, first the agent saw the observation [0.65, 0.297] from the previous slide, then it took action 0, got a reward of 0.65, and the episode was not done.\n",
    "\n",
    "There is more information stored in the dataset than just the above, but these are the key points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e35709e-45fd-4d70-a111-172580296fea",
   "metadata": {},
   "source": [
    "#### Offline RL training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8049ef58-4f5e-4116-abd7-4aad06f65ee0",
   "metadata": {},
   "source": [
    "- Lots of info on offline RL with RLlib [here](https://docs.ray.io/en/latest/rllib/rllib-offline.html)\n",
    "- First we need our trainer config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "56e22e98-6ca9-4c34-850e-488c4bb38db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_candidates = 2\n",
    "\n",
    "offline_trainer_config = {\n",
    "    # These should look familiar:\n",
    "    \"framework\"             : \"torch\",\n",
    "    \"create_env_on_driver\"  : True,\n",
    "    \"seed\"                  : 0,\n",
    "    \"model\"                 : {\n",
    "        \"fcnet_hiddens\"     : [64, 64]\n",
    "    },\n",
    "    \n",
    "    # These are new for offline RL:\n",
    "    \"input\": [json_dataset_file],\n",
    "    \"observation_space\": gym.spaces.Box(low=0, high=1, shape=(num_candidates,)),\n",
    "    \"action_space\": gym.spaces.Discrete(num_candidates),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108dd79c-dddb-4153-a89d-258ee623da8a",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "- The config items on the top should look familiar. On the second half, things are a bit different:\n",
    "  - We need to give it the path to the dataset file\n",
    "  - Because there is no env, we need to manually specify the observation and action spaces\n",
    "- We don't have an environment config because there is no environment!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c811603-4264-479d-bd0a-b5ec9ff38de2",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89a9af6-8f77-4115-9668-3c807031aa2f",
   "metadata": {},
   "source": [
    "- For offline RL we can't use `PPO`.\n",
    "- We'll use the `MARWIL` algorithm that is included with RLlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "f9465ecd-1562-4ae6-a2be-8950c56a988b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.agents.marwil import MARWILTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "7280c730-eeee-49a2-a84b-7c42388df308",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = MARWILTrainer(config=offline_trainer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "f8172e9d-bc26-4e00-b431-c9438863028d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    out = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fe0e89-2c77-4a99-98e7-307e7502693f",
   "metadata": {},
   "source": [
    "Ok, we apparently did some training. How do we evaluate without a simulator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "d9565ff9-257b-4e5e-bdad-cf1fc28c7d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'episode_reward_max': nan,\n",
       " 'episode_reward_min': nan,\n",
       " 'episode_reward_mean': nan,\n",
       " 'episode_len_mean': nan,\n",
       " 'episode_media': {},\n",
       " 'episodes_this_iter': 0,\n",
       " 'policy_reward_min': {},\n",
       " 'policy_reward_max': {},\n",
       " 'policy_reward_mean': {},\n",
       " 'custom_metrics': {},\n",
       " 'hist_stats': {'episode_reward': [], 'episode_lengths': []},\n",
       " 'sampler_perf': {},\n",
       " 'off_policy_estimator': {'is': {'V_prev': 16.231008984326884,\n",
       "   'V_step_IS': 16.923409830694006,\n",
       "   'V_gain_est': 1.0409941273679744},\n",
       "  'wis': {'V_prev': 16.231008984326884,\n",
       "   'V_step_WIS': 16.741032437886666,\n",
       "   'V_gain_est': 1.0298169594045956}},\n",
       " 'num_healthy_workers': 0,\n",
       " 'timesteps_total': 3200,\n",
       " 'timesteps_this_iter': 2000,\n",
       " 'agent_timesteps_total': 3200,\n",
       " 'timers': {'sample_time_ms': 1640.838,\n",
       "  'sample_throughput': 1218.89,\n",
       "  'learn_time_ms': 3.779,\n",
       "  'learn_throughput': 529192.957},\n",
       " 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0,\n",
       "     'policy_loss': 1.916463851928711,\n",
       "     'total_loss': 54.55511474609375,\n",
       "     'moving_average_sqd_adv_norm': array([100.], dtype=float32),\n",
       "     'vf_explained_var': -0.0001423358917236328,\n",
       "     'vf_loss': 52.63865280151367},\n",
       "    'model': {},\n",
       "    'custom_metrics': {}}},\n",
       "  'num_steps_sampled': 3200,\n",
       "  'num_agent_steps_sampled': 3200,\n",
       "  'num_steps_trained': 32000,\n",
       "  'num_steps_trained_this_iter': 2000,\n",
       "  'num_agent_steps_trained': 32000},\n",
       " 'done': False,\n",
       " 'episodes_total': 0,\n",
       " 'training_iteration': 16,\n",
       " 'trial_id': 'default',\n",
       " 'experiment_id': '31bfeb7473384a61b194a05bdc4008f7',\n",
       " 'date': '2022-07-14_11-49-06',\n",
       " 'timestamp': 1657824546,\n",
       " 'time_this_iter_s': 0.0663449764251709,\n",
       " 'time_total_s': 6.39565372467041,\n",
       " 'pid': 84282,\n",
       " 'hostname': 'MacBook-Air.local',\n",
       " 'node_ip': '127.0.0.1',\n",
       " 'config': {'num_workers': 0,\n",
       "  'num_envs_per_worker': 1,\n",
       "  'create_env_on_driver': True,\n",
       "  'rollout_fragment_length': 200,\n",
       "  'batch_mode': 'truncate_episodes',\n",
       "  'gamma': 0.99,\n",
       "  'lr': 0.0001,\n",
       "  'train_batch_size': 2000,\n",
       "  'model': {'_use_default_native_models': False,\n",
       "   '_disable_preprocessor_api': False,\n",
       "   '_disable_action_flattening': False,\n",
       "   'fcnet_hiddens': [64, 64],\n",
       "   'fcnet_activation': 'tanh',\n",
       "   'conv_filters': None,\n",
       "   'conv_activation': 'relu',\n",
       "   'post_fcnet_hiddens': [],\n",
       "   'post_fcnet_activation': 'relu',\n",
       "   'free_log_std': False,\n",
       "   'no_final_linear': False,\n",
       "   'vf_share_layers': True,\n",
       "   'use_lstm': False,\n",
       "   'max_seq_len': 20,\n",
       "   'lstm_cell_size': 256,\n",
       "   'lstm_use_prev_action': False,\n",
       "   'lstm_use_prev_reward': False,\n",
       "   '_time_major': False,\n",
       "   'use_attention': False,\n",
       "   'attention_num_transformer_units': 1,\n",
       "   'attention_dim': 64,\n",
       "   'attention_num_heads': 1,\n",
       "   'attention_head_dim': 32,\n",
       "   'attention_memory_inference': 50,\n",
       "   'attention_memory_training': 50,\n",
       "   'attention_position_wise_mlp_dim': 32,\n",
       "   'attention_init_gru_gate_bias': 2.0,\n",
       "   'attention_use_n_prev_actions': 0,\n",
       "   'attention_use_n_prev_rewards': 0,\n",
       "   'framestack': True,\n",
       "   'dim': 84,\n",
       "   'grayscale': False,\n",
       "   'zero_mean': True,\n",
       "   'custom_model': None,\n",
       "   'custom_model_config': {},\n",
       "   'custom_action_dist': None,\n",
       "   'custom_preprocessor': None,\n",
       "   'lstm_use_prev_action_reward': -1},\n",
       "  'optimizer': {},\n",
       "  'horizon': None,\n",
       "  'soft_horizon': False,\n",
       "  'no_done_at_end': False,\n",
       "  'env': None,\n",
       "  'observation_space': Box([0. 0.], [1. 1.], (2,), float32),\n",
       "  'action_space': Discrete(2),\n",
       "  'env_config': {},\n",
       "  'remote_worker_envs': False,\n",
       "  'remote_env_batch_wait_ms': 0,\n",
       "  'env_task_fn': None,\n",
       "  'render_env': False,\n",
       "  'record_env': False,\n",
       "  'clip_rewards': None,\n",
       "  'normalize_actions': True,\n",
       "  'clip_actions': False,\n",
       "  'preprocessor_pref': 'deepmind',\n",
       "  'log_level': 'WARN',\n",
       "  'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
       "  'ignore_worker_failures': False,\n",
       "  'log_sys_usage': True,\n",
       "  'fake_sampler': False,\n",
       "  'framework': 'torch',\n",
       "  'eager_tracing': False,\n",
       "  'eager_max_retraces': 20,\n",
       "  'explore': True,\n",
       "  'exploration_config': {'type': 'StochasticSampling'},\n",
       "  'evaluation_interval': None,\n",
       "  'evaluation_duration': 10,\n",
       "  'evaluation_duration_unit': 'episodes',\n",
       "  'evaluation_parallel_to_training': False,\n",
       "  'in_evaluation': False,\n",
       "  'evaluation_config': {'num_workers': 0,\n",
       "   'num_envs_per_worker': 1,\n",
       "   'create_env_on_driver': True,\n",
       "   'rollout_fragment_length': 200,\n",
       "   'batch_mode': 'truncate_episodes',\n",
       "   'gamma': 0.99,\n",
       "   'lr': 0.0001,\n",
       "   'train_batch_size': 2000,\n",
       "   'model': {'_use_default_native_models': False,\n",
       "    '_disable_preprocessor_api': False,\n",
       "    '_disable_action_flattening': False,\n",
       "    'fcnet_hiddens': [64, 64],\n",
       "    'fcnet_activation': 'tanh',\n",
       "    'conv_filters': None,\n",
       "    'conv_activation': 'relu',\n",
       "    'post_fcnet_hiddens': [],\n",
       "    'post_fcnet_activation': 'relu',\n",
       "    'free_log_std': False,\n",
       "    'no_final_linear': False,\n",
       "    'vf_share_layers': True,\n",
       "    'use_lstm': False,\n",
       "    'max_seq_len': 20,\n",
       "    'lstm_cell_size': 256,\n",
       "    'lstm_use_prev_action': False,\n",
       "    'lstm_use_prev_reward': False,\n",
       "    '_time_major': False,\n",
       "    'use_attention': False,\n",
       "    'attention_num_transformer_units': 1,\n",
       "    'attention_dim': 64,\n",
       "    'attention_num_heads': 1,\n",
       "    'attention_head_dim': 32,\n",
       "    'attention_memory_inference': 50,\n",
       "    'attention_memory_training': 50,\n",
       "    'attention_position_wise_mlp_dim': 32,\n",
       "    'attention_init_gru_gate_bias': 2.0,\n",
       "    'attention_use_n_prev_actions': 0,\n",
       "    'attention_use_n_prev_rewards': 0,\n",
       "    'framestack': True,\n",
       "    'dim': 84,\n",
       "    'grayscale': False,\n",
       "    'zero_mean': True,\n",
       "    'custom_model': None,\n",
       "    'custom_model_config': {},\n",
       "    'custom_action_dist': None,\n",
       "    'custom_preprocessor': None,\n",
       "    'lstm_use_prev_action_reward': -1},\n",
       "   'optimizer': {},\n",
       "   'horizon': None,\n",
       "   'soft_horizon': False,\n",
       "   'no_done_at_end': False,\n",
       "   'env': None,\n",
       "   'observation_space': Box([0. 0.], [1. 1.], (2,), float32),\n",
       "   'action_space': Discrete(2),\n",
       "   'env_config': {},\n",
       "   'remote_worker_envs': False,\n",
       "   'remote_env_batch_wait_ms': 0,\n",
       "   'env_task_fn': None,\n",
       "   'render_env': False,\n",
       "   'record_env': False,\n",
       "   'clip_rewards': None,\n",
       "   'normalize_actions': True,\n",
       "   'clip_actions': False,\n",
       "   'preprocessor_pref': 'deepmind',\n",
       "   'log_level': 'WARN',\n",
       "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
       "   'ignore_worker_failures': False,\n",
       "   'log_sys_usage': True,\n",
       "   'fake_sampler': False,\n",
       "   'framework': 'torch',\n",
       "   'eager_tracing': False,\n",
       "   'eager_max_retraces': 20,\n",
       "   'explore': True,\n",
       "   'exploration_config': {'type': 'StochasticSampling'},\n",
       "   'evaluation_interval': None,\n",
       "   'evaluation_duration': 10,\n",
       "   'evaluation_duration_unit': 'episodes',\n",
       "   'evaluation_parallel_to_training': False,\n",
       "   'in_evaluation': False,\n",
       "   'evaluation_config': {},\n",
       "   'evaluation_num_workers': 0,\n",
       "   'custom_eval_function': None,\n",
       "   'always_attach_evaluation_results': False,\n",
       "   'sample_async': False,\n",
       "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       "   'observation_filter': 'NoFilter',\n",
       "   'synchronize_filters': True,\n",
       "   'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "    'inter_op_parallelism_threads': 2,\n",
       "    'gpu_options': {'allow_growth': True},\n",
       "    'log_device_placement': False,\n",
       "    'device_count': {'CPU': 1},\n",
       "    'allow_soft_placement': True},\n",
       "   'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "    'inter_op_parallelism_threads': 8},\n",
       "   'compress_observations': False,\n",
       "   'metrics_episode_collection_timeout_s': 180,\n",
       "   'metrics_num_episodes_for_smoothing': 100,\n",
       "   'min_time_s_per_reporting': None,\n",
       "   'min_train_timesteps_per_reporting': None,\n",
       "   'min_sample_timesteps_per_reporting': 0,\n",
       "   'seed': 0,\n",
       "   'extra_python_environs_for_driver': {},\n",
       "   'extra_python_environs_for_worker': {},\n",
       "   'num_gpus': 0,\n",
       "   '_fake_gpus': False,\n",
       "   'num_cpus_per_worker': 1,\n",
       "   'num_gpus_per_worker': 0,\n",
       "   'custom_resources_per_worker': {},\n",
       "   'num_cpus_for_driver': 1,\n",
       "   'placement_strategy': 'PACK',\n",
       "   'input': ['data/offline/recommender_offline.json'],\n",
       "   'input_config': {},\n",
       "   'actions_in_input_normalized': False,\n",
       "   'input_evaluation': ['is', 'wis'],\n",
       "   'postprocess_inputs': True,\n",
       "   'shuffle_buffer_size': 0,\n",
       "   'output': None,\n",
       "   'output_compress_columns': ['obs', 'new_obs'],\n",
       "   'output_max_file_size': 67108864,\n",
       "   'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.policy_template.MARWILTorchPolicy'>, observation_space=Box([0. 0.], [1. 1.], (2,), float32), action_space=Discrete(2), config={})},\n",
       "    'policy_map_capacity': 100,\n",
       "    'policy_map_cache': None,\n",
       "    'policy_mapping_fn': None,\n",
       "    'policies_to_train': None,\n",
       "    'observation_fn': None,\n",
       "    'replay_mode': 'independent',\n",
       "    'count_steps_by': 'env_steps'},\n",
       "   'logger_config': None,\n",
       "   '_tf_policy_handles_more_than_one_loss': False,\n",
       "   '_disable_preprocessor_api': False,\n",
       "   '_disable_action_flattening': False,\n",
       "   '_disable_execution_plan_api': False,\n",
       "   'simple_optimizer': False,\n",
       "   'monitor': -1,\n",
       "   'evaluation_num_episodes': -1,\n",
       "   'metrics_smoothing_episodes': -1,\n",
       "   'timesteps_per_iteration': 0,\n",
       "   'min_iter_time_s': -1,\n",
       "   'collect_metrics_timeout': -1,\n",
       "   'use_gae': True,\n",
       "   'beta': 1.0,\n",
       "   'vf_coeff': 1.0,\n",
       "   'grad_clip': None,\n",
       "   'moving_average_sqd_adv_norm_update_rate': 1e-08,\n",
       "   'moving_average_sqd_adv_norm_start': 100.0,\n",
       "   'replay_buffer_size': 10000,\n",
       "   'learning_starts': 0},\n",
       "  'evaluation_num_workers': 0,\n",
       "  'custom_eval_function': None,\n",
       "  'always_attach_evaluation_results': False,\n",
       "  'sample_async': False,\n",
       "  'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       "  'observation_filter': 'NoFilter',\n",
       "  'synchronize_filters': True,\n",
       "  'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "   'inter_op_parallelism_threads': 2,\n",
       "   'gpu_options': {'allow_growth': True},\n",
       "   'log_device_placement': False,\n",
       "   'device_count': {'CPU': 1},\n",
       "   'allow_soft_placement': True},\n",
       "  'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "   'inter_op_parallelism_threads': 8},\n",
       "  'compress_observations': False,\n",
       "  'metrics_episode_collection_timeout_s': 180,\n",
       "  'metrics_num_episodes_for_smoothing': 100,\n",
       "  'min_time_s_per_reporting': None,\n",
       "  'min_train_timesteps_per_reporting': None,\n",
       "  'min_sample_timesteps_per_reporting': 0,\n",
       "  'seed': 0,\n",
       "  'extra_python_environs_for_driver': {},\n",
       "  'extra_python_environs_for_worker': {},\n",
       "  'num_gpus': 0,\n",
       "  '_fake_gpus': False,\n",
       "  'num_cpus_per_worker': 1,\n",
       "  'num_gpus_per_worker': 0,\n",
       "  'custom_resources_per_worker': {},\n",
       "  'num_cpus_for_driver': 1,\n",
       "  'placement_strategy': 'PACK',\n",
       "  'input': ['data/offline/recommender_offline.json'],\n",
       "  'input_config': {},\n",
       "  'actions_in_input_normalized': False,\n",
       "  'input_evaluation': ['is', 'wis'],\n",
       "  'postprocess_inputs': True,\n",
       "  'shuffle_buffer_size': 0,\n",
       "  'output': None,\n",
       "  'output_compress_columns': ['obs', 'new_obs'],\n",
       "  'output_max_file_size': 67108864,\n",
       "  'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.policy_template.MARWILTorchPolicy'>, observation_space=Box([0. 0.], [1. 1.], (2,), float32), action_space=Discrete(2), config={})},\n",
       "   'policy_map_capacity': 100,\n",
       "   'policy_map_cache': None,\n",
       "   'policy_mapping_fn': None,\n",
       "   'policies_to_train': None,\n",
       "   'observation_fn': None,\n",
       "   'replay_mode': 'independent',\n",
       "   'count_steps_by': 'env_steps'},\n",
       "  'logger_config': None,\n",
       "  '_tf_policy_handles_more_than_one_loss': False,\n",
       "  '_disable_preprocessor_api': False,\n",
       "  '_disable_action_flattening': False,\n",
       "  '_disable_execution_plan_api': False,\n",
       "  'simple_optimizer': False,\n",
       "  'monitor': -1,\n",
       "  'evaluation_num_episodes': -1,\n",
       "  'metrics_smoothing_episodes': -1,\n",
       "  'timesteps_per_iteration': 0,\n",
       "  'min_iter_time_s': -1,\n",
       "  'collect_metrics_timeout': -1,\n",
       "  'use_gae': True,\n",
       "  'beta': 1.0,\n",
       "  'vf_coeff': 1.0,\n",
       "  'grad_clip': None,\n",
       "  'moving_average_sqd_adv_norm_update_rate': 1e-08,\n",
       "  'moving_average_sqd_adv_norm_start': 100.0,\n",
       "  'replay_buffer_size': 10000,\n",
       "  'learning_starts': 0},\n",
       " 'time_since_restore': 6.39565372467041,\n",
       " 'timesteps_since_restore': 32000,\n",
       " 'iterations_since_restore': 16,\n",
       " 'perf': {}}"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e979de95-c039-4493-b958-e2e5d85098ec",
   "metadata": {},
   "source": [
    "#### Let's apply what we learned!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rl-course-dev-2]",
   "language": "python",
   "name": "conda-env-rl-course-dev-2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
